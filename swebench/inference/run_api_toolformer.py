#!/usr/bin/env python3

"""This python script is designed to run inference on a dataset using either the OpenAI or Anthropic API, depending on the model specified. 
It sorts instances by length and continually writes the outputs to a specified file, so that the script can be stopped and restarted without losing progress.
"""

import json
import os, re
import time
import dotenv
import traceback
from pathlib import Path
from tqdm.auto import tqdm
import numpy as np
import tiktoken
import openai
from anthropic import HUMAN_PROMPT, AI_PROMPT, Anthropic
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)
from typing import Optional, Tuple
from datasets import load_dataset, load_from_disk
from swebench.inference.make_datasets.utils import extract_diff
from argparse import ArgumentParser
import logging

# from langchain import Cohere, PromptTemplate
from langchain_community.llms import Cohere  # This is the current recommended way  # Need to install langchain-cohere
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import os

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(message)s")
logger = logging.getLogger(__name__)
dotenv.load_dotenv()

MODEL_LIMITS = {
    "claude-instant-1": 100_000,
    "claude-2": 100_000,
    "claude-3-opus-20240229": 200_000,
    "claude-3-sonnet-20240229": 200_000,
    "claude-3-haiku-20240307": 200_000,
    "gpt-3.5-turbo-16k-0613": 16_385,
    "gpt-3.5-turbo-0613": 4_097,
    "gpt-3.5-turbo-1106": 16_385,
    "gpt-4-32k-0613": 32_768,
    "gpt-4-0613": 8_192,
    "gpt-4-1106-preview": 128_000,
    "gpt-4-0125-preview": 128_000,
    "gpt-4o-mini" : 100_000,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBG2x5": 16_384, # finetuned gpt-4o-mini on toolformer dataset generated by codellama-7B
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-real-mistral:AfXAxGlO": 16_384, # finetuned gpt-4o-mini on toolformer dataset generated by Mistral-7B
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBGp86:ckpt-step-212": 16_384, # finetuned gpt-4o-mini on toolformer dataset generated by codellama-7B / step-212
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBFIol:ckpt-step-106": 16_384, # finetuned gpt-4o-mini on toolformer dataset generated by codellama-7B / step-106
}

# The cost per token for each model input.
MODEL_COST_PER_INPUT = {
    "claude-instant-1": 0.00000163,
    "claude-2": 0.00001102,
    "claude-3-opus-20240229": 0.000015,
    "claude-3-sonnet-20240229": 0.000003,
    "claude-3-haiku-20240307": 0.00000025,
    "gpt-3.5-turbo-16k-0613": 0.0000015,
    "gpt-3.5-turbo-0613": 0.0000015,
    "gpt-3.5-turbo-1106": 0.000001,
    "gpt-35-turbo-0613": 0.0000015,
    "gpt-35-turbo": 0.0000015,  # probably still 0613
    "gpt-4-0613": 0.00003,
    "gpt-4-32k-0613": 0.00006,
    "gpt-4-32k": 0.00006,
    "gpt-4-1106-preview": 0.00001,
    "gpt-4-0125-preview": 0.00001,
    "gpt-4o-mini-2024-07-18":0.00000015,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBG2x5":0.00000015,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-real-mistral:AfXAxGlO":0.00000015,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBGp86:ckpt-step-212":0.00000015,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBFIol:ckpt-step-106":0.00000015,
}

# The cost per token for each model output.
MODEL_COST_PER_OUTPUT = {
    "claude-instant-1": 0.00000551,
    "claude-2": 0.00003268,
    "claude-3-opus-20240229": 0.000075,
    "claude-3-sonnet-20240229": 0.000015,
    "claude-3-haiku-20240307": 0.00000125,
    "gpt-3.5-turbo-16k-0613": 0.000002,
    "gpt-3.5-turbo-16k": 0.000002,
    "gpt-3.5-turbo-1106": 0.000002,
    "gpt-35-turbo-0613": 0.000002,
    "gpt-35-turbo": 0.000002,
    "gpt-4-0613": 0.00006,
    "gpt-4-32k-0613": 0.00012,
    "gpt-4-32k": 0.00012,
    "gpt-4-1106-preview": 0.00003,
    "gpt-4-0125-preview": 0.00003,
    "gpt-4o-mini-2024-07-18":0.00000060,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBG2x5":0.00000060,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-real-mistral:AfXAxGlO":0.00000060,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBGp86:ckpt-step-212":0.00000060,
    "ft:gpt-4o-mini-2024-07-18:personal:toolformer-mistral:AfWBFIol:ckpt-step-106":0.00000060,
}

# used for azure
ENGINES = {
    "gpt-3.5-turbo-16k-0613": "gpt-35-turbo-16k",
    "gpt-4-0613": "gpt-4",
    "gpt-4-32k-0613": "gpt-4-32k",
}


def calc_cost(model_name, input_tokens, output_tokens):
    """
    Calculates the cost of a response from the openai API.

    Args:
    response (openai.ChatCompletion): The response from the API.

    Returns:
    float: The cost of the response.
    """
    cost = (
        MODEL_COST_PER_INPUT[model_name] * input_tokens
        + MODEL_COST_PER_OUTPUT[model_name] * output_tokens
    )
    logger.info(
        f"input_tokens={input_tokens}, output_tokens={output_tokens}, cost={cost:.2f}"
    )
    return cost


class ToolformerHandler:
    """Handles Toolformer API calls during model inference using Cohere LLMChain."""
    
    def __init__(self):
        """Initialize the handler with the tool pattern regex."""
        self.tool_pattern = r'<TOOLFORMER_API_START>\s*LLMChain\((.*?)\)\s*<TOOLFORMER_API_RESPONSE>'
    
    def langchain_llmchain(self, input_question):
    
        if "COHERE_API_KEY" not in os.environ:
            raise ValueError("Please set the COHERE_API_KEY environment variable")
        
        # TODO: Check succinct if it's good once we don't have rate limited APIs
        template = """Please be succinct in your answer to this question.
                    Question: {question}

                    Answer: Let's think step by step."""
        prompt = PromptTemplate(template=template, input_variables=["question"])
        # old model name
        # llm = Cohere(model="command-xlarge-nightly")
        # recent stable model
        llm = Cohere(model="command")
        
        # create and use the chain
        chain = LLMChain(llm=llm, prompt=prompt)
        
        return chain.predict(question=input_question)
    
    def find_next_tool_call(self, text: str) -> Tuple[Optional[str], int, int]:
        """
        Find the next tool call in the text.
        Returns (question, start_index, end_index) or (None, -1, -1) if no tool call found.
        """
        match = re.search(self.tool_pattern, text)
        if not match:
            return None, -1, -1
            
        question = match.group(1).strip()
        return question, match.start(), match.end()



@retry(wait=wait_random_exponential(min=1, max=10), stop=stop_after_attempt(3))
def make_openai_call(model_name_or_path, messages, use_azure, temperature, top_p, max_tokens=None, **model_args):
    """Makes the actual API call to OpenAI with retry logic."""
    try:
        # Create a new dict for API arguments to avoid modifying the input
        api_args = {
            'frequency_penalty': 0.8,
            'temperature': temperature,
            'top_p': top_p,
            'max_tokens': max_tokens
        }
        
        # Add any additional args that aren't already set
        for key, value in model_args.items():
            if key not in api_args :
                api_args[key] = value
                
        if use_azure:
            return openai.chat.completions.create(
                engine=ENGINES[model_name_or_path],
                messages=messages,
                **api_args
            )
        else:
            return openai.chat.completions.create(
                model=model_name_or_path,
                messages=messages,
                **api_args
            )
    except openai.BadRequestError as e:
        if e.code == "context_length_exceeded":
            print("Context length exceeded")
            return None
        raise e


def call_chat(model_name_or_path, inputs, use_azure, temperature, top_p, toolformer=False, handler=None, **model_args):
    """
    Calls the openai API to generate completions for the given inputs.

    Args:
    model_name_or_path (str): The name or path of the model to use.
    inputs (str): The inputs to generate completions for.
    use_azure (bool): Whether to use the azure API.
    temperature (float): The temperature to use.
    top_p (float): The top_p to use.
    **model_args (dict): A dictionary of model arguments.
    """
    # Get original system message and user message
    system_messages = inputs.split("\n", 1)[0]
    user_message = inputs.split("\n", 1)[1]
    
    # Building instruction message
    instruction_msg ="""
        A code snippet with issues has been provided in the previous context.
        The issue is specified between <issue> and </issue> tags.
        PLEASE FOLLOW THESE STEPS IN ORDER :

        1. First, repeat the issue message that was between <issue> and </issue> tags
        
        2. Then, keeping the issue in mind, rewrite the code snippet
        
    """
    
    # Add toolformer instructions if enabled
    if toolformer:
        toolformer_msg = """
            During code snippet rewrite:
            - Use the LLMChain tool if you need additional information or clarification
            - Tool usage format: <TOOLFORMER_API_START> LLMChain(your specific question) <TOOLFORMER_API_RESPONSE>
    
        """
        instruction_msg = instruction_msg + toolformer_msg
        print("Toolformer instruction added in the middle.")
    
    
    instruction_msg += """
            3. Finally,
            - Provide a solution to the issues as a unified diff for the given code snippet
            - The diff should be the final section of your answer
            - Example format:
                --- a/filepath
                +++ b/filepath
                @@ line numbers @@
                - removed lines
                + added lines

            Note : When generating the code, avoid repeating the same or similar lines multiple times. 
            Consolidate your output and avoid unnecessary repetition. 
            
            Important: The diff section should only contain diff syntax and code changes, without any explanatory text.
            Your response should follow this sequence: issue restatement → code snippet → diff.
            Once the diff section begins, do NOT add any additional commentary or tool calls.
        """
    
    # Add instruction message at the end of the input.
    user_message = user_message + instruction_msg
    print("Instruction added to the end of the input.")
    
    # Create messages for API call
    messages = [
        {"role": "system", "content": system_messages},
        {"role": "user", "content": user_message}
    ]
    
    try:
        # Make API call with tool handling if toolformer is enabled
        if toolformer:
            model_args['stream'] = True
            return call_chat_with_tool(
                model_name_or_path,
                messages,
                use_azure,
                temperature,
                top_p,
                handler,
                **model_args
            )
            
        else:
            model_args['stream'] = False
            # Regular API call without tool processing
            response = make_openai_call(
                model_name_or_path,
                messages,
                use_azure,
                temperature,
                top_p,
                **model_args
            )
            input_tokens = response.usage.prompt_tokens
            output_tokens = response.usage.completion_tokens
            cost = calc_cost(response.model, input_tokens, output_tokens)
            return response, cost
            
    except openai.BadRequestError as e:
        if e.code == "context_length_exceeded":
            print("Context length exceeded")
            return None
        raise e
    

def call_chat_with_tool(model_name_or_path, messages, use_azure, temperature, top_p, handler, **model_args):
    """
    Makes OpenAI API calls with streaming to handle tool calls immediately.
    """
    print("-"*100+"\n")
    print("new call chat with tool")
    print("-"*100+"\n")

    total_input_tokens = 0
    total_output_tokens = 0
    full_response = ""
    max_tokens = MODEL_LIMITS[model_name_or_path]  # Get model's token limit
    max_iterations = 10  # Prevent infinite loops
    iteration_count = 0
    tokens_since_last_tool_call = 0
    min_tokens_before_tool_call = 300
    done = False
    
    while iteration_count < max_iterations:  
        iteration_count += 1
        print(f"\nIteration {iteration_count}/{max_iterations}")
        print(f"Last few messages : .....{messages[-1]['content'][-300:]}\n")
        print(f"start again... \n")
        
        # Check total tokens
        if total_output_tokens >= max_tokens * 0.8:  # 80% of model's limit
            print(f"Warning: Approaching token limit ({total_output_tokens}/{max_tokens}). Stopping early.")
            break
        
        # Make API call with retry
        response_stream = make_openai_call(
            model_name_or_path,
            messages,
            use_azure,
            temperature,
            top_p,
            max_tokens=max_tokens - total_output_tokens,
            **model_args
        )
        
        if response_stream is None:
            break

        current_chunk = ""
        tool_call_detected = False
        
        # Process the stream
        for chunk in response_stream:
            if chunk.choices[0].delta.content:
                chunk_text = chunk.choices[0].delta.content
                current_chunk += chunk_text
                tokens_since_last_tool_call += len(chunk_text.split())
                print(chunk_text, end='', flush=True)  # Show output in real-time
                
                # Check for tool call only if enough tokens have been processed
                if tokens_since_last_tool_call >= min_tokens_before_tool_call and "<TOOLFORMER_API_START>" in current_chunk:
                    # Wait for full tool call pattern before proceeding
                    complete_pattern = re.search(r'<TOOLFORMER_API_START>\s*LLMChain\((.*?)\)\s*<TOOLFORMER_API_RESPONSE>', current_chunk)
                    if complete_pattern:
                        tool_call_detected = True
                        break
                    
        if tool_call_detected:
            # Extract the question
            question_match = re.search(r'<TOOLFORMER_API_START>\s*LLMChain\((.*?)\)\s*<TOOLFORMER_API_RESPONSE>', current_chunk)
            if question_match:
                question = question_match.group(1)
                print(f"\nProcessing tool call: {question}")
                
                # Get tool response
                tool_response = handler.langchain_llmchain(question)
                print(f"Tool response: {tool_response}")
                
                # Format the interaction
                tool_interaction = f"<TOOLFORMER_API_START> LLMChain({question}) <TOOLFORMER_API_RESPONSE>{tool_response}<TOOLFORMER_API_END>"
                
                # Update messages, preserving context
                full_response += current_chunk[:question_match.start()] + tool_interaction
                messages = [
                    *messages[:2],  # Keep original system and user messages
                    {"role": "assistant", "content": full_response},
                    {"role": "user", "content": """
                        Apologies, the previous output was interrupted.
                    Please continue from where you left off and complete the response.
                    Please do NOT use API call for a while.
                    Don't forget the original plan I gave you.
                    """}
                ]
                
                # Reset token counter
                tokens_since_last_tool_call = 0
                
                continue  # Continue to next iteration with updated messages
        else:
            # No tool call detected, we're done
            full_response += current_chunk
            done = True
            break
    
    if not done:
        print("All iterations used. Completing output with streaming...")
        final_messages = [
            *messages[:2],
            {"role": "assistant", "content": full_response},
            {"role": "user", "content": """
             Please complete the response WITHOUT using any more tools.
             Don't forget the original plan I gave you.
             """}
        ]
        
        # Final completion without tool processing
        final_stream = make_openai_call(
            model_name_or_path,
            final_messages,
            use_azure,
            temperature,
            top_p,
            max_tokens=max_tokens - total_output_tokens,
            **model_args
        )
        
        for chunk in final_stream:
            if chunk.choices[0].delta.content:
                chunk_text = chunk.choices[0].delta.content
                full_response += chunk_text
                total_output_tokens += len(chunk_text.split())
                print(chunk_text, end='', flush=True)
    
    return full_response, calc_cost(model_name_or_path, total_input_tokens, total_output_tokens)



def gpt_tokenize(string: str, encoding) -> int:
    """Returns the number of tokens in a text string."""
    num_tokens = len(encoding.encode(string))
    return num_tokens


def claude_tokenize(string: str, api) -> int:
    """Returns the number of tokens in a text string."""
    num_tokens = api.count_tokens(string)
    return num_tokens


def openai_inference(
    test_dataset,
    model_name_or_path,
    output_file,
    model_args,
    existing_ids,
    max_cost,
    toolformer=False,
):
    """
    Runs inference on a dataset using the openai API.

    Args:
    test_dataset (datasets.Dataset): The dataset to run inference on.
    model_name_or_path (str): The name or path of the model to use.
    output_file (str): The path to the output file.
    model_args (dict): A dictionary of model arguments.
    existing_ids (set): A set of ids that have already been processed.
    max_cost (float): The maximum cost to spend on inference.
    """
    # Initialize tokenizer and filter dataset
    encoding = tiktoken.encoding_for_model(model_name_or_path)
    test_dataset = test_dataset.filter(
        lambda x: gpt_tokenize(x["text"], encoding) <= MODEL_LIMITS[model_name_or_path],
        desc="Filtering",
        load_from_cache_file=False,
    )
    
    # Setup OpenAI API
    openai_key = os.environ.get("OPENAI_API_KEY", None)
    if openai_key is None:
        raise ValueError(
            "Must provide an api key. Expected in OPENAI_API_KEY environment variable."
        )
    openai.api_key = openai_key
    print(f"Using OpenAI key {'*' * max(0, len(openai_key)-5) + openai_key[-5:]}")
    
    # Setup Azure if needed
    use_azure = model_args.pop("use_azure", False)
    if use_azure:
        openai.api_type = "azure"
        openai.api_base = "https://pnlpopenai3.openai.azure.com/"
        openai.api_version = "2023-05-15"
        
    # Setup model parameters
    temperature = model_args.pop("temperature", 0.2)
    top_p = model_args.pop("top_p", 0.95 if temperature > 0 else 1)
    print(f"Using temperature={temperature}, top_p={top_p}")
    
    # Initialize Toolformer handler if needed
    handler = ToolformerHandler() if toolformer else None
    
    total_cost = 0
    print(f"Filtered to {len(test_dataset)} instances")
    
    # Process dataset
    with open(output_file, "a+") as f:
        for datum in tqdm(test_dataset, desc=f"Inference for {model_name_or_path}"):
            instance_id = datum["instance_id"]
            print(f"processing instance_id : {instance_id}")
            # Skip if already processed
            if instance_id in existing_ids:
                continue
            
            try:
                # Prepare output dictionary
                output_dict = {
                    "instance_id": datum["instance_id"],
                    "model_name_or_path": model_name_or_path,
                    "text": f"{datum['text']}\n\n"
                }
                
                # Call API - call_chat will handle toolformer routing internally
                response, cost = call_chat(
                    model_name_or_path,
                    output_dict["text"],
                    use_azure,
                    temperature,
                    top_p,
                    toolformer=toolformer,
                    handler=handler,
                    **model_args
                )
                
                print("data gathered")
                
                if response is None:
                    continue
                
                # Get completion from response
                if not toolformer :
                    response = response.choices[0].message.content
                    
                # Update costs and output
                total_cost += cost
                print(f"Total Cost: {total_cost:.2f}")
                
                # Save results
                output_dict["full_output"] = response
                output_dict["model_patch"] = extract_diff(response)
                print(json.dumps(output_dict), file=f, flush=True)
                
                # Check cost limit
                if max_cost is not None and total_cost >= max_cost:
                    print(f"Reached max cost {max_cost}, exiting")
                    break
                
            except Exception as e:
                logger.error(f"Error processing instance {datum['instance_id']}: {str(e)}")
                traceback.print_exc()
                continue


@retry(wait=wait_random_exponential(min=60, max=600), stop=stop_after_attempt(6))
def call_anthropic(
    inputs, anthropic, model_name_or_path, temperature, top_p, **model_args
):
    """
    Calls the anthropic API to generate completions for the given inputs.

    Args:
    inputs (str): The inputs to generate completions for.
    anthropic (Anthropic): The anthropic API object.
    model_name_or_path (str): The name or path of the model to use.
    temperature (float): The temperature to use.
    top_p (float): The top_p to use.
    model_args (dict): A dictionary of model arguments.
    """
    try:
        completion = anthropic.completions.create(
            model=model_name_or_path,
            max_tokens_to_sample=6000,
            prompt=inputs,
            temperature=temperature,
            top_p=top_p,
            **model_args,
        )
        response = completion.completion
        input_tokens = anthropic.count_tokens(inputs)
        output_tokens = anthropic.count_tokens(response)
        cost = calc_cost(model_name_or_path, input_tokens, output_tokens)
        return completion, cost
    except Exception as e:
        logger.error(e)
        logger.error(f"Inputs: {inputs}")
        traceback.print_exc()
        time.sleep(20)
        return None
    

@retry(wait=wait_random_exponential(min=60, max=600), stop=stop_after_attempt(6))
def call_anthropic_v2(
    inputs, anthropic, model_name_or_path, temperature, top_p, toolformer=False, **model_args
):
    """
    Calls the anthropic API to generate completions for the given inputs.

    Args:
    inputs list(str): The inputs to generate completions for.
    anthropic (Anthropic): The anthropic API object.
    model_name_or_path (str): The name or path of the model to use.
    temperature (float): The temperature to use.
    top_p (float): The top_p to use.
    model_args (dict): A dictionary of model arguments.
    """
    system_messages = inputs.split("\n", 1)[0]
    user_message = inputs.split("\n", 1)[1]
    
    if toolformer:
        toolformer_msg = """

            When providing solutions, follow these exact steps:

            1. First, restate:
            - The issue statement
            - The problematic code base

            2. During restate:
            - Use LLMChain tool when needed by printing:
                <TOOLFORMER_API_START> LLMChain(your specific question) <TOOLFORMER_API_RESPONSE>

            3. Finally:
            - Provide solution in diff file format, using ONLY proper diff syntax:
                --- a/filepath
                +++ b/filepath
                @@ line numbers @@
                - removed lines
                + added lines

            Note: Do NOT include any API calls or explanatory text within the diff output itself.
                The diff should contain only the standard diff format with file paths and changes.
        """
        user_message = user_message + toolformer_msg
        print("toolformer instruction added to the end of the input")
    
    try:
        messages = [
            {"role": "user", "content": user_message},
        ]
        response = anthropic.messages.create(
            messages=messages,
            max_tokens=4096,
            model=model_name_or_path,
            temperature=temperature,
            top_p=top_p,
            system=system_messages,
        )
        input_tokens = response.usage.input_tokens
        output_tokens = response.usage.output_tokens
        cost = calc_cost(response.model, input_tokens, output_tokens)
        return response, cost
    except Exception as e:
        logger.error(e)
        logger.error(f"Inputs: {inputs}")
        traceback.print_exc()
        time.sleep(20)
        return None


def anthropic_inference(
    test_dataset,
    model_name_or_path,
    output_file,
    model_args,
    existing_ids,
    max_cost,
    toolformer=False,
):
    """
    Runs inference on a dataset using the anthropic API.

    Args:
    test_dataset (datasets.Dataset): The dataset to run inference on.
    model_name_or_path (str): The name or path of the model to use.
    output_file (str): The path to the output file.
    model_args (dict): A dictionary of model arguments.
    existing_ids (set): A set of ids that have already been processed.
    max_cost (float): The maximum cost to spend on inference.
    """
    api_key = os.environ.get("ANTHROPIC_API_KEY", None)
    if api_key is None:
        raise ValueError(
            "Must provide an api key. Expected in ANTHROPIC_API_KEY environment variable."
        )
    print(f"Using Anthropic key {'*' * max(0, len(api_key)-5) + api_key[-5:]}")
    anthropic = Anthropic(api_key=api_key)
    test_dataset = test_dataset.filter(
        lambda x: claude_tokenize(x["text"], anthropic)
        <= MODEL_LIMITS[model_name_or_path],
        desc="Filtering",
        load_from_cache_file=False,
    )
    temperature = model_args.pop("temperature", 0.2)
    top_p = model_args.pop("top_p", 0.95 if temperature > 0 else 1)
    print(f"Using temperature={temperature}, top_p={top_p}")
    basic_args = {
        "model_name_or_path": model_name_or_path,
    }
    total_cost = 0
    print(f"Filtered to {len(test_dataset)} instances")
    if 'claude-3' in model_name_or_path.lower():
        call_api = call_anthropic_v2
    else:
        call_api = call_anthropic
    with open(output_file, "a+") as f:
        for datum in tqdm(test_dataset, desc=f"Inference for {model_name_or_path}"):
            instance_id = datum["instance_id"]
            if instance_id in existing_ids:
                continue
            output_dict = {"instance_id": instance_id}
            output_dict.update(basic_args)
            if 'claude-3' in model_name_or_path.lower():
                output_dict["text_inputs"] = f"{datum['text']}\n"
            else:
                output_dict[
                    "text_inputs"
                ] = f"{HUMAN_PROMPT} {datum['text']}\n\n{AI_PROMPT}"
            try:
                completion, cost = call_api(
                    output_dict["text_inputs"],
                    anthropic,
                    model_name_or_path,
                    temperature,
                    top_p,
                    **model_args,
                )
            except Exception as e:
                logger.error(e)
                traceback.print_exc()
                continue
            total_cost += cost
            print(f"Total Cost: {total_cost:.2f}")
            if "claude-3" in model_name_or_path.lower():
                output_dict["full_output"] = completion.content[0].text
            else:
                output_dict["full_output"] = completion.completion
            output_dict["model_patch"] = extract_diff(output_dict["full_output"])
            print(json.dumps(output_dict), file=f, flush=True)
            if max_cost is not None and total_cost >= max_cost:
                print(f"Reached max cost {max_cost}, exiting")
                break


def parse_model_args(model_args):
    """
    Parses a string of model arguments and returns a dictionary of keyword arguments.

    Args:
        model_args (str): A string of comma-separated key-value pairs representing model arguments.

    Returns:
        dict: A dictionary of keyword arguments parsed from the input string.
    """
    kwargs = dict()
    if model_args is not None:
        for arg in model_args.split(","):
            key, value = arg.split("=")
            # infer value type
            if value in {"True", "False"}:
                kwargs[key] = value == "True"
            elif value.isnumeric():
                kwargs[key] = int(value)
            elif value.replace(".", "", 1).isnumeric():
                kwargs[key] = float(value)
            elif value in {"None"}:
                kwargs[key] = None
            elif value in {"[]"}:
                kwargs[key] = []
            elif value in {"{}"}:
                kwargs[key] = {}
            elif value.startswith("'") and value.endswith("'"):
                kwargs[key] = value[1:-1]
            elif value.startswith('"') and value.endswith('"'):
                kwargs[key] = value[1:-1]
            else:
                kwargs[key] = value
    return kwargs


def main(
    dataset_name_or_path,
    split,
    model_name_or_path,
    shard_id,
    num_shards,
    output_dir,
    model_args,
    max_cost,
    my_set,
    toolformer,
):  
    # choose data choice if available
    if my_set in ["test","train","testtest"]:
        shard_id = None
        num_shards = None
        
        if my_set == "test":
            indices = [0,6,12,18,24,30,36,42,48,54,60,66,72,78,84,90]
        elif my_set == "train":
            indices = [96,102,108,114,120,126,132,138,144,150,156,162,168,174,180,185]
        elif my_set == "testtest":
            indices = [1,7]
            
    # using shard if designated
    if shard_id is None and num_shards is not None:
        logger.warning(
            f"Received num_shards={num_shards} but shard_id is None, ignoring"
        )
    if shard_id is not None and num_shards is None:
        logger.warning(f"Received shard_id={shard_id} but num_shards is None, ignoring")
    
    # setup output file
    model_args = parse_model_args(model_args)
    model_nickname = model_name_or_path
    if "checkpoint" in Path(model_name_or_path).name:
        model_nickname = Path(model_name_or_path).parent.name
    else:
        model_nickname = Path(model_name_or_path).name
    output_file = f"{model_nickname}__{dataset_name_or_path.split('/')[-1]}__{split}"
    if my_set in ["test","train","testtest"]:
        output_file += f"__{my_set}"
    if shard_id is not None and num_shards is not None:
        output_file += f"__shard-{shard_id}__num_shards-{num_shards}"
    output_file = Path(output_dir, output_file + ".jsonl")
    logger.info(f"Will write to {output_file}")
    
    # check previously calculated cases
    existing_ids = set()
    if os.path.exists(output_file):
        with open(output_file) as f:
            for line in f:
                data = json.loads(line)
                instance_id = data["instance_id"]
                existing_ids.add(instance_id)
    logger.info(f"Read {len(existing_ids)} already completed ids from {output_file}")
    if Path(dataset_name_or_path).exists():
        dataset = load_from_disk(dataset_name_or_path)
    else:
        dataset = load_dataset(dataset_name_or_path)
        
    # load and setup dataset
    if not split in dataset:
        raise ValueError(f"Invalid split {split} for dataset {dataset_name_or_path}")
    dataset = dataset[split]
    
    logger.info(f"type of dataset : {type(dataset)}")
    
    if my_set in ["test","train","testtest"]:
        dataset = dataset.select(indices)
        logger.info(f"{my_set} set is chosen")
    else:
        lens = np.array(list(map(len, dataset["text"])))
        dataset = dataset.select(np.argsort(lens))
        if shard_id is not None and num_shards is not None:
            dataset = dataset.shard(num_shards, shard_id, contiguous=True)
            
    if len(existing_ids) > 0:
        dataset = dataset.filter(
            lambda x: x["instance_id"] not in existing_ids,
            desc="Filtering out existing ids",
            load_from_cache_file=False,
        )
    
    # Model inference
    inference_args = {
        "test_dataset": dataset,
        "model_name_or_path": model_name_or_path,
        "output_file": output_file,
        "model_args": model_args,
        "existing_ids": existing_ids,
        "max_cost": max_cost,
        "toolformer":toolformer
    }
    if model_name_or_path.startswith("claude"):
        anthropic_inference(**inference_args)
    elif model_name_or_path.startswith("gpt") or model_name_or_path.startswith("ft:"):  # Add fine-tuned model check
        openai_inference(**inference_args)
    else:
        raise ValueError(f"Invalid model name or path {model_name_or_path}")
    logger.info(f"Done!")


if __name__ == "__main__":
    parser = ArgumentParser(description=__doc__)
    parser.add_argument(
        "--dataset_name_or_path",
        type=str,
        required=True,
        help="HuggingFace dataset name or local path",
    )
    parser.add_argument(
        "--split",
        type=str,
        default="test",
        help="Dataset split to use",
    )
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        help="Name of API model. Update MODEL* constants in this file to add new models.",
        choices=sorted(list(MODEL_LIMITS.keys())),
    )
    parser.add_argument(
        "--shard_id",
        type=int,
        default=None,
        help="Shard id to process. If None, process all shards.",
    )
    parser.add_argument(
        "--num_shards",
        type=int,
        default=None,
        help="Number of shards. If None, process all shards.",
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        required=True,
        help="Path to the output file.",
    )
    parser.add_argument(
        "--model_args",
        type=str,
        default=None,
        help="List of model arguments separated by commas. (e.g. 'top_p=0.95,temperature=0.70')",
    )
    parser.add_argument(
        "--max_cost",
        type=float,
        default=None,
        help="Maximum cost to spend on inference.",
    )
    parser.add_argument(
        "--my_set",
        type=str,
        default=None,
        help="which data I would choose",
    )
    parser.add_argument(
    "--toolformer",
    action="store_true",  # Default is False, --toolformer sets it to True
    help="Whether to use Toolformer system message for code fixes",
    )
    args = parser.parse_args()
    main(**vars(args))
