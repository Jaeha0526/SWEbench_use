{"instance_id": "sqlfluff__sqlfluff-2862", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nfix keep adding new line on wrong place \n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo replicate this issue you can create a file eg. test.template.sql \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n```\r\n\r\nthen run:\r\n```\r\nsqlfluff fix test.template.sql  \r\n```\r\n\r\nThis will give you:\r\n```\r\nL:   2 | P:  12 | L009 | Files must end with a trailing newline.\r\n```\r\n\r\nAnd the result of the file is now:\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n{%- endif %}\r\n```\r\n\r\nIf i run it again it will complain on the same issue and the result of the file would be: \r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n\r\n\r\n{%- endif %}\r\n```\r\n\r\nAnd so on. \n\n### Expected Behaviour\n\nThe expected behavior would be to add the new line at the end of the file, that is after `{%- endif %}` instead of adding the new line at the end of the SQL query - so the result should look like this: \r\n\r\n```\r\n{% if true %}\r\nSELECT 1 + 1\r\n{%- endif %}\r\n\r\n```\n\n### Observed Behaviour\n\nAdds a new line to the end of the SQL query instead of in the end of the file. \n\n### How to reproduce\n\nAlready mentioned above (in What Happened section).\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.6.2\n\n### Configuration\n\n[sqlfluff]\r\nverbose = 1\r\ndialect = snowflake\r\ntemplater = jinja\r\nexclude_rules = L027,L031,L032,L036,L044,L046,L034\r\noutput_line_length = 121\r\nsql_file_exts=.sql\r\n\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nmax_line_length = 250\r\nindent_unit = space\r\ncomma_style = trailing\r\nallow_scalar = True\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = aliases\r\n\r\n\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n[sqlfluff:rules:L014]\r\nextended_capitalisation_policy = lower\r\n\r\n[sqlfluff:rules:L030]  # function names\r\ncapitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 \n16 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n17 \n18 ## Dialects Supported\n19 \n20 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n21 \n22 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n23 - [BigQuery](https://cloud.google.com/bigquery/)\n24 - [Exasol](https://www.exasol.com/)\n25 - [Hive](https://hive.apache.org/)\n26 - [MySQL](https://www.mysql.com/)\n27 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n30 - [Snowflake](https://www.snowflake.com/)\n31 - [Spark3](https://spark.apache.org/docs/latest/)\n32 - [SQLite](https://www.sqlite.org/)\n33 - [Teradata](https://www.teradata.com/)\n34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n35 \n36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n37 \n38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n39 \n40 ## Templates Supported\n41 \n42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n43 \n44 **SQLFluff** supports the following templates:\n45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n46 - [dbt](https://www.getdbt.com/)\n47 \n48 Again, please raise issues if you wish to support more templating languages/syntaxes.\n49 \n50 # Getting Started\n51 \n52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n53 \n54 ```shell\n55 $ pip install sqlfluff\n56 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n57 $ sqlfluff lint test.sql\n58 == [test.sql] FAIL\n59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4\n60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line\n61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace\n62 ```\n63 \n64 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n65 \n66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n67 \n68 # Documentation\n69 \n70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n71 \n72 # Releases\n73 \n74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).\n75 \n76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n77 \n78 # SQLFluff on Slack\n79 \n80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n81 \n82 # SQLFluff on Twitter\n83 \n84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n85 \n86 # Contributing\n87 \n88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n89 \n90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n91 \n92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n93 \n[end of README.md]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import (\n8     Any,\n9     List,\n10     Sequence,\n11     Optional,\n12     Tuple,\n13     cast,\n14     Iterable,\n15     Iterator,\n16 )\n17 \n18 import pathspec\n19 import regex\n20 from tqdm import tqdm\n21 \n22 from sqlfluff.core.errors import (\n23     SQLBaseError,\n24     SQLLexError,\n25     SQLLintError,\n26     SQLParseError,\n27     SQLTemplaterSkipFile,\n28 )\n29 from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n30 from sqlfluff.core.file_helpers import get_encoding\n31 from sqlfluff.core.templaters import TemplatedFile\n32 from sqlfluff.core.rules import get_ruleset\n33 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n34 \n35 # Classes needed only for type checking\n36 from sqlfluff.core.parser.segments.base import BaseSegment\n37 from sqlfluff.core.parser.segments.meta import MetaSegment\n38 from sqlfluff.core.parser.segments.raw import RawSegment\n39 from sqlfluff.core.rules.base import BaseRule\n40 \n41 from sqlfluff.core.linter.common import (\n42     RuleTuple,\n43     ParsedString,\n44     NoQaDirective,\n45     RenderedFile,\n46 )\n47 from sqlfluff.core.linter.linted_file import LintedFile\n48 from sqlfluff.core.linter.linted_dir import LintedDir\n49 from sqlfluff.core.linter.linting_result import LintingResult\n50 \n51 \n52 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n53 \n54 # Instantiate the linter logger\n55 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n56 \n57 \n58 class Linter:\n59     \"\"\"The interface class to interact with the linter.\"\"\"\n60 \n61     # Default to allowing process parallelism\n62     allow_process_parallelism = True\n63 \n64     def __init__(\n65         self,\n66         config: Optional[FluffConfig] = None,\n67         formatter: Any = None,\n68         dialect: Optional[str] = None,\n69         rules: Optional[List[str]] = None,\n70         user_rules: Optional[List[BaseRule]] = None,\n71         exclude_rules: Optional[List[str]] = None,\n72     ) -> None:\n73         # Store the config object\n74         self.config = FluffConfig.from_kwargs(\n75             config=config,\n76             dialect=dialect,\n77             rules=rules,\n78             exclude_rules=exclude_rules,\n79         )\n80         # Get the dialect and templater\n81         self.dialect = self.config.get(\"dialect_obj\")\n82         self.templater = self.config.get(\"templater_obj\")\n83         # Store the formatter for output\n84         self.formatter = formatter\n85         # Store references to user rule classes\n86         self.user_rules = user_rules or []\n87 \n88     def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n89         \"\"\"Get hold of a set of rules.\"\"\"\n90         rs = get_ruleset()\n91         # Register any user rules\n92         for rule in self.user_rules:\n93             rs.register(rule)\n94         cfg = config or self.config\n95         return rs.get_rulelist(config=cfg)\n96 \n97     def rule_tuples(self) -> List[RuleTuple]:\n98         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n99         rs = self.get_ruleset()\n100         return [RuleTuple(rule.code, rule.description) for rule in rs]\n101 \n102     # #### Static methods\n103     # These are the building blocks of the linting process.\n104 \n105     @staticmethod\n106     def _load_raw_file_and_config(\n107         fname: str, root_config: FluffConfig\n108     ) -> Tuple[str, FluffConfig, str]:\n109         \"\"\"Load a raw file and the associated config.\"\"\"\n110         file_config = root_config.make_child_from_path(fname)\n111         encoding = get_encoding(fname=fname, config=file_config)\n112         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n113             raw_file = target_file.read()\n114         # Scan the raw file for config commands.\n115         file_config.process_raw_file_for_config(raw_file)\n116         # Return the raw file and config\n117         return raw_file, file_config, encoding\n118 \n119     @staticmethod\n120     def _normalise_newlines(string: str) -> str:\n121         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n122         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n123 \n124     @staticmethod\n125     def _lex_templated_file(\n126         templated_file: TemplatedFile, config: FluffConfig\n127     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n128         \"\"\"Lex a templated file.\n129 \n130         NOTE: This potentially mutates the config, so make sure to\n131         use the returned one.\n132         \"\"\"\n133         violations = []\n134         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n135         # Get the lexer\n136         lexer = Lexer(config=config)\n137         # Lex the file and log any problems\n138         try:\n139             tokens, lex_vs = lexer.lex(templated_file)\n140             # We might just get the violations as a list\n141             violations += lex_vs\n142             linter_logger.info(\n143                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n144             )\n145         except SQLLexError as err:\n146             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n147             violations.append(err)\n148             return None, violations, config\n149 \n150         if not tokens:  # pragma: no cover TODO?\n151             return None, violations, config\n152 \n153         # Check that we've got sensible indentation from the lexer.\n154         # We might need to suppress if it's a complicated file.\n155         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n156         if isinstance(templating_blocks_indent, str):\n157             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n158         else:\n159             force_block_indent = False\n160         templating_blocks_indent = bool(templating_blocks_indent)\n161         # If we're forcing it through we don't check.\n162         if templating_blocks_indent and not force_block_indent:\n163             indent_balance = sum(\n164                 getattr(elem, \"indent_val\", 0)\n165                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n166             )\n167             if indent_balance != 0:\n168                 linter_logger.debug(\n169                     \"Indent balance test failed for %r. Template indents will not be \"\n170                     \"linted for this file.\",\n171                     templated_file.fname,\n172                 )\n173                 # Don't enable the templating blocks.\n174                 templating_blocks_indent = False\n175 \n176         # The file will have been lexed without config, so check all indents\n177         # are enabled.\n178         new_tokens = []\n179         for token in cast(Tuple[BaseSegment, ...], tokens):\n180             if token.is_meta:\n181                 token = cast(MetaSegment, token)\n182                 if token.indent_val != 0:\n183                     # Don't allow it if we're not linting templating block indents.\n184                     if not templating_blocks_indent:\n185                         continue\n186             new_tokens.append(token)\n187 \n188         # Return new buffer\n189         return new_tokens, violations, config\n190 \n191     @staticmethod\n192     def _parse_tokens(\n193         tokens: Sequence[BaseSegment],\n194         config: FluffConfig,\n195         recurse: bool = True,\n196         fname: Optional[str] = None,\n197     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n198         parser = Parser(config=config)\n199         violations = []\n200         # Parse the file and log any problems\n201         try:\n202             parsed: Optional[BaseSegment] = parser.parse(\n203                 tokens,\n204                 recurse=recurse,\n205                 fname=fname,\n206             )\n207         except SQLParseError as err:\n208             linter_logger.info(\"PARSING FAILED! : %s\", err)\n209             violations.append(err)\n210             return None, violations\n211 \n212         if parsed:\n213             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n214             linter_logger.info(\"\\n\" + parsed.stringify())\n215             # We may succeed parsing, but still have unparsable segments. Extract them\n216             # here.\n217             for unparsable in parsed.iter_unparsables():\n218                 # No exception has been raised explicitly, but we still create one here\n219                 # so that we can use the common interface\n220                 violations.append(\n221                     SQLParseError(\n222                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n223                         \"{1!r}\".format(\n224                             unparsable.pos_marker.working_loc,\n225                             unparsable.raw\n226                             if len(unparsable.raw) < 40\n227                             else unparsable.raw[:40] + \"...\",\n228                         ),\n229                         segment=unparsable,\n230                     )\n231                 )\n232                 linter_logger.info(\"Found unparsable segment...\")\n233                 linter_logger.info(unparsable.stringify())\n234         return parsed, violations\n235 \n236     @staticmethod\n237     def parse_noqa(\n238         comment: str,\n239         line_no: int,\n240         rule_codes: List[str],\n241     ):\n242         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n243         # Also trim any whitespace afterward\n244 \n245         # Comment lines can also have noqa e.g.\n246         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: L016\n247         # Therefore extract last possible inline ignore.\n248         comment = [c.strip() for c in comment.split(\"--\")][-1]\n249 \n250         if comment.startswith(\"noqa\"):\n251             # This is an ignore identifier\n252             comment_remainder = comment[4:]\n253             if comment_remainder:\n254                 if not comment_remainder.startswith(\":\"):\n255                     return SQLParseError(\n256                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n257                         line_no=line_no,\n258                     )\n259                 comment_remainder = comment_remainder[1:].strip()\n260                 if comment_remainder:\n261                     action: Optional[str]\n262                     if \"=\" in comment_remainder:\n263                         action, rule_part = comment_remainder.split(\"=\", 1)\n264                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n265                             return SQLParseError(\n266                                 \"Malformed 'noqa' section. \"\n267                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n268                                 \"or 'noqa: disable=<rule>[,...] | all\",\n269                                 line_no=line_no,\n270                             )\n271                     else:\n272                         action = None\n273                         rule_part = comment_remainder\n274                         if rule_part in {\"disable\", \"enable\"}:\n275                             return SQLParseError(\n276                                 \"Malformed 'noqa' section. \"\n277                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n278                                 \"or 'noqa: disable=<rule>[,...] | all\",\n279                                 line_no=line_no,\n280                             )\n281                     rules: Optional[Tuple[str, ...]]\n282                     if rule_part != \"all\":\n283                         # Rules can be globs therefore we compare to the rule_set to\n284                         # expand the globs.\n285                         unexpanded_rules = tuple(\n286                             r.strip() for r in rule_part.split(\",\")\n287                         )\n288                         expanded_rules = []\n289                         for r in unexpanded_rules:\n290                             expanded_rule = [\n291                                 x\n292                                 for x in fnmatch.filter(rule_codes, r)\n293                                 if x not in expanded_rules\n294                             ]\n295                             if expanded_rule:\n296                                 expanded_rules.extend(expanded_rule)\n297                             elif r not in expanded_rules:\n298                                 # We were unable to expand the glob.\n299                                 # Therefore assume the user is referencing\n300                                 # a special error type (e.g. PRS, LXR, or TMP)\n301                                 # and add this to the list of rules to ignore.\n302                                 expanded_rules.append(r)\n303                         rules = tuple(expanded_rules)\n304                     else:\n305                         rules = None\n306                     return NoQaDirective(line_no, rules, action)\n307             return NoQaDirective(line_no, None, None)\n308         return None\n309 \n310     @staticmethod\n311     def remove_templated_errors(\n312         linting_errors: List[SQLBaseError],\n313     ) -> List[SQLBaseError]:\n314         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n315         # Filter out any linting errors in templated sections if relevant.\n316         result: List[SQLBaseError] = []\n317         for e in linting_errors:\n318             if isinstance(e, SQLLintError):\n319                 if (\n320                     # Is it in a literal section?\n321                     e.segment.pos_marker.is_literal()\n322                     # Is it a rule that is designed to work on templated sections?\n323                     or e.rule.targets_templated\n324                 ):\n325                     result.append(e)\n326             else:\n327                 # If it's another type, just keep it. (E.g. SQLParseError from\n328                 # malformed \"noqa\" comment).\n329                 result.append(e)\n330         return result\n331 \n332     @staticmethod\n333     def _report_conflicting_fixes_same_anchor(message: str):  # pragma: no cover\n334         # This function exists primarily in order to let us monkeypatch it at\n335         # runtime (replacing it with a function that raises an exception).\n336         linter_logger.critical(message)\n337 \n338     @staticmethod\n339     def _warn_unfixable(code: str):\n340         linter_logger.warning(\n341             f\"One fix for {code} not applied, it would re-cause the same error.\"\n342         )\n343 \n344     # ### Class Methods\n345     # These compose the base static methods into useful recipes.\n346 \n347     @classmethod\n348     def parse_rendered(\n349         cls,\n350         rendered: RenderedFile,\n351         recurse: bool = True,\n352     ) -> ParsedString:\n353         \"\"\"Parse a rendered file.\"\"\"\n354         t0 = time.monotonic()\n355         violations = cast(List[SQLBaseError], rendered.templater_violations)\n356         tokens: Optional[Sequence[BaseSegment]]\n357         if rendered.templated_file:\n358             tokens, lvs, config = cls._lex_templated_file(\n359                 rendered.templated_file, rendered.config\n360             )\n361             violations += lvs\n362         else:\n363             tokens = None\n364 \n365         t1 = time.monotonic()\n366         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n367 \n368         if tokens:\n369             parsed, pvs = cls._parse_tokens(\n370                 tokens,\n371                 rendered.config,\n372                 recurse=recurse,\n373                 fname=rendered.fname,\n374             )\n375             violations += pvs\n376         else:\n377             parsed = None\n378 \n379         time_dict = {\n380             **rendered.time_dict,\n381             \"lexing\": t1 - t0,\n382             \"parsing\": time.monotonic() - t1,\n383         }\n384         return ParsedString(\n385             parsed,\n386             violations,\n387             time_dict,\n388             rendered.templated_file,\n389             rendered.config,\n390             rendered.fname,\n391             rendered.source_str,\n392         )\n393 \n394     @classmethod\n395     def extract_ignore_from_comment(\n396         cls,\n397         comment: RawSegment,\n398         rule_codes: List[str],\n399     ):\n400         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n401         # Also trim any whitespace afterward\n402         comment_content = comment.raw_trimmed().strip()\n403         comment_line, _ = comment.pos_marker.source_position()\n404         result = cls.parse_noqa(comment_content, comment_line, rule_codes)\n405         if isinstance(result, SQLParseError):\n406             result.segment = comment\n407         return result\n408 \n409     @classmethod\n410     def extract_ignore_mask_tree(\n411         cls,\n412         tree: BaseSegment,\n413         rule_codes: List[str],\n414     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n415         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n416         ignore_buff: List[NoQaDirective] = []\n417         violations: List[SQLBaseError] = []\n418         for comment in tree.recursive_crawl(\"comment\"):\n419             if comment.name == \"inline_comment\":\n420                 ignore_entry = cls.extract_ignore_from_comment(comment, rule_codes)\n421                 if isinstance(ignore_entry, SQLParseError):\n422                     violations.append(ignore_entry)\n423                 elif ignore_entry:\n424                     ignore_buff.append(ignore_entry)\n425         if ignore_buff:\n426             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n427         return ignore_buff, violations\n428 \n429     @classmethod\n430     def extract_ignore_mask_source(\n431         cls,\n432         source: str,\n433         inline_comment_regex: RegexLexer,\n434         rule_codes: List[str],\n435     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n436         \"\"\"Look for inline ignore comments and return NoQaDirectives.\n437 \n438         Very similar to extract_ignore_mask_tree(), but can be run on raw source\n439         (i.e. does not require the code to have parsed successfully).\n440         \"\"\"\n441         ignore_buff: List[NoQaDirective] = []\n442         violations: List[SQLBaseError] = []\n443         for idx, line in enumerate(source.split(\"\\n\")):\n444             match = inline_comment_regex.search(line) if line else None\n445             if match:\n446                 ignore_entry = cls.parse_noqa(\n447                     line[match[0] : match[1]], idx + 1, rule_codes\n448                 )\n449                 if isinstance(ignore_entry, SQLParseError):\n450                     violations.append(ignore_entry)  # pragma: no cover\n451                 elif ignore_entry:\n452                     ignore_buff.append(ignore_entry)\n453         if ignore_buff:\n454             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n455         return ignore_buff, violations\n456 \n457     @classmethod\n458     def lint_fix_parsed(\n459         cls,\n460         tree: BaseSegment,\n461         config: FluffConfig,\n462         rule_set: List[BaseRule],\n463         fix: bool = False,\n464         fname: Optional[str] = None,\n465         templated_file: Optional[TemplatedFile] = None,\n466         formatter: Any = None,\n467     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n468         \"\"\"Lint and optionally fix a tree object.\"\"\"\n469         # Keep track of the linting errors\n470         all_linting_errors = []\n471         # A placeholder for the fixes we had on the previous loop\n472         last_fixes = None\n473         # Keep a set of previous versions to catch infinite loops.\n474         previous_versions = {tree.raw}\n475 \n476         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n477         # once for linting.\n478         loop_limit = config.get(\"runaway_limit\") if fix else 1\n479 \n480         # Dispatch the output for the lint header\n481         if formatter:\n482             formatter.dispatch_lint_header(fname)\n483 \n484         # Look for comment segments which might indicate lines to ignore.\n485         if not config.get(\"disable_noqa\"):\n486             rule_codes = [r.code for r in rule_set]\n487             ignore_buff, ivs = cls.extract_ignore_mask_tree(tree, rule_codes)\n488             all_linting_errors += ivs\n489         else:\n490             ignore_buff = []\n491 \n492         save_tree = tree\n493         for loop in range(loop_limit):\n494             changed = False\n495 \n496             progress_bar_crawler = tqdm(\n497                 rule_set,\n498                 desc=\"lint by rules\",\n499                 leave=False,\n500                 disable=progress_bar_configuration.disable_progress_bar,\n501             )\n502 \n503             for crawler in progress_bar_crawler:\n504                 progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n505 \n506                 # fixes should be a dict {} with keys edit, delete, create\n507                 # delete is just a list of segments to delete\n508                 # edit and create are list of tuples. The first element is the\n509                 # \"anchor\", the segment to look for either to edit or to insert BEFORE.\n510                 # The second is the element to insert or create.\n511                 linting_errors, _, fixes, _ = crawler.crawl(\n512                     tree,\n513                     ignore_mask=ignore_buff,\n514                     dialect=config.get(\"dialect_obj\"),\n515                     fname=fname,\n516                     templated_file=templated_file,\n517                 )\n518                 all_linting_errors += linting_errors\n519 \n520                 if fix and fixes:\n521                     linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n522                     # Do some sanity checks on the fixes before applying.\n523                     anchor_info = BaseSegment.compute_anchor_edit_info(fixes)\n524                     if any(\n525                         not info.is_valid for info in anchor_info.values()\n526                     ):  # pragma: no cover\n527                         message = (\n528                             f\"Rule {crawler.code} returned conflicting fixes with the \"\n529                             f\"same anchor. This is only supported for create_before+\"\n530                             f\"create_after, so the fixes will not be applied. {fixes!r}\"\n531                         )\n532                         cls._report_conflicting_fixes_same_anchor(message)\n533                         for lint_result in linting_errors:\n534                             lint_result.fixes = []\n535                     elif fixes == last_fixes:  # pragma: no cover\n536                         # If we generate the same fixes two times in a row,\n537                         # that means we're in a loop, and we want to stop.\n538                         # (Fixes should address issues, hence different\n539                         # and/or fewer fixes next time.)\n540                         cls._warn_unfixable(crawler.code)\n541                     else:\n542                         # This is the happy path. We have fixes, now we want to\n543                         # apply them.\n544                         last_fixes = fixes\n545                         new_tree, _ = tree.apply_fixes(\n546                             config.get(\"dialect_obj\"), crawler.code, anchor_info\n547                         )\n548                         # Check for infinite loops\n549                         if new_tree.raw not in previous_versions:\n550                             # We've not seen this version of the file so far. Continue.\n551                             tree = new_tree\n552                             previous_versions.add(tree.raw)\n553                             changed = True\n554                             continue\n555                         else:\n556                             # Applying these fixes took us back to a state which we've\n557                             # seen before. We're in a loop, so we want to stop.\n558                             cls._warn_unfixable(crawler.code)\n559 \n560             if loop == 0:\n561                 # Keep track of initial errors for reporting.\n562                 initial_linting_errors = all_linting_errors.copy()\n563 \n564             if fix and not changed:\n565                 # We did not change the file. Either the file is clean (no fixes), or\n566                 # any fixes which are present will take us back to a previous state.\n567                 linter_logger.info(\n568                     f\"Fix loop complete. Stability achieved after {loop}/{loop_limit} \"\n569                     \"loops.\"\n570                 )\n571                 break\n572         else:\n573             if fix:\n574                 # The linter loop hit the limit before reaching a stable point\n575                 # (i.e. free of lint errors). If this happens, it's usually\n576                 # because one or more rules produced fixes which did not address\n577                 # the original issue **or** created new issues.\n578                 linter_logger.warning(f\"Loop limit on fixes reached [{loop_limit}].\")\n579 \n580                 # Discard any fixes for the linting errors, since they caused a\n581                 # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n582                 # these linting errors are \"unfixable\". This is important,\n583                 # because when \"sqlfluff fix\" encounters unfixable lint errors,\n584                 # it exits with a \"failure\" exit code, which is exactly what we\n585                 # want in this situation. (Reason: Although this is more of an\n586                 # internal SQLFluff issue, users deserve to know about it,\n587                 # because it means their file(s) weren't fixed.\n588                 for violation in initial_linting_errors:\n589                     if isinstance(violation, SQLLintError):\n590                         violation.fixes = []\n591 \n592                 # Return the original parse tree, before any fixes were applied.\n593                 # Reason: When the linter hits the loop limit, the file is often\n594                 # messy, e.g. some of the fixes were applied repeatedly, possibly\n595                 # other weird things. We don't want the user to see this junk!\n596                 return save_tree, initial_linting_errors, ignore_buff\n597 \n598         if config.get(\"ignore_templated_areas\", default=True):\n599             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n600 \n601         return tree, initial_linting_errors, ignore_buff\n602 \n603     @classmethod\n604     def lint_parsed(\n605         cls,\n606         parsed: ParsedString,\n607         rule_set: List[BaseRule],\n608         fix: bool = False,\n609         formatter: Any = None,\n610         encoding: str = \"utf8\",\n611     ):\n612         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n613         violations = parsed.violations\n614         time_dict = parsed.time_dict\n615         tree: Optional[BaseSegment]\n616         if parsed.tree:\n617             t0 = time.monotonic()\n618             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n619             tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n620                 parsed.tree,\n621                 config=parsed.config,\n622                 rule_set=rule_set,\n623                 fix=fix,\n624                 fname=parsed.fname,\n625                 templated_file=parsed.templated_file,\n626                 formatter=formatter,\n627             )\n628             # Update the timing dict\n629             time_dict[\"linting\"] = time.monotonic() - t0\n630 \n631             # We're only going to return the *initial* errors, rather\n632             # than any generated during the fixing cycle.\n633             violations += initial_linting_errors\n634         else:\n635             # If no parsed tree, set to None\n636             tree = None\n637             ignore_buff = []\n638             if not parsed.config.get(\"disable_noqa\"):\n639                 # Templating and/or parsing have failed. Look for \"noqa\"\n640                 # comments (the normal path for identifying these comments\n641                 # requires access to the parse tree, and because of the failure,\n642                 # we don't have a parse tree).\n643                 ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n644                     parsed.source_str,\n645                     [\n646                         lm\n647                         for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n648                         if lm.name == \"inline_comment\"\n649                     ][0],\n650                     [r.code for r in rule_set],\n651                 )\n652                 violations += ignore_violations\n653 \n654         # We process the ignore config here if appropriate\n655         for violation in violations:\n656             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n657 \n658         linted_file = LintedFile(\n659             parsed.fname,\n660             violations,\n661             time_dict,\n662             tree,\n663             ignore_mask=ignore_buff,\n664             templated_file=parsed.templated_file,\n665             encoding=encoding,\n666         )\n667 \n668         # This is the main command line output from linting.\n669         if formatter:\n670             formatter.dispatch_file_violations(\n671                 parsed.fname, linted_file, only_fixable=fix\n672             )\n673 \n674         # Safety flag for unset dialects\n675         if parsed.config.get(\"dialect\") == \"ansi\" and linted_file.get_violations(\n676             fixable=True if fix else None, types=SQLParseError\n677         ):\n678             if formatter:  # pragma: no cover TODO?\n679                 formatter.dispatch_dialect_warning()\n680 \n681         return linted_file\n682 \n683     @classmethod\n684     def lint_rendered(\n685         cls,\n686         rendered: RenderedFile,\n687         rule_set: List[BaseRule],\n688         fix: bool = False,\n689         formatter: Any = None,\n690     ) -> LintedFile:\n691         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n692         parsed = cls.parse_rendered(rendered)\n693         return cls.lint_parsed(\n694             parsed,\n695             rule_set=rule_set,\n696             fix=fix,\n697             formatter=formatter,\n698             encoding=rendered.encoding,\n699         )\n700 \n701     # ### Instance Methods\n702     # These are tied to a specific instance and so are not necessarily\n703     # safe to use in parallel operations.\n704 \n705     def render_string(\n706         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n707     ) -> RenderedFile:\n708         \"\"\"Template the file.\"\"\"\n709         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n710 \n711         # Start the templating timer\n712         t0 = time.monotonic()\n713 \n714         # Newlines are normalised to unix-style line endings (\\n).\n715         # The motivation is that Jinja normalises newlines during templating and\n716         # we want consistent mapping between the raw and templated slices.\n717         in_str = self._normalise_newlines(in_str)\n718 \n719         if not config.get(\"templater_obj\") == self.templater:\n720             linter_logger.warning(\n721                 (\n722                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n723                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n724                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n725                     \"working directory. It can be set in a .sqlfluff in the current \"\n726                     \"working directory. See Nesting section of the docs for more \"\n727                     \"details.\"\n728                 )\n729             )\n730         try:\n731             templated_file, templater_violations = self.templater.process(\n732                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n733             )\n734         except SQLTemplaterSkipFile as s:  # pragma: no cover\n735             linter_logger.warning(str(s))\n736             templated_file = None\n737             templater_violations = []\n738 \n739         if not templated_file:\n740             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n741 \n742         # Record time\n743         time_dict = {\"templating\": time.monotonic() - t0}\n744 \n745         return RenderedFile(\n746             templated_file,\n747             templater_violations,\n748             config,\n749             time_dict,\n750             fname,\n751             encoding,\n752             in_str,\n753         )\n754 \n755     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n756         \"\"\"Load and render a file with relevant config.\"\"\"\n757         # Load the raw file.\n758         raw_file, config, encoding = self._load_raw_file_and_config(fname, root_config)\n759         # Render the file\n760         return self.render_string(raw_file, fname, config, encoding)\n761 \n762     def parse_string(\n763         self,\n764         in_str: str,\n765         fname: str = \"<string>\",\n766         recurse: bool = True,\n767         config: Optional[FluffConfig] = None,\n768         encoding: str = \"utf-8\",\n769     ) -> ParsedString:\n770         \"\"\"Parse a string.\"\"\"\n771         violations: List[SQLBaseError] = []\n772 \n773         # Dispatch the output for the template header (including the config diff)\n774         if self.formatter:\n775             self.formatter.dispatch_template_header(fname, self.config, config)\n776 \n777         # Just use the local config from here:\n778         config = config or self.config\n779 \n780         # Scan the raw file for config commands.\n781         config.process_raw_file_for_config(in_str)\n782         rendered = self.render_string(in_str, fname, config, encoding)\n783         violations += rendered.templater_violations\n784 \n785         # Dispatch the output for the parse header\n786         if self.formatter:\n787             self.formatter.dispatch_parse_header(fname)\n788 \n789         return self.parse_rendered(rendered, recurse=recurse)\n790 \n791     def fix(\n792         self,\n793         tree: BaseSegment,\n794         config: Optional[FluffConfig] = None,\n795         fname: Optional[str] = None,\n796         templated_file: Optional[TemplatedFile] = None,\n797     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n798         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n799         config = config or self.config\n800         rule_set = self.get_ruleset(config=config)\n801         fixed_tree, violations, _ = self.lint_fix_parsed(\n802             tree,\n803             config,\n804             rule_set,\n805             fix=True,\n806             fname=fname,\n807             templated_file=templated_file,\n808             formatter=self.formatter,\n809         )\n810         return fixed_tree, violations\n811 \n812     def lint(\n813         self,\n814         tree: BaseSegment,\n815         config: Optional[FluffConfig] = None,\n816         fname: Optional[str] = None,\n817         templated_file: Optional[TemplatedFile] = None,\n818     ) -> List[SQLBaseError]:\n819         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n820         config = config or self.config\n821         rule_set = self.get_ruleset(config=config)\n822         _, violations, _ = self.lint_fix_parsed(\n823             tree,\n824             config,\n825             rule_set,\n826             fix=False,\n827             fname=fname,\n828             templated_file=templated_file,\n829             formatter=self.formatter,\n830         )\n831         return violations\n832 \n833     def lint_string(\n834         self,\n835         in_str: str = \"\",\n836         fname: str = \"<string input>\",\n837         fix: bool = False,\n838         config: Optional[FluffConfig] = None,\n839         encoding: str = \"utf8\",\n840     ) -> LintedFile:\n841         \"\"\"Lint a string.\n842 \n843         Returns:\n844             :obj:`LintedFile`: an object representing that linted file.\n845 \n846         \"\"\"\n847         # Sort out config, defaulting to the built in config if no override\n848         config = config or self.config\n849         # Parse the string.\n850         parsed = self.parse_string(\n851             in_str=in_str,\n852             fname=fname,\n853             config=config,\n854         )\n855         # Get rules as appropriate\n856         rule_set = self.get_ruleset(config=config)\n857         # Lint the file and return the LintedFile\n858         return self.lint_parsed(\n859             parsed,\n860             rule_set,\n861             fix=fix,\n862             formatter=self.formatter,\n863             encoding=encoding,\n864         )\n865 \n866     def paths_from_path(\n867         self,\n868         path: str,\n869         ignore_file_name: str = \".sqlfluffignore\",\n870         ignore_non_existent_files: bool = False,\n871         ignore_files: bool = True,\n872         working_path: str = os.getcwd(),\n873     ) -> List[str]:\n874         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n875 \n876         Here we also deal with the .sqlfluffignore file if present.\n877 \n878         When a path to a file to be linted is explicitly passed\n879         we look for ignore files in all directories that are parents of the file,\n880         up to the current directory.\n881 \n882         If the current directory is not a parent of the file we only\n883         look for an ignore file in the direct parent of the file.\n884 \n885         \"\"\"\n886         if not os.path.exists(path):\n887             if ignore_non_existent_files:\n888                 return []\n889             else:\n890                 raise OSError(\"Specified path does not exist\")\n891 \n892         # Files referred to exactly are also ignored if\n893         # matched, but we warn the users when that happens\n894         is_exact_file = os.path.isfile(path)\n895 \n896         path_walk: WalkableType\n897         if is_exact_file:\n898             # When the exact file to lint is passed, we\n899             # fill path_walk with an input that follows\n900             # the structure of `os.walk`:\n901             #   (root, directories, files)\n902             dirpath = os.path.dirname(path)\n903             files = [os.path.basename(path)]\n904             path_walk = [(dirpath, None, files)]\n905         else:\n906             path_walk = list(os.walk(path))\n907 \n908         ignore_file_paths = ConfigLoader.find_ignore_config_files(\n909             path=path, working_path=working_path, ignore_file_name=ignore_file_name\n910         )\n911         # Add paths that could contain \"ignore files\"\n912         # to the path_walk list\n913         path_walk_ignore_file = [\n914             (\n915                 os.path.dirname(ignore_file_path),\n916                 None,\n917                 # Only one possible file, since we only\n918                 # have one \"ignore file name\"\n919                 [os.path.basename(ignore_file_path)],\n920             )\n921             for ignore_file_path in ignore_file_paths\n922         ]\n923         path_walk += path_walk_ignore_file\n924 \n925         # If it's a directory then expand the path!\n926         buffer = []\n927         ignores = {}\n928         for dirpath, _, filenames in path_walk:\n929             for fname in filenames:\n930                 fpath = os.path.join(dirpath, fname)\n931                 # Handle potential .sqlfluffignore files\n932                 if ignore_files and fname == ignore_file_name:\n933                     with open(fpath) as fh:\n934                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n935                         ignores[dirpath] = spec\n936                     # We don't need to process the ignore file any futher\n937                     continue\n938 \n939                 # We won't purge files *here* because there's an edge case\n940                 # that the ignore file is processed after the sql file.\n941 \n942                 # Scan for remaining files\n943                 for ext in (\n944                     self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n945                 ):\n946                     # is it a sql file?\n947                     if fname.lower().endswith(ext):\n948                         buffer.append(fpath)\n949 \n950         if not ignore_files:\n951             return sorted(buffer)\n952 \n953         # Check the buffer for ignore items and normalise the rest.\n954         filtered_buffer = []\n955 \n956         for fpath in buffer:\n957             abs_fpath = os.path.abspath(fpath)\n958             for ignore_base, ignore_spec in ignores.items():\n959                 abs_ignore_base = os.path.abspath(ignore_base)\n960                 if abs_fpath.startswith(\n961                     abs_ignore_base + os.sep\n962                 ) and ignore_spec.match_file(\n963                     os.path.relpath(abs_fpath, abs_ignore_base)\n964                 ):\n965                     # This file is ignored, skip it.\n966                     if is_exact_file:\n967                         linter_logger.warning(\n968                             \"Exact file path %s was given but \"\n969                             \"it was ignored by a %s pattern in %s, \"\n970                             \"re-run with `--disregard-sqlfluffignores` to \"\n971                             \"skip %s\"\n972                             % (\n973                                 path,\n974                                 ignore_file_name,\n975                                 ignore_base,\n976                                 ignore_file_name,\n977                             )\n978                         )\n979                     break\n980             else:\n981                 filtered_buffer.append(os.path.normpath(fpath))\n982 \n983         # Return\n984         return sorted(filtered_buffer)\n985 \n986     def lint_string_wrapped(\n987         self,\n988         string: str,\n989         fname: str = \"<string input>\",\n990         fix: bool = False,\n991     ) -> LintingResult:\n992         \"\"\"Lint strings directly.\"\"\"\n993         result = LintingResult()\n994         linted_path = LintedDir(fname)\n995         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n996         result.add(linted_path)\n997         result.stop_timer()\n998         return result\n999 \n1000     def lint_path(\n1001         self,\n1002         path: str,\n1003         fix: bool = False,\n1004         ignore_non_existent_files: bool = False,\n1005         ignore_files: bool = True,\n1006         processes: int = 1,\n1007     ) -> LintedDir:\n1008         \"\"\"Lint a path.\"\"\"\n1009         linted_path = LintedDir(path)\n1010         if self.formatter:\n1011             self.formatter.dispatch_path(path)\n1012         fnames = list(\n1013             self.paths_from_path(\n1014                 path,\n1015                 ignore_non_existent_files=ignore_non_existent_files,\n1016                 ignore_files=ignore_files,\n1017             )\n1018         )\n1019 \n1020         # to avoid circular import\n1021         from sqlfluff.core.linter.runner import get_runner\n1022 \n1023         runner = get_runner(\n1024             self,\n1025             self.config,\n1026             processes=processes,\n1027             allow_process_parallelism=self.allow_process_parallelism,\n1028         )\n1029 \n1030         # Show files progress bar only when there is more than one.\n1031         files_count = len(fnames)\n1032         progress_bar_files = tqdm(\n1033             total=files_count,\n1034             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n1035             leave=False,\n1036             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1037         )\n1038 \n1039         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n1040             linted_path.add(linted_file)\n1041             # If any fatal errors, then stop iteration.\n1042             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n1043                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n1044                 break\n1045 \n1046             # Progress bar for files is rendered only when there is more than one file.\n1047             # Additionally as it's updated after each loop, we need to get file name\n1048             # from the next loop. This is why `enumerate` starts with `1` and there\n1049             # is `i < len` to not exceed files list length.\n1050             progress_bar_files.update(n=1)\n1051             if i < len(fnames):\n1052                 progress_bar_files.set_description(\n1053                     f\"file {os.path.basename(fnames[i])}\"\n1054                 )\n1055 \n1056         return linted_path\n1057 \n1058     def lint_paths(\n1059         self,\n1060         paths: Tuple[str, ...],\n1061         fix: bool = False,\n1062         ignore_non_existent_files: bool = False,\n1063         ignore_files: bool = True,\n1064         processes: int = 1,\n1065     ) -> LintingResult:\n1066         \"\"\"Lint an iterable of paths.\"\"\"\n1067         paths_count = len(paths)\n1068 \n1069         # If no paths specified - assume local\n1070         if not paths_count:  # pragma: no cover\n1071             paths = (os.getcwd(),)\n1072         # Set up the result to hold what we get back\n1073         result = LintingResult()\n1074 \n1075         progress_bar_paths = tqdm(\n1076             total=paths_count,\n1077             desc=\"path\",\n1078             leave=False,\n1079             disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1080         )\n1081         for path in paths:\n1082             progress_bar_paths.set_description(f\"path {path}\")\n1083 \n1084             # Iterate through files recursively in the specified directory (if it's a\n1085             # directory) or read the file directly if it's not\n1086             result.add(\n1087                 self.lint_path(\n1088                     path,\n1089                     fix=fix,\n1090                     ignore_non_existent_files=ignore_non_existent_files,\n1091                     ignore_files=ignore_files,\n1092                     processes=processes,\n1093                 )\n1094             )\n1095 \n1096             progress_bar_paths.update(1)\n1097 \n1098         result.stop_timer()\n1099         return result\n1100 \n1101     def parse_path(\n1102         self,\n1103         path: str,\n1104         recurse: bool = True,\n1105     ) -> Iterator[ParsedString]:\n1106         \"\"\"Parse a path of sql files.\n1107 \n1108         NB: This a generator which will yield the result of each file\n1109         within the path iteratively.\n1110         \"\"\"\n1111         for fname in self.paths_from_path(path):\n1112             if self.formatter:\n1113                 self.formatter.dispatch_path(path)\n1114             # Load the file with the config and yield the result.\n1115             raw_file, config, encoding = self._load_raw_file_and_config(\n1116                 fname, self.config\n1117             )\n1118             yield self.parse_string(\n1119                 raw_file,\n1120                 fname=fname,\n1121                 recurse=recurse,\n1122                 config=config,\n1123                 encoding=encoding,\n1124             )\n1125 \n[end of src/sqlfluff/core/linter/linter.py]\n[start of src/sqlfluff/core/rules/base.py]\n1 \"\"\"Implements the base rule class.\n2 \n3 Rules crawl through the trees returned by the parser and evaluate particular\n4 rules.\n5 \n6 The intent is that it should be possible for the rules to be expressed\n7 as simply as possible, with as much of the complexity abstracted away.\n8 \n9 The evaluation function should take enough arguments that it can evaluate\n10 the position of the given segment in relation to its neighbors, and that\n11 the segment which finally \"triggers\" the error, should be the one that would\n12 be corrected OR if the rule relates to something that is missing, then it\n13 should flag on the segment FOLLOWING, the place that the desired element is\n14 missing.\n15 \"\"\"\n16 \n17 import bdb\n18 import copy\n19 import fnmatch\n20 import logging\n21 import pathlib\n22 import regex\n23 from typing import cast, Iterable, Optional, List, Set, Tuple, Union, Any\n24 from collections import namedtuple\n25 from dataclasses import dataclass\n26 \n27 from sqlfluff.core.cached_property import cached_property\n28 \n29 from sqlfluff.core.linter import LintedFile\n30 from sqlfluff.core.parser import BaseSegment, PositionMarker, RawSegment\n31 from sqlfluff.core.dialects import Dialect\n32 from sqlfluff.core.errors import SQLLintError\n33 from sqlfluff.core.rules.functional import Segments\n34 from sqlfluff.core.templaters.base import RawFileSlice, TemplatedFile\n35 \n36 # The ghost of a rule (mostly used for testing)\n37 RuleGhost = namedtuple(\"RuleGhost\", [\"code\", \"description\"])\n38 \n39 # Instantiate the rules logger\n40 rules_logger = logging.getLogger(\"sqlfluff.rules\")\n41 \n42 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n43 \n44 \n45 class RuleLoggingAdapter(logging.LoggerAdapter):\n46     \"\"\"A LoggingAdapter for rules which adds the code of the rule to it.\"\"\"\n47 \n48     def process(self, msg, kwargs):\n49         \"\"\"Add the code element to the logging message before emit.\"\"\"\n50         return \"[{}] {}\".format(self.extra[\"code\"], msg), kwargs\n51 \n52 \n53 class LintResult:\n54     \"\"\"A class to hold the results of a rule evaluation.\n55 \n56     Args:\n57         anchor (:obj:`BaseSegment`, optional): A segment which represents\n58             the *position* of the a problem. NB: Each fix will also hold\n59             its own reference to position, so this position is mostly for\n60             alerting the user to where the *problem* is.\n61         fixes (:obj:`list` of :obj:`LintFix`, optional): An array of any\n62             fixes which would correct this issue. If not present then it's\n63             assumed that this issue will have to manually fixed.\n64         memory (:obj:`dict`, optional): An object which stores any working\n65             memory for the rule. The `memory` returned in any `LintResult`\n66             will be passed as an input to the next segment to be crawled.\n67         description (:obj:`str`, optional): A description of the problem\n68             identified as part of this result. This will override the\n69             description of the rule as what gets reported to the user\n70             with the problem if provided.\n71 \n72     \"\"\"\n73 \n74     def __init__(\n75         self,\n76         anchor: Optional[BaseSegment] = None,\n77         fixes: Optional[List[\"LintFix\"]] = None,\n78         memory=None,\n79         description=None,\n80     ):\n81         # An anchor of none, means no issue\n82         self.anchor = anchor\n83         # Fixes might be blank\n84         self.fixes = fixes or []\n85         # When instantiating the result, we filter any fixes which are \"trivial\".\n86         self.fixes = [f for f in self.fixes if not f.is_trivial()]\n87         # Memory is passed back in the linting result\n88         self.memory = memory\n89         # store a description_override for later\n90         self.description = description\n91 \n92     def to_linting_error(self, rule) -> Optional[SQLLintError]:\n93         \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n94         if self.anchor:\n95             # Allow description override from the LintResult\n96             description = self.description or rule.description\n97             return SQLLintError(\n98                 rule=rule,\n99                 segment=self.anchor,\n100                 fixes=self.fixes,\n101                 description=description,\n102             )\n103         else:\n104             return None\n105 \n106 \n107 class LintFix:\n108     \"\"\"A class to hold a potential fix to a linting violation.\n109 \n110     Args:\n111         edit_type (:obj:`str`): One of `create_before`, `create_after`,\n112             `replace`, `delete` to indicate the kind of fix this represents.\n113         anchor (:obj:`BaseSegment`): A segment which represents\n114             the *position* that this fix should be applied at. For deletions\n115             it represents the segment to delete, for creations it implies the\n116             position to create at (with the existing element at this position\n117             to be moved *after* the edit), for a `replace` it implies the\n118             segment to be replaced.\n119         edit (:obj:`BaseSegment`, optional): For `replace` and `create` fixes,\n120             this holds the iterable of segments to create or replace at the\n121             given `anchor` point.\n122         source (:obj:`BaseSegment`, optional): For `replace` and `create` fixes,\n123             this holds iterable of segments that provided code. IMPORTANT: The\n124             linter uses this to prevent copying material from templated areas.\n125 \n126     \"\"\"\n127 \n128     def __init__(\n129         self,\n130         edit_type: str,\n131         anchor: BaseSegment,\n132         edit: Optional[Iterable[BaseSegment]] = None,\n133         source: Optional[Iterable[BaseSegment]] = None,\n134     ) -> None:\n135         if edit_type not in (\n136             \"create_before\",\n137             \"create_after\",\n138             \"replace\",\n139             \"delete\",\n140         ):  # pragma: no cover\n141             raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n142         self.edit_type = edit_type\n143         if not anchor:  # pragma: no cover\n144             raise ValueError(\"Fixes must provide an anchor.\")\n145         self.anchor = anchor\n146         self.edit: Optional[List[BaseSegment]] = None\n147         if edit is not None:\n148             # Coerce edit iterable to list\n149             edit = list(edit)\n150             # Copy all the elements of edit to stop contamination.\n151             # We're about to start stripping the position markers\n152             # off some of the elements and we don't want to end up\n153             # stripping the positions of the original elements of\n154             # the parsed structure.\n155             self.edit = copy.deepcopy(edit)\n156             # Check that any edits don't have a position marker set.\n157             # We should rely on realignment to make position markers.\n158             # Strip position markers of anything enriched, otherwise things can get\n159             # blurry\n160             for seg in self.edit:\n161                 if seg.pos_marker:\n162                     # Developer warning.\n163                     rules_logger.debug(\n164                         \"Developer Note: Edit segment found with preset position \"\n165                         \"marker. These should be unset and calculated later.\"\n166                     )\n167                     seg.pos_marker = None\n168             # Once stripped, we shouldn't replace any markers because\n169             # later code may rely on them being accurate, which we\n170             # can't guarantee with edits.\n171         self.source = [seg for seg in source if seg.pos_marker] if source else []\n172 \n173     def is_trivial(self):\n174         \"\"\"Return true if the fix is trivial.\n175 \n176         Trivial edits are:\n177         - Anything of zero length.\n178         - Any edits which result in themselves.\n179 \n180         Removing these makes the routines which process fixes much faster.\n181         \"\"\"\n182         if self.edit_type in (\"create_before\", \"create_after\"):\n183             if isinstance(self.edit, BaseSegment):\n184                 if len(self.edit.raw) == 0:  # pragma: no cover TODO?\n185                     return True\n186             elif all(len(elem.raw) == 0 for elem in self.edit):\n187                 return True\n188         elif self.edit_type == \"replace\" and self.edit == self.anchor:\n189             return True  # pragma: no cover TODO?\n190         return False\n191 \n192     def __repr__(self):\n193         if self.edit_type == \"delete\":\n194             detail = f\"delete:{self.anchor.raw!r}\"\n195         elif self.edit_type in (\"replace\", \"create_before\", \"create_after\"):\n196             if hasattr(self.edit, \"raw\"):\n197                 new_detail = self.edit.raw  # pragma: no cover TODO?\n198             else:\n199                 new_detail = \"\".join(s.raw for s in self.edit)\n200 \n201             if self.edit_type == \"replace\":\n202                 detail = f\"edt:{self.anchor.raw!r}->{new_detail!r}\"\n203             else:\n204                 detail = f\"create:{new_detail!r}\"\n205         else:\n206             detail = \"\"  # pragma: no cover TODO?\n207         return \"<LintFix: {} @{} {}>\".format(\n208             self.edit_type, self.anchor.pos_marker, detail\n209         )\n210 \n211     def __eq__(self, other):\n212         \"\"\"Compare equality with another fix.\n213 \n214         A fix is equal to another if is in the same place (position), with the\n215         same type and (if appropriate) the same edit values.\n216 \n217         \"\"\"\n218         if not self.edit_type == other.edit_type:\n219             return False\n220         if not self.anchor == other.anchor:\n221             return False\n222         if not self.edit == other.edit:\n223             return False\n224         return True  # pragma: no cover TODO?\n225 \n226     @classmethod\n227     def delete(cls, anchor_segment: BaseSegment) -> \"LintFix\":\n228         \"\"\"Delete supplied anchor segment.\"\"\"\n229         return cls(\"delete\", anchor_segment)\n230 \n231     @classmethod\n232     def replace(\n233         cls,\n234         anchor_segment: BaseSegment,\n235         edit_segments: Iterable[BaseSegment],\n236         source: Optional[Iterable[BaseSegment]] = None,\n237     ) -> \"LintFix\":\n238         \"\"\"Replace supplied anchor segment with the edit segments.\"\"\"\n239         return cls(\"replace\", anchor_segment, edit_segments, source)\n240 \n241     @classmethod\n242     def create_before(\n243         cls,\n244         anchor_segment: BaseSegment,\n245         edit_segments: Iterable[BaseSegment],\n246         source: Optional[Iterable[BaseSegment]] = None,\n247     ) -> \"LintFix\":\n248         \"\"\"Create edit segments before the supplied anchor segment.\"\"\"\n249         return cls(\"create_before\", anchor_segment, edit_segments, source)\n250 \n251     @classmethod\n252     def create_after(\n253         cls,\n254         anchor_segment: BaseSegment,\n255         edit_segments: Iterable[BaseSegment],\n256         source: Optional[Iterable[BaseSegment]] = None,\n257     ) -> \"LintFix\":\n258         \"\"\"Create edit segments after the supplied anchor segment.\"\"\"\n259         return cls(\"create_after\", anchor_segment, edit_segments, source)\n260 \n261     def has_template_conflicts(self, templated_file: TemplatedFile) -> bool:\n262         \"\"\"Does this fix conflict with (i.e. touch) templated code?\"\"\"\n263         # Goal: Find the raw slices touched by the fix. Two cases, based on\n264         # edit type:\n265         # 1. \"delete\", \"replace\": Raw slices touching the anchor segment. If\n266         #    ANY are templated, discard the fix.\n267         # 2. \"create_before\", \"create_after\": Raw slices encompassing the two\n268         #    character positions surrounding the insertion point (**NOT** the\n269         #    whole anchor segment, because we're not *touching* the anchor\n270         #    segment, we're inserting **RELATIVE** to it. If ALL are templated,\n271         #    discard the fix.\n272         assert self.anchor.pos_marker\n273         anchor_slice = self.anchor.pos_marker.templated_slice\n274         templated_slices = [anchor_slice]\n275         check_fn = any\n276 \n277         if self.edit_type == \"create_before\":\n278             # Consider the first position of the anchor segment and the\n279             # position just before it.\n280             templated_slices = [\n281                 slice(anchor_slice.start, anchor_slice.start + 1),\n282                 slice(anchor_slice.start - 1, anchor_slice.start),\n283             ]\n284             check_fn = all\n285         elif self.edit_type == \"create_after\":\n286             # Consider the last position of the anchor segment and the\n287             # character just after it.\n288             templated_slices = [\n289                 slice(anchor_slice.stop - 1, anchor_slice.stop),\n290                 slice(anchor_slice.stop, anchor_slice.stop + 1),\n291             ]\n292             check_fn = all\n293         # TRICKY: For creations at the end of the file, there won't be an\n294         # existing slice. In this case, the function adds file_end_slice to the\n295         # result, as a sort of placeholder or sentinel value. We pass a literal\n296         # slice for \"file_end_slice\" so that later in this function, the LintFix\n297         # is interpreted as literal code. Otherwise, it could be interpreted as\n298         # a fix to *templated* code and incorrectly discarded.\n299         fix_slices = self._raw_slices_from_templated_slices(\n300             templated_file,\n301             templated_slices,\n302             file_end_slice=RawFileSlice(\"\", \"literal\", -1),\n303         )\n304 \n305         # We have the fix slices. Now check for conflicts.\n306         result = check_fn(fs.slice_type == \"templated\" for fs in fix_slices)\n307         if result or not self.source:\n308             return result\n309 \n310         # Fix slices were okay. Now check template safety of the \"source\" field.\n311         templated_slices = [\n312             cast(PositionMarker, source.pos_marker).templated_slice\n313             for source in self.source\n314         ]\n315         raw_slices = self._raw_slices_from_templated_slices(\n316             templated_file, templated_slices\n317         )\n318         return any(fs.slice_type == \"templated\" for fs in raw_slices)\n319 \n320     @staticmethod\n321     def _raw_slices_from_templated_slices(\n322         templated_file: TemplatedFile,\n323         templated_slices: List[slice],\n324         file_end_slice: Optional[RawFileSlice] = None,\n325     ) -> Set[RawFileSlice]:\n326         raw_slices: Set[RawFileSlice] = set()\n327         for templated_slice in templated_slices:\n328             try:\n329                 raw_slices.update(\n330                     templated_file.raw_slices_spanning_source_slice(\n331                         templated_file.templated_slice_to_source_slice(templated_slice)\n332                     )\n333                 )\n334             except (IndexError, ValueError):\n335                 # These errors will happen with \"create_before\" at the beginning\n336                 # of the file or \"create_after\" at the end of the file. By\n337                 # default, we ignore this situation. If the caller passed\n338                 # \"file_end_slice\", add that to the result. In effect,\n339                 # file_end_slice serves as a placeholder or sentinel value.\n340                 if file_end_slice is not None:\n341                     raw_slices.add(file_end_slice)\n342         return raw_slices\n343 \n344 \n345 EvalResultType = Union[LintResult, List[LintResult], None]\n346 \n347 \n348 @dataclass\n349 class RuleContext:\n350     \"\"\"Class for holding the context passed to rule eval functions.\"\"\"\n351 \n352     segment: BaseSegment\n353     parent_stack: Tuple[BaseSegment, ...]\n354     siblings_pre: Tuple[BaseSegment, ...]\n355     siblings_post: Tuple[BaseSegment, ...]\n356     raw_stack: Tuple[RawSegment, ...]\n357     memory: Any\n358     dialect: Dialect\n359     path: Optional[pathlib.Path]\n360     templated_file: Optional[TemplatedFile]\n361 \n362     @cached_property\n363     def functional(self):\n364         \"\"\"Returns a Surrogates object that simplifies writing rules.\"\"\"\n365         return FunctionalRuleContext(self)\n366 \n367 \n368 class FunctionalRuleContext:\n369     \"\"\"RuleContext written in a \"functional\" style; simplifies writing rules.\"\"\"\n370 \n371     def __init__(self, context: RuleContext):\n372         self.context = context\n373 \n374     @cached_property\n375     def segment(self) -> \"Segments\":\n376         \"\"\"Returns a Segments object for context.segment.\"\"\"\n377         return Segments(\n378             self.context.segment, templated_file=self.context.templated_file\n379         )\n380 \n381     @property\n382     def parent_stack(self) -> \"Segments\":  # pragma: no cover\n383         \"\"\"Returns a Segments object for context.parent_stack.\"\"\"\n384         return Segments(\n385             *self.context.parent_stack, templated_file=self.context.templated_file\n386         )\n387 \n388     @property\n389     def siblings_pre(self) -> \"Segments\":  # pragma: no cover\n390         \"\"\"Returns a Segments object for context.siblings_pre.\"\"\"\n391         return Segments(\n392             *self.context.siblings_pre, templated_file=self.context.templated_file\n393         )\n394 \n395     @property\n396     def siblings_post(self) -> \"Segments\":  # pragma: no cover\n397         \"\"\"Returns a Segments object for context.siblings_post.\"\"\"\n398         return Segments(\n399             *self.context.siblings_post, templated_file=self.context.templated_file\n400         )\n401 \n402     @cached_property\n403     def raw_stack(self) -> \"Segments\":\n404         \"\"\"Returns a Segments object for context.raw_stack.\"\"\"\n405         return Segments(\n406             *self.context.raw_stack, templated_file=self.context.templated_file\n407         )\n408 \n409     @cached_property\n410     def raw_segments(self):\n411         \"\"\"Returns a Segments object for all the raw segments in the file.\"\"\"\n412         file_segment = self.context.parent_stack[0]\n413         return Segments(\n414             *file_segment.get_raw_segments(), templated_file=self.context.templated_file\n415         )\n416 \n417 \n418 class BaseRule:\n419     \"\"\"The base class for a rule.\n420 \n421     Args:\n422         code (:obj:`str`): The identifier for this rule, used in inclusion\n423             or exclusion.\n424         description (:obj:`str`): A human readable description of what this\n425             rule does. It will be displayed when any violations are found.\n426 \n427     \"\"\"\n428 \n429     _check_docstring = True\n430     _works_on_unparsable = True\n431     _adjust_anchors = False\n432     targets_templated = False\n433 \n434     def __init__(self, code, description, **kwargs):\n435         self.description = description\n436         self.code = code\n437         # kwargs represents the config passed to the rule. Add all kwargs as class\n438         # attributes so they can be accessed in rules which inherit from this class\n439         for key, value in kwargs.items():\n440             self.__dict__[key] = value\n441 \n442         # We also define a custom logger here, which also includes the code\n443         # of the rule in the logging.\n444         self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n445         # Validate that declared configuration options exist\n446         try:\n447             for keyword in self.config_keywords:\n448                 if keyword not in kwargs.keys():\n449                     raise ValueError(\n450                         (\n451                             \"Unrecognized config '{}' for Rule {}. If this \"\n452                             \"is a new option, please add it to \"\n453                             \"`default_config.cfg`\"\n454                         ).format(keyword, code)\n455                     )\n456         except AttributeError:\n457             self.logger.info(f\"No config_keywords defined for {code}\")\n458 \n459     def _eval(self, context: RuleContext) -> EvalResultType:\n460         \"\"\"Evaluate this rule against the current context.\n461 \n462         This should indicate whether a linting violation has occurred and/or\n463         whether there is something to remember from this evaluation.\n464 \n465         Note that an evaluate function should always accept `**kwargs`, but\n466         if it relies on any available kwargs, it should explicitly call\n467         them out at definition.\n468 \n469         Returns:\n470             :obj:`LintResult`, list of :obj:`LintResult` or :obj:`None`.\n471 \n472         The reason that this method is called :meth:`_eval` and not `eval` is\n473         a bit of a hack with sphinx autodoc, to make it so that the rule\n474         documentation auto-generates nicely.\n475 \n476         \"\"\"\n477         raise NotImplementedError(\n478             (\n479                 \"{} has not had its `eval` function defined. This is a problem \"\n480                 \"with the rule setup.\"\n481             ).format(self.__class__.__name__)\n482         )  # pragma: no cover\n483 \n484     def crawl(\n485         self,\n486         segment,\n487         ignore_mask,\n488         dialect,\n489         parent_stack=None,\n490         siblings_pre=None,\n491         siblings_post=None,\n492         raw_stack=None,\n493         memory=None,\n494         fname=None,\n495         templated_file: Optional[\"TemplatedFile\"] = None,\n496     ):\n497         \"\"\"Recursively perform the crawl operation on a given segment.\n498 \n499         Returns:\n500             A tuple of (vs, raw_stack, fixes, memory)\n501 \n502         \"\"\"\n503         # parent stack should be a tuple if it exists\n504 \n505         # Rules should evaluate on segments FIRST, before evaluating on their\n506         # children. They should also return a list of violations.\n507 \n508         parent_stack = parent_stack or ()\n509         raw_stack = raw_stack or ()\n510         siblings_post = siblings_post or ()\n511         siblings_pre = siblings_pre or ()\n512         memory = memory or {}\n513         vs: List[SQLLintError] = []\n514         fixes: List[LintFix] = []\n515 \n516         # First, check whether we're looking at an unparsable and whether\n517         # this rule will still operate on that.\n518         if not self._works_on_unparsable and segment.is_type(\"unparsable\"):\n519             # Abort here if it doesn't. Otherwise we'll get odd results.\n520             return vs, raw_stack, [], memory\n521 \n522         # TODO: Document what options are available to the evaluation function.\n523         context = RuleContext(\n524             segment=segment,\n525             parent_stack=parent_stack,\n526             siblings_pre=siblings_pre,\n527             siblings_post=siblings_post,\n528             raw_stack=raw_stack,\n529             memory=memory,\n530             dialect=dialect,\n531             path=pathlib.Path(fname) if fname else None,\n532             templated_file=templated_file,\n533         )\n534         try:\n535             res = self._eval(context=context)\n536         except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n537             raise\n538         # Any exception at this point would halt the linter and\n539         # cause the user to get no results\n540         except Exception as e:\n541             self.logger.critical(\n542                 f\"Applying rule {self.code} threw an Exception: {e}\", exc_info=True\n543             )\n544             exception_line, _ = segment.pos_marker.source_position()\n545             self._log_critical_errors(e)\n546             vs.append(\n547                 SQLLintError(\n548                     rule=self,\n549                     segment=segment,\n550                     fixes=[],\n551                     description=(\n552                         f\"Unexpected exception: {str(e)};\\n\"\n553                         \"Could you open an issue at \"\n554                         \"https://github.com/sqlfluff/sqlfluff/issues ?\\n\"\n555                         \"You can ignore this exception for now, by adding \"\n556                         f\"'-- noqa: {self.code}' at the end\\n\"\n557                         f\"of line {exception_line}\\n\"\n558                     ),\n559                 )\n560             )\n561             return vs, raw_stack, fixes, memory\n562 \n563         new_lerrs: List[SQLLintError] = []\n564         new_fixes: List[LintFix] = []\n565 \n566         if res is None:\n567             # Assume this means no problems (also means no memory)\n568             pass\n569         elif isinstance(res, LintResult):\n570             # Extract any memory\n571             memory = res.memory\n572             self._adjust_anchors_for_fixes(context, res)\n573             self._process_lint_result(\n574                 res, templated_file, ignore_mask, new_lerrs, new_fixes\n575             )\n576         elif isinstance(res, list) and all(\n577             isinstance(elem, LintResult) for elem in res\n578         ):\n579             # Extract any memory from the *last* one, assuming\n580             # it was the last to be added\n581             memory = res[-1].memory\n582             for elem in res:\n583                 self._adjust_anchors_for_fixes(context, elem)\n584                 self._process_lint_result(\n585                     elem, templated_file, ignore_mask, new_lerrs, new_fixes\n586                 )\n587         else:  # pragma: no cover\n588             raise TypeError(\n589                 \"Got unexpected result [{!r}] back from linting rule: {!r}\".format(\n590                     res, self.code\n591                 )\n592             )\n593 \n594         for lerr in new_lerrs:\n595             self.logger.debug(\"!! Violation Found: %r\", lerr.description)\n596         for fix in new_fixes:\n597             self.logger.debug(\"!! Fix Proposed: %r\", fix)\n598 \n599         # Consume the new results\n600         vs += new_lerrs\n601         fixes += new_fixes\n602 \n603         # The raw stack only keeps track of the previous raw segments\n604         if len(segment.segments) == 0:\n605             raw_stack += (segment,)\n606         # Parent stack keeps track of all the parent segments\n607         parent_stack += (segment,)\n608 \n609         for idx, child in enumerate(segment.segments):\n610             dvs, raw_stack, child_fixes, memory = self.crawl(\n611                 segment=child,\n612                 ignore_mask=ignore_mask,\n613                 parent_stack=parent_stack,\n614                 siblings_pre=segment.segments[:idx],\n615                 siblings_post=segment.segments[idx + 1 :],\n616                 raw_stack=raw_stack,\n617                 memory=memory,\n618                 dialect=dialect,\n619                 fname=fname,\n620                 templated_file=templated_file,\n621             )\n622             vs += dvs\n623             fixes += child_fixes\n624         return vs, raw_stack, fixes, memory\n625 \n626     # HELPER METHODS --------\n627     @staticmethod\n628     def _log_critical_errors(error: Exception):  # pragma: no cover\n629         \"\"\"This method is monkey patched into a \"raise\" for certain tests.\"\"\"\n630         pass\n631 \n632     def _process_lint_result(\n633         self, res, templated_file, ignore_mask, new_lerrs, new_fixes\n634     ):\n635         self.discard_unsafe_fixes(res, templated_file)\n636         lerr = res.to_linting_error(rule=self)\n637         ignored = False\n638         if lerr:\n639             if ignore_mask:\n640                 filtered = LintedFile.ignore_masked_violations([lerr], ignore_mask)\n641                 if not filtered:\n642                     lerr = None\n643                     ignored = True\n644         if lerr:\n645             new_lerrs.append(lerr)\n646         if not ignored:\n647             new_fixes.extend(res.fixes)\n648 \n649     @cached_property\n650     def indent(self) -> str:\n651         \"\"\"String for a single indent, based on configuration.\"\"\"\n652         self.tab_space_size: int\n653         self.indent_unit: str\n654 \n655         tab = \"\\t\"\n656         space = \" \"\n657         return space * self.tab_space_size if self.indent_unit == \"space\" else tab\n658 \n659     def is_final_segment(self, context: RuleContext) -> bool:\n660         \"\"\"Is the current segment the final segment in the parse tree.\"\"\"\n661         if len(self.filter_meta(context.siblings_post)) > 0:\n662             # This can only fail on the last segment\n663             return False\n664         elif len(context.segment.segments) > 0:\n665             # This can only fail on the last base segment\n666             return False\n667         elif context.segment.is_meta:\n668             # We can't fail on a meta segment\n669             return False\n670         else:\n671             # We know we are at a leaf of the tree but not necessarily at the end of the\n672             # tree. Therefore we look backwards up the parent stack and ask if any of\n673             # the parent segments have another non-meta child segment after the current\n674             # one.\n675             child_segment = context.segment\n676             for parent_segment in context.parent_stack[::-1]:\n677                 possible_children = [\n678                     s for s in parent_segment.segments if not s.is_meta\n679                 ]\n680                 if len(possible_children) > possible_children.index(child_segment) + 1:\n681                     return False\n682                 child_segment = parent_segment\n683 \n684         return True\n685 \n686     @staticmethod\n687     def filter_meta(segments, keep_meta=False):\n688         \"\"\"Filter the segments to non-meta.\n689 \n690         Or optionally the opposite if keep_meta is True.\n691         \"\"\"\n692         buff = []\n693         for elem in segments:\n694             if elem.is_meta is keep_meta:\n695                 buff.append(elem)\n696         return tuple(buff)\n697 \n698     @classmethod\n699     def get_parent_of(cls, segment, root_segment):  # pragma: no cover TODO?\n700         \"\"\"Return the segment immediately containing segment.\n701 \n702         NB: This is recursive.\n703 \n704         Args:\n705             segment: The segment to look for.\n706             root_segment: Some known parent of the segment\n707                 we're looking for (although likely not the\n708                 direct parent in question).\n709 \n710         \"\"\"\n711         if segment in root_segment.segments:\n712             return root_segment\n713         elif root_segment.segments:\n714             # try each of the subsegments\n715             for sub in root_segment.segments:\n716                 p = cls.get_parent_of(segment, sub)\n717                 if p:\n718                     return p\n719         # Not directly in the segment and\n720         # no subsegments to check. Return None.\n721         return None\n722 \n723     @staticmethod\n724     def matches_target_tuples(seg: BaseSegment, target_tuples: List[Tuple[str, str]]):\n725         \"\"\"Does the given segment match any of the given type tuples.\"\"\"\n726         if seg.name in [elem[1] for elem in target_tuples if elem[0] == \"name\"]:\n727             return True\n728         elif seg.is_type(*[elem[1] for elem in target_tuples if elem[0] == \"type\"]):\n729             return True\n730         return False\n731 \n732     @staticmethod\n733     def discard_unsafe_fixes(\n734         lint_result: LintResult, templated_file: Optional[TemplatedFile]\n735     ):\n736         \"\"\"Remove (discard) LintResult fixes if they are \"unsafe\".\n737 \n738         By removing its fixes, a LintResult will still be reported, but it\n739         will be treated as _unfixable_.\n740         \"\"\"\n741         if not lint_result.fixes or not templated_file:\n742             return\n743 \n744         # Get the set of slices touched by any of the fixes.\n745         fix_slices: Set[RawFileSlice] = set()\n746         for fix in lint_result.fixes:\n747             if fix.anchor:\n748                 assert fix.anchor.pos_marker\n749                 fix_slices.update(\n750                     templated_file.raw_slices_spanning_source_slice(\n751                         fix.anchor.pos_marker.source_slice\n752                     )\n753                 )\n754 \n755         for fix in lint_result.fixes:\n756             if fix.has_template_conflicts(templated_file):\n757                 linter_logger.info(\n758                     \"      * Discarding fixes that touch templated code: %s\",\n759                     lint_result.fixes,\n760                 )\n761                 lint_result.fixes = []\n762                 return\n763 \n764     @classmethod\n765     def _adjust_anchors_for_fixes(cls, context, lint_result):\n766         \"\"\"Makes simple fixes to the anchor position for fixes.\n767 \n768         Some rules return fixes where the anchor is too low in the tree. These\n769         are most often rules like L003 and L016 that make whitespace changes\n770         without a \"deep\" understanding of the parse structure. This function\n771         attempts to correct those issues automatically. It may not be perfect,\n772         but it should be an improvement over the old behavior, where rules like\n773         L003 often corrupted the parse tree, placing spaces in weird places that\n774         caused issues with other rules. For more context, see issue #1304.\n775         \"\"\"\n776         if not cls._adjust_anchors:\n777             return\n778 \n779         fix: LintFix\n780         for fix in lint_result.fixes:\n781             if fix.anchor:\n782                 fix.anchor = cls._choose_anchor_segment(\n783                     context, fix.edit_type, fix.anchor\n784                 )\n785 \n786     @staticmethod\n787     def _choose_anchor_segment(\n788         context: RuleContext,\n789         edit_type: str,\n790         segment: BaseSegment,\n791         filter_meta: bool = False,\n792     ):\n793         \"\"\"Choose the anchor point for a lint fix, i.e. where to apply the fix.\n794 \n795         From a grammar perspective, segments near the leaf of the tree are\n796         generally less likely to allow general edits such as whitespace\n797         insertion.\n798 \n799         This function avoids such issues by taking a proposed anchor point\n800         (assumed to be near the leaf of the tree) and walking \"up\" the parse\n801         tree as long as the ancestor segments have the same start or end point\n802         (depending on the edit type) as \"segment\". This newly chosen anchor\n803         is more likely to be a valid anchor point for the fix.\n804         \"\"\"\n805         if edit_type not in (\"create_before\", \"create_after\"):\n806             return segment\n807 \n808         anchor: BaseSegment = segment\n809         child: BaseSegment = segment\n810         parent: BaseSegment = context.parent_stack[0]\n811         path: Optional[List[BaseSegment]] = parent.path_to(segment) if parent else None\n812         inner_path: Optional[List[BaseSegment]] = path[1:-1] if path else None\n813         if inner_path:\n814             for seg in inner_path[::-1]:\n815                 # Which lists of children to check against.\n816                 children_lists: List[List[BaseSegment]] = []\n817                 if filter_meta:\n818                     # Optionally check against filtered (non-meta only) children.\n819                     children_lists.append(\n820                         [child for child in seg.segments if not child.is_meta]\n821                     )\n822                 # Always check against the full set of children.\n823                 children_lists.append(seg.segments)\n824                 children: List[BaseSegment]\n825                 for children in children_lists:\n826                     if edit_type == \"create_before\" and children[0] is child:\n827                         anchor = seg\n828                         assert anchor.raw.startswith(segment.raw)\n829                         child = seg\n830                         break\n831                     elif edit_type == \"create_after\" and children[-1] is child:\n832                         anchor = seg\n833                         assert anchor.raw.endswith(segment.raw)\n834                         child = seg\n835                         break\n836         return anchor\n837 \n838     @staticmethod\n839     def split_comma_separated_string(raw_str: str) -> List[str]:\n840         \"\"\"Converts comma separated string to List, stripping whitespace.\"\"\"\n841         return [s.strip() for s in raw_str.split(\",\") if s.strip()]\n842 \n843 \n844 class RuleSet:\n845     \"\"\"Class to define a ruleset.\n846 \n847     A rule set is instantiated on module load, but the references\n848     to each of its classes are instantiated at runtime. This means\n849     that configuration values can be passed to those rules live\n850     and be responsive to any changes in configuration from the\n851     path that the file is in.\n852 \n853     Rules should be fetched using the :meth:`get_rulelist` command which\n854     also handles any filtering (i.e. allowlisting and denylisting).\n855 \n856     New rules should be added to the instance of this class using the\n857     :meth:`register` decorator. That decorator registers the class, but also\n858     performs basic type and name-convention checks.\n859 \n860     The code for the rule will be parsed from the name, the description\n861     from the docstring. The eval function is assumed that it will be\n862     overriden by the subclass, and the parent class raises an error on\n863     this function if not overriden.\n864 \n865     \"\"\"\n866 \n867     def __init__(self, name, config_info):\n868         self.name = name\n869         self.config_info = config_info\n870         self._register = {}\n871 \n872     def _validate_config_options(self, config, rule=None):\n873         \"\"\"Ensure that all config options are valid.\n874 \n875         Config options can also be checked for a specific rule e.g L010.\n876         \"\"\"\n877         rule_config = config.get_section(\"rules\")\n878         for config_name, info_dict in self.config_info.items():\n879             config_option = (\n880                 rule_config.get(config_name)\n881                 if not rule\n882                 else rule_config.get(rule).get(config_name)\n883             )\n884             valid_options = info_dict.get(\"validation\")\n885             if (\n886                 valid_options\n887                 and config_option not in valid_options\n888                 and config_option is not None\n889             ):\n890                 raise ValueError(\n891                     (\n892                         \"Invalid option '{}' for {} configuration. Must be one of {}\"\n893                     ).format(\n894                         config_option,\n895                         config_name,\n896                         valid_options,\n897                     )\n898                 )\n899 \n900     @property\n901     def valid_rule_name_regex(self):\n902         \"\"\"Defines the accepted pattern for rule names.\n903 \n904         The first group captures the plugin name (optional), which\n905         must be capitalized.\n906         The second group captures the rule code.\n907 \n908         Examples of valid rule names:\n909 \n910         * Rule_PluginName_L001\n911         * Rule_L001\n912         \"\"\"\n913         return regex.compile(r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z][0-9]{3})\")\n914 \n915     def register(self, cls, plugin=None):\n916         \"\"\"Decorate a class with this to add it to the ruleset.\n917 \n918         .. code-block:: python\n919 \n920            @myruleset.register\n921            class Rule_L001(BaseRule):\n922                \"Description of rule.\"\n923 \n924                def eval(self, **kwargs):\n925                    return LintResult()\n926 \n927         We expect that rules are defined as classes with the name `Rule_XXXX`\n928         where `XXXX` is of the form `LNNN`, where L is a letter (literally L for\n929         *linting* by default) and N is a three digit number.\n930 \n931         If this receives classes by any other name, then it will raise a\n932         :exc:`ValueError`.\n933 \n934         \"\"\"\n935         rule_name_match = self.valid_rule_name_regex.match(cls.__name__)\n936         # Validate the name\n937         if not rule_name_match:  # pragma: no cover\n938             raise ValueError(\n939                 (\n940                     \"Tried to register rule on set {!r} with unexpected \"\n941                     \"format: {}, format should be: Rule_PluginName_L123 (for plugins) \"\n942                     \"or Rule_L123 (for core rules).\"\n943                 ).format(self.name, cls.__name__)\n944             )\n945 \n946         plugin_name, code = rule_name_match.groups()\n947         # If the docstring is multiline, then we extract just summary.\n948         description = cls.__doc__.replace(\"``\", \"'\").split(\"\\n\")[0]\n949 \n950         if plugin_name:\n951             code = f\"{plugin_name}_{code}\"\n952 \n953         # Keep track of the *class* in the register. Don't instantiate yet.\n954         if code in self._register:  # pragma: no cover\n955             raise ValueError(\n956                 \"Rule {!r} has already been registered on RuleSet {!r}!\".format(\n957                     code, self.name\n958                 )\n959             )\n960         self._register[code] = dict(code=code, description=description, cls=cls)\n961 \n962         # Make sure we actually return the original class\n963         return cls\n964 \n965     def _expand_config_rule_glob_list(self, glob_list: List[str]) -> List[str]:\n966         \"\"\"Expand a list of rule globs into a list of rule codes.\n967 \n968         Returns:\n969             :obj:`list` of :obj:`str` rule codes.\n970 \n971         \"\"\"\n972         expanded_glob_list = []\n973         for r in glob_list:\n974             expanded_glob_list.extend(\n975                 [\n976                     x\n977                     for x in fnmatch.filter(self._register, r)\n978                     if x not in expanded_glob_list\n979                 ]\n980             )\n981 \n982         return expanded_glob_list\n983 \n984     def get_rulelist(self, config) -> List[BaseRule]:\n985         \"\"\"Use the config to return the appropriate rules.\n986 \n987         We use the config both for allowlisting and denylisting, but also\n988         for configuring the rules given the given config.\n989 \n990         Returns:\n991             :obj:`list` of instantiated :obj:`BaseRule`.\n992 \n993         \"\"\"\n994         # Validate all generic rule configs\n995         self._validate_config_options(config)\n996         # default the allowlist to all the rules if not set\n997         allowlist = config.get(\"rule_allowlist\") or list(self._register.keys())\n998         denylist = config.get(\"rule_denylist\") or []\n999 \n1000         allowlisted_unknown_rule_codes = [\n1001             r for r in allowlist if not fnmatch.filter(self._register, r)\n1002         ]\n1003         if any(allowlisted_unknown_rule_codes):\n1004             rules_logger.warning(\n1005                 \"Tried to allowlist unknown rules: {!r}\".format(\n1006                     allowlisted_unknown_rule_codes\n1007                 )\n1008             )\n1009 \n1010         denylisted_unknown_rule_codes = [\n1011             r for r in denylist if not fnmatch.filter(self._register, r)\n1012         ]\n1013         if any(denylisted_unknown_rule_codes):  # pragma: no cover\n1014             rules_logger.warning(\n1015                 \"Tried to denylist unknown rules: {!r}\".format(\n1016                     denylisted_unknown_rule_codes\n1017                 )\n1018             )\n1019 \n1020         keylist = sorted(self._register.keys())\n1021 \n1022         # First we expand the allowlist and denylist globs\n1023         expanded_allowlist = self._expand_config_rule_glob_list(allowlist)\n1024         expanded_denylist = self._expand_config_rule_glob_list(denylist)\n1025 \n1026         # Then we filter the rules\n1027         keylist = [\n1028             r for r in keylist if r in expanded_allowlist and r not in expanded_denylist\n1029         ]\n1030 \n1031         # Construct the kwargs for instantiation before we actually do it.\n1032         rule_kwargs = {}\n1033         for k in keylist:\n1034             kwargs = {}\n1035             generic_rule_config = config.get_section(\"rules\")\n1036             specific_rule_config = config.get_section(\n1037                 (\"rules\", self._register[k][\"code\"])\n1038             )\n1039             if generic_rule_config:\n1040                 kwargs.update(generic_rule_config)\n1041             if specific_rule_config:\n1042                 # Validate specific rule config before adding\n1043                 self._validate_config_options(config, self._register[k][\"code\"])\n1044                 kwargs.update(specific_rule_config)\n1045             kwargs[\"code\"] = self._register[k][\"code\"]\n1046             # Allow variable substitution in making the description\n1047             kwargs[\"description\"] = self._register[k][\"description\"].format(**kwargs)\n1048             rule_kwargs[k] = kwargs\n1049 \n1050         # Instantiate in the final step\n1051         return [self._register[k][\"cls\"](**rule_kwargs[k]) for k in keylist]\n1052 \n1053     def copy(self):\n1054         \"\"\"Return a copy of self with a separate register.\"\"\"\n1055         new_ruleset = copy.copy(self)\n1056         new_ruleset._register = self._register.copy()\n1057         return new_ruleset\n1058 \n[end of src/sqlfluff/core/rules/base.py]\n[start of src/sqlfluff/core/rules/config_info.py]\n1 \"\"\"Documenting and validating rule configuration.\n2 \n3 Provide a mapping with all configuration options, with information\n4 on valid inputs and definitions.\n5 \n6 This mapping is used to validate rule config inputs, as well\n7 as document rule configuration.\n8 \"\"\"\n9 \n10 from sqlfluff.core.plugin.host import get_plugin_manager\n11 \n12 STANDARD_CONFIG_INFO_DICT = {\n13     \"tab_space_size\": {\n14         \"validation\": range(100),\n15         \"definition\": (\n16             \"The number of spaces to consider equal to one tab. \"\n17             \"Used in the fixing step of this rule.\"\n18         ),\n19     },\n20     \"max_line_length\": {\n21         \"validation\": range(1000),\n22         \"definition\": (\n23             \"The maximum length of a line to allow without raising a violation.\"\n24         ),\n25     },\n26     \"indent_unit\": {\n27         \"validation\": [\"space\", \"tab\"],\n28         \"definition\": \"Whether to use tabs or spaces to add new indents.\",\n29     },\n30     \"comma_style\": {\n31         \"validation\": [\"leading\", \"trailing\"],\n32         \"definition\": \"The comma style to enforce.\",\n33     },\n34     \"allow_scalar\": {\n35         \"validation\": [True, False],\n36         \"definition\": (\n37             \"Whether or not to allow a single element in the \"\n38             \" select clause to be without an alias.\"\n39         ),\n40     },\n41     \"single_table_references\": {\n42         \"validation\": [\"consistent\", \"qualified\", \"unqualified\"],\n43         \"definition\": \"The expectation for references in single-table select.\",\n44     },\n45     \"force_enable\": {\n46         \"validation\": [True, False],\n47         \"definition\": (\n48             \"Run this rule even for dialects where this rule is disabled by default.\"\n49         ),\n50     },\n51     \"unquoted_identifiers_policy\": {\n52         \"validation\": [\"all\", \"aliases\", \"column_aliases\"],\n53         \"definition\": \"Types of unquoted identifiers to flag violations for.\",\n54     },\n55     \"quoted_identifiers_policy\": {\n56         \"validation\": [\"all\", \"aliases\", \"column_aliases\", \"none\"],\n57         \"definition\": \"Types of quoted identifiers to flag violations for.\",\n58     },\n59     \"capitalisation_policy\": {\n60         \"validation\": [\"consistent\", \"upper\", \"lower\", \"capitalise\"],\n61         \"definition\": \"The capitalisation policy to enforce.\",\n62     },\n63     \"extended_capitalisation_policy\": {\n64         \"validation\": [\"consistent\", \"upper\", \"lower\", \"pascal\", \"capitalise\"],\n65         \"definition\": (\n66             \"The capitalisation policy to enforce, extended with PascalCase. \"\n67             \"This is separate from ``capitalisation_policy`` as it should not be \"\n68             \"applied to keywords.\"\n69         ),\n70     },\n71     \"select_clause_trailing_comma\": {\n72         \"validation\": [\"forbid\", \"require\"],\n73         \"definition\": (\n74             \"Should trailing commas within select clauses be required or forbidden?\"\n75         ),\n76     },\n77     \"ignore_comment_lines\": {\n78         \"validation\": [True, False],\n79         \"definition\": (\n80             \"Should lines that contain only whitespace and comments\"\n81             \" be ignored when linting line lengths?\"\n82         ),\n83     },\n84     \"ignore_comment_clauses\": {\n85         \"validation\": [True, False],\n86         \"definition\": (\n87             \"Should comment clauses (e.g. column comments) be ignored\"\n88             \" when linting line lengths?\"\n89         ),\n90     },\n91     \"ignore_words\": {\n92         \"definition\": (\"Comma separated list of words to ignore from rule\"),\n93     },\n94     \"forbid_subquery_in\": {\n95         \"validation\": [\"join\", \"from\", \"both\"],\n96         \"definition\": \"Which clauses should be linted for subqueries?\",\n97     },\n98     \"prefer_count_1\": {\n99         \"validation\": [True, False],\n100         \"definition\": (\"Should count(1) be preferred over count(*) and count(0)?\"),\n101     },\n102     \"prefer_count_0\": {\n103         \"validation\": [True, False],\n104         \"definition\": (\"Should count(0) be preferred over count(*) and count(1)?\"),\n105     },\n106     \"operator_new_lines\": {\n107         \"validation\": [\"before\", \"after\"],\n108         \"definition\": (\"Should operator be placed before or after newlines?\"),\n109     },\n110     \"aliasing\": {\n111         \"validation\": [\"implicit\", \"explicit\"],\n112         \"definition\": (\n113             \"Should alias have an explict AS or is implicit aliasing required?\"\n114         ),\n115     },\n116     \"fully_qualify_join_types\": {\n117         \"validation\": [\"inner\", \"outer\", \"both\"],\n118         \"definition\": (\"Which types of JOIN clauses should be fully qualified?\"),\n119     },\n120     \"multiline_newline\": {\n121         \"validation\": [True, False],\n122         \"definition\": (\n123             \"Should semi-colons be placed on a new line after multi-line statements?\"\n124         ),\n125     },\n126     \"require_final_semicolon\": {\n127         \"validation\": [True, False],\n128         \"definition\": (\n129             \"Should final semi-colons be required? \"\n130             \"(N.B. forcing trailing semi-colons is not recommended for dbt users \"\n131             \"as it can cause issues when wrapping the query within other SQL queries).\"\n132         ),\n133     },\n134     \"group_by_and_order_by_style\": {\n135         \"validation\": [\"consistent\", \"implicit\", \"explicit\"],\n136         \"definition\": (\n137             \"The expectation for using explicit column name references \"\n138             \"or implicit positional references.\"\n139         ),\n140     },\n141     \"allow_space_in_identifier\": {\n142         \"validation\": [True, False],\n143         \"definition\": (\"Should spaces in identifiers be allowed?\"),\n144     },\n145     \"additional_allowed_characters\": {\n146         \"definition\": (\n147             \"Optional list of extra allowed characters, \"\n148             \"in addition to alphanumerics (A-Z, a-z, 0-9) and underscores.\"\n149         ),\n150     },\n151     \"prefer_quoted_identifiers\": {\n152         \"validation\": [True, False],\n153         \"definition\": (\n154             \"If ``True``, requires every identifier to be quoted. \"\n155             \"Defaults to ``False``.\"\n156         ),\n157     },\n158     \"blocked_words\": {\n159         \"definition\": (\n160             \"Optional, comma-separated list of blocked words which should not be used \"\n161             \"in statements.\"\n162         ),\n163     },\n164 }\n165 \n166 \n167 def get_config_info() -> dict:\n168     \"\"\"Gets the config from core sqlfluff and sqlfluff plugins and merges them.\"\"\"\n169     plugin_manager = get_plugin_manager()\n170     configs_info = plugin_manager.hook.get_configs_info()\n171     return {\n172         k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n173     }\n174 \n[end of src/sqlfluff/core/rules/config_info.py]\n[start of src/sqlfluff/rules/L019.py]\n1 \"\"\"Implementation of Rule L019.\"\"\"\n2 \n3 from typing import Any, Dict, Optional, Tuple\n4 \n5 from sqlfluff.core.parser import RawSegment, WhitespaceSegment\n6 from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult, RuleContext\n7 from sqlfluff.core.rules.doc_decorators import (\n8     document_fix_compatible,\n9     document_configuration,\n10 )\n11 \n12 \n13 @document_fix_compatible\n14 @document_configuration\n15 class Rule_L019(BaseRule):\n16     \"\"\"Leading/Trailing comma enforcement.\n17 \n18     **Anti-pattern**\n19 \n20     There is a mixture of leading and trailing commas.\n21 \n22     .. code-block:: sql\n23 \n24         SELECT\n25             a\n26             , b,\n27             c\n28         FROM foo\n29 \n30     **Best practice**\n31 \n32     By default, `SQLFluff` prefers trailing commas. However it\n33     is configurable for leading commas. The chosen style must be used\n34     consistently throughout your SQL.\n35 \n36     .. code-block:: sql\n37 \n38         SELECT\n39             a,\n40             b,\n41             c\n42         FROM foo\n43 \n44         -- Alternatively, set the configuration file to 'leading'\n45         -- and then the following would be acceptable:\n46 \n47         SELECT\n48             a\n49             , b\n50             , c\n51         FROM foo\n52     \"\"\"\n53 \n54     _works_on_unparsable = False\n55     config_keywords = [\"comma_style\"]\n56 \n57     @staticmethod\n58     def _last_comment_seg(raw_stack):\n59         \"\"\"Trace the raw stack back to the most recent comment segment.\n60 \n61         A return value of `None` indicates no code segments preceding the current\n62         position.\n63         \"\"\"\n64         for segment in raw_stack[::-1]:\n65             if segment.is_comment:\n66                 return segment\n67         return None\n68 \n69     @staticmethod\n70     def _last_code_seg(raw_stack: Tuple[RawSegment, ...]) -> Optional[RawSegment]:\n71         \"\"\"Trace the raw stack back to the most recent code segment.\n72 \n73         A return value of `None` indicates no code segments preceding the current\n74         position.\n75         \"\"\"\n76         for segment in raw_stack[::-1]:\n77             if segment.is_code or segment.is_type(\"newline\"):\n78                 return segment\n79         return None\n80 \n81     @staticmethod\n82     def _get_following_seg(\n83         raw_stack: Tuple[RawSegment, ...], segment: RawSegment\n84     ) -> RawSegment:\n85         \"\"\"Given a segment in raw_stack, return the segment following.\"\"\"\n86         idx = raw_stack.index(segment)\n87         if idx < len(raw_stack):\n88             return raw_stack[idx + 1]\n89         raise ValueError(\"No following segment available\")  # pragma: no cover\n90 \n91     def _eval(self, context: RuleContext) -> Optional[LintResult]:\n92         \"\"\"Enforce comma placement.\n93 \n94         For leading commas we're looking for trailing commas, so\n95         we look for newline segments. For trailing commas we're\n96         looking for leading commas, so we look for the comma itself.\n97 \n98         We also want to handle proper whitespace removal/addition. We remove\n99         any trailing whitespace after the leading comma, when converting a\n100         leading comma to a trailing comma. We add whitespace after the leading\n101         comma when converting a trailing comma to a leading comma.\n102         \"\"\"\n103         # Config type hints\n104         self.comma_style: str\n105 \n106         if not context.memory:\n107             memory: Dict[str, Any] = {\n108                 # Trailing comma keys\n109                 #\n110                 # Do we have a fix in place for removing a leading\n111                 # comma violation, and inserting a new trailing comma?\n112                 \"insert_trailing_comma\": False,\n113                 # A list of whitespace segments that come after a\n114                 # leading comma violation, to be removed during fixing.\n115                 \"whitespace_deletions\": None,\n116                 # The leading comma violation segment to be removed during fixing\n117                 \"last_leading_comma_seg\": None,\n118                 # The newline segment where we're going to insert our new trailing\n119                 # comma during fixing\n120                 \"anchor_for_new_trailing_comma_seg\": None,\n121                 #\n122                 # Leading comma keys\n123                 #\n124                 # Do we have a fix in place for removing a trailing\n125                 # comma violation, and inserting a new leading comma?\n126                 \"insert_leading_comma\": False,\n127                 # The trailing comma violation segment to be removed during fixing\n128                 \"last_trailing_comma_segment\": None,\n129             }\n130         else:\n131             memory = context.memory\n132 \n133         if self.comma_style == \"trailing\":\n134             # A comma preceded by a new line == a leading comma\n135             if context.segment.is_type(\"comma\"):\n136                 last_seg = self._last_code_seg(context.raw_stack)\n137                 if (\n138                     last_seg\n139                     and last_seg.is_type(\"newline\")\n140                     and not last_seg.is_templated\n141                 ):\n142                     # Recorded where the fix should be applied\n143                     memory[\"last_leading_comma_seg\"] = context.segment\n144                     last_comment_seg = self._last_comment_seg(context.raw_stack)\n145                     inline_comment = (\n146                         last_comment_seg.pos_marker.line_no\n147                         == last_seg.pos_marker.line_no\n148                         if last_comment_seg\n149                         else False\n150                     )\n151                     # If we have a comment right before the newline, then anchor\n152                     # the fix at the comment instead\n153                     memory[\"anchor_for_new_trailing_comma_seg\"] = (\n154                         last_seg if not inline_comment else last_comment_seg\n155                     )\n156                     # Trigger fix routine\n157                     memory[\"insert_trailing_comma\"] = True\n158                     memory[\"whitespace_deletions\"] = []\n159                     return LintResult(memory=memory)\n160             # Have we found a leading comma violation?\n161             if memory[\"insert_trailing_comma\"]:\n162                 # Search for trailing whitespace to delete after the leading\n163                 # comma violation\n164                 if context.segment.is_type(\"whitespace\"):\n165                     memory[\"whitespace_deletions\"] += [context.segment]\n166                     return LintResult(memory=memory)\n167                 else:\n168                     # We've run out of whitespace to delete, time to fix\n169                     last_leading_comma_seg = memory[\"last_leading_comma_seg\"]\n170                     # Scan backwards to find the last code segment, skipping\n171                     # over lines that are either entirely blank or just a\n172                     # comment. We want to place the comma immediately after it.\n173                     last_code_seg = None\n174                     while last_code_seg is None or last_code_seg.is_type(\"newline\"):\n175                         last_code_seg = self._last_code_seg(\n176                             context.raw_stack[\n177                                 : context.raw_stack.index(\n178                                     last_code_seg\n179                                     if last_code_seg\n180                                     else memory[\"last_leading_comma_seg\"]\n181                                 )\n182                             ]\n183                         )\n184                     return LintResult(\n185                         anchor=last_leading_comma_seg,\n186                         description=\"Found leading comma. Expected only trailing.\",\n187                         fixes=[\n188                             LintFix.delete(last_leading_comma_seg),\n189                             *[\n190                                 LintFix.delete(d)\n191                                 for d in memory[\"whitespace_deletions\"]\n192                             ],\n193                             LintFix.create_before(\n194                                 anchor_segment=self._get_following_seg(\n195                                     context.raw_stack, last_code_seg\n196                                 ),\n197                                 edit_segments=[last_leading_comma_seg],\n198                             ),\n199                         ],\n200                     )\n201 \n202         elif self.comma_style == \"leading\":\n203             # A new line preceded by a comma == a trailing comma\n204             if context.segment.is_type(\"newline\"):\n205                 last_seg = self._last_code_seg(context.raw_stack)\n206                 # no code precedes the current position: no issue\n207                 if last_seg is None:\n208                     return None\n209                 if last_seg.is_type(\"comma\") and not context.segment.is_templated:\n210                     # Trigger fix routine\n211                     memory[\"insert_leading_comma\"] = True\n212                     # Record where the fix should be applied\n213                     memory[\"last_trailing_comma_segment\"] = last_seg\n214                     return LintResult(memory=memory)\n215             # Have we found a trailing comma violation?\n216             if memory[\"insert_leading_comma\"]:\n217                 # Only insert the comma here if this isn't a comment/whitespace segment\n218                 if context.segment.is_code:\n219                     last_comma_seg = memory[\"last_trailing_comma_segment\"]\n220                     # Create whitespace to insert after the new leading comma\n221                     new_whitespace_seg = WhitespaceSegment()\n222                     return LintResult(\n223                         anchor=last_comma_seg,\n224                         description=\"Found trailing comma. Expected only leading.\",\n225                         fixes=[\n226                             LintFix.delete(last_comma_seg),\n227                             LintFix.create_before(\n228                                 anchor_segment=context.segment,\n229                                 edit_segments=[last_comma_seg, new_whitespace_seg],\n230                             ),\n231                         ],\n232                     )\n233         # Otherwise, no issue\n234         return None\n235 \n[end of src/sqlfluff/rules/L019.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "<issue>\nfix keep adding new line on wrong place \n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nTo replicate this issue you can create a file eg. test.template.sql \n\n```\n{% if true %}\nSELECT 1 + 1\n{%- endif %}\n```\n\nthen run:\n```\nsqlfluff fix test.template.sql  \n```\n\nThis will give you:\n```\nL:   2 | P:  12 | L009 | Files must end with a trailing newline.\n```\n\nAnd the result of the file is now:\n```\n{% if true %}\nSELECT 1 + 1\n\n{%- endif %}\n```\n\nIf i run it again it will complain on the same issue and the result of the file would be: \n```\n{% if true %}\nSELECT 1 + 1\n\n\n{%- endif %}\n```\n\nAnd so on. \n\n### Expected Behaviour\n\nThe expected behavior would be to add the new line at the end of the file, that is after `{%- endif %}` instead of adding the new line at the end of the SQL query - so the result should look like this: \n\n```\n{% if true %}\nSELECT 1 + 1\n{%- endif %}\n\n```\n\n### Observed Behaviour\n\nAdds a new line to the end of the SQL query instead of in the end of the file. \n\n### How to reproduce\n\nAlready mentioned above (in What Happened section).\n\n### Dialect\n\nsnowflake\n\n### Version\n\nsqlfluff, version 0.6.2\n\n### Configuration\n\n[sqlfluff]\nverbose = 1\ndialect = snowflake\ntemplater = jinja\nexclude_rules = L027,L031,L032,L036,L044,L046,L034\noutput_line_length = 121\nsql_file_exts=.sql\n\n[sqlfluff:rules]\ntab_space_size = 4\nmax_line_length = 250\nindent_unit = space\ncomma_style = trailing\nallow_scalar = True\nsingle_table_references = consistent\nunquoted_identifiers_policy = aliases\n\n\n[sqlfluff:rules:L010]  # Keywords\ncapitalisation_policy = upper\n\n[sqlfluff:rules:L014]\nextended_capitalisation_policy = lower\n\n[sqlfluff:rules:L030]  # function names\ncapitalisation_policy = upper\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n{% if true %}\nSELECT 1 + 1\n{%- endif %}\n</code>\n<patch>\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,5 +1,5 @@\n {% if true %}\n SELECT 1 + 1\n-{%- endif %}\n+{%- endif %}\n\n </patch>", "model_patch": "\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,5 +1,5 @@\n {% if true %}\n SELECT 1 + 1\n-{%- endif %}\n+{%- endif %}\n\n "}
{"instance_id": "sqlfluff__sqlfluff-4151", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\r\n```\r\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\r\n        Try 'sqlfluff fix -h' for help.\r\n\r\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\r\n```\n\n### How to reproduce\n\nSql file:\r\n```\r\nSELECT foo FROM bar;\r\n```\r\n\r\nCommand:\r\n```\r\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\r\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\r\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![GitHub Workflow Status](https://img.shields.io/github/workflow/status/sqlfluff/sqlfluff/CI%20Tests?logo=github&style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 [![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)\n16 \n17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n18 \n19 ## Dialects Supported\n20 \n21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n22 \n23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n24 - [Athena](https://aws.amazon.com/athena/)\n25 - [BigQuery](https://cloud.google.com/bigquery/)\n26 - [ClickHouse](https://clickhouse.com/)\n27 - [Databricks](https://databricks.com/) (note: currently this is just an alias for the `sparksql` dialect).\n28 - [Db2](https://www.ibm.com/analytics/db2)\n29 - [Exasol](https://www.exasol.com/)\n30 - [Hive](https://hive.apache.org/)\n31 - [Materialize](https://materialize.com/)\n32 - [MySQL](https://www.mysql.com/)\n33 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n34 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n35 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n36 - [Snowflake](https://www.snowflake.com/)\n37 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n38 - [SparkSQL](https://spark.apache.org/docs/latest/)\n39 - [SQLite](https://www.sqlite.org/)\n40 - [Teradata](https://www.teradata.com/)\n41 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n42 \n43 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n44 \n45 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n46 \n47 ## Templates Supported\n48 \n49 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n50 \n51 **SQLFluff** supports the following templates:\n52 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n53 - [dbt](https://www.getdbt.com/)\n54 \n55 Again, please raise issues if you wish to support more templating languages/syntaxes.\n56 \n57 # Getting Started\n58 \n59 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n60 \n61 ```shell\n62 $ pip install sqlfluff\n63 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n64 $ sqlfluff lint test.sql --dialect ansi\n65 == [test.sql] FAIL\n66 L:   1 | P:   1 | L050 | Files must not begin with newlines or whitespace.\n67 L:   1 | P:   3 | L003 | First line has unexpected indent\n68 L:   1 | P:  11 | L039 | Unnecessary whitespace found.\n69 L:   1 | P:  14 | L039 | Unnecessary whitespace found.\n70 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace.\n71 ```\n72 \n73 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n74 \n75 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n76 \n77 # Documentation\n78 \n79 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n80 \n81 # Releases\n82 \n83 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html), so breaking changes\n84 should be restricted to major versions releases. Some elements (such as the python API) are in a less\n85 stable state and may see more significant changes more often. See the [changelog](CHANGELOG.md) for more details.\n86 If you would like to join in please consider [contributing](CONTRIBUTING.md).\n87 \n88 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n89 \n90 # SQLFluff on Slack\n91 \n92 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n93 \n94 # SQLFluff on Twitter\n95 \n96 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n97 \n98 # Contributing\n99 \n100 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n101 \n102 If you want to understand more about the architecture of **SQLFluff**, you can\n103 find [more here](https://docs.sqlfluff.com/en/latest/internals.html#architecture).\n104 \n105 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n106 \n[end of README.md]\n[start of src/sqlfluff/cli/commands.py]\n1 \"\"\"Contains the CLI.\"\"\"\n2 \n3 from itertools import chain\n4 import os\n5 import sys\n6 import json\n7 import logging\n8 import time\n9 from logging import LogRecord\n10 from typing import Callable, Tuple, Optional, cast\n11 \n12 import yaml\n13 \n14 import click\n15 \n16 # For the profiler\n17 import pstats\n18 from io import StringIO\n19 \n20 # To enable colour cross platform\n21 import colorama\n22 from tqdm import tqdm\n23 from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n24 \n25 from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n26 from sqlfluff.cli.click_deprecated_option import (\n27     DeprecatedOption,\n28     DeprecatedOptionsCommand,\n29 )\n30 from sqlfluff.cli.formatters import (\n31     format_linting_result_header,\n32     OutputStreamFormatter,\n33 )\n34 from sqlfluff.cli.helpers import get_package_version\n35 from sqlfluff.cli.outputstream import make_output_stream, OutputStream\n36 \n37 # Import from sqlfluff core.\n38 from sqlfluff.core import (\n39     Linter,\n40     FluffConfig,\n41     SQLLintError,\n42     SQLTemplaterError,\n43     SQLFluffUserError,\n44     dialect_selector,\n45     dialect_readout,\n46 )\n47 from sqlfluff.core.config import progress_bar_configuration\n48 \n49 from sqlfluff.core.enums import FormatType, Color\n50 from sqlfluff.core.plugin.host import get_plugin_manager\n51 \n52 \n53 class StreamHandlerTqdm(logging.StreamHandler):\n54     \"\"\"Modified StreamHandler which takes care of writing within `tqdm` context.\n55 \n56     It uses `tqdm` write which takes care of conflicting prints with progressbar.\n57     Without it, there were left artifacts in DEBUG mode (not sure about another ones,\n58     but probably would happen somewhere).\n59     \"\"\"\n60 \n61     def emit(self, record: LogRecord) -> None:\n62         \"\"\"Behaves like original one except uses `tqdm` to write.\"\"\"\n63         try:\n64             msg = self.format(record)\n65             tqdm.write(msg, file=self.stream)\n66             self.flush()\n67         except Exception:  # pragma: no cover\n68             self.handleError(record)\n69 \n70 \n71 def set_logging_level(\n72     verbosity: int,\n73     formatter: OutputStreamFormatter,\n74     logger: Optional[logging.Logger] = None,\n75     stderr_output: bool = False,\n76 ) -> None:\n77     \"\"\"Set up logging for the CLI.\n78 \n79     We either set up global logging based on the verbosity\n80     or, if `logger` is specified, we only limit to a single\n81     sqlfluff logger. Verbosity is applied in the same way.\n82 \n83     Implementation: If `logger` is not specified, the handler\n84     is attached to the `sqlfluff` logger. If it is specified\n85     then it attaches the the logger in question. In addition\n86     if `logger` is specified, then that logger will also\n87     not propagate.\n88     \"\"\"\n89     fluff_logger = logging.getLogger(\"sqlfluff\")\n90     # Don't propagate logging\n91     fluff_logger.propagate = False\n92 \n93     # Enable colorama\n94     colorama.init()\n95 \n96     # Set up the log handler which is able to print messages without overlapping\n97     # with progressbars.\n98     handler = StreamHandlerTqdm(stream=sys.stderr if stderr_output else sys.stdout)\n99     # NB: the unicode character at the beginning is to squash any badly\n100     # tamed ANSI colour statements, and return us to normality.\n101     handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n102 \n103     # Set up a handler to colour warnings red.\n104     # See: https://docs.python.org/3/library/logging.html#filter-objects\n105     def red_log_filter(record: logging.LogRecord) -> bool:\n106         if record.levelno >= logging.WARNING:\n107             record.msg = f\"{formatter.colorize(record.msg, Color.red)} \"\n108         return True\n109 \n110     handler.addFilter(red_log_filter)\n111 \n112     if logger:\n113         focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n114         focus_logger.addHandler(handler)\n115     else:\n116         fluff_logger.addHandler(handler)\n117 \n118     # NB: We treat the parser logger slightly differently because it's noisier.\n119     # It's important that we set levels for all each time so\n120     # that we don't break tests by changing the granularity\n121     # between tests.\n122     parser_logger = logging.getLogger(\"sqlfluff.parser\")\n123     if verbosity < 3:\n124         fluff_logger.setLevel(logging.WARNING)\n125         parser_logger.setLevel(logging.NOTSET)\n126     elif verbosity == 3:\n127         fluff_logger.setLevel(logging.INFO)\n128         parser_logger.setLevel(logging.WARNING)\n129     elif verbosity == 4:\n130         fluff_logger.setLevel(logging.DEBUG)\n131         parser_logger.setLevel(logging.INFO)\n132     elif verbosity > 4:\n133         fluff_logger.setLevel(logging.DEBUG)\n134         parser_logger.setLevel(logging.DEBUG)\n135 \n136 \n137 class PathAndUserErrorHandler:\n138     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n139 \n140     def __init__(self, formatter):\n141         self.formatter = formatter\n142 \n143     def __enter__(self):\n144         return self\n145 \n146     def __exit__(self, exc_type, exc_val, exc_tb):\n147         if exc_type is SQLFluffUserError:\n148             click.echo(\n149                 \"\\nUser Error: \"\n150                 + self.formatter.colorize(\n151                     str(exc_val),\n152                     Color.red,\n153                 )\n154             )\n155             sys.exit(EXIT_ERROR)\n156 \n157 \n158 def common_options(f: Callable) -> Callable:\n159     \"\"\"Add common options to commands via a decorator.\n160 \n161     These are applied to all of the cli commands.\n162     \"\"\"\n163     f = click.version_option()(f)\n164     f = click.option(\n165         \"-v\",\n166         \"--verbose\",\n167         count=True,\n168         default=None,\n169         help=(\n170             \"Verbosity, how detailed should the output be. This is *stackable*, so \"\n171             \"`-vv` is more verbose than `-v`. For the most verbose option try `-vvvv` \"\n172             \"or `-vvvvv`.\"\n173         ),\n174     )(f)\n175     f = click.option(\n176         \"-n\",\n177         \"--nocolor\",\n178         is_flag=True,\n179         default=None,\n180         help=\"No color - output will be without ANSI color codes.\",\n181     )(f)\n182 \n183     return f\n184 \n185 \n186 def core_options(f: Callable) -> Callable:\n187     \"\"\"Add core operation options to commands via a decorator.\n188 \n189     These are applied to the main (but not all) cli commands like\n190     `parse`, `lint` and `fix`.\n191     \"\"\"\n192     # Only enable dialect completion if on version of click\n193     # that supports it\n194     if shell_completion_enabled:\n195         f = click.option(\n196             \"-d\",\n197             \"--dialect\",\n198             default=None,\n199             help=\"The dialect of SQL to lint\",\n200             shell_complete=dialect_shell_complete,\n201         )(f)\n202     else:  # pragma: no cover\n203         f = click.option(\n204             \"-d\",\n205             \"--dialect\",\n206             default=None,\n207             help=\"The dialect of SQL to lint\",\n208         )(f)\n209     f = click.option(\n210         \"-t\",\n211         \"--templater\",\n212         default=None,\n213         help=\"The templater to use (default=jinja)\",\n214         type=click.Choice(\n215             [\n216                 templater.name\n217                 for templater in chain.from_iterable(\n218                     get_plugin_manager().hook.get_templaters()\n219                 )\n220             ]\n221         ),\n222     )(f)\n223     f = click.option(\n224         \"-r\",\n225         \"--rules\",\n226         default=None,\n227         help=(\n228             \"Narrow the search to only specific rules. For example \"\n229             \"specifying `--rules L001` will only search for rule `L001` (Unnecessary \"\n230             \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n231             \"`--rules L001,L002` will specify only looking for violations of rule \"\n232             \"`L001` and rule `L002`.\"\n233         ),\n234     )(f)\n235     f = click.option(\n236         \"-e\",\n237         \"--exclude-rules\",\n238         default=None,\n239         help=(\n240             \"Exclude specific rules. For example \"\n241             \"specifying `--exclude-rules L001` will remove rule `L001` (Unnecessary \"\n242             \"trailing whitespace) from the set of considered rules. This could either \"\n243             \"be the allowlist, or the general set if there is no specific allowlist. \"\n244             \"Multiple rules can be specified with commas e.g. \"\n245             \"`--exclude-rules L001,L002` will exclude violations of rule \"\n246             \"`L001` and rule `L002`.\"\n247         ),\n248     )(f)\n249     f = click.option(\n250         \"--config\",\n251         \"extra_config_path\",\n252         default=None,\n253         help=(\n254             \"Include additional config file. By default the config is generated \"\n255             \"from the standard configuration files described in the documentation. \"\n256             \"This argument allows you to specify an additional configuration file that \"\n257             \"overrides the standard configuration files. N.B. cfg format is required.\"\n258         ),\n259         type=click.Path(),\n260     )(f)\n261     f = click.option(\n262         \"--ignore-local-config\",\n263         is_flag=True,\n264         help=(\n265             \"Ignore config files in default search path locations. \"\n266             \"This option allows the user to lint with the default config \"\n267             \"or can be used in conjunction with --config to only \"\n268             \"reference the custom config file.\"\n269         ),\n270     )(f)\n271     f = click.option(\n272         \"--encoding\",\n273         default=None,\n274         help=(\n275             \"Specify encoding to use when reading and writing files. Defaults to \"\n276             \"autodetect.\"\n277         ),\n278     )(f)\n279     f = click.option(\n280         \"-i\",\n281         \"--ignore\",\n282         default=None,\n283         help=(\n284             \"Ignore particular families of errors so that they don't cause a failed \"\n285             \"run. For example `--ignore parsing` would mean that any parsing errors \"\n286             \"are ignored and don't influence the success or fail of a run. \"\n287             \"`--ignore` behaves somewhat like `noqa` comments, except it \"\n288             \"applies globally. Multiple options are possible if comma separated: \"\n289             \"e.g. `--ignore parsing,templating`.\"\n290         ),\n291     )(f)\n292     f = click.option(\n293         \"--bench\",\n294         is_flag=True,\n295         help=\"Set this flag to engage the benchmarking tool output.\",\n296     )(f)\n297     f = click.option(\n298         \"--logger\",\n299         type=click.Choice(\n300             [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\", \"config\"],\n301             case_sensitive=False,\n302         ),\n303         help=\"Choose to limit the logging to one of the loggers.\",\n304     )(f)\n305     f = click.option(\n306         \"--disable-noqa\",\n307         is_flag=True,\n308         default=None,\n309         help=\"Set this flag to ignore inline noqa comments.\",\n310     )(f)\n311     return f\n312 \n313 \n314 def get_config(\n315     extra_config_path: Optional[str] = None,\n316     ignore_local_config: bool = False,\n317     **kwargs,\n318 ) -> FluffConfig:\n319     \"\"\"Get a config object from kwargs.\"\"\"\n320     plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n321     if kwargs.get(\"dialect\"):\n322         try:\n323             # We're just making sure it exists at this stage.\n324             # It will be fetched properly in the linter.\n325             dialect_selector(kwargs[\"dialect\"])\n326         except SQLFluffUserError as err:\n327             click.echo(\n328                 OutputStreamFormatter.colorize_helper(\n329                     plain_output,\n330                     f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n331                     color=Color.red,\n332                 )\n333             )\n334             sys.exit(EXIT_ERROR)\n335         except KeyError:\n336             click.echo(\n337                 OutputStreamFormatter.colorize_helper(\n338                     plain_output,\n339                     f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n340                     color=Color.red,\n341                 )\n342             )\n343             sys.exit(EXIT_ERROR)\n344     from_root_kwargs = {}\n345     if \"require_dialect\" in kwargs:\n346         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n347     # Instantiate a config object (filtering out the nulls)\n348     overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n349     try:\n350         return FluffConfig.from_root(\n351             extra_config_path=extra_config_path,\n352             ignore_local_config=ignore_local_config,\n353             overrides=overrides,\n354             **from_root_kwargs,\n355         )\n356     except SQLFluffUserError as err:  # pragma: no cover\n357         click.echo(\n358             OutputStreamFormatter.colorize_helper(\n359                 plain_output,\n360                 f\"Error loading config: {str(err)}\",\n361                 color=Color.red,\n362             )\n363         )\n364         sys.exit(EXIT_ERROR)\n365 \n366 \n367 def get_linter_and_formatter(\n368     cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n369 ) -> Tuple[Linter, OutputStreamFormatter]:\n370     \"\"\"Get a linter object given a config.\"\"\"\n371     try:\n372         # We're just making sure it exists at this stage.\n373         # It will be fetched properly in the linter.\n374         dialect = cfg.get(\"dialect\")\n375         if dialect:\n376             dialect_selector(dialect)\n377     except KeyError:  # pragma: no cover\n378         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n379         sys.exit(EXIT_ERROR)\n380     formatter = OutputStreamFormatter(\n381         output_stream=output_stream or make_output_stream(cfg),\n382         nocolor=cfg.get(\"nocolor\"),\n383         verbosity=cfg.get(\"verbose\"),\n384         output_line_length=cfg.get(\"output_line_length\"),\n385     )\n386     return Linter(config=cfg, formatter=formatter), formatter\n387 \n388 \n389 @click.group(\n390     context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n391     epilog=\"\"\"\\b\\bExamples:\\n\n392   sqlfluff lint --dialect postgres .\\n\n393   sqlfluff lint --dialect postgres --rules L042 .\\n\n394   sqlfluff fix --dialect sqlite --rules L041,L042 src/queries\\n\n395   sqlfluff parse --dialect sqlite --templater jinja src/queries/common.sql\n396 \"\"\",\n397 )\n398 @click.version_option()\n399 def cli():\n400     \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n401 \n402 \n403 @cli.command()\n404 @common_options\n405 def version(**kwargs) -> None:\n406     \"\"\"Show the version of sqlfluff.\"\"\"\n407     c = get_config(**kwargs, require_dialect=False)\n408     if c.get(\"verbose\") > 0:\n409         # Instantiate the linter\n410         lnt, formatter = get_linter_and_formatter(c)\n411         # Dispatch the detailed config from the linter.\n412         formatter.dispatch_config(lnt)\n413     else:\n414         # Otherwise just output the package version.\n415         click.echo(get_package_version(), color=c.get(\"color\"))\n416 \n417 \n418 @cli.command()\n419 @common_options\n420 def rules(**kwargs) -> None:\n421     \"\"\"Show the current rules in use.\"\"\"\n422     c = get_config(**kwargs, dialect=\"ansi\")\n423     lnt, formatter = get_linter_and_formatter(c)\n424     click.echo(formatter.format_rules(lnt), color=c.get(\"color\"))\n425 \n426 \n427 @cli.command()\n428 @common_options\n429 def dialects(**kwargs) -> None:\n430     \"\"\"Show the current dialects available.\"\"\"\n431     c = get_config(**kwargs, require_dialect=False)\n432     _, formatter = get_linter_and_formatter(c)\n433     click.echo(formatter.format_dialects(dialect_readout), color=c.get(\"color\"))\n434 \n435 \n436 def dump_file_payload(filename: Optional[str], payload: str):\n437     \"\"\"Write the output file content to stdout or file.\"\"\"\n438     # If there's a file specified to write to, write to it.\n439     if filename:\n440         with open(filename, \"w\") as out_file:\n441             out_file.write(payload)\n442     # Otherwise write to stdout\n443     else:\n444         click.echo(payload)\n445 \n446 \n447 @cli.command(cls=DeprecatedOptionsCommand)\n448 @common_options\n449 @core_options\n450 @click.option(\n451     \"-f\",\n452     \"--format\",\n453     \"format\",\n454     default=\"human\",\n455     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n456     help=\"What format to return the lint result in (default=human).\",\n457 )\n458 @click.option(\n459     \"--write-output\",\n460     help=(\n461         \"Optionally provide a filename to write the results to, mostly used in \"\n462         \"tandem with --format. NB: Setting an output file re-enables normal \"\n463         \"stdout logging.\"\n464     ),\n465 )\n466 @click.option(\n467     \"--annotation-level\",\n468     default=\"notice\",\n469     type=click.Choice([\"notice\", \"warning\", \"failure\", \"error\"], case_sensitive=False),\n470     help=(\n471         \"When format is set to github-annotation or github-annotation-native, \"\n472         \"default annotation level (default=notice). failure and error are equivalent.\"\n473     ),\n474 )\n475 @click.option(\n476     \"--nofail\",\n477     is_flag=True,\n478     help=(\n479         \"If set, the exit code will always be zero, regardless of violations \"\n480         \"found. This is potentially useful during rollout.\"\n481     ),\n482 )\n483 @click.option(\n484     \"--disregard-sqlfluffignores\",\n485     is_flag=True,\n486     help=\"Perform the operation regardless of .sqlfluffignore configurations\",\n487 )\n488 @click.option(\n489     \"-p\",\n490     \"--processes\",\n491     type=int,\n492     default=None,\n493     help=(\n494         \"The number of parallel processes to run. Positive numbers work as \"\n495         \"expected. Zero and negative numbers will work as number_of_cpus - \"\n496         \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n497     ),\n498 )\n499 @click.option(\n500     \"--disable_progress_bar\",\n501     \"--disable-progress-bar\",\n502     is_flag=True,\n503     help=\"Disables progress bars.\",\n504     cls=DeprecatedOption,\n505     deprecated=[\"--disable_progress_bar\"],\n506 )\n507 @click.option(\n508     \"--persist-timing\",\n509     default=None,\n510     help=(\n511         \"A filename to persist the timing information for a linting run to \"\n512         \"in csv format for external analysis. NOTE: This feature should be \"\n513         \"treated as beta, and the format of the csv file may change in \"\n514         \"future releases without warning.\"\n515     ),\n516 )\n517 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n518 def lint(\n519     paths: Tuple[str],\n520     format: str,\n521     write_output: Optional[str],\n522     annotation_level: str,\n523     nofail: bool,\n524     disregard_sqlfluffignores: bool,\n525     logger: Optional[logging.Logger] = None,\n526     bench: bool = False,\n527     processes: Optional[int] = None,\n528     disable_progress_bar: Optional[bool] = False,\n529     extra_config_path: Optional[str] = None,\n530     ignore_local_config: bool = False,\n531     persist_timing: Optional[str] = None,\n532     **kwargs,\n533 ) -> None:\n534     \"\"\"Lint SQL files via passing a list of files or using stdin.\n535 \n536     PATH is the path to a sql file or directory to lint. This can be either a\n537     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n538     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n539     be interpreted like passing the current working directory as a path argument.\n540 \n541     Linting SQL files:\n542 \n543         sqlfluff lint path/to/file.sql\n544         sqlfluff lint directory/of/sql/files\n545 \n546     Linting a file via stdin (note the lone '-' character):\n547 \n548         cat path/to/file.sql | sqlfluff lint -\n549         echo 'select col from tbl' | sqlfluff lint -\n550 \n551     \"\"\"\n552     config = get_config(\n553         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n554     )\n555     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n556     file_output = None\n557     output_stream = make_output_stream(config, format, write_output)\n558     lnt, formatter = get_linter_and_formatter(config, output_stream)\n559 \n560     verbose = config.get(\"verbose\")\n561     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n562 \n563     formatter.dispatch_config(lnt)\n564 \n565     # Set up logging.\n566     set_logging_level(\n567         verbosity=verbose,\n568         formatter=formatter,\n569         logger=logger,\n570         stderr_output=non_human_output,\n571     )\n572 \n573     # Output the results as we go\n574     if verbose >= 1:\n575         click.echo(format_linting_result_header())\n576 \n577     with PathAndUserErrorHandler(formatter):\n578         # add stdin if specified via lone '-'\n579         if (\"-\",) == paths:\n580             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n581         else:\n582             result = lnt.lint_paths(\n583                 paths,\n584                 ignore_non_existent_files=False,\n585                 ignore_files=not disregard_sqlfluffignores,\n586                 processes=processes,\n587             )\n588 \n589     # Output the final stats\n590     if verbose >= 1:\n591         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n592 \n593     if format == FormatType.json.value:\n594         file_output = json.dumps(result.as_records())\n595     elif format == FormatType.yaml.value:\n596         file_output = yaml.dump(result.as_records(), sort_keys=False)\n597     elif format == FormatType.github_annotation.value:\n598         if annotation_level == \"error\":\n599             annotation_level = \"failure\"\n600 \n601         github_result = []\n602         for record in result.as_records():\n603             filepath = record[\"filepath\"]\n604             for violation in record[\"violations\"]:\n605                 # NOTE: The output format is designed for this GitHub action:\n606                 # https://github.com/yuzutech/annotations-action\n607                 # It is similar, but not identical, to the native GitHub format:\n608                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n609                 github_result.append(\n610                     {\n611                         \"file\": filepath,\n612                         \"line\": violation[\"line_no\"],\n613                         \"start_column\": violation[\"line_pos\"],\n614                         \"end_column\": violation[\"line_pos\"],\n615                         \"title\": \"SQLFluff\",\n616                         \"message\": f\"{violation['code']}: {violation['description']}\",\n617                         \"annotation_level\": annotation_level,\n618                     }\n619                 )\n620         file_output = json.dumps(github_result)\n621     elif format == FormatType.github_annotation_native.value:\n622         if annotation_level == \"failure\":\n623             annotation_level = \"error\"\n624 \n625         github_result_native = []\n626         for record in result.as_records():\n627             filepath = record[\"filepath\"]\n628             for violation in record[\"violations\"]:\n629                 # NOTE: The output format is designed for GitHub action:\n630                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n631                 line = f\"::{annotation_level} \"\n632                 line += \"title=SQLFluff,\"\n633                 line += f\"file={filepath},\"\n634                 line += f\"line={violation['line_no']},\"\n635                 line += f\"col={violation['line_pos']}\"\n636                 line += \"::\"\n637                 line += f\"{violation['code']}: {violation['description']}\"\n638 \n639                 github_result_native.append(line)\n640 \n641         file_output = \"\\n\".join(github_result_native)\n642 \n643     if file_output:\n644         dump_file_payload(write_output, cast(str, file_output))\n645 \n646     if persist_timing:\n647         result.persist_timing_records(persist_timing)\n648 \n649     output_stream.close()\n650     if bench:\n651         click.echo(\"==== overall timings ====\")\n652         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n653         timing_summary = result.timing_summary()\n654         for step in timing_summary:\n655             click.echo(f\"=== {step} ===\")\n656             click.echo(formatter.cli_table(timing_summary[step].items()))\n657 \n658     if not nofail:\n659         if not non_human_output:\n660             formatter.completion_message()\n661         sys.exit(result.stats()[\"exit code\"])\n662     else:\n663         sys.exit(EXIT_SUCCESS)\n664 \n665 \n666 def do_fixes(lnt, result, formatter=None, **kwargs):\n667     \"\"\"Actually do the fixes.\"\"\"\n668     click.echo(\"Persisting Changes...\")\n669     res = result.persist_changes(formatter=formatter, **kwargs)\n670     if all(res.values()):\n671         click.echo(\"Done. Please check your files to confirm.\")\n672         return True\n673     # If some failed then return false\n674     click.echo(\n675         \"Done. Some operations failed. Please check your files to confirm.\"\n676     )  # pragma: no cover\n677     click.echo(\n678         \"Some errors cannot be fixed or there is another error blocking it.\"\n679     )  # pragma: no cover\n680     return False  # pragma: no cover\n681 \n682 \n683 @cli.command()\n684 @common_options\n685 @core_options\n686 @click.option(\n687     \"-f\",\n688     \"--force\",\n689     is_flag=True,\n690     help=(\n691         \"skip the confirmation prompt and go straight to applying \"\n692         \"fixes. **Use this with caution.**\"\n693     ),\n694 )\n695 @click.option(\n696     \"-x\",\n697     \"--fixed-suffix\",\n698     default=None,\n699     help=\"An optional suffix to add to fixed files.\",\n700 )\n701 @click.option(\n702     \"-p\",\n703     \"--processes\",\n704     type=int,\n705     default=None,\n706     help=(\n707         \"The number of parallel processes to run. Positive numbers work as \"\n708         \"expected. Zero and negative numbers will work as number_of_cpus - \"\n709         \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n710     ),\n711 )\n712 @click.option(\n713     \"--disable-progress-bar\",\n714     is_flag=True,\n715     help=\"Disables progress bars.\",\n716 )\n717 @click.option(\n718     \"--FIX-EVEN-UNPARSABLE\",\n719     is_flag=True,\n720     default=None,\n721     help=(\n722         \"Enables fixing of files that have templating or parse errors. \"\n723         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n724         \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n725         \"command will not make any fixes in files that have templating or parse \"\n726         \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n727         \"or in the .sqlfluff config file.\"\n728     ),\n729 )\n730 @click.option(\n731     \"--show-lint-violations\",\n732     is_flag=True,\n733     help=\"Show lint violations\",\n734 )\n735 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n736 def fix(\n737     force: bool,\n738     paths: Tuple[str],\n739     bench: bool = False,\n740     fixed_suffix: str = \"\",\n741     logger: Optional[logging.Logger] = None,\n742     processes: Optional[int] = None,\n743     disable_progress_bar: Optional[bool] = False,\n744     extra_config_path: Optional[str] = None,\n745     ignore_local_config: bool = False,\n746     show_lint_violations: bool = False,\n747     **kwargs,\n748 ) -> None:\n749     \"\"\"Fix SQL files.\n750 \n751     PATH is the path to a sql file or directory to lint. This can be either a\n752     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n753     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n754     be interpreted like passing the current working directory as a path argument.\n755     \"\"\"\n756     # some quick checks\n757     fixing_stdin = (\"-\",) == paths\n758 \n759     config = get_config(\n760         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n761     )\n762     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n763     output_stream = make_output_stream(\n764         config, None, os.devnull if fixing_stdin else None\n765     )\n766     lnt, formatter = get_linter_and_formatter(config, output_stream)\n767 \n768     verbose = config.get(\"verbose\")\n769     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n770 \n771     exit_code = EXIT_SUCCESS\n772 \n773     formatter.dispatch_config(lnt)\n774 \n775     # Set up logging.\n776     set_logging_level(\n777         verbosity=verbose,\n778         formatter=formatter,\n779         logger=logger,\n780         stderr_output=fixing_stdin,\n781     )\n782 \n783     # handle stdin case. should output formatted sql to stdout and nothing else.\n784     if fixing_stdin:\n785         stdin = sys.stdin.read()\n786 \n787         result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n788         templater_error = result.num_violations(types=SQLTemplaterError) > 0\n789         unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n790         if not fix_even_unparsable:\n791             exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n792 \n793         if result.num_violations(types=SQLLintError, fixable=True) > 0:\n794             stdout = result.paths[0].files[0].fix_string()[0]\n795         else:\n796             stdout = stdin\n797 \n798         if templater_error:\n799             click.echo(\n800                 formatter.colorize(\n801                     \"Fix aborted due to unparsable template variables.\",\n802                     Color.red,\n803                 ),\n804                 err=True,\n805             )\n806             click.echo(\n807                 formatter.colorize(\n808                     \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n809                     Color.red,\n810                 ),\n811                 err=True,\n812             )\n813 \n814         if unfixable_error:\n815             click.echo(\n816                 formatter.colorize(\"Unfixable violations detected.\", Color.red),\n817                 err=True,\n818             )\n819 \n820         click.echo(stdout, nl=False)\n821         sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n822 \n823     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n824     click.echo(\"==== finding fixable violations ====\")\n825 \n826     with PathAndUserErrorHandler(formatter):\n827         result = lnt.lint_paths(\n828             paths,\n829             fix=True,\n830             ignore_non_existent_files=False,\n831             processes=processes,\n832         )\n833 \n834     if not fix_even_unparsable:\n835         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n836 \n837     # NB: We filter to linting violations here, because they're\n838     # the only ones which can be potentially fixed.\n839     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n840         click.echo(\"==== fixing violations ====\")\n841         click.echo(\n842             f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\n843             \"linting violations found\"\n844         )\n845         if force:\n846             click.echo(\n847                 f\"{formatter.colorize('FORCE MODE', Color.red)}: Attempting fixes...\"\n848             )\n849             success = do_fixes(\n850                 lnt,\n851                 result,\n852                 formatter,\n853                 types=SQLLintError,\n854                 fixed_file_suffix=fixed_suffix,\n855             )\n856             if not success:\n857                 sys.exit(EXIT_FAIL)  # pragma: no cover\n858         else:\n859             click.echo(\n860                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n861             )\n862             c = click.getchar().lower()\n863             click.echo(\"...\")\n864             if c in (\"y\", \"\\r\", \"\\n\"):\n865                 click.echo(\"Attempting fixes...\")\n866                 success = do_fixes(\n867                     lnt,\n868                     result,\n869                     formatter,\n870                     types=SQLLintError,\n871                     fixed_file_suffix=fixed_suffix,\n872                 )\n873                 if not success:\n874                     sys.exit(EXIT_FAIL)  # pragma: no cover\n875                 else:\n876                     formatter.completion_message()\n877             elif c == \"n\":\n878                 click.echo(\"Aborting...\")\n879                 exit_code = EXIT_FAIL\n880             else:  # pragma: no cover\n881                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n882                 click.echo(\"Aborting...\")\n883                 exit_code = EXIT_FAIL\n884     else:\n885         click.echo(\"==== no fixable linting violations found ====\")\n886         formatter.completion_message()\n887 \n888     error_types = [\n889         (\n890             dict(types=SQLLintError, fixable=False),\n891             \"  [{} unfixable linting violations found]\",\n892             EXIT_FAIL,\n893         ),\n894     ]\n895     for num_violations_kwargs, message_format, error_level in error_types:\n896         num_violations = result.num_violations(**num_violations_kwargs)\n897         if num_violations > 0:\n898             click.echo(message_format.format(num_violations))\n899             exit_code = max(exit_code, error_level)\n900 \n901     if bench:\n902         click.echo(\"==== overall timings ====\")\n903         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n904         timing_summary = result.timing_summary()\n905         for step in timing_summary:\n906             click.echo(f\"=== {step} ===\")\n907             click.echo(formatter.cli_table(timing_summary[step].items()))\n908 \n909     if show_lint_violations:\n910         click.echo(\"==== lint for unfixable violations ====\")\n911         all_results = result.violation_dict(**num_violations_kwargs)\n912         sorted_files = sorted(all_results.keys())\n913         for file in sorted_files:\n914             violations = all_results.get(file, [])\n915             click.echo(formatter.format_filename(file, success=(not violations)))\n916             for violation in violations:\n917                 click.echo(formatter.format_violation(violation))\n918 \n919     sys.exit(exit_code)\n920 \n921 \n922 def quoted_presenter(dumper, data):\n923     \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n924     if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n925         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n926     else:\n927         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n928 \n929 \n930 @cli.command()\n931 @common_options\n932 @core_options\n933 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n934 @click.option(\n935     \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n936 )\n937 @click.option(\n938     \"-c\",\n939     \"--code-only\",\n940     is_flag=True,\n941     help=\"Output only the code elements of the parse tree.\",\n942 )\n943 @click.option(\n944     \"-m\",\n945     \"--include-meta\",\n946     is_flag=True,\n947     help=(\n948         \"Include meta segments (indents, dedents and placeholders) in the output. \"\n949         \"This only applies when outputting json or yaml.\"\n950     ),\n951 )\n952 @click.option(\n953     \"-f\",\n954     \"--format\",\n955     default=FormatType.human.value,\n956     type=click.Choice(\n957         [\n958             FormatType.human.value,\n959             FormatType.json.value,\n960             FormatType.yaml.value,\n961         ],\n962         case_sensitive=False,\n963     ),\n964     help=\"What format to return the parse result in.\",\n965 )\n966 @click.option(\n967     \"--write-output\",\n968     help=(\n969         \"Optionally provide a filename to write the results to, mostly used in \"\n970         \"tandem with --format. NB: Setting an output file re-enables normal \"\n971         \"stdout logging.\"\n972     ),\n973 )\n974 @click.option(\n975     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n976 )\n977 @click.option(\n978     \"--nofail\",\n979     is_flag=True,\n980     help=(\n981         \"If set, the exit code will always be zero, regardless of violations \"\n982         \"found. This is potentially useful during rollout.\"\n983     ),\n984 )\n985 def parse(\n986     path: str,\n987     code_only: bool,\n988     include_meta: bool,\n989     format: str,\n990     write_output: Optional[str],\n991     profiler: bool,\n992     bench: bool,\n993     nofail: bool,\n994     logger: Optional[logging.Logger] = None,\n995     extra_config_path: Optional[str] = None,\n996     ignore_local_config: bool = False,\n997     **kwargs,\n998 ) -> None:\n999     \"\"\"Parse SQL files and just spit out the result.\n1000 \n1001     PATH is the path to a sql file or directory to lint. This can be either a\n1002     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n1003     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n1004     be interpreted like passing the current working directory as a path argument.\n1005     \"\"\"\n1006     c = get_config(\n1007         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1008     )\n1009     # We don't want anything else to be logged if we want json or yaml output\n1010     # unless we're writing to a file.\n1011     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n1012     output_stream = make_output_stream(c, format, write_output)\n1013     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1014     verbose = c.get(\"verbose\")\n1015     recurse = c.get(\"recurse\")\n1016 \n1017     progress_bar_configuration.disable_progress_bar = True\n1018 \n1019     formatter.dispatch_config(lnt)\n1020 \n1021     # Set up logging.\n1022     set_logging_level(\n1023         verbosity=verbose,\n1024         formatter=formatter,\n1025         logger=logger,\n1026         stderr_output=non_human_output,\n1027     )\n1028 \n1029     # TODO: do this better\n1030 \n1031     if profiler:\n1032         # Set up the profiler if required\n1033         try:\n1034             import cProfile\n1035         except ImportError:  # pragma: no cover\n1036             click.echo(\"The cProfiler is not available on your platform.\")\n1037             sys.exit(EXIT_ERROR)\n1038         pr = cProfile.Profile()\n1039         pr.enable()\n1040 \n1041     t0 = time.monotonic()\n1042 \n1043     # handle stdin if specified via lone '-'\n1044     with PathAndUserErrorHandler(formatter):\n1045         if \"-\" == path:\n1046             parsed_strings = [\n1047                 lnt.parse_string(\n1048                     sys.stdin.read(),\n1049                     \"stdin\",\n1050                     recurse=recurse,\n1051                     config=lnt.config,\n1052                 ),\n1053             ]\n1054         else:\n1055             # A single path must be specified for this command\n1056             parsed_strings = list(\n1057                 lnt.parse_path(\n1058                     path=path,\n1059                     recurse=recurse,\n1060                 )\n1061             )\n1062 \n1063     total_time = time.monotonic() - t0\n1064     violations_count = 0\n1065 \n1066     # iterative print for human readout\n1067     if format == FormatType.human.value:\n1068         violations_count = formatter.print_out_violations_and_timing(\n1069             output_stream, bench, code_only, total_time, verbose, parsed_strings\n1070         )\n1071     else:\n1072         parsed_strings_dict = [\n1073             dict(\n1074                 filepath=linted_result.fname,\n1075                 segments=linted_result.tree.as_record(\n1076                     code_only=code_only, show_raw=True, include_meta=include_meta\n1077                 )\n1078                 if linted_result.tree\n1079                 else None,\n1080             )\n1081             for linted_result in parsed_strings\n1082         ]\n1083 \n1084         if format == FormatType.yaml.value:\n1085             # For yaml dumping always dump double quoted strings if they contain\n1086             # tabs or newlines.\n1087             yaml.add_representer(str, quoted_presenter)\n1088             file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n1089         elif format == FormatType.json.value:\n1090             file_output = json.dumps(parsed_strings_dict)\n1091 \n1092         # Dump the output to stdout or to file as appropriate.\n1093         dump_file_payload(write_output, file_output)\n1094     if profiler:\n1095         pr.disable()\n1096         profiler_buffer = StringIO()\n1097         ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n1098         ps.print_stats()\n1099         click.echo(\"==== profiler stats ====\")\n1100         # Only print the first 50 lines of it\n1101         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n1102 \n1103     if violations_count > 0 and not nofail:\n1104         sys.exit(EXIT_FAIL)  # pragma: no cover\n1105     else:\n1106         sys.exit(EXIT_SUCCESS)\n1107 \n1108 \n1109 @cli.command()\n1110 @common_options\n1111 @core_options\n1112 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n1113 def render(\n1114     path: str,\n1115     bench: bool,\n1116     logger: Optional[logging.Logger] = None,\n1117     extra_config_path: Optional[str] = None,\n1118     ignore_local_config: bool = False,\n1119     **kwargs,\n1120 ) -> None:\n1121     \"\"\"Render SQL files and just spit out the result.\n1122 \n1123     PATH is the path to a sql file. This should be either a single file\n1124     file ('path/to/file.sql') or a single ('-') character to indicate reading\n1125     from *stdin*.\n1126     \"\"\"\n1127     c = get_config(\n1128         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1129     )\n1130     # We don't want anything else to be logged if we want json or yaml output\n1131     # unless we're writing to a file.\n1132     output_stream = make_output_stream(c, None, None)\n1133     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1134     verbose = c.get(\"verbose\")\n1135 \n1136     progress_bar_configuration.disable_progress_bar = True\n1137 \n1138     formatter.dispatch_config(lnt)\n1139 \n1140     # Set up logging.\n1141     set_logging_level(\n1142         verbosity=verbose,\n1143         formatter=formatter,\n1144         logger=logger,\n1145         stderr_output=False,\n1146     )\n1147 \n1148     # handle stdin if specified via lone '-'\n1149     with PathAndUserErrorHandler(formatter):\n1150         if \"-\" == path:\n1151             raw_sql = sys.stdin.read()\n1152             fname = \"stdin\"\n1153             file_config = lnt.config\n1154         else:\n1155             raw_sql, file_config, _ = lnt.load_raw_file_and_config(path, lnt.config)\n1156             fname = path\n1157 \n1158     # Get file specific config\n1159     file_config.process_raw_file_for_config(raw_sql)\n1160     rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n1161 \n1162     if rendered.templater_violations:\n1163         for v in rendered.templater_violations:\n1164             click.echo(formatter.format_violation(v))\n1165         sys.exit(EXIT_FAIL)\n1166     else:\n1167         click.echo(rendered.templated_file.templated_str)\n1168         sys.exit(EXIT_SUCCESS)\n1169 \n1170 \n1171 # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n1172 # simplifies the use of cProfile, e.g.:\n1173 # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\n1174 if __name__ == \"__main__\":\n1175     cli.main(sys.argv[1:])  # pragma: no cover\n1176 \n[end of src/sqlfluff/cli/commands.py]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import (\n8     Any,\n9     Iterable,\n10     Iterator,\n11     List,\n12     Optional,\n13     Sequence,\n14     Set,\n15     Tuple,\n16     Type,\n17     cast,\n18 )\n19 \n20 import pathspec\n21 import regex\n22 from tqdm import tqdm\n23 \n24 from sqlfluff.core.errors import (\n25     SQLBaseError,\n26     SQLLexError,\n27     SQLLintError,\n28     SQLParseError,\n29     SQLFluffSkipFile,\n30     SQLFluffUserError,\n31 )\n32 from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n33 from sqlfluff.core.file_helpers import get_encoding\n34 from sqlfluff.core.templaters import TemplatedFile\n35 from sqlfluff.core.rules import get_ruleset\n36 from sqlfluff.core.rules.doc_decorators import is_fix_compatible\n37 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n38 \n39 # Classes needed only for type checking\n40 from sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n41 from sqlfluff.core.parser.segments.meta import MetaSegment\n42 from sqlfluff.core.parser.segments.raw import RawSegment\n43 from sqlfluff.core.rules import BaseRule\n44 \n45 from sqlfluff.core.linter.common import (\n46     RuleTuple,\n47     ParsedString,\n48     NoQaDirective,\n49     RenderedFile,\n50 )\n51 from sqlfluff.core.linter.linted_file import LintedFile\n52 from sqlfluff.core.linter.linted_dir import LintedDir\n53 from sqlfluff.core.linter.linting_result import LintingResult\n54 \n55 \n56 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n57 \n58 # Instantiate the linter logger\n59 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n60 \n61 \n62 class Linter:\n63     \"\"\"The interface class to interact with the linter.\"\"\"\n64 \n65     # Default to allowing process parallelism\n66     allow_process_parallelism = True\n67 \n68     def __init__(\n69         self,\n70         config: Optional[FluffConfig] = None,\n71         formatter: Any = None,\n72         dialect: Optional[str] = None,\n73         rules: Optional[List[str]] = None,\n74         user_rules: Optional[List[Type[BaseRule]]] = None,\n75         exclude_rules: Optional[List[str]] = None,\n76     ) -> None:\n77         # Store the config object\n78         self.config = FluffConfig.from_kwargs(\n79             config=config,\n80             dialect=dialect,\n81             rules=rules,\n82             exclude_rules=exclude_rules,\n83             # Don't require a dialect to be provided yet. Defer this until we\n84             # are actually linting something, since the directory we are linting\n85             # from may provide additional configuration, including a dialect.\n86             require_dialect=False,\n87         )\n88         # Get the dialect and templater\n89         self.dialect = self.config.get(\"dialect_obj\")\n90         self.templater = self.config.get(\"templater_obj\")\n91         # Store the formatter for output\n92         self.formatter = formatter\n93         # Store references to user rule classes\n94         self.user_rules = user_rules or []\n95 \n96     def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n97         \"\"\"Get hold of a set of rules.\"\"\"\n98         rs = get_ruleset()\n99         # Register any user rules\n100         for rule in self.user_rules:\n101             rs.register(rule)\n102         cfg = config or self.config\n103         return rs.get_rulelist(config=cfg)\n104 \n105     def rule_tuples(self) -> List[RuleTuple]:\n106         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n107         rs = self.get_ruleset()\n108         return [RuleTuple(rule.code, rule.description) for rule in rs]\n109 \n110     # #### Static methods\n111     # These are the building blocks of the linting process.\n112 \n113     @staticmethod\n114     def load_raw_file_and_config(\n115         fname: str, root_config: FluffConfig\n116     ) -> Tuple[str, FluffConfig, str]:\n117         \"\"\"Load a raw file and the associated config.\"\"\"\n118         file_config = root_config.make_child_from_path(fname)\n119         encoding = get_encoding(fname=fname, config=file_config)\n120         # Check file size before loading.\n121         limit = file_config.get(\"large_file_skip_byte_limit\")\n122         if limit:\n123             # Get the file size\n124             file_size = os.path.getsize(fname)\n125             if file_size > limit:\n126                 raise SQLFluffSkipFile(\n127                     f\"Length of file {fname!r} is {file_size} bytes which is over \"\n128                     f\"the limit of {limit} bytes. Skipping to avoid parser lock. \"\n129                     \"Users can increase this limit in their config by setting the \"\n130                     \"'large_file_skip_byte_limit' value, or disable by setting it \"\n131                     \"to zero.\"\n132                 )\n133         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n134             raw_file = target_file.read()\n135         # Scan the raw file for config commands.\n136         file_config.process_raw_file_for_config(raw_file)\n137         # Return the raw file and config\n138         return raw_file, file_config, encoding\n139 \n140     @staticmethod\n141     def _normalise_newlines(string: str) -> str:\n142         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n143         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n144 \n145     @staticmethod\n146     def _lex_templated_file(\n147         templated_file: TemplatedFile, config: FluffConfig\n148     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n149         \"\"\"Lex a templated file.\n150 \n151         NOTE: This potentially mutates the config, so make sure to\n152         use the returned one.\n153         \"\"\"\n154         violations = []\n155         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n156         # Get the lexer\n157         lexer = Lexer(config=config)\n158         # Lex the file and log any problems\n159         try:\n160             tokens, lex_vs = lexer.lex(templated_file)\n161             # We might just get the violations as a list\n162             violations += lex_vs\n163             linter_logger.info(\n164                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n165             )\n166         except SQLLexError as err:  # pragma: no cover\n167             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n168             violations.append(err)\n169             return None, violations, config\n170 \n171         if not tokens:  # pragma: no cover TODO?\n172             return None, violations, config\n173 \n174         # Check that we've got sensible indentation from the lexer.\n175         # We might need to suppress if it's a complicated file.\n176         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n177         if isinstance(templating_blocks_indent, str):\n178             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n179         else:\n180             force_block_indent = False\n181         templating_blocks_indent = bool(templating_blocks_indent)\n182         # If we're forcing it through we don't check.\n183         if templating_blocks_indent and not force_block_indent:\n184             indent_balance = sum(\n185                 getattr(elem, \"indent_val\", 0)\n186                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n187             )\n188             if indent_balance != 0:\n189                 linter_logger.debug(\n190                     \"Indent balance test failed for %r. Template indents will not be \"\n191                     \"linted for this file.\",\n192                     templated_file.fname,\n193                 )\n194                 # Don't enable the templating blocks.\n195                 templating_blocks_indent = False\n196 \n197         # The file will have been lexed without config, so check all indents\n198         # are enabled.\n199         new_tokens = []\n200         for token in cast(Tuple[BaseSegment, ...], tokens):\n201             if token.is_meta:\n202                 token = cast(MetaSegment, token)\n203                 if token.indent_val != 0:\n204                     # Don't allow it if we're not linting templating block indents.\n205                     if not templating_blocks_indent:\n206                         continue\n207             new_tokens.append(token)\n208 \n209         # Return new buffer\n210         return new_tokens, violations, config\n211 \n212     @staticmethod\n213     def _parse_tokens(\n214         tokens: Sequence[BaseSegment],\n215         config: FluffConfig,\n216         recurse: bool = True,\n217         fname: Optional[str] = None,\n218     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n219         parser = Parser(config=config)\n220         violations = []\n221         # Parse the file and log any problems\n222         try:\n223             parsed: Optional[BaseSegment] = parser.parse(\n224                 tokens,\n225                 recurse=recurse,\n226                 fname=fname,\n227             )\n228         except SQLParseError as err:\n229             linter_logger.info(\"PARSING FAILED! : %s\", err)\n230             violations.append(err)\n231             return None, violations\n232 \n233         if parsed:\n234             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n235             linter_logger.info(\"\\n\" + parsed.stringify())\n236             # We may succeed parsing, but still have unparsable segments. Extract them\n237             # here.\n238             for unparsable in parsed.iter_unparsables():\n239                 # No exception has been raised explicitly, but we still create one here\n240                 # so that we can use the common interface\n241                 violations.append(\n242                     SQLParseError(\n243                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n244                         \"{1!r}\".format(\n245                             unparsable.pos_marker.working_loc,\n246                             unparsable.raw\n247                             if len(unparsable.raw) < 40\n248                             else unparsable.raw[:40] + \"...\",\n249                         ),\n250                         segment=unparsable,\n251                     )\n252                 )\n253                 linter_logger.info(\"Found unparsable segment...\")\n254                 linter_logger.info(unparsable.stringify())\n255         return parsed, violations\n256 \n257     @staticmethod\n258     def parse_noqa(\n259         comment: str,\n260         line_no: int,\n261         rule_codes: List[str],\n262     ):\n263         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n264         # Also trim any whitespace afterward\n265 \n266         # Comment lines can also have noqa e.g.\n267         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: L016\n268         # Therefore extract last possible inline ignore.\n269         comment = [c.strip() for c in comment.split(\"--\")][-1]\n270 \n271         if comment.startswith(\"noqa\"):\n272             # This is an ignore identifier\n273             comment_remainder = comment[4:]\n274             if comment_remainder:\n275                 if not comment_remainder.startswith(\":\"):\n276                     return SQLParseError(\n277                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n278                         line_no=line_no,\n279                     )\n280                 comment_remainder = comment_remainder[1:].strip()\n281                 if comment_remainder:\n282                     action: Optional[str]\n283                     if \"=\" in comment_remainder:\n284                         action, rule_part = comment_remainder.split(\"=\", 1)\n285                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n286                             return SQLParseError(\n287                                 \"Malformed 'noqa' section. \"\n288                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n289                                 \"or 'noqa: disable=<rule>[,...] | all\",\n290                                 line_no=line_no,\n291                             )\n292                     else:\n293                         action = None\n294                         rule_part = comment_remainder\n295                         if rule_part in {\"disable\", \"enable\"}:\n296                             return SQLParseError(\n297                                 \"Malformed 'noqa' section. \"\n298                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n299                                 \"or 'noqa: disable=<rule>[,...] | all\",\n300                                 line_no=line_no,\n301                             )\n302                     rules: Optional[Tuple[str, ...]]\n303                     if rule_part != \"all\":\n304                         # Rules can be globs therefore we compare to the rule_set to\n305                         # expand the globs.\n306                         unexpanded_rules = tuple(\n307                             r.strip() for r in rule_part.split(\",\")\n308                         )\n309                         expanded_rules = []\n310                         for r in unexpanded_rules:\n311                             expanded_rule = [\n312                                 x\n313                                 for x in fnmatch.filter(rule_codes, r)\n314                                 if x not in expanded_rules\n315                             ]\n316                             if expanded_rule:\n317                                 expanded_rules.extend(expanded_rule)\n318                             elif r not in expanded_rules:\n319                                 # We were unable to expand the glob.\n320                                 # Therefore assume the user is referencing\n321                                 # a special error type (e.g. PRS, LXR, or TMP)\n322                                 # and add this to the list of rules to ignore.\n323                                 expanded_rules.append(r)\n324                         rules = tuple(expanded_rules)\n325                     else:\n326                         rules = None\n327                     return NoQaDirective(line_no, rules, action)\n328             return NoQaDirective(line_no, None, None)\n329         return None\n330 \n331     @staticmethod\n332     def remove_templated_errors(\n333         linting_errors: List[SQLBaseError],\n334     ) -> List[SQLBaseError]:\n335         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n336         # Filter out any linting errors in templated sections if relevant.\n337         result: List[SQLBaseError] = []\n338         for e in linting_errors:\n339             if isinstance(e, SQLLintError):\n340                 if (\n341                     # Is it in a literal section?\n342                     e.segment.pos_marker.is_literal()\n343                     # Is it a rule that is designed to work on templated sections?\n344                     or e.rule.targets_templated\n345                 ):\n346                     result.append(e)\n347             else:\n348                 # If it's another type, just keep it. (E.g. SQLParseError from\n349                 # malformed \"noqa\" comment).\n350                 result.append(e)\n351         return result\n352 \n353     @staticmethod\n354     def _report_conflicting_fixes_same_anchor(message: str):  # pragma: no cover\n355         # This function exists primarily in order to let us monkeypatch it at\n356         # runtime (replacing it with a function that raises an exception).\n357         linter_logger.critical(message)\n358 \n359     @staticmethod\n360     def _warn_unfixable(code: str):\n361         linter_logger.warning(\n362             f\"One fix for {code} not applied, it would re-cause the same error.\"\n363         )\n364 \n365     # ### Class Methods\n366     # These compose the base static methods into useful recipes.\n367 \n368     @classmethod\n369     def parse_rendered(\n370         cls,\n371         rendered: RenderedFile,\n372         recurse: bool = True,\n373     ) -> ParsedString:\n374         \"\"\"Parse a rendered file.\"\"\"\n375         t0 = time.monotonic()\n376         violations = cast(List[SQLBaseError], rendered.templater_violations)\n377         tokens: Optional[Sequence[BaseSegment]]\n378         if rendered.templated_file:\n379             tokens, lvs, config = cls._lex_templated_file(\n380                 rendered.templated_file, rendered.config\n381             )\n382             violations += lvs\n383         else:\n384             tokens = None\n385 \n386         t1 = time.monotonic()\n387         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n388 \n389         if tokens:\n390             parsed, pvs = cls._parse_tokens(\n391                 tokens,\n392                 rendered.config,\n393                 recurse=recurse,\n394                 fname=rendered.fname,\n395             )\n396             violations += pvs\n397         else:\n398             parsed = None\n399 \n400         time_dict = {\n401             **rendered.time_dict,\n402             \"lexing\": t1 - t0,\n403             \"parsing\": time.monotonic() - t1,\n404         }\n405         return ParsedString(\n406             parsed,\n407             violations,\n408             time_dict,\n409             rendered.templated_file,\n410             rendered.config,\n411             rendered.fname,\n412             rendered.source_str,\n413         )\n414 \n415     @classmethod\n416     def extract_ignore_from_comment(\n417         cls,\n418         comment: RawSegment,\n419         rule_codes: List[str],\n420     ):\n421         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n422         # Also trim any whitespace afterward\n423         comment_content = comment.raw_trimmed().strip()\n424         comment_line, _ = comment.pos_marker.source_position()\n425         result = cls.parse_noqa(comment_content, comment_line, rule_codes)\n426         if isinstance(result, SQLParseError):\n427             result.segment = comment\n428         return result\n429 \n430     @classmethod\n431     def extract_ignore_mask_tree(\n432         cls,\n433         tree: BaseSegment,\n434         rule_codes: List[str],\n435     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n436         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n437         ignore_buff: List[NoQaDirective] = []\n438         violations: List[SQLBaseError] = []\n439         for comment in tree.recursive_crawl(\"comment\"):\n440             if comment.is_type(\"inline_comment\"):\n441                 ignore_entry = cls.extract_ignore_from_comment(comment, rule_codes)\n442                 if isinstance(ignore_entry, SQLParseError):\n443                     violations.append(ignore_entry)\n444                 elif ignore_entry:\n445                     ignore_buff.append(ignore_entry)\n446         if ignore_buff:\n447             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n448         return ignore_buff, violations\n449 \n450     @classmethod\n451     def extract_ignore_mask_source(\n452         cls,\n453         source: str,\n454         inline_comment_regex: RegexLexer,\n455         rule_codes: List[str],\n456     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n457         \"\"\"Look for inline ignore comments and return NoQaDirectives.\n458 \n459         Very similar to extract_ignore_mask_tree(), but can be run on raw source\n460         (i.e. does not require the code to have parsed successfully).\n461         \"\"\"\n462         ignore_buff: List[NoQaDirective] = []\n463         violations: List[SQLBaseError] = []\n464         for idx, line in enumerate(source.split(\"\\n\")):\n465             match = inline_comment_regex.search(line) if line else None\n466             if match:\n467                 ignore_entry = cls.parse_noqa(\n468                     line[match[0] : match[1]], idx + 1, rule_codes\n469                 )\n470                 if isinstance(ignore_entry, SQLParseError):\n471                     violations.append(ignore_entry)  # pragma: no cover\n472                 elif ignore_entry:\n473                     ignore_buff.append(ignore_entry)\n474         if ignore_buff:\n475             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n476         return ignore_buff, violations\n477 \n478     @classmethod\n479     def lint_fix_parsed(\n480         cls,\n481         tree: BaseSegment,\n482         config: FluffConfig,\n483         rule_set: List[BaseRule],\n484         fix: bool = False,\n485         fname: Optional[str] = None,\n486         templated_file: Optional[TemplatedFile] = None,\n487         formatter: Any = None,\n488     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n489         \"\"\"Lint and optionally fix a tree object.\"\"\"\n490         # Keep track of the linting errors on the very first linter pass. The\n491         # list of issues output by \"lint\" and \"fix\" only includes issues present\n492         # in the initial SQL code, EXCLUDING any issues that may be created by\n493         # the fixes themselves.\n494         initial_linting_errors = []\n495         # A placeholder for the fixes we had on the previous loop\n496         last_fixes = None\n497         # Keep a set of previous versions to catch infinite loops.\n498         previous_versions: Set[Tuple[str, Tuple[SourceFix, ...]]] = {(tree.raw, ())}\n499 \n500         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n501         # once for linting.\n502         loop_limit = config.get(\"runaway_limit\") if fix else 1\n503 \n504         # Dispatch the output for the lint header\n505         if formatter:\n506             formatter.dispatch_lint_header(fname)\n507 \n508         # Look for comment segments which might indicate lines to ignore.\n509         if not config.get(\"disable_noqa\"):\n510             rule_codes = [r.code for r in rule_set]\n511             ignore_buff, ivs = cls.extract_ignore_mask_tree(tree, rule_codes)\n512             initial_linting_errors += ivs\n513         else:\n514             ignore_buff = []\n515 \n516         save_tree = tree\n517         # There are two phases of rule running.\n518         # 1. The main loop is for most rules. These rules are assumed to\n519         # interact and cause a cascade of fixes requiring multiple passes.\n520         # These are run the `runaway_limit` number of times (default 10).\n521         # 2. The post loop is for post-processing rules, not expected to trigger\n522         # any downstream rules, e.g. capitalization fixes. They are run on the\n523         # first loop and then twice at the end (once to fix, and once again to\n524         # check result of fixes), but not in the intervening loops.\n525         phases = [\"main\"]\n526         if fix:\n527             phases.append(\"post\")\n528         for phase in phases:\n529             if len(phases) > 1:\n530                 rules_this_phase = [\n531                     rule for rule in rule_set if rule.lint_phase == phase\n532                 ]\n533             else:\n534                 rules_this_phase = rule_set\n535             for loop in range(loop_limit if phase == \"main\" else 2):\n536 \n537                 def is_first_linter_pass():\n538                     return phase == phases[0] and loop == 0\n539 \n540                 # Additional newlines are to assist in scanning linting loops\n541                 # during debugging.\n542                 linter_logger.info(\n543                     f\"\\n\\nEntering linter phase {phase}, loop {loop+1}/{loop_limit}\\n\"\n544                 )\n545                 changed = False\n546 \n547                 if is_first_linter_pass():\n548                     # In order to compute initial_linting_errors correctly, need\n549                     # to run all rules on the first loop of the main phase.\n550                     rules_this_phase = rule_set\n551                 progress_bar_crawler = tqdm(\n552                     rules_this_phase,\n553                     desc=\"lint by rules\",\n554                     leave=False,\n555                     disable=progress_bar_configuration.disable_progress_bar,\n556                 )\n557 \n558                 for crawler in progress_bar_crawler:\n559                     # Performance: After first loop pass, skip rules that don't\n560                     # do fixes. Any results returned won't be seen by the user\n561                     # anyway (linting errors ADDED by rules changing SQL, are\n562                     # not reported back to the user - only initial linting errors),\n563                     # so there's absolutely no reason to run them.\n564                     if (\n565                         fix\n566                         and not is_first_linter_pass()\n567                         and not is_fix_compatible(crawler)\n568                     ):\n569                         continue\n570 \n571                     progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n572 \n573                     # fixes should be a dict {} with keys edit, delete, create\n574                     # delete is just a list of segments to delete\n575                     # edit and create are list of tuples. The first element is\n576                     # the \"anchor\", the segment to look for either to edit or to\n577                     # insert BEFORE. The second is the element to insert or create.\n578                     linting_errors, _, fixes, _ = crawler.crawl(\n579                         tree,\n580                         dialect=config.get(\"dialect_obj\"),\n581                         fix=fix,\n582                         templated_file=templated_file,\n583                         ignore_mask=ignore_buff,\n584                         fname=fname,\n585                         config=config,\n586                     )\n587                     if is_first_linter_pass():\n588                         initial_linting_errors += linting_errors\n589 \n590                     if fix and fixes:\n591                         linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n592                         # Do some sanity checks on the fixes before applying.\n593                         anchor_info = BaseSegment.compute_anchor_edit_info(fixes)\n594                         if any(\n595                             not info.is_valid for info in anchor_info.values()\n596                         ):  # pragma: no cover\n597                             message = (\n598                                 f\"Rule {crawler.code} returned conflicting \"\n599                                 \"fixes with the same anchor. This is only \"\n600                                 \"supported for create_before+create_after, so \"\n601                                 f\"the fixes will not be applied. {fixes!r}\"\n602                             )\n603                             cls._report_conflicting_fixes_same_anchor(message)\n604                             for lint_result in linting_errors:\n605                                 lint_result.fixes = []\n606                         elif fixes == last_fixes:  # pragma: no cover\n607                             # If we generate the same fixes two times in a row,\n608                             # that means we're in a loop, and we want to stop.\n609                             # (Fixes should address issues, hence different\n610                             # and/or fewer fixes next time.)\n611                             cls._warn_unfixable(crawler.code)\n612                         else:\n613                             # This is the happy path. We have fixes, now we want to\n614                             # apply them.\n615                             last_fixes = fixes\n616                             new_tree, _, _ = tree.apply_fixes(\n617                                 config.get(\"dialect_obj\"), crawler.code, anchor_info\n618                             )\n619                             # Check for infinite loops. We use a combination of the\n620                             # fixed templated file and the list of source fixes to\n621                             # apply.\n622                             loop_check_tuple = (\n623                                 new_tree.raw,\n624                                 tuple(new_tree.source_fixes),\n625                             )\n626                             if loop_check_tuple not in previous_versions:\n627                                 # We've not seen this version of the file so\n628                                 # far. Continue.\n629                                 tree = new_tree\n630                                 previous_versions.add(loop_check_tuple)\n631                                 changed = True\n632                                 continue\n633                             else:\n634                                 # Applying these fixes took us back to a state\n635                                 # which we've seen before. We're in a loop, so\n636                                 # we want to stop.\n637                                 cls._warn_unfixable(crawler.code)\n638 \n639                 if fix and not changed:\n640                     # We did not change the file. Either the file is clean (no\n641                     # fixes), or any fixes which are present will take us back\n642                     # to a previous state.\n643                     linter_logger.info(\n644                         f\"Fix loop complete for {phase} phase. Stability \"\n645                         f\"achieved after {loop}/{loop_limit} loops.\"\n646                     )\n647                     break\n648             else:\n649                 if fix:\n650                     # The linter loop hit the limit before reaching a stable point\n651                     # (i.e. free of lint errors). If this happens, it's usually\n652                     # because one or more rules produced fixes which did not address\n653                     # the original issue **or** created new issues.\n654                     linter_logger.warning(\n655                         f\"Loop limit on fixes reached [{loop_limit}].\"\n656                     )\n657 \n658                     # Discard any fixes for the linting errors, since they caused a\n659                     # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n660                     # these linting errors are \"unfixable\". This is important,\n661                     # because when \"sqlfluff fix\" encounters unfixable lint errors,\n662                     # it exits with a \"failure\" exit code, which is exactly what we\n663                     # want in this situation. (Reason: Although this is more of an\n664                     # internal SQLFluff issue, users deserve to know about it,\n665                     # because it means their file(s) weren't fixed.\n666                     for violation in initial_linting_errors:\n667                         if isinstance(violation, SQLLintError):\n668                             violation.fixes = []\n669 \n670                     # Return the original parse tree, before any fixes were applied.\n671                     # Reason: When the linter hits the loop limit, the file is often\n672                     # messy, e.g. some of the fixes were applied repeatedly, possibly\n673                     # other weird things. We don't want the user to see this junk!\n674                     return save_tree, initial_linting_errors, ignore_buff\n675 \n676         if config.get(\"ignore_templated_areas\", default=True):\n677             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n678 \n679         return tree, initial_linting_errors, ignore_buff\n680 \n681     @classmethod\n682     def lint_parsed(\n683         cls,\n684         parsed: ParsedString,\n685         rule_set: List[BaseRule],\n686         fix: bool = False,\n687         formatter: Any = None,\n688         encoding: str = \"utf8\",\n689     ):\n690         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n691         violations = parsed.violations\n692         time_dict = parsed.time_dict\n693         tree: Optional[BaseSegment]\n694         if parsed.tree:\n695             t0 = time.monotonic()\n696             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n697             tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n698                 parsed.tree,\n699                 config=parsed.config,\n700                 rule_set=rule_set,\n701                 fix=fix,\n702                 fname=parsed.fname,\n703                 templated_file=parsed.templated_file,\n704                 formatter=formatter,\n705             )\n706             # Update the timing dict\n707             time_dict[\"linting\"] = time.monotonic() - t0\n708 \n709             # We're only going to return the *initial* errors, rather\n710             # than any generated during the fixing cycle.\n711             violations += initial_linting_errors\n712         else:\n713             # If no parsed tree, set to None\n714             tree = None\n715             ignore_buff = []\n716             if not parsed.config.get(\"disable_noqa\"):\n717                 # Templating and/or parsing have failed. Look for \"noqa\"\n718                 # comments (the normal path for identifying these comments\n719                 # requires access to the parse tree, and because of the failure,\n720                 # we don't have a parse tree).\n721                 ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n722                     parsed.source_str,\n723                     [\n724                         lm\n725                         for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n726                         if lm.name == \"inline_comment\"\n727                     ][0],\n728                     [r.code for r in rule_set],\n729                 )\n730                 violations += ignore_violations\n731 \n732         # We process the ignore config here if appropriate\n733         for violation in violations:\n734             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n735             violation.warning_if_in(parsed.config.get(\"warnings\"))\n736 \n737         linted_file = LintedFile(\n738             parsed.fname,\n739             # Deduplicate violations\n740             LintedFile.deduplicate_in_source_space(violations),\n741             time_dict,\n742             tree,\n743             ignore_mask=ignore_buff,\n744             templated_file=parsed.templated_file,\n745             encoding=encoding,\n746         )\n747 \n748         # This is the main command line output from linting.\n749         if formatter:\n750             formatter.dispatch_file_violations(\n751                 parsed.fname, linted_file, only_fixable=fix\n752             )\n753 \n754         # Safety flag for unset dialects\n755         if linted_file.get_violations(\n756             fixable=True if fix else None, types=SQLParseError\n757         ):\n758             if formatter:  # pragma: no cover TODO?\n759                 formatter.dispatch_dialect_warning(parsed.config.get(\"dialect\"))\n760 \n761         return linted_file\n762 \n763     @classmethod\n764     def lint_rendered(\n765         cls,\n766         rendered: RenderedFile,\n767         rule_set: List[BaseRule],\n768         fix: bool = False,\n769         formatter: Any = None,\n770     ) -> LintedFile:\n771         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n772         parsed = cls.parse_rendered(rendered)\n773         return cls.lint_parsed(\n774             parsed,\n775             rule_set=rule_set,\n776             fix=fix,\n777             formatter=formatter,\n778             encoding=rendered.encoding,\n779         )\n780 \n781     # ### Instance Methods\n782     # These are tied to a specific instance and so are not necessarily\n783     # safe to use in parallel operations.\n784 \n785     def render_string(\n786         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n787     ) -> RenderedFile:\n788         \"\"\"Template the file.\"\"\"\n789         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n790 \n791         # Start the templating timer\n792         t0 = time.monotonic()\n793 \n794         # Newlines are normalised to unix-style line endings (\\n).\n795         # The motivation is that Jinja normalises newlines during templating and\n796         # we want consistent mapping between the raw and templated slices.\n797         in_str = self._normalise_newlines(in_str)\n798 \n799         # Since Linter.__init__() does not require a dialect to be specified,\n800         # check for one now. (We're processing a string, not a file, so we're\n801         # not going to pick up a .sqlfluff or other config file to provide a\n802         # missing dialect at this point.)\n803         config.verify_dialect_specified()\n804         if not config.get(\"templater_obj\") == self.templater:\n805             linter_logger.warning(\n806                 (\n807                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n808                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n809                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n810                     \"working directory. It can be set in a .sqlfluff in the current \"\n811                     \"working directory. See Nesting section of the docs for more \"\n812                     \"details.\"\n813                 )\n814             )\n815         try:\n816             templated_file, templater_violations = self.templater.process(\n817                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n818             )\n819         except SQLFluffSkipFile as s:  # pragma: no cover\n820             linter_logger.warning(str(s))\n821             templated_file = None\n822             templater_violations = []\n823 \n824         if not templated_file:\n825             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n826 \n827         # Record time\n828         time_dict = {\"templating\": time.monotonic() - t0}\n829 \n830         return RenderedFile(\n831             templated_file,\n832             templater_violations,\n833             config,\n834             time_dict,\n835             fname,\n836             encoding,\n837             in_str,\n838         )\n839 \n840     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n841         \"\"\"Load and render a file with relevant config.\"\"\"\n842         # Load the raw file.\n843         raw_file, config, encoding = self.load_raw_file_and_config(fname, root_config)\n844         # Render the file\n845         return self.render_string(raw_file, fname, config, encoding)\n846 \n847     def parse_string(\n848         self,\n849         in_str: str,\n850         fname: str = \"<string>\",\n851         recurse: bool = True,\n852         config: Optional[FluffConfig] = None,\n853         encoding: str = \"utf-8\",\n854     ) -> ParsedString:\n855         \"\"\"Parse a string.\"\"\"\n856         violations: List[SQLBaseError] = []\n857 \n858         # Dispatch the output for the template header (including the config diff)\n859         if self.formatter:\n860             self.formatter.dispatch_template_header(fname, self.config, config)\n861 \n862         # Just use the local config from here:\n863         config = config or self.config\n864 \n865         # Scan the raw file for config commands.\n866         config.process_raw_file_for_config(in_str)\n867         rendered = self.render_string(in_str, fname, config, encoding)\n868         violations += rendered.templater_violations\n869 \n870         # Dispatch the output for the parse header\n871         if self.formatter:\n872             self.formatter.dispatch_parse_header(fname)\n873 \n874         return self.parse_rendered(rendered, recurse=recurse)\n875 \n876     def fix(\n877         self,\n878         tree: BaseSegment,\n879         config: Optional[FluffConfig] = None,\n880         fname: Optional[str] = None,\n881         templated_file: Optional[TemplatedFile] = None,\n882     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n883         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n884         config = config or self.config\n885         rule_set = self.get_ruleset(config=config)\n886         fixed_tree, violations, _ = self.lint_fix_parsed(\n887             tree,\n888             config,\n889             rule_set,\n890             fix=True,\n891             fname=fname,\n892             templated_file=templated_file,\n893             formatter=self.formatter,\n894         )\n895         return fixed_tree, violations\n896 \n897     def lint(\n898         self,\n899         tree: BaseSegment,\n900         config: Optional[FluffConfig] = None,\n901         fname: Optional[str] = None,\n902         templated_file: Optional[TemplatedFile] = None,\n903     ) -> List[SQLBaseError]:\n904         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n905         config = config or self.config\n906         rule_set = self.get_ruleset(config=config)\n907         _, violations, _ = self.lint_fix_parsed(\n908             tree,\n909             config,\n910             rule_set,\n911             fix=False,\n912             fname=fname,\n913             templated_file=templated_file,\n914             formatter=self.formatter,\n915         )\n916         return violations\n917 \n918     def lint_string(\n919         self,\n920         in_str: str = \"\",\n921         fname: str = \"<string input>\",\n922         fix: bool = False,\n923         config: Optional[FluffConfig] = None,\n924         encoding: str = \"utf8\",\n925     ) -> LintedFile:\n926         \"\"\"Lint a string.\n927 \n928         Returns:\n929             :obj:`LintedFile`: an object representing that linted file.\n930 \n931         \"\"\"\n932         # Sort out config, defaulting to the built in config if no override\n933         config = config or self.config\n934         # Parse the string.\n935         parsed = self.parse_string(\n936             in_str=in_str,\n937             fname=fname,\n938             config=config,\n939         )\n940         # Get rules as appropriate\n941         rule_set = self.get_ruleset(config=config)\n942         # Lint the file and return the LintedFile\n943         return self.lint_parsed(\n944             parsed,\n945             rule_set,\n946             fix=fix,\n947             formatter=self.formatter,\n948             encoding=encoding,\n949         )\n950 \n951     def paths_from_path(\n952         self,\n953         path: str,\n954         ignore_file_name: str = \".sqlfluffignore\",\n955         ignore_non_existent_files: bool = False,\n956         ignore_files: bool = True,\n957         working_path: str = os.getcwd(),\n958     ) -> List[str]:\n959         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n960 \n961         Here we also deal with the .sqlfluffignore file if present.\n962 \n963         When a path to a file to be linted is explicitly passed\n964         we look for ignore files in all directories that are parents of the file,\n965         up to the current directory.\n966 \n967         If the current directory is not a parent of the file we only\n968         look for an ignore file in the direct parent of the file.\n969 \n970         \"\"\"\n971         if not os.path.exists(path):\n972             if ignore_non_existent_files:\n973                 return []\n974             else:\n975                 raise SQLFluffUserError(\n976                     f\"Specified path does not exist. Check it/they exist(s): {path}.\"\n977                 )\n978 \n979         # Files referred to exactly are also ignored if\n980         # matched, but we warn the users when that happens\n981         is_exact_file = os.path.isfile(path)\n982 \n983         path_walk: WalkableType\n984         if is_exact_file:\n985             # When the exact file to lint is passed, we\n986             # fill path_walk with an input that follows\n987             # the structure of `os.walk`:\n988             #   (root, directories, files)\n989             dirpath = os.path.dirname(path)\n990             files = [os.path.basename(path)]\n991             path_walk = [(dirpath, None, files)]\n992         else:\n993             path_walk = list(os.walk(path))\n994 \n995         ignore_file_paths = ConfigLoader.find_ignore_config_files(\n996             path=path, working_path=working_path, ignore_file_name=ignore_file_name\n997         )\n998         # Add paths that could contain \"ignore files\"\n999         # to the path_walk list\n1000         path_walk_ignore_file = [\n1001             (\n1002                 os.path.dirname(ignore_file_path),\n1003                 None,\n1004                 # Only one possible file, since we only\n1005                 # have one \"ignore file name\"\n1006                 [os.path.basename(ignore_file_path)],\n1007             )\n1008             for ignore_file_path in ignore_file_paths\n1009         ]\n1010         path_walk += path_walk_ignore_file\n1011 \n1012         # If it's a directory then expand the path!\n1013         buffer = []\n1014         ignores = {}\n1015         for dirpath, _, filenames in path_walk:\n1016             for fname in filenames:\n1017                 fpath = os.path.join(dirpath, fname)\n1018                 # Handle potential .sqlfluffignore files\n1019                 if ignore_files and fname == ignore_file_name:\n1020                     with open(fpath) as fh:\n1021                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n1022                         ignores[dirpath] = spec\n1023                     # We don't need to process the ignore file any further\n1024                     continue\n1025 \n1026                 # We won't purge files *here* because there's an edge case\n1027                 # that the ignore file is processed after the sql file.\n1028 \n1029                 # Scan for remaining files\n1030                 for ext in (\n1031                     self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n1032                 ):\n1033                     # is it a sql file?\n1034                     if fname.lower().endswith(ext):\n1035                         buffer.append(fpath)\n1036 \n1037         if not ignore_files:\n1038             return sorted(buffer)\n1039 \n1040         # Check the buffer for ignore items and normalise the rest.\n1041         # It's a set, so we can do natural deduplication.\n1042         filtered_buffer = set()\n1043 \n1044         for fpath in buffer:\n1045             abs_fpath = os.path.abspath(fpath)\n1046             for ignore_base, ignore_spec in ignores.items():\n1047                 abs_ignore_base = os.path.abspath(ignore_base)\n1048                 if abs_fpath.startswith(\n1049                     abs_ignore_base + os.sep\n1050                 ) and ignore_spec.match_file(\n1051                     os.path.relpath(abs_fpath, abs_ignore_base)\n1052                 ):\n1053                     # This file is ignored, skip it.\n1054                     if is_exact_file:\n1055                         linter_logger.warning(\n1056                             \"Exact file path %s was given but \"\n1057                             \"it was ignored by a %s pattern in %s, \"\n1058                             \"re-run with `--disregard-sqlfluffignores` to \"\n1059                             \"skip %s\"\n1060                             % (\n1061                                 path,\n1062                                 ignore_file_name,\n1063                                 ignore_base,\n1064                                 ignore_file_name,\n1065                             )\n1066                         )\n1067                     break\n1068             else:\n1069                 npath = os.path.normpath(fpath)\n1070                 # For debugging, log if we already have the file.\n1071                 if npath in filtered_buffer:\n1072                     linter_logger.debug(  # pragma: no cover\n1073                         \"Developer Warning: Path crawler attempted to \"\n1074                         \"requeue the same file twice. %s is already in \"\n1075                         \"filtered buffer.\",\n1076                         npath,\n1077                     )\n1078                 filtered_buffer.add(npath)\n1079 \n1080         # Return a sorted list\n1081         return sorted(filtered_buffer)\n1082 \n1083     def lint_string_wrapped(\n1084         self,\n1085         string: str,\n1086         fname: str = \"<string input>\",\n1087         fix: bool = False,\n1088     ) -> LintingResult:\n1089         \"\"\"Lint strings directly.\"\"\"\n1090         result = LintingResult()\n1091         linted_path = LintedDir(fname)\n1092         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n1093         result.add(linted_path)\n1094         result.stop_timer()\n1095         return result\n1096 \n1097     def lint_path(\n1098         self,\n1099         path: str,\n1100         fix: bool = False,\n1101         ignore_non_existent_files: bool = False,\n1102         ignore_files: bool = True,\n1103         processes: Optional[int] = None,\n1104     ) -> LintedDir:\n1105         \"\"\"Lint a path.\"\"\"\n1106         return self.lint_paths(\n1107             (path,), fix, ignore_non_existent_files, ignore_files, processes\n1108         ).paths[0]\n1109 \n1110     def lint_paths(\n1111         self,\n1112         paths: Tuple[str, ...],\n1113         fix: bool = False,\n1114         ignore_non_existent_files: bool = False,\n1115         ignore_files: bool = True,\n1116         processes: Optional[int] = None,\n1117     ) -> LintingResult:\n1118         \"\"\"Lint an iterable of paths.\"\"\"\n1119         # If no paths specified - assume local\n1120         if not paths:  # pragma: no cover\n1121             paths = (os.getcwd(),)\n1122         # Set up the result to hold what we get back\n1123         result = LintingResult()\n1124 \n1125         expanded_paths: List[str] = []\n1126         expanded_path_to_linted_dir = {}\n1127         for path in paths:\n1128             linted_dir = LintedDir(path)\n1129             result.add(linted_dir)\n1130             for fname in self.paths_from_path(\n1131                 path,\n1132                 ignore_non_existent_files=ignore_non_existent_files,\n1133                 ignore_files=ignore_files,\n1134             ):\n1135                 expanded_paths.append(fname)\n1136                 expanded_path_to_linted_dir[fname] = linted_dir\n1137 \n1138         files_count = len(expanded_paths)\n1139         if processes is None:\n1140             processes = self.config.get(\"processes\", default=1)\n1141 \n1142         # to avoid circular import\n1143         from sqlfluff.core.linter.runner import get_runner\n1144 \n1145         runner, effective_processes = get_runner(\n1146             self,\n1147             self.config,\n1148             processes=processes,\n1149             allow_process_parallelism=self.allow_process_parallelism,\n1150         )\n1151 \n1152         if self.formatter and effective_processes != 1:\n1153             self.formatter.dispatch_processing_header(effective_processes)\n1154 \n1155         # Show files progress bar only when there is more than one.\n1156         first_path = expanded_paths[0] if expanded_paths else \"\"\n1157         progress_bar_files = tqdm(\n1158             total=files_count,\n1159             desc=f\"file {first_path}\",\n1160             leave=False,\n1161             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1162         )\n1163 \n1164         for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n1165             linted_dir = expanded_path_to_linted_dir[linted_file.path]\n1166             linted_dir.add(linted_file)\n1167             # If any fatal errors, then stop iteration.\n1168             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n1169                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n1170                 break\n1171 \n1172             # Progress bar for files is rendered only when there is more than one file.\n1173             # Additionally, as it's updated after each loop, we need to get file name\n1174             # from the next loop. This is why `enumerate` starts with `1` and there\n1175             # is `i < len` to not exceed files list length.\n1176             progress_bar_files.update(n=1)\n1177             if i < len(expanded_paths):\n1178                 progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n1179 \n1180         result.stop_timer()\n1181         return result\n1182 \n1183     def parse_path(\n1184         self,\n1185         path: str,\n1186         recurse: bool = True,\n1187     ) -> Iterator[ParsedString]:\n1188         \"\"\"Parse a path of sql files.\n1189 \n1190         NB: This a generator which will yield the result of each file\n1191         within the path iteratively.\n1192         \"\"\"\n1193         for fname in self.paths_from_path(path):\n1194             if self.formatter:\n1195                 self.formatter.dispatch_path(path)\n1196             # Load the file with the config and yield the result.\n1197             try:\n1198                 raw_file, config, encoding = self.load_raw_file_and_config(\n1199                     fname, self.config\n1200                 )\n1201             except SQLFluffSkipFile as s:\n1202                 linter_logger.warning(str(s))\n1203                 continue\n1204             yield self.parse_string(\n1205                 raw_file,\n1206                 fname=fname,\n1207                 recurse=recurse,\n1208                 config=config,\n1209                 encoding=encoding,\n1210             )\n1211 \n[end of src/sqlfluff/core/linter/linter.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "--disable_progress_bar Flag Broken for Fix\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nI ran `sqlfluff fix ${target} --dialect ansi --disable_progress_bar --force` on version 1.4.0 and got an error with exit code 2. Running with `--disable-progress-bar` appears to work fine, but it appears that compatibility with underscores was broken in version 1.4.0.\n\n### Expected Behaviour\n\nShould run as expected, with no error and no progress bar.\n\n### Observed Behaviour\n\nExit code 2 and stderr:\n```\nUsage: sqlfluff fix [OPTIONS] [PATHS]...\n        Try 'sqlfluff fix -h' for help.\n\n        Error: No such option: --disable_progress_bar (Possible options: --disable-noqa, --disable-progress-bar)\n```\n\n### How to reproduce\n\nSql file:\n```\nSELECT foo FROM bar;\n```\n\nCommand:\n```\nsqlfluff fix ${target} --dialect ansi --disable_progress_bar --force\n```\n\n### Dialect\n\nansi\n\n### Version\n\npython 3.10.3\nsqlfluff 1.4.0 and up appears to have this problem (tested through 1.4.2)\n\n### Configuration\n\nNo special configuration. Ran hermetically with `trunk`.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n```diff\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -500,7 +500,7 @@\n     \"--disable_progress_bar\",\n     \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n-    cls=DeprecatedOption,\n+    cls=click.Option,\n     deprecated=[\"--disable_progress_bar\"],\n )\n```", "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -500,7 +500,7 @@\n     \"--disable_progress_bar\",\n     \"--disable-progress-bar\",\n     is_flag=True,\n     help=\"Disables progress bars.\",\n-    cls=DeprecatedOption,\n+    cls=click.Option,\n     deprecated=[\"--disable_progress_bar\"],\n )\n"}
