{"instance_id": "pvlib__pvlib-python-1764", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nAllow user to set tol and maxiter for singlediode newton method\nThe first few lines of `pvlib.singlediode` set `tol` and `maxiter` for all the solvers using the newton method:\r\n\r\n```\r\nfrom scipy.optimize import brentq, newton\r\nfrom scipy.special import lambertw\r\n\r\n# set keyword arguments for all uses of newton in this module\r\nnewton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\r\n```\r\n\r\nHowever, I would like to change `tol` and `maxiter` for my application. It would be great if these could be added instead as keyword arguments to the various functions so they can be adjusted by the user. Using a variety of singlediode model params, I have found that by setting tol=0.1 and maxiter=10, I can realize a 1.4x speedup in the `singeldiode.bishop88_mpp` algorithm while incurring a maximum error of 0.007038% and a mean absolute error of  0.000042% in calculated V_mp.\r\n\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/main/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amain\">\n32       <img src=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=main\" alt=\"GitHub Actions Testing Status\" />\n33     </a>\n34     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n35     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/main/graph/badge.svg\" alt=\"codecov coverage\" />\n36     </a>\n37   </td>\n38 </tr>\n39 <tr>\n40   <td>Benchmarks</td>\n41   <td>\n42     <a href=\"https://pvlib.github.io/pvlib-benchmarks/\">\n43     <img src=\"https://img.shields.io/badge/benchmarks-asv-lightgrey\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Publications</td>\n49   <td>\n50     <a href=\"https://doi.org/10.5281/zenodo.593284\">\n51     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg\" alt=\"zenodo reference\">\n52     </a>\n53     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n54     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Downloads</td>\n60   <td>\n61     <a href=\"https://pypi.org/project/pvlib/\">\n62     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n63     </a>\n64     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n65     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n66     </a>\n67   </td>\n68 </tr>\n69 </table>\n70 \n71 \n72 pvlib python is a community supported tool that provides a set of\n73 functions and classes for simulating the performance of photovoltaic\n74 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n75 toolbox developed at Sandia National Laboratories and it implements many\n76 of the models and methods developed at the Labs. More information on\n77 Sandia Labs PV performance modeling programs can be found at\n78 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n79 but operate independently of it.\n80 \n81 \n82 Documentation\n83 =============\n84 \n85 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),\n86 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.\n87 \n88 Installation\n89 ============\n90 \n91 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n92 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.\n93 \n94 \n95 Contributing\n96 ============\n97 \n98 We need your help to make pvlib-python a great tool!\n99 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n100 The long-term success of pvlib-python requires substantial community support.\n101 \n102 \n103 Citing\n104 ======\n105 \n106 If you use pvlib-python in a published work, please cite:\n107 \n108   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n109   \"pvlib python: a python package for modeling solar energy systems.\"\n110   Journal of Open Source Software, 3(29), 884, (2018).\n111   https://doi.org/10.21105/joss.00884\n112 \n113 Please also cite the DOI corresponding to the specific version of\n114 pvlib-python that you used. pvlib-python DOIs are listed at\n115 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n116 \n117 If you use pvlib-python in a commercial or publicly-available application, please\n118 consider displaying one of the \"powered by pvlib\" logos:\n119 \n120 <img src=\"docs/sphinx/source/_images/pvlib_powered_logo_vert.png\" width=\"300\"><img src=\"docs/sphinx/source/_images/pvlib_powered_logo_horiz.png\" width=\"300\">\n121 \n122 Getting support\n123 ===============\n124 \n125 pvlib usage questions can be asked on\n126 [Stack Overflow](http://stackoverflow.com) and tagged with\n127 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n128 \n129 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n130 is used for discussing various topics of interest to the pvlib-python\n131 community. We also make new version announcements on the google group.\n132 \n133 If you suspect that you may have discovered a bug or if you'd like to\n134 change something about pvlib, then please make an issue on our\n135 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n136 \n137 \n138 \n139 License\n140 =======\n141 \n142 BSD 3-clause.\n143 \n144 \n145 NumFOCUS\n146 ========\n147 \n148 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n149 \n150 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n151 \n[end of README.md]\n[start of pvlib/singlediode.py]\n1 \"\"\"\n2 Low-level functions for solving the single diode equation.\n3 \"\"\"\n4 \n5 from functools import partial\n6 import numpy as np\n7 from pvlib.tools import _golden_sect_DataFrame\n8 \n9 from scipy.optimize import brentq, newton\n10 from scipy.special import lambertw\n11 \n12 # set keyword arguments for all uses of newton in this module\n13 newton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\n14 \n15 # intrinsic voltage per cell junction for a:Si, CdTe, Mertens et al.\n16 VOLTAGE_BUILTIN = 0.9  # [V]\n17 \n18 \n19 def estimate_voc(photocurrent, saturation_current, nNsVth):\n20     \"\"\"\n21     Rough estimate of open circuit voltage useful for bounding searches for\n22     ``i`` of ``v`` when using :func:`~pvlib.pvsystem.singlediode`.\n23 \n24     Parameters\n25     ----------\n26     photocurrent : numeric\n27         photo-generated current [A]\n28     saturation_current : numeric\n29         diode reverse saturation current [A]\n30     nNsVth : numeric\n31         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n32         and number of series cells ``Ns``\n33 \n34     Returns\n35     -------\n36     numeric\n37         rough estimate of open circuit voltage [V]\n38 \n39     Notes\n40     -----\n41     Calculating the open circuit voltage, :math:`V_{oc}`, of an ideal device\n42     with infinite shunt resistance, :math:`R_{sh} \\\\to \\\\infty`, and zero\n43     series resistance, :math:`R_s = 0`, yields the following equation [1]. As\n44     an estimate of :math:`V_{oc}` it is useful as an upper bound for the\n45     bisection method.\n46 \n47     .. math::\n48 \n49         V_{oc, est}=n Ns V_{th} \\\\log \\\\left( \\\\frac{I_L}{I_0} + 1 \\\\right)\n50 \n51     .. [1] http://www.pveducation.org/pvcdrom/open-circuit-voltage\n52     \"\"\"\n53 \n54     return nNsVth * np.log(np.asarray(photocurrent) / saturation_current + 1.0)\n55 \n56 \n57 def bishop88(diode_voltage, photocurrent, saturation_current,\n58              resistance_series, resistance_shunt, nNsVth, d2mutau=0,\n59              NsVbi=np.Inf, breakdown_factor=0., breakdown_voltage=-5.5,\n60              breakdown_exp=3.28, gradients=False):\n61     r\"\"\"\n62     Explicit calculation of points on the IV curve described by the single\n63     diode equation.  Values are calculated as described in [1]_.\n64 \n65     The single diode equation with recombination current and reverse bias\n66     breakdown is\n67 \n68     .. math::\n69 \n70         I = I_{L} - I_{0} \\left (\\exp \\frac{V_{d}}{nN_{s}V_{th}} - 1 \\right )\n71         - \\frac{V_{d}}{R_{sh}}\n72         - \\frac{I_{L} \\frac{d^{2}}{\\mu \\tau}}{N_{s} V_{bi} - V_{d}}\n73         - a \\frac{V_{d}}{R_{sh}} \\left (1 - \\frac{V_{d}}{V_{br}} \\right )^{-m}\n74 \n75     The input `diode_voltage` must be :math:`V + I R_{s}`.\n76 \n77 \n78     .. warning::\n79        * Usage of ``d2mutau`` is required with PVSyst\n80          coefficients for cadmium-telluride (CdTe) and amorphous-silicon\n81          (a:Si) PV modules only.\n82        * Do not use ``d2mutau`` with CEC coefficients.\n83 \n84     Parameters\n85     ----------\n86     diode_voltage : numeric\n87         diode voltage :math:`V_d` [V]\n88     photocurrent : numeric\n89         photo-generated current :math:`I_{L}` [A]\n90     saturation_current : numeric\n91         diode reverse saturation current :math:`I_{0}` [A]\n92     resistance_series : numeric\n93         series resistance :math:`R_{s}` [ohms]\n94     resistance_shunt: numeric\n95         shunt resistance :math:`R_{sh}` [ohms]\n96     nNsVth : numeric\n97         product of thermal voltage :math:`V_{th}` [V], diode ideality factor\n98         :math:`n`, and number of series cells :math:`N_{s}` [V]\n99     d2mutau : numeric, default 0\n100         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n101         (a-Si) modules that accounts for recombination current in the\n102         intrinsic layer. The value is the ratio of intrinsic layer thickness\n103         squared :math:`d^2` to the diffusion length of charge carriers\n104         :math:`\\mu \\tau`. [V]\n105     NsVbi : numeric, default np.inf\n106         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n107         (a-Si) modules that is the product of the PV module number of series\n108         cells :math:`N_{s}` and the builtin voltage :math:`V_{bi}` of the\n109         intrinsic layer. [V].\n110     breakdown_factor : numeric, default 0\n111         fraction of ohmic current involved in avalanche breakdown :math:`a`.\n112         Default of 0 excludes the reverse bias term from the model. [unitless]\n113     breakdown_voltage : numeric, default -5.5\n114         reverse breakdown voltage of the photovoltaic junction :math:`V_{br}`\n115         [V]\n116     breakdown_exp : numeric, default 3.28\n117         avalanche breakdown exponent :math:`m` [unitless]\n118     gradients : bool\n119         False returns only I, V, and P. True also returns gradients\n120 \n121     Returns\n122     -------\n123     tuple\n124         currents [A], voltages [V], power [W], and optionally\n125         :math:`\\frac{dI}{dV_d}`, :math:`\\frac{dV}{dV_d}`,\n126         :math:`\\frac{dI}{dV}`, :math:`\\frac{dP}{dV}`, and\n127         :math:`\\frac{d^2 P}{dV dV_d}`\n128 \n129     Notes\n130     -----\n131     The PVSyst thin-film recombination losses parameters ``d2mutau`` and\n132     ``NsVbi`` should only be applied to cadmium-telluride (CdTe) and amorphous-\n133     silicon (a-Si) PV modules, [2]_, [3]_. The builtin voltage :math:`V_{bi}`\n134     should account for all junctions. For example: tandem and triple junction\n135     cells would have builtin voltages of 1.8[V] and 2.7[V] respectively, based\n136     on the default of 0.9[V] for a single junction. The parameter ``NsVbi``\n137     should only account for the number of series cells in a single parallel\n138     sub-string if the module has cells in parallel greater than 1.\n139 \n140     References\n141     ----------\n142     .. [1] \"Computer simulation of the effects of electrical mismatches in\n143        photovoltaic cell interconnection circuits\" JW Bishop, Solar Cell (1988)\n144        :doi:`10.1016/0379-6787(88)90059-2`\n145 \n146     .. [2] \"Improved equivalent circuit and Analytical Model for Amorphous\n147        Silicon Solar Cells and Modules.\" J. Mertens, et al., IEEE Transactions\n148        on Electron Devices, Vol 45, No 2, Feb 1998.\n149        :doi:`10.1109/16.658676`\n150 \n151     .. [3] \"Performance assessment of a simulation model for PV modules of any\n152        available technology\", Andr\u00e9 Mermoud and Thibault Lejeune, 25th EUPVSEC,\n153        2010\n154        :doi:`10.4229/25thEUPVSEC2010-4BV.1.114`\n155     \"\"\"\n156     # calculate recombination loss current where d2mutau > 0\n157     is_recomb = d2mutau > 0  # True where there is thin-film recombination loss\n158     v_recomb = np.where(is_recomb, NsVbi - diode_voltage, np.inf)\n159     i_recomb = np.where(is_recomb, photocurrent * d2mutau / v_recomb, 0)\n160     # calculate temporary values to simplify calculations\n161     v_star = diode_voltage / nNsVth  # non-dimensional diode voltage\n162     g_sh = 1.0 / resistance_shunt  # conductance\n163     if breakdown_factor > 0:  # reverse bias is considered\n164         brk_term = 1 - diode_voltage / breakdown_voltage\n165         brk_pwr = np.power(brk_term, -breakdown_exp)\n166         i_breakdown = breakdown_factor * diode_voltage * g_sh * brk_pwr\n167     else:\n168         i_breakdown = 0.\n169     i = (photocurrent - saturation_current * np.expm1(v_star)  # noqa: W503\n170          - diode_voltage * g_sh - i_recomb - i_breakdown)   # noqa: W503\n171     v = diode_voltage - i * resistance_series\n172     retval = (i, v, i*v)\n173     if gradients:\n174         # calculate recombination loss current gradients where d2mutau > 0\n175         grad_i_recomb = np.where(is_recomb, i_recomb / v_recomb, 0)\n176         grad_2i_recomb = np.where(is_recomb, 2 * grad_i_recomb / v_recomb, 0)\n177         g_diode = saturation_current * np.exp(v_star) / nNsVth  # conductance\n178         if breakdown_factor > 0:  # reverse bias is considered\n179             brk_pwr_1 = np.power(brk_term, -breakdown_exp - 1)\n180             brk_pwr_2 = np.power(brk_term, -breakdown_exp - 2)\n181             brk_fctr = breakdown_factor * g_sh\n182             grad_i_brk = brk_fctr * (brk_pwr + diode_voltage *\n183                                      -breakdown_exp * brk_pwr_1)\n184             grad2i_brk = (brk_fctr * -breakdown_exp        # noqa: W503\n185                           * (2 * brk_pwr_1 + diode_voltage   # noqa: W503\n186                              * (-breakdown_exp - 1) * brk_pwr_2))  # noqa: W503\n187         else:\n188             grad_i_brk = 0.\n189             grad2i_brk = 0.\n190         grad_i = -g_diode - g_sh - grad_i_recomb - grad_i_brk  # di/dvd\n191         grad_v = 1.0 - grad_i * resistance_series  # dv/dvd\n192         # dp/dv = d(iv)/dv = v * di/dv + i\n193         grad = grad_i / grad_v  # di/dv\n194         grad_p = v * grad + i  # dp/dv\n195         grad2i = -g_diode / nNsVth - grad_2i_recomb - grad2i_brk  # d2i/dvd\n196         grad2v = -grad2i * resistance_series  # d2v/dvd\n197         grad2p = (\n198             grad_v * grad + v * (grad2i/grad_v - grad_i*grad2v/grad_v**2)\n199             + grad_i\n200         )  # d2p/dv/dvd\n201         retval += (grad_i, grad_v, grad, grad_p, grad2p)\n202     return retval\n203 \n204 \n205 def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n206                       resistance_series, resistance_shunt, nNsVth,\n207                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n208                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n209                       method='newton'):\n210     \"\"\"\n211     Find current given any voltage.\n212 \n213     Parameters\n214     ----------\n215     voltage : numeric\n216         voltage (V) in volts [V]\n217     photocurrent : numeric\n218         photogenerated current (Iph or IL) [A]\n219     saturation_current : numeric\n220         diode dark or saturation current (Io or Isat) [A]\n221     resistance_series : numeric\n222         series resistance (Rs) in [Ohm]\n223     resistance_shunt : numeric\n224         shunt resistance (Rsh) [Ohm]\n225     nNsVth : numeric\n226         product of diode ideality factor (n), number of series cells (Ns), and\n227         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n228     d2mutau : numeric, default 0\n229         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n230         (a-Si) modules that accounts for recombination current in the\n231         intrinsic layer. The value is the ratio of intrinsic layer thickness\n232         squared :math:`d^2` to the diffusion length of charge carriers\n233         :math:`\\\\mu \\\\tau`. [V]\n234     NsVbi : numeric, default np.inf\n235         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n236         (a-Si) modules that is the product of the PV module number of series\n237         cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n238         [V].\n239     breakdown_factor : numeric, default 0\n240         fraction of ohmic current involved in avalanche breakdown :math:`a`.\n241         Default of 0 excludes the reverse bias term from the model. [unitless]\n242     breakdown_voltage : numeric, default -5.5\n243         reverse breakdown voltage of the photovoltaic junction :math:`V_{br}`\n244         [V]\n245     breakdown_exp : numeric, default 3.28\n246         avalanche breakdown exponent :math:`m` [unitless]\n247     method : str, default 'newton'\n248        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n249        if ``breakdown_factor`` is not 0.\n250 \n251     Returns\n252     -------\n253     current : numeric\n254         current (I) at the specified voltage (V). [A]\n255     \"\"\"\n256     # collect args\n257     args = (photocurrent, saturation_current, resistance_series,\n258             resistance_shunt, nNsVth, d2mutau, NsVbi,\n259             breakdown_factor, breakdown_voltage, breakdown_exp)\n260 \n261     def fv(x, v, *a):\n262         # calculate voltage residual given diode voltage \"x\"\n263         return bishop88(x, *a)[1] - v\n264 \n265     if method.lower() == 'brentq':\n266         # first bound the search using voc\n267         voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n268 \n269         # brentq only works with scalar inputs, so we need a set up function\n270         # and np.vectorize to repeatedly call the optimizer with the right\n271         # arguments for possible array input\n272         def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n273                           breakdown_factor, breakdown_voltage, breakdown_exp):\n274             return brentq(fv, 0.0, voc,\n275                           args=(v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n276                                 breakdown_factor, breakdown_voltage,\n277                                 breakdown_exp))\n278 \n279         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n280         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n281     elif method.lower() == 'newton':\n282         # make sure all args are numpy arrays if max size > 1\n283         # if voltage is an array, then make a copy to use for initial guess, v0\n284         args, v0 = _prepare_newton_inputs((voltage,), args, voltage)\n285         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n286                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n287                     args=args)\n288     else:\n289         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n290     return bishop88(vd, *args)[0]\n291 \n292 \n293 def bishop88_v_from_i(current, photocurrent, saturation_current,\n294                       resistance_series, resistance_shunt, nNsVth,\n295                       d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n296                       breakdown_voltage=-5.5, breakdown_exp=3.28,\n297                       method='newton'):\n298     \"\"\"\n299     Find voltage given any current.\n300 \n301     Parameters\n302     ----------\n303     current : numeric\n304         current (I) in amperes [A]\n305     photocurrent : numeric\n306         photogenerated current (Iph or IL) [A]\n307     saturation_current : numeric\n308         diode dark or saturation current (Io or Isat) [A]\n309     resistance_series : numeric\n310         series resistance (Rs) in [Ohm]\n311     resistance_shunt : numeric\n312         shunt resistance (Rsh) [Ohm]\n313     nNsVth : numeric\n314         product of diode ideality factor (n), number of series cells (Ns), and\n315         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n316     d2mutau : numeric, default 0\n317         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n318         (a-Si) modules that accounts for recombination current in the\n319         intrinsic layer. The value is the ratio of intrinsic layer thickness\n320         squared :math:`d^2` to the diffusion length of charge carriers\n321         :math:`\\\\mu \\\\tau`. [V]\n322     NsVbi : numeric, default np.inf\n323         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n324         (a-Si) modules that is the product of the PV module number of series\n325         cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n326         [V].\n327     breakdown_factor : numeric, default 0\n328         fraction of ohmic current involved in avalanche breakdown :math:`a`.\n329         Default of 0 excludes the reverse bias term from the model. [unitless]\n330     breakdown_voltage : numeric, default -5.5\n331         reverse breakdown voltage of the photovoltaic junction :math:`V_{br}`\n332         [V]\n333     breakdown_exp : numeric, default 3.28\n334         avalanche breakdown exponent :math:`m` [unitless]\n335     method : str, default 'newton'\n336        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n337        if ``breakdown_factor`` is not 0.\n338 \n339     Returns\n340     -------\n341     voltage : numeric\n342         voltage (V) at the specified current (I) in volts [V]\n343     \"\"\"\n344     # collect args\n345     args = (photocurrent, saturation_current, resistance_series,\n346             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n347             breakdown_voltage, breakdown_exp)\n348     # first bound the search using voc\n349     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n350 \n351     def fi(x, i, *a):\n352         # calculate current residual given diode voltage \"x\"\n353         return bishop88(x, *a)[0] - i\n354 \n355     if method.lower() == 'brentq':\n356         # brentq only works with scalar inputs, so we need a set up function\n357         # and np.vectorize to repeatedly call the optimizer with the right\n358         # arguments for possible array input\n359         def vd_from_brent(voc, i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n360                           breakdown_factor, breakdown_voltage, breakdown_exp):\n361             return brentq(fi, 0.0, voc,\n362                           args=(i, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n363                                 breakdown_factor, breakdown_voltage,\n364                                 breakdown_exp))\n365 \n366         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n367         vd = vd_from_brent_vectorized(voc_est, current, *args)\n368     elif method.lower() == 'newton':\n369         # make sure all args are numpy arrays if max size > 1\n370         # if voc_est is an array, then make a copy to use for initial guess, v0\n371         args, v0 = _prepare_newton_inputs((current,), args, voc_est)\n372         vd = newton(func=lambda x, *a: fi(x, current, *a), x0=v0,\n373                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n374                     args=args)\n375     else:\n376         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n377     return bishop88(vd, *args)[1]\n378 \n379 \n380 def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n381                  resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n382                  breakdown_factor=0., breakdown_voltage=-5.5,\n383                  breakdown_exp=3.28, method='newton'):\n384     \"\"\"\n385     Find max power point.\n386 \n387     Parameters\n388     ----------\n389     photocurrent : numeric\n390         photogenerated current (Iph or IL) [A]\n391     saturation_current : numeric\n392         diode dark or saturation current (Io or Isat) [A]\n393     resistance_series : numeric\n394         series resistance (Rs) in [Ohm]\n395     resistance_shunt : numeric\n396         shunt resistance (Rsh) [Ohm]\n397     nNsVth : numeric\n398         product of diode ideality factor (n), number of series cells (Ns), and\n399         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n400     d2mutau : numeric, default 0\n401         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n402         (a-Si) modules that accounts for recombination current in the\n403         intrinsic layer. The value is the ratio of intrinsic layer thickness\n404         squared :math:`d^2` to the diffusion length of charge carriers\n405         :math:`\\\\mu \\\\tau`. [V]\n406     NsVbi : numeric, default np.inf\n407         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n408         (a-Si) modules that is the product of the PV module number of series\n409         cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n410         [V].\n411     breakdown_factor : numeric, default 0\n412         fraction of ohmic current involved in avalanche breakdown :math:`a`.\n413         Default of 0 excludes the reverse bias term from the model. [unitless]\n414     breakdown_voltage : numeric, default -5.5\n415         reverse breakdown voltage of the photovoltaic junction :math:`V_{br}`\n416         [V]\n417     breakdown_exp : numeric, default 3.28\n418         avalanche breakdown exponent :math:`m` [unitless]\n419     method : str, default 'newton'\n420        Either ``'newton'`` or ``'brentq'``. ''method'' must be ``'newton'``\n421        if ``breakdown_factor`` is not 0.\n422 \n423     Returns\n424     -------\n425     tuple\n426         max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n427         max power ``p_mp`` [W]\n428     \"\"\"\n429     # collect args\n430     args = (photocurrent, saturation_current, resistance_series,\n431             resistance_shunt, nNsVth, d2mutau, NsVbi, breakdown_factor,\n432             breakdown_voltage, breakdown_exp)\n433     # first bound the search using voc\n434     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n435 \n436     def fmpp(x, *a):\n437         return bishop88(x, *a, gradients=True)[6]\n438 \n439     if method.lower() == 'brentq':\n440         # break out arguments for numpy.vectorize to handle broadcasting\n441         vec_fun = np.vectorize(\n442             lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi, vbr_a, vbr,\n443             vbr_exp: brentq(fmpp, 0.0, voc,\n444                             args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n445                                   vbr_a, vbr, vbr_exp))\n446         )\n447         vd = vec_fun(voc_est, *args)\n448     elif method.lower() == 'newton':\n449         # make sure all args are numpy arrays if max size > 1\n450         # if voc_est is an array, then make a copy to use for initial guess, v0\n451         args, v0 = _prepare_newton_inputs((), args, voc_est)\n452         vd = newton(\n453             func=fmpp, x0=v0,\n454             fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n455         )\n456     else:\n457         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n458     return bishop88(vd, *args)\n459 \n460 \n461 def _get_size_and_shape(args):\n462     # find the right size and shape for returns\n463     size, shape = 0, None  # 0 or None both mean scalar\n464     for arg in args:\n465         try:\n466             this_shape = arg.shape  # try to get shape\n467         except AttributeError:\n468             this_shape = None\n469             try:\n470                 this_size = len(arg)  # try to get the size\n471             except TypeError:\n472                 this_size = 0\n473         else:\n474             this_size = arg.size  # if it has shape then it also has size\n475             if shape is None:\n476                 shape = this_shape  # set the shape if None\n477         # update size and shape\n478         if this_size > size:\n479             size = this_size\n480             if this_shape is not None:\n481                 shape = this_shape\n482     return size, shape\n483 \n484 \n485 def _prepare_newton_inputs(i_or_v_tup, args, v0):\n486     # broadcast arguments for newton method\n487     # the first argument should be a tuple, eg: (i,), (v,) or ()\n488     size, shape = _get_size_and_shape(i_or_v_tup + args)\n489     if size > 1:\n490         args = [np.asarray(arg) for arg in args]\n491     # newton uses initial guess for the output shape\n492     # copy v0 to a new array and broadcast it to the shape of max size\n493     if shape is not None:\n494         v0 = np.broadcast_to(v0, shape).copy()\n495     return args, v0\n496 \n497 \n498 def _lambertw_v_from_i(current, photocurrent, saturation_current,\n499                        resistance_series, resistance_shunt, nNsVth):\n500     # Record if inputs were all scalar\n501     output_is_scalar = all(map(np.isscalar,\n502                                (current, photocurrent, saturation_current,\n503                                 resistance_series, resistance_shunt, nNsVth)))\n504 \n505     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n506     #  is generally more numerically stable\n507     conductance_shunt = 1. / resistance_shunt\n508 \n509     # Ensure that we are working with read-only views of numpy arrays\n510     # Turns Series into arrays so that we don't have to worry about\n511     #  multidimensional broadcasting failing\n512     I, IL, I0, Rs, Gsh, a = \\\n513         np.broadcast_arrays(current, photocurrent, saturation_current,\n514                             resistance_series, conductance_shunt, nNsVth)\n515 \n516     # Intitalize output V (I might not be float64)\n517     V = np.full_like(I, np.nan, dtype=np.float64)\n518 \n519     # Determine indices where 0 < Gsh requires implicit model solution\n520     idx_p = 0. < Gsh\n521 \n522     # Determine indices where 0 = Gsh allows explicit model solution\n523     idx_z = 0. == Gsh\n524 \n525     # Explicit solutions where Gsh=0\n526     if np.any(idx_z):\n527         V[idx_z] = a[idx_z] * np.log1p((IL[idx_z] - I[idx_z]) / I0[idx_z]) - \\\n528                    I[idx_z] * Rs[idx_z]\n529 \n530     # Only compute using LambertW if there are cases with Gsh>0\n531     if np.any(idx_p):\n532         # LambertW argument, cannot be float128, may overflow to np.inf\n533         # overflow is explicitly handled below, so ignore warnings here\n534         with np.errstate(over='ignore'):\n535             argW = (I0[idx_p] / (Gsh[idx_p] * a[idx_p]) *\n536                     np.exp((-I[idx_p] + IL[idx_p] + I0[idx_p]) /\n537                            (Gsh[idx_p] * a[idx_p])))\n538 \n539         # lambertw typically returns complex value with zero imaginary part\n540         # may overflow to np.inf\n541         lambertwterm = lambertw(argW).real\n542 \n543         # Record indices where lambertw input overflowed output\n544         idx_inf = np.logical_not(np.isfinite(lambertwterm))\n545 \n546         # Only re-compute LambertW if it overflowed\n547         if np.any(idx_inf):\n548             # Calculate using log(argW) in case argW is really big\n549             logargW = (np.log(I0[idx_p]) - np.log(Gsh[idx_p]) -\n550                        np.log(a[idx_p]) +\n551                        (-I[idx_p] + IL[idx_p] + I0[idx_p]) /\n552                        (Gsh[idx_p] * a[idx_p]))[idx_inf]\n553 \n554             # Three iterations of Newton-Raphson method to solve\n555             #  w+log(w)=logargW. The initial guess is w=logargW. Where direct\n556             #  evaluation (above) results in NaN from overflow, 3 iterations\n557             #  of Newton's method gives approximately 8 digits of precision.\n558             w = logargW\n559             for _ in range(0, 3):\n560                 w = w * (1. - np.log(w) + logargW) / (1. + w)\n561             lambertwterm[idx_inf] = w\n562 \n563         # Eqn. 3 in Jain and Kapoor, 2004\n564         #  V = -I*(Rs + Rsh) + IL*Rsh - a*lambertwterm + I0*Rsh\n565         # Recast in terms of Gsh=1/Rsh for better numerical stability.\n566         V[idx_p] = (IL[idx_p] + I0[idx_p] - I[idx_p]) / Gsh[idx_p] - \\\n567             I[idx_p] * Rs[idx_p] - a[idx_p] * lambertwterm\n568 \n569     if output_is_scalar:\n570         return V.item()\n571     else:\n572         return V\n573 \n574 \n575 def _lambertw_i_from_v(voltage, photocurrent, saturation_current,\n576                        resistance_series, resistance_shunt, nNsVth):\n577     # Record if inputs were all scalar\n578     output_is_scalar = all(map(np.isscalar,\n579                                (voltage, photocurrent, saturation_current,\n580                                 resistance_series, resistance_shunt, nNsVth)))\n581 \n582     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n583     #  is generally more numerically stable\n584     conductance_shunt = 1. / resistance_shunt\n585 \n586     # Ensure that we are working with read-only views of numpy arrays\n587     # Turns Series into arrays so that we don't have to worry about\n588     #  multidimensional broadcasting failing\n589     V, IL, I0, Rs, Gsh, a = \\\n590         np.broadcast_arrays(voltage, photocurrent, saturation_current,\n591                             resistance_series, conductance_shunt, nNsVth)\n592 \n593     # Intitalize output I (V might not be float64)\n594     I = np.full_like(V, np.nan, dtype=np.float64)           # noqa: E741, N806\n595 \n596     # Determine indices where 0 < Rs requires implicit model solution\n597     idx_p = 0. < Rs\n598 \n599     # Determine indices where 0 = Rs allows explicit model solution\n600     idx_z = 0. == Rs\n601 \n602     # Explicit solutions where Rs=0\n603     if np.any(idx_z):\n604         I[idx_z] = IL[idx_z] - I0[idx_z] * np.expm1(V[idx_z] / a[idx_z]) - \\\n605                    Gsh[idx_z] * V[idx_z]\n606 \n607     # Only compute using LambertW if there are cases with Rs>0\n608     # Does NOT handle possibility of overflow, github issue 298\n609     if np.any(idx_p):\n610         # LambertW argument, cannot be float128, may overflow to np.inf\n611         argW = Rs[idx_p] * I0[idx_p] / (\n612                     a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)) * \\\n613                np.exp((Rs[idx_p] * (IL[idx_p] + I0[idx_p]) + V[idx_p]) /\n614                       (a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)))\n615 \n616         # lambertw typically returns complex value with zero imaginary part\n617         # may overflow to np.inf\n618         lambertwterm = lambertw(argW).real\n619 \n620         # Eqn. 2 in Jain and Kapoor, 2004\n621         #  I = -V/(Rs + Rsh) - (a/Rs)*lambertwterm + Rsh*(IL + I0)/(Rs + Rsh)\n622         # Recast in terms of Gsh=1/Rsh for better numerical stability.\n623         I[idx_p] = (IL[idx_p] + I0[idx_p] - V[idx_p] * Gsh[idx_p]) / \\\n624                    (Rs[idx_p] * Gsh[idx_p] + 1.) - (\n625                                a[idx_p] / Rs[idx_p]) * lambertwterm\n626 \n627     if output_is_scalar:\n628         return I.item()\n629     else:\n630         return I\n631 \n632 \n633 def _lambertw(photocurrent, saturation_current, resistance_series,\n634               resistance_shunt, nNsVth, ivcurve_pnts=None):\n635     # collect args\n636     params = {'photocurrent': photocurrent,\n637               'saturation_current': saturation_current,\n638               'resistance_series': resistance_series,\n639               'resistance_shunt': resistance_shunt, 'nNsVth': nNsVth}\n640 \n641     # Compute short circuit current\n642     i_sc = _lambertw_i_from_v(0., **params)\n643 \n644     # Compute open circuit voltage\n645     v_oc = _lambertw_v_from_i(0., **params)\n646 \n647     # Find the voltage, v_mp, where the power is maximized.\n648     # Start the golden section search at v_oc * 1.14\n649     p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14, _pwr_optfcn)\n650 \n651     # Find Imp using Lambert W\n652     i_mp = _lambertw_i_from_v(v_mp, **params)\n653 \n654     # Find Ix and Ixx using Lambert W\n655     i_x = _lambertw_i_from_v(0.5 * v_oc, **params)\n656 \n657     i_xx = _lambertw_i_from_v(0.5 * (v_oc + v_mp), **params)\n658 \n659     out = (i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx)\n660 \n661     # create ivcurve\n662     if ivcurve_pnts:\n663         ivcurve_v = (np.asarray(v_oc)[..., np.newaxis] *\n664                      np.linspace(0, 1, ivcurve_pnts))\n665 \n666         ivcurve_i = _lambertw_i_from_v(ivcurve_v.T, **params).T\n667 \n668         out += (ivcurve_i, ivcurve_v)\n669 \n670     return out\n671 \n672 \n673 def _pwr_optfcn(df, loc):\n674     '''\n675     Function to find power from ``i_from_v``.\n676     '''\n677 \n678     current = _lambertw_i_from_v(df[loc], df['photocurrent'],\n679                                  df['saturation_current'],\n680                                  df['resistance_series'],\n681                                  df['resistance_shunt'], df['nNsVth'])\n682 \n683     return current * df[loc]\n684 \n[end of pvlib/singlediode.py]\n[start of pvlib/solarposition.py]\n1 \"\"\"\n2 Calculate the solar position using a variety of methods/packages.\n3 \"\"\"\n4 \n5 # Contributors:\n6 # Rob Andrews (@Calama-Consulting), Calama Consulting, 2014\n7 # Will Holmgren (@wholmgren), University of Arizona, 2014\n8 # Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n9 # Cliff hansen (@cwhanse), Sandia National Laboratories, 2018\n10 \n11 import os\n12 import datetime as dt\n13 try:\n14     from importlib import reload\n15 except ImportError:\n16     try:\n17         from imp import reload\n18     except ImportError:\n19         pass\n20 \n21 import numpy as np\n22 import pandas as pd\n23 import scipy.optimize as so\n24 import warnings\n25 import datetime\n26 \n27 from pvlib import atmosphere\n28 from pvlib.tools import datetime_to_djd, djd_to_datetime\n29 \n30 \n31 NS_PER_HR = 1.e9 * 3600.  # nanoseconds per hour\n32 \n33 \n34 def get_solarposition(time, latitude, longitude,\n35                       altitude=None, pressure=None,\n36                       method='nrel_numpy',\n37                       temperature=12, **kwargs):\n38     \"\"\"\n39     A convenience wrapper for the solar position calculators.\n40 \n41     Parameters\n42     ----------\n43     time : pandas.DatetimeIndex\n44         Must be localized or UTC will be assumed.\n45 \n46     latitude : float\n47         Latitude in decimal degrees. Positive north of equator, negative\n48         to south.\n49 \n50     longitude : float\n51         Longitude in decimal degrees. Positive east of prime meridian,\n52         negative to west.\n53 \n54     altitude : None or float, default None\n55         If None, computed from pressure. Assumed to be 0 m\n56         if pressure is also None.\n57 \n58     pressure : None or float, default None\n59         If None, computed from altitude. Assumed to be 101325 Pa\n60         if altitude is also None.\n61 \n62     method : string, default 'nrel_numpy'\n63         'nrel_numpy' uses an implementation of the NREL SPA algorithm\n64         described in [1] (default, recommended): :py:func:`spa_python`\n65 \n66         'nrel_numba' uses an implementation of the NREL SPA algorithm\n67         described in [1], but also compiles the code first:\n68         :py:func:`spa_python`\n69 \n70         'pyephem' uses the PyEphem package: :py:func:`pyephem`\n71 \n72         'ephemeris' uses the pvlib ephemeris code: :py:func:`ephemeris`\n73 \n74         'nrel_c' uses the NREL SPA C code [3]: :py:func:`spa_c`\n75 \n76     temperature : float, default 12\n77         Degrees C.\n78 \n79     kwargs\n80         Other keywords are passed to the solar position function\n81         specified by the ``method`` argument.\n82 \n83     References\n84     ----------\n85     .. [1] I. Reda and A. Andreas, Solar position algorithm for solar radiation\n86        applications. Solar Energy, vol. 76, no. 5, pp. 577-589, 2004.\n87 \n88     .. [2] I. Reda and A. Andreas, Corrigendum to Solar position algorithm for\n89        solar radiation applications. Solar Energy, vol. 81, no. 6, p. 838,\n90        2007.\n91 \n92     .. [3] NREL SPA code: http://rredc.nrel.gov/solar/codesandalgorithms/spa/\n93     \"\"\"\n94 \n95     if altitude is None and pressure is None:\n96         altitude = 0.\n97         pressure = 101325.\n98     elif altitude is None:\n99         altitude = atmosphere.pres2alt(pressure)\n100     elif pressure is None:\n101         pressure = atmosphere.alt2pres(altitude)\n102 \n103     method = method.lower()\n104     if isinstance(time, dt.datetime):\n105         time = pd.DatetimeIndex([time, ])\n106 \n107     if method == 'nrel_c':\n108         ephem_df = spa_c(time, latitude, longitude, pressure, temperature,\n109                          **kwargs)\n110     elif method == 'nrel_numba':\n111         ephem_df = spa_python(time, latitude, longitude, altitude,\n112                               pressure, temperature,\n113                               how='numba', **kwargs)\n114     elif method == 'nrel_numpy':\n115         ephem_df = spa_python(time, latitude, longitude, altitude,\n116                               pressure, temperature,\n117                               how='numpy', **kwargs)\n118     elif method == 'pyephem':\n119         ephem_df = pyephem(time, latitude, longitude,\n120                            altitude=altitude,\n121                            pressure=pressure,\n122                            temperature=temperature, **kwargs)\n123     elif method == 'ephemeris':\n124         ephem_df = ephemeris(time, latitude, longitude, pressure, temperature,\n125                              **kwargs)\n126     else:\n127         raise ValueError('Invalid solar position method')\n128 \n129     return ephem_df\n130 \n131 \n132 def spa_c(time, latitude, longitude, pressure=101325, altitude=0,\n133           temperature=12, delta_t=67.0,\n134           raw_spa_output=False):\n135     \"\"\"\n136     Calculate the solar position using the C implementation of the NREL\n137     SPA code.\n138 \n139     The source files for this code are located in './spa_c_files/', along with\n140     a README file which describes how the C code is wrapped in Python.\n141     Due to license restrictions, the C code must be downloaded seperately\n142     and used in accordance with it's license.\n143 \n144     This function is slower and no more accurate than :py:func:`spa_python`.\n145 \n146     Parameters\n147     ----------\n148     time : pandas.DatetimeIndex\n149         Must be localized or UTC will be assumed.\n150     latitude : float\n151         Latitude in decimal degrees. Positive north of equator, negative\n152         to south.\n153     longitude : float\n154         Longitude in decimal degrees. Positive east of prime meridian,\n155         negative to west.\n156     pressure : float, default 101325\n157         Pressure in Pascals\n158     altitude : float, default 0\n159         Height above sea level. [m]\n160     temperature : float, default 12\n161         Temperature in C\n162     delta_t : float, default 67.0\n163         Difference between terrestrial time and UT1.\n164         USNO has previous values and predictions.\n165     raw_spa_output : bool, default False\n166         If true, returns the raw SPA output.\n167 \n168     Returns\n169     -------\n170     DataFrame\n171         The DataFrame will have the following columns:\n172         elevation,\n173         azimuth,\n174         zenith,\n175         apparent_elevation,\n176         apparent_zenith.\n177 \n178     References\n179     ----------\n180     .. [1] NREL SPA reference:\n181        http://rredc.nrel.gov/solar/codesandalgorithms/spa/\n182        NREL SPA C files: https://midcdmz.nrel.gov/spa/\n183 \n184     Note: The ``timezone`` field in the SPA C files is replaced with\n185     ``time_zone`` to avoid a nameclash with the function ``__timezone`` that is\n186     redefined by Python>=3.5. This issue is\n187     `Python bug 24643 <https://bugs.python.org/issue24643>`_.\n188 \n189     .. [2] USNO delta T:\n190        http://www.usno.navy.mil/USNO/earth-orientation/eo-products/long-term\n191 \n192     See also\n193     --------\n194     pyephem, spa_python, ephemeris\n195     \"\"\"\n196 \n197     # Added by Rob Andrews (@Calama-Consulting), Calama Consulting, 2014\n198     # Edited by Will Holmgren (@wholmgren), University of Arizona, 2014\n199     # Edited by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n200 \n201     try:\n202         from pvlib.spa_c_files.spa_py import spa_calc\n203     except ImportError:\n204         raise ImportError('Could not import built-in SPA calculator. ' +\n205                           'You may need to recompile the SPA code.')\n206 \n207     # if localized, convert to UTC. otherwise, assume UTC.\n208     try:\n209         time_utc = time.tz_convert('UTC')\n210     except TypeError:\n211         time_utc = time\n212 \n213     spa_out = []\n214 \n215     for date in time_utc:\n216         spa_out.append(spa_calc(year=date.year,\n217                                 month=date.month,\n218                                 day=date.day,\n219                                 hour=date.hour,\n220                                 minute=date.minute,\n221                                 second=date.second,\n222                                 time_zone=0,  # date uses utc time\n223                                 latitude=latitude,\n224                                 longitude=longitude,\n225                                 elevation=altitude,\n226                                 pressure=pressure / 100,\n227                                 temperature=temperature,\n228                                 delta_t=delta_t\n229                                 ))\n230 \n231     spa_df = pd.DataFrame(spa_out, index=time)\n232 \n233     if raw_spa_output:\n234         # rename \"time_zone\" from raw output from spa_c_files.spa_py.spa_calc()\n235         # to \"timezone\" to match the API of pvlib.solarposition.spa_c()\n236         return spa_df.rename(columns={'time_zone': 'timezone'})\n237     else:\n238         dfout = pd.DataFrame({'azimuth': spa_df['azimuth'],\n239                               'apparent_zenith': spa_df['zenith'],\n240                               'apparent_elevation': spa_df['e'],\n241                               'elevation': spa_df['e0'],\n242                               'zenith': 90 - spa_df['e0']})\n243 \n244         return dfout\n245 \n246 \n247 def _spa_python_import(how):\n248     \"\"\"Compile spa.py appropriately\"\"\"\n249 \n250     from pvlib import spa\n251 \n252     # check to see if the spa module was compiled with numba\n253     using_numba = spa.USE_NUMBA\n254 \n255     if how == 'numpy' and using_numba:\n256         # the spa module was compiled to numba code, so we need to\n257         # reload the module without compiling\n258         # the PVLIB_USE_NUMBA env variable is used to tell the module\n259         # to not compile with numba\n260         warnings.warn('Reloading spa to use numpy')\n261         os.environ['PVLIB_USE_NUMBA'] = '0'\n262         spa = reload(spa)\n263         del os.environ['PVLIB_USE_NUMBA']\n264     elif how == 'numba' and not using_numba:\n265         # The spa module was not compiled to numba code, so set\n266         # PVLIB_USE_NUMBA so it does compile to numba on reload.\n267         warnings.warn('Reloading spa to use numba')\n268         os.environ['PVLIB_USE_NUMBA'] = '1'\n269         spa = reload(spa)\n270         del os.environ['PVLIB_USE_NUMBA']\n271     elif how != 'numba' and how != 'numpy':\n272         raise ValueError(\"how must be either 'numba' or 'numpy'\")\n273 \n274     return spa\n275 \n276 \n277 def spa_python(time, latitude, longitude,\n278                altitude=0, pressure=101325, temperature=12, delta_t=67.0,\n279                atmos_refract=None, how='numpy', numthreads=4):\n280     \"\"\"\n281     Calculate the solar position using a python implementation of the\n282     NREL SPA algorithm.\n283 \n284     The details of the NREL SPA algorithm are described in [1]_.\n285 \n286     If numba is installed, the functions can be compiled to\n287     machine code and the function can be multithreaded.\n288     Without numba, the function evaluates via numpy with\n289     a slight performance hit.\n290 \n291     Parameters\n292     ----------\n293     time : pandas.DatetimeIndex\n294         Must be localized or UTC will be assumed.\n295     latitude : float\n296         Latitude in decimal degrees. Positive north of equator, negative\n297         to south.\n298     longitude : float\n299         Longitude in decimal degrees. Positive east of prime meridian,\n300         negative to west.\n301     altitude : float, default 0\n302         Distance above sea level.\n303     pressure : int or float, optional, default 101325\n304         avg. yearly air pressure in Pascals.\n305     temperature : int or float, optional, default 12\n306         avg. yearly air temperature in degrees C.\n307     delta_t : float, optional, default 67.0\n308         Difference between terrestrial time and UT1.\n309         If delta_t is None, uses spa.calculate_deltat\n310         using time.year and time.month from pandas.DatetimeIndex.\n311         For most simulations the default delta_t is sufficient.\n312         *Note: delta_t = None will break code using nrel_numba,\n313         this will be fixed in a future version.*\n314         The USNO has historical and forecasted delta_t [3]_.\n315     atmos_refrac : None or float, optional, default None\n316         The approximate atmospheric refraction (in degrees)\n317         at sunrise and sunset.\n318     how : str, optional, default 'numpy'\n319         Options are 'numpy' or 'numba'. If numba >= 0.17.0\n320         is installed, how='numba' will compile the spa functions\n321         to machine code and run them multithreaded.\n322     numthreads : int, optional, default 4\n323         Number of threads to use if how == 'numba'.\n324 \n325     Returns\n326     -------\n327     DataFrame\n328         The DataFrame will have the following columns:\n329         apparent_zenith (degrees),\n330         zenith (degrees),\n331         apparent_elevation (degrees),\n332         elevation (degrees),\n333         azimuth (degrees),\n334         equation_of_time (minutes).\n335 \n336 \n337     References\n338     ----------\n339     .. [1] I. Reda and A. Andreas, Solar position algorithm for solar\n340        radiation applications. Solar Energy, vol. 76, no. 5, pp. 577-589, 2004.\n341 \n342     .. [2] I. Reda and A. Andreas, Corrigendum to Solar position algorithm for\n343        solar radiation applications. Solar Energy, vol. 81, no. 6, p. 838,\n344        2007.\n345 \n346     .. [3] USNO delta T:\n347        http://www.usno.navy.mil/USNO/earth-orientation/eo-products/long-term\n348 \n349     See also\n350     --------\n351     pyephem, spa_c, ephemeris\n352     \"\"\"\n353 \n354     # Added by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n355 \n356     lat = latitude\n357     lon = longitude\n358     elev = altitude\n359     pressure = pressure / 100  # pressure must be in millibars for calculation\n360 \n361     atmos_refract = atmos_refract or 0.5667\n362 \n363     if not isinstance(time, pd.DatetimeIndex):\n364         try:\n365             time = pd.DatetimeIndex(time)\n366         except (TypeError, ValueError):\n367             time = pd.DatetimeIndex([time, ])\n368 \n369     unixtime = np.array(time.view(np.int64)/10**9)\n370 \n371     spa = _spa_python_import(how)\n372 \n373     delta_t = delta_t or spa.calculate_deltat(time.year, time.month)\n374 \n375     app_zenith, zenith, app_elevation, elevation, azimuth, eot = \\\n376         spa.solar_position(unixtime, lat, lon, elev, pressure, temperature,\n377                            delta_t, atmos_refract, numthreads)\n378 \n379     result = pd.DataFrame({'apparent_zenith': app_zenith, 'zenith': zenith,\n380                            'apparent_elevation': app_elevation,\n381                            'elevation': elevation, 'azimuth': azimuth,\n382                            'equation_of_time': eot},\n383                           index=time)\n384 \n385     return result\n386 \n387 \n388 def sun_rise_set_transit_spa(times, latitude, longitude, how='numpy',\n389                              delta_t=67.0, numthreads=4):\n390     \"\"\"\n391     Calculate the sunrise, sunset, and sun transit times using the\n392     NREL SPA algorithm.\n393 \n394     The details of the NREL SPA algorithm are described in [1]_.\n395 \n396     If numba is installed, the functions can be compiled to\n397     machine code and the function can be multithreaded.\n398     Without numba, the function evaluates via numpy with\n399     a slight performance hit.\n400 \n401     Parameters\n402     ----------\n403     times : pandas.DatetimeIndex\n404         Must be localized to the timezone for ``latitude`` and ``longitude``.\n405     latitude : float\n406         Latitude in degrees, positive north of equator, negative to south\n407     longitude : float\n408         Longitude in degrees, positive east of prime meridian, negative to west\n409     how : str, optional, default 'numpy'\n410         Options are 'numpy' or 'numba'. If numba >= 0.17.0\n411         is installed, how='numba' will compile the spa functions\n412         to machine code and run them multithreaded.\n413     delta_t : float, optional, default 67.0\n414         Difference between terrestrial time and UT1.\n415         If delta_t is None, uses spa.calculate_deltat\n416         using times.year and times.month from pandas.DatetimeIndex.\n417         For most simulations the default delta_t is sufficient.\n418         *Note: delta_t = None will break code using nrel_numba,\n419         this will be fixed in a future version.*\n420     numthreads : int, optional, default 4\n421         Number of threads to use if how == 'numba'.\n422 \n423     Returns\n424     -------\n425     pandas.DataFrame\n426         index is the same as input `times` argument\n427         columns are 'sunrise', 'sunset', and 'transit'\n428 \n429     References\n430     ----------\n431     .. [1] Reda, I., Andreas, A., 2003. Solar position algorithm for solar\n432        radiation applications. Technical report: NREL/TP-560- 34302. Golden,\n433        USA, http://www.nrel.gov.\n434     \"\"\"\n435     # Added by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n436 \n437     lat = latitude\n438     lon = longitude\n439 \n440     # times must be localized\n441     if times.tz:\n442         tzinfo = times.tz\n443     else:\n444         raise ValueError('times must be localized')\n445 \n446     # must convert to midnight UTC on day of interest\n447     utcday = pd.DatetimeIndex(times.date).tz_localize('UTC')\n448     unixtime = np.array(utcday.view(np.int64)/10**9)\n449 \n450     spa = _spa_python_import(how)\n451 \n452     delta_t = delta_t or spa.calculate_deltat(times.year, times.month)\n453 \n454     transit, sunrise, sunset = spa.transit_sunrise_sunset(\n455         unixtime, lat, lon, delta_t, numthreads)\n456 \n457     # arrays are in seconds since epoch format, need to conver to timestamps\n458     transit = pd.to_datetime(transit*1e9, unit='ns', utc=True).tz_convert(\n459         tzinfo).tolist()\n460     sunrise = pd.to_datetime(sunrise*1e9, unit='ns', utc=True).tz_convert(\n461         tzinfo).tolist()\n462     sunset = pd.to_datetime(sunset*1e9, unit='ns', utc=True).tz_convert(\n463         tzinfo).tolist()\n464 \n465     return pd.DataFrame(index=times, data={'sunrise': sunrise,\n466                                            'sunset': sunset,\n467                                            'transit': transit})\n468 \n469 \n470 def _ephem_convert_to_seconds_and_microseconds(date):\n471     # utility from unreleased PyEphem 3.6.7.1\n472     \"\"\"Converts a PyEphem date into seconds\"\"\"\n473     microseconds = int(round(24 * 60 * 60 * 1000000 * date))\n474     seconds, microseconds = divmod(microseconds, 1000000)\n475     seconds -= 2209032000  # difference between epoch 1900 and epoch 1970\n476     return seconds, microseconds\n477 \n478 \n479 def _ephem_to_timezone(date, tzinfo):\n480     # utility from unreleased PyEphem 3.6.7.1\n481     \"\"\"\"Convert a PyEphem Date into a timezone aware python datetime\"\"\"\n482     seconds, microseconds = _ephem_convert_to_seconds_and_microseconds(date)\n483     date = dt.datetime.fromtimestamp(seconds, tzinfo)\n484     date = date.replace(microsecond=microseconds)\n485     return date\n486 \n487 \n488 def _ephem_setup(latitude, longitude, altitude, pressure, temperature,\n489                  horizon):\n490     import ephem\n491     # initialize a PyEphem observer\n492     obs = ephem.Observer()\n493     obs.lat = str(latitude)\n494     obs.lon = str(longitude)\n495     obs.elevation = altitude\n496     obs.pressure = pressure / 100.  # convert to mBar\n497     obs.temp = temperature\n498     obs.horizon = horizon\n499 \n500     # the PyEphem sun\n501     sun = ephem.Sun()\n502     return obs, sun\n503 \n504 \n505 def sun_rise_set_transit_ephem(times, latitude, longitude,\n506                                next_or_previous='next',\n507                                altitude=0,\n508                                pressure=101325,\n509                                temperature=12, horizon='0:00'):\n510     \"\"\"\n511     Calculate the next sunrise and sunset times using the PyEphem package.\n512 \n513     Parameters\n514     ----------\n515     time : pandas.DatetimeIndex\n516         Must be localized\n517     latitude : float\n518         Latitude in degrees, positive north of equator, negative to south\n519     longitude : float\n520         Longitude in degrees, positive east of prime meridian, negative to west\n521     next_or_previous : str\n522         'next' or 'previous' sunrise and sunset relative to time\n523     altitude : float, default 0\n524         distance above sea level in meters.\n525     pressure : int or float, optional, default 101325\n526         air pressure in Pascals.\n527     temperature : int or float, optional, default 12\n528         air temperature in degrees C.\n529     horizon : string, format +/-X:YY\n530         arc degrees:arc minutes from geometrical horizon for sunrise and\n531         sunset, e.g., horizon='+0:00' to use sun center crossing the\n532         geometrical horizon to define sunrise and sunset,\n533         horizon='-0:34' for when the sun's upper edge crosses the\n534         geometrical horizon\n535 \n536     Returns\n537     -------\n538     pandas.DataFrame\n539         index is the same as input `time` argument\n540         columns are 'sunrise', 'sunset', and 'transit'\n541 \n542     See also\n543     --------\n544     pyephem\n545     \"\"\"\n546 \n547     try:\n548         import ephem\n549     except ImportError:\n550         raise ImportError('PyEphem must be installed')\n551 \n552     # times must be localized\n553     if times.tz:\n554         tzinfo = times.tz\n555     else:\n556         raise ValueError('times must be localized')\n557 \n558     obs, sun = _ephem_setup(latitude, longitude, altitude,\n559                             pressure, temperature, horizon)\n560     # create lists of sunrise and sunset time localized to time.tz\n561     if next_or_previous.lower() == 'next':\n562         rising = obs.next_rising\n563         setting = obs.next_setting\n564         transit = obs.next_transit\n565     elif next_or_previous.lower() == 'previous':\n566         rising = obs.previous_rising\n567         setting = obs.previous_setting\n568         transit = obs.previous_transit\n569     else:\n570         raise ValueError(\"next_or_previous must be either 'next' or\" +\n571                          \" 'previous'\")\n572 \n573     sunrise = []\n574     sunset = []\n575     trans = []\n576     for thetime in times:\n577         thetime = thetime.to_pydatetime()\n578         # older versions of pyephem ignore timezone when converting to its\n579         # internal datetime format, so convert to UTC here to support\n580         # all versions.  GH #1449\n581         obs.date = ephem.Date(thetime.astimezone(datetime.timezone.utc))\n582         sunrise.append(_ephem_to_timezone(rising(sun), tzinfo))\n583         sunset.append(_ephem_to_timezone(setting(sun), tzinfo))\n584         trans.append(_ephem_to_timezone(transit(sun), tzinfo))\n585 \n586     return pd.DataFrame(index=times, data={'sunrise': sunrise,\n587                                            'sunset': sunset,\n588                                            'transit': trans})\n589 \n590 \n591 def pyephem(time, latitude, longitude, altitude=0, pressure=101325,\n592             temperature=12, horizon='+0:00'):\n593     \"\"\"\n594     Calculate the solar position using the PyEphem package.\n595 \n596     Parameters\n597     ----------\n598     time : pandas.DatetimeIndex\n599         Must be localized or UTC will be assumed.\n600     latitude : float\n601         Latitude in decimal degrees. Positive north of equator, negative\n602         to south.\n603     longitude : float\n604         Longitude in decimal degrees. Positive east of prime meridian,\n605         negative to west.\n606     altitude : float, default 0\n607         Height above sea level in meters. [m]\n608     pressure : int or float, optional, default 101325\n609         air pressure in Pascals.\n610     temperature : int or float, optional, default 12\n611         air temperature in degrees C.\n612     horizon : string, optional, default '+0:00'\n613         arc degrees:arc minutes from geometrical horizon for sunrise and\n614         sunset, e.g., horizon='+0:00' to use sun center crossing the\n615         geometrical horizon to define sunrise and sunset,\n616         horizon='-0:34' for when the sun's upper edge crosses the\n617         geometrical horizon\n618 \n619     Returns\n620     -------\n621     pandas.DataFrame\n622         index is the same as input `time` argument\n623         The DataFrame will have the following columns:\n624         apparent_elevation, elevation,\n625         apparent_azimuth, azimuth,\n626         apparent_zenith, zenith.\n627 \n628     See also\n629     --------\n630     spa_python, spa_c, ephemeris\n631     \"\"\"\n632 \n633     # Written by Will Holmgren (@wholmgren), University of Arizona, 2014\n634     try:\n635         import ephem\n636     except ImportError:\n637         raise ImportError('PyEphem must be installed')\n638 \n639     # if localized, convert to UTC. otherwise, assume UTC.\n640     try:\n641         time_utc = time.tz_convert('UTC')\n642     except TypeError:\n643         time_utc = time\n644 \n645     sun_coords = pd.DataFrame(index=time)\n646 \n647     obs, sun = _ephem_setup(latitude, longitude, altitude,\n648                             pressure, temperature, horizon)\n649 \n650     # make and fill lists of the sun's altitude and azimuth\n651     # this is the pressure and temperature corrected apparent alt/az.\n652     alts = []\n653     azis = []\n654     for thetime in time_utc:\n655         obs.date = ephem.Date(thetime)\n656         sun.compute(obs)\n657         alts.append(sun.alt)\n658         azis.append(sun.az)\n659 \n660     sun_coords['apparent_elevation'] = alts\n661     sun_coords['apparent_azimuth'] = azis\n662 \n663     # redo it for p=0 to get no atmosphere alt/az\n664     obs.pressure = 0\n665     alts = []\n666     azis = []\n667     for thetime in time_utc:\n668         obs.date = ephem.Date(thetime)\n669         sun.compute(obs)\n670         alts.append(sun.alt)\n671         azis.append(sun.az)\n672 \n673     sun_coords['elevation'] = alts\n674     sun_coords['azimuth'] = azis\n675 \n676     # convert to degrees. add zenith\n677     sun_coords = np.rad2deg(sun_coords)\n678     sun_coords['apparent_zenith'] = 90 - sun_coords['apparent_elevation']\n679     sun_coords['zenith'] = 90 - sun_coords['elevation']\n680 \n681     return sun_coords\n682 \n683 \n684 def ephemeris(time, latitude, longitude, pressure=101325, temperature=12):\n685     \"\"\"\n686     Python-native solar position calculator.\n687     The accuracy of this code is not guaranteed.\n688     Consider using the built-in spa_c code or the PyEphem library.\n689 \n690     Parameters\n691     ----------\n692     time : pandas.DatetimeIndex\n693         Must be localized or UTC will be assumed.\n694     latitude : float\n695         Latitude in decimal degrees. Positive north of equator, negative\n696         to south.\n697     longitude : float\n698         Longitude in decimal degrees. Positive east of prime meridian,\n699         negative to west.\n700     pressure : float or Series, default 101325\n701         Ambient pressure (Pascals)\n702     temperature : float or Series, default 12\n703         Ambient temperature (C)\n704 \n705     Returns\n706     -------\n707 \n708     DataFrame with the following columns:\n709 \n710         * apparent_elevation : apparent sun elevation accounting for\n711           atmospheric refraction.\n712         * elevation : actual elevation (not accounting for refraction)\n713           of the sun in decimal degrees, 0 = on horizon.\n714           The complement of the zenith angle.\n715         * azimuth : Azimuth of the sun in decimal degrees East of North.\n716           This is the complement of the apparent zenith angle.\n717         * apparent_zenith : apparent sun zenith accounting for atmospheric\n718           refraction.\n719         * zenith : Solar zenith angle\n720         * solar_time : Solar time in decimal hours (solar noon is 12.00).\n721 \n722     References\n723     -----------\n724 \n725     .. [1] Grover Hughes' class and related class materials on Engineering\n726        Astronomy at Sandia National Laboratories, 1985.\n727 \n728     See also\n729     --------\n730     pyephem, spa_c, spa_python\n731 \n732     \"\"\"\n733 \n734     # Added by Rob Andrews (@Calama-Consulting), Calama Consulting, 2014\n735     # Edited by Will Holmgren (@wholmgren), University of Arizona, 2014\n736 \n737     # Most comments in this function are from PVLIB_MATLAB or from\n738     # pvlib-python's attempt to understand and fix problems with the\n739     # algorithm. The comments are *not* based on the reference material.\n740     # This helps a little bit:\n741     # http://www.cv.nrao.edu/~rfisher/Ephemerides/times.html\n742 \n743     # the inversion of longitude is due to the fact that this code was\n744     # originally written for the convention that positive longitude were for\n745     # locations west of the prime meridian. However, the correct convention (as\n746     # of 2009) is to use negative longitudes for locations west of the prime\n747     # meridian. Therefore, the user should input longitude values under the\n748     # correct convention (e.g. Albuquerque is at -106 longitude), but it needs\n749     # to be inverted for use in the code.\n750 \n751     Latitude = latitude\n752     Longitude = -1 * longitude\n753 \n754     Abber = 20 / 3600.\n755     LatR = np.radians(Latitude)\n756 \n757     # the SPA algorithm needs time to be expressed in terms of\n758     # decimal UTC hours of the day of the year.\n759 \n760     # if localized, convert to UTC. otherwise, assume UTC.\n761     try:\n762         time_utc = time.tz_convert('UTC')\n763     except TypeError:\n764         time_utc = time\n765 \n766     # strip out the day of the year and calculate the decimal hour\n767     DayOfYear = time_utc.dayofyear\n768     DecHours = (time_utc.hour + time_utc.minute/60. + time_utc.second/3600. +\n769                 time_utc.microsecond/3600.e6)\n770 \n771     # np.array needed for pandas > 0.20\n772     UnivDate = np.array(DayOfYear)\n773     UnivHr = np.array(DecHours)\n774 \n775     Yr = np.array(time_utc.year) - 1900\n776     YrBegin = 365 * Yr + np.floor((Yr - 1) / 4.) - 0.5\n777 \n778     Ezero = YrBegin + UnivDate\n779     T = Ezero / 36525.\n780 \n781     # Calculate Greenwich Mean Sidereal Time (GMST)\n782     GMST0 = 6 / 24. + 38 / 1440. + (\n783         45.836 + 8640184.542 * T + 0.0929 * T ** 2) / 86400.\n784     GMST0 = 360 * (GMST0 - np.floor(GMST0))\n785     GMSTi = np.mod(GMST0 + 360 * (1.0027379093 * UnivHr / 24.), 360)\n786 \n787     # Local apparent sidereal time\n788     LocAST = np.mod((360 + GMSTi - Longitude), 360)\n789 \n790     EpochDate = Ezero + UnivHr / 24.\n791     T1 = EpochDate / 36525.\n792 \n793     ObliquityR = np.radians(\n794         23.452294 - 0.0130125 * T1 - 1.64e-06 * T1 ** 2 + 5.03e-07 * T1 ** 3)\n795     MlPerigee = 281.22083 + 4.70684e-05 * EpochDate + 0.000453 * T1 ** 2 + (\n796         3e-06 * T1 ** 3)\n797     MeanAnom = np.mod((358.47583 + 0.985600267 * EpochDate - 0.00015 *\n798                        T1 ** 2 - 3e-06 * T1 ** 3), 360)\n799     Eccen = 0.01675104 - 4.18e-05 * T1 - 1.26e-07 * T1 ** 2\n800     EccenAnom = MeanAnom\n801     E = 0\n802 \n803     while np.max(abs(EccenAnom - E)) > 0.0001:\n804         E = EccenAnom\n805         EccenAnom = MeanAnom + np.degrees(Eccen)*np.sin(np.radians(E))\n806 \n807     TrueAnom = (\n808         2 * np.mod(np.degrees(np.arctan2(((1 + Eccen) / (1 - Eccen)) ** 0.5 *\n809                    np.tan(np.radians(EccenAnom) / 2.), 1)), 360))\n810     EcLon = np.mod(MlPerigee + TrueAnom, 360) - Abber\n811     EcLonR = np.radians(EcLon)\n812     DecR = np.arcsin(np.sin(ObliquityR)*np.sin(EcLonR))\n813 \n814     RtAscen = np.degrees(np.arctan2(np.cos(ObliquityR)*np.sin(EcLonR),\n815                                     np.cos(EcLonR)))\n816 \n817     HrAngle = LocAST - RtAscen\n818     HrAngleR = np.radians(HrAngle)\n819     HrAngle = HrAngle - (360 * (abs(HrAngle) > 180))\n820 \n821     SunAz = np.degrees(np.arctan2(-np.sin(HrAngleR),\n822                                   np.cos(LatR)*np.tan(DecR) -\n823                                   np.sin(LatR)*np.cos(HrAngleR)))\n824     SunAz[SunAz < 0] += 360\n825 \n826     SunEl = np.degrees(np.arcsin(\n827         np.cos(LatR) * np.cos(DecR) * np.cos(HrAngleR) +\n828         np.sin(LatR) * np.sin(DecR)))\n829 \n830     SolarTime = (180 + HrAngle) / 15.\n831 \n832     # Calculate refraction correction\n833     Elevation = SunEl\n834     TanEl = pd.Series(np.tan(np.radians(Elevation)), index=time_utc)\n835     Refract = pd.Series(0, index=time_utc)\n836 \n837     Refract[(Elevation > 5) & (Elevation <= 85)] = (\n838         58.1/TanEl - 0.07/(TanEl**3) + 8.6e-05/(TanEl**5))\n839 \n840     Refract[(Elevation > -0.575) & (Elevation <= 5)] = (\n841         Elevation *\n842         (-518.2 + Elevation*(103.4 + Elevation*(-12.79 + Elevation*0.711))) +\n843         1735)\n844 \n845     Refract[(Elevation > -1) & (Elevation <= -0.575)] = -20.774 / TanEl\n846 \n847     Refract *= (283/(273. + temperature)) * (pressure/101325.) / 3600.\n848 \n849     ApparentSunEl = SunEl + Refract\n850 \n851     # make output DataFrame\n852     DFOut = pd.DataFrame(index=time_utc)\n853     DFOut['apparent_elevation'] = ApparentSunEl\n854     DFOut['elevation'] = SunEl\n855     DFOut['azimuth'] = SunAz\n856     DFOut['apparent_zenith'] = 90 - ApparentSunEl\n857     DFOut['zenith'] = 90 - SunEl\n858     DFOut['solar_time'] = SolarTime\n859     DFOut.index = time\n860 \n861     return DFOut\n862 \n863 \n864 def calc_time(lower_bound, upper_bound, latitude, longitude, attribute, value,\n865               altitude=0, pressure=101325, temperature=12, horizon='+0:00',\n866               xtol=1.0e-12):\n867     \"\"\"\n868     Calculate the time between lower_bound and upper_bound\n869     where the attribute is equal to value. Uses PyEphem for\n870     solar position calculations.\n871 \n872     Parameters\n873     ----------\n874     lower_bound : datetime.datetime\n875     upper_bound : datetime.datetime\n876     latitude : float\n877         Latitude in decimal degrees. Positive north of equator, negative\n878         to south.\n879     longitude : float\n880         Longitude in decimal degrees. Positive east of prime meridian,\n881         negative to west.\n882     attribute : str\n883         The attribute of a pyephem.Sun object that\n884         you want to solve for. Likely options are 'alt'\n885         and 'az' (which must be given in radians).\n886     value : int or float\n887         The value of the attribute to solve for\n888     altitude : float, default 0\n889         Distance above sea level.\n890     pressure : int or float, optional, default 101325\n891         Air pressure in Pascals. Set to 0 for no\n892         atmospheric correction.\n893     temperature : int or float, optional, default 12\n894         Air temperature in degrees C.\n895     horizon : string, optional, default '+0:00'\n896         arc degrees:arc minutes from geometrical horizon for sunrise and\n897         sunset, e.g., horizon='+0:00' to use sun center crossing the\n898         geometrical horizon to define sunrise and sunset,\n899         horizon='-0:34' for when the sun's upper edge crosses the\n900         geometrical horizon\n901     xtol : float, optional, default 1.0e-12\n902         The allowed error in the result from value\n903 \n904     Returns\n905     -------\n906     datetime.datetime\n907 \n908     Raises\n909     ------\n910     ValueError\n911         If the value is not contained between the bounds.\n912     AttributeError\n913         If the given attribute is not an attribute of a\n914         PyEphem.Sun object.\n915     \"\"\"\n916     obs, sun = _ephem_setup(latitude, longitude, altitude,\n917                             pressure, temperature, horizon)\n918 \n919     def compute_attr(thetime, target, attr):\n920         obs.date = thetime\n921         sun.compute(obs)\n922         return getattr(sun, attr) - target\n923 \n924     lb = datetime_to_djd(lower_bound)\n925     ub = datetime_to_djd(upper_bound)\n926 \n927     djd_root = so.brentq(compute_attr, lb, ub,\n928                          (value, attribute), xtol=xtol)\n929 \n930     return djd_to_datetime(djd_root)\n931 \n932 \n933 def pyephem_earthsun_distance(time):\n934     \"\"\"\n935     Calculates the distance from the earth to the sun using pyephem.\n936 \n937     Parameters\n938     ----------\n939     time : pandas.DatetimeIndex\n940         Must be localized or UTC will be assumed.\n941 \n942     Returns\n943     -------\n944     pd.Series. Earth-sun distance in AU.\n945     \"\"\"\n946 \n947     import ephem\n948 \n949     sun = ephem.Sun()\n950     earthsun = []\n951     for thetime in time:\n952         sun.compute(ephem.Date(thetime))\n953         earthsun.append(sun.earth_distance)\n954 \n955     return pd.Series(earthsun, index=time)\n956 \n957 \n958 def nrel_earthsun_distance(time, how='numpy', delta_t=67.0, numthreads=4):\n959     \"\"\"\n960     Calculates the distance from the earth to the sun using the\n961     NREL SPA algorithm.\n962 \n963     The details of the NREL SPA algorithm are described in [1]_.\n964 \n965     Parameters\n966     ----------\n967     time : pandas.DatetimeIndex\n968         Must be localized or UTC will be assumed.\n969 \n970     how : str, optional, default 'numpy'\n971         Options are 'numpy' or 'numba'. If numba >= 0.17.0\n972         is installed, how='numba' will compile the spa functions\n973         to machine code and run them multithreaded.\n974 \n975     delta_t : float, optional, default 67.0\n976         Difference between terrestrial time and UT1.\n977         If delta_t is None, uses spa.calculate_deltat\n978         using time.year and time.month from pandas.DatetimeIndex.\n979         For most simulations the default delta_t is sufficient.\n980         *Note: delta_t = None will break code using nrel_numba,\n981         this will be fixed in a future version.*\n982 \n983     numthreads : int, optional, default 4\n984         Number of threads to use if how == 'numba'.\n985 \n986     Returns\n987     -------\n988     dist : pd.Series\n989         Earth-sun distance in AU.\n990 \n991     References\n992     ----------\n993     .. [1] Reda, I., Andreas, A., 2003. Solar position algorithm for solar\n994        radiation applications. Technical report: NREL/TP-560- 34302. Golden,\n995        USA, http://www.nrel.gov.\n996     \"\"\"\n997 \n998     if not isinstance(time, pd.DatetimeIndex):\n999         try:\n1000             time = pd.DatetimeIndex(time)\n1001         except (TypeError, ValueError):\n1002             time = pd.DatetimeIndex([time, ])\n1003 \n1004     unixtime = np.array(time.view(np.int64)/10**9)\n1005 \n1006     spa = _spa_python_import(how)\n1007 \n1008     delta_t = delta_t or spa.calculate_deltat(time.year, time.month)\n1009 \n1010     dist = spa.earthsun_distance(unixtime, delta_t, numthreads)\n1011 \n1012     dist = pd.Series(dist, index=time)\n1013 \n1014     return dist\n1015 \n1016 \n1017 def _calculate_simple_day_angle(dayofyear, offset=1):\n1018     \"\"\"\n1019     Calculates the day angle for the Earth's orbit around the Sun.\n1020 \n1021     Parameters\n1022     ----------\n1023     dayofyear : numeric\n1024     offset : int, default 1\n1025         For the Spencer method, offset=1; for the ASCE method, offset=0\n1026 \n1027     Returns\n1028     -------\n1029     day_angle : numeric\n1030     \"\"\"\n1031     return (2. * np.pi / 365.) * (dayofyear - offset)\n1032 \n1033 \n1034 def equation_of_time_spencer71(dayofyear):\n1035     \"\"\"\n1036     Equation of time from Duffie & Beckman and attributed to Spencer\n1037     (1971) and Iqbal (1983).\n1038 \n1039     The coefficients correspond to the online copy of the `Fourier\n1040     paper`_ [1]_ in the Sundial Mailing list that was posted in 1998 by\n1041     Mac Oglesby from his correspondence with Macquarie University Prof.\n1042     John Pickard who added the following note.\n1043 \n1044         In the early 1970s, I contacted Dr Spencer about this method because I\n1045         was trying to use a hand calculator for calculating solar positions,\n1046         etc. He was extremely helpful and gave me a reprint of this paper. He\n1047         also pointed out an error in the original: in the series for E, the\n1048         constant was printed as 0.000075 rather than 0.0000075. I have\n1049         corrected the error in this version.\n1050 \n1051     There appears to be another error in formula as printed in both\n1052     Duffie & Beckman's [2]_ and Frank Vignola's [3]_ books in which the\n1053     coefficient 0.04089 is printed instead of 0.040849, corresponding to\n1054     the value used in the Bird Clear Sky model implemented by Daryl\n1055     Myers [4]_ and printed in both the Fourier paper from the Sundial\n1056     Mailing List and R. Hulstrom's [5]_ book.\n1057 \n1058     .. _Fourier paper: http://www.mail-archive.com/sundial@uni-koeln.de/msg01050.html\n1059 \n1060     Parameters\n1061     ----------\n1062     dayofyear : numeric\n1063 \n1064     Returns\n1065     -------\n1066     equation_of_time : numeric\n1067         Difference in time between solar time and mean solar time in minutes.\n1068 \n1069     References\n1070     ----------\n1071     .. [1] J. W. Spencer, \"Fourier series representation of the position of the\n1072        sun\" in Search 2 (5), p. 172 (1971)\n1073 \n1074     .. [2] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1075        Processes, 3rd Edition\" pp. 9-11, J. Wiley and Sons, New York (2006)\n1076 \n1077     .. [3] Frank Vignola et al., \"Solar And Infrared Radiation Measurements\",\n1078        p. 13, CRC Press (2012)\n1079 \n1080     .. [4] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n1081        Energy Applications\", p. 5 CRC Press (2013)\n1082 \n1083     .. [5] Roland Hulstrom, \"Solar Resources\" p. 66, MIT Press (1989)\n1084 \n1085     See Also\n1086     --------\n1087     equation_of_time_pvcdrom\n1088     \"\"\"\n1089     day_angle = _calculate_simple_day_angle(dayofyear)\n1090     # convert from radians to minutes per day = 24[h/day] * 60[min/h] / 2 / pi\n1091     eot = (1440.0 / 2 / np.pi) * (\n1092         0.0000075 +\n1093         0.001868 * np.cos(day_angle) - 0.032077 * np.sin(day_angle) -\n1094         0.014615 * np.cos(2.0 * day_angle) - 0.040849 * np.sin(2.0 * day_angle)\n1095     )\n1096     return eot\n1097 \n1098 \n1099 def equation_of_time_pvcdrom(dayofyear):\n1100     \"\"\"\n1101     Equation of time from PVCDROM.\n1102 \n1103     `PVCDROM`_ is a website by Solar Power Lab at Arizona State\n1104     University (ASU)\n1105 \n1106     .. _PVCDROM: http://www.pveducation.org/pvcdrom/2-properties-sunlight/solar-time\n1107 \n1108     Parameters\n1109     ----------\n1110     dayofyear : numeric\n1111 \n1112     Returns\n1113     -------\n1114     equation_of_time : numeric\n1115         Difference in time between solar time and mean solar time in minutes.\n1116 \n1117     References\n1118     ----------\n1119     .. [1] Soteris A. Kalogirou, \"Solar Energy Engineering Processes and\n1120        Systems, 2nd Edition\" Elselvier/Academic Press (2009).\n1121 \n1122     See Also\n1123     --------\n1124     equation_of_time_spencer71\n1125     \"\"\"\n1126     # day angle relative to Vernal Equinox, typically March 22 (day number 81)\n1127     bday = \\\n1128         _calculate_simple_day_angle(dayofyear) - (2.0 * np.pi / 365.0) * 80.0\n1129     # same value but about 2x faster than Spencer (1971)\n1130     return 9.87 * np.sin(2.0 * bday) - 7.53 * np.cos(bday) - 1.5 * np.sin(bday)\n1131 \n1132 \n1133 def declination_spencer71(dayofyear):\n1134     \"\"\"\n1135     Solar declination from Duffie & Beckman and attributed to\n1136     Spencer (1971) and Iqbal (1983).\n1137 \n1138     See [1]_ for details.\n1139 \n1140     .. warning::\n1141         Return units are radians, not degrees.\n1142 \n1143     Parameters\n1144     ----------\n1145     dayofyear : numeric\n1146 \n1147     Returns\n1148     -------\n1149     declination (radians) : numeric\n1150         Angular position of the sun at solar noon relative to the plane of the\n1151         equator, approximately between +/-23.45 (degrees).\n1152 \n1153     References\n1154     ----------\n1155     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1156        Processes, 3rd Edition\" pp. 13-14, J. Wiley and Sons, New York (2006)\n1157 \n1158     .. [2] J. W. Spencer, \"Fourier series representation of the position of the\n1159        sun\" in Search 2 (5), p. 172 (1971)\n1160 \n1161     .. [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n1162        Energy Applications\", p. 4 CRC Press (2013)\n1163 \n1164     See Also\n1165     --------\n1166     declination_cooper69\n1167     \"\"\"\n1168     day_angle = _calculate_simple_day_angle(dayofyear)\n1169     return (\n1170         0.006918 -\n1171         0.399912 * np.cos(day_angle) + 0.070257 * np.sin(day_angle) -\n1172         0.006758 * np.cos(2. * day_angle) + 0.000907 * np.sin(2. * day_angle) -\n1173         0.002697 * np.cos(3. * day_angle) + 0.00148 * np.sin(3. * day_angle)\n1174     )\n1175 \n1176 \n1177 def declination_cooper69(dayofyear):\n1178     \"\"\"\n1179     Solar declination from Duffie & Beckman and attributed to Cooper (1969).\n1180 \n1181     See [1]_ for details.\n1182 \n1183     .. warning::\n1184         Return units are radians, not degrees.\n1185 \n1186     Declination can be expressed using either sine or cosine:\n1187 \n1188     .. math::\n1189 \n1190        \\\\delta = 23.45 \\\\sin \\\\left( \\\\frac{2 \\\\pi}{365} \\\\left(n_{day} + 284\n1191        \\\\right) \\\\right) = -23.45 \\\\cos \\\\left( \\\\frac{2 \\\\pi}{365}\n1192        \\\\left(n_{day} + 10 \\\\right) \\\\right)\n1193 \n1194     Parameters\n1195     ----------\n1196     dayofyear : numeric\n1197 \n1198     Returns\n1199     -------\n1200     declination (radians) : numeric\n1201         Angular position of the sun at solar noon relative to the plane of the\n1202         equator, approximately between +/-23.45 (degrees).\n1203 \n1204     References\n1205     ----------\n1206     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1207        Processes, 3rd Edition\" pp. 13-14, J. Wiley and Sons, New York (2006)\n1208 \n1209     .. [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and Physics\"\n1210        p. 129, J. Wiley (1998)\n1211 \n1212     .. [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n1213        Energy Applications\", p. 4 CRC Press (2013)\n1214 \n1215     See Also\n1216     --------\n1217     declination_spencer71\n1218     \"\"\"\n1219     day_angle = _calculate_simple_day_angle(dayofyear)\n1220     dec = np.deg2rad(23.45 * np.sin(day_angle + (2.0 * np.pi / 365.0) * 285.0))\n1221     return dec\n1222 \n1223 \n1224 def solar_azimuth_analytical(latitude, hourangle, declination, zenith):\n1225     \"\"\"\n1226     Analytical expression of solar azimuth angle based on spherical\n1227     trigonometry.\n1228 \n1229     Parameters\n1230     ----------\n1231     latitude : numeric\n1232         Latitude of location in radians.\n1233     hourangle : numeric\n1234         Hour angle in the local solar time in radians.\n1235     declination : numeric\n1236         Declination of the sun in radians.\n1237     zenith : numeric\n1238         Solar zenith angle in radians.\n1239 \n1240     Returns\n1241     -------\n1242     azimuth : numeric\n1243         Solar azimuth angle in radians.\n1244 \n1245     References\n1246     ----------\n1247     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1248        Processes, 3rd Edition\" pp. 14, J. Wiley and Sons, New York (2006)\n1249 \n1250     .. [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and Physics\"\n1251        p. 132, J. Wiley (1998)\n1252 \n1253     .. [3] `Wikipedia: Solar Azimuth Angle\n1254        <https://en.wikipedia.org/wiki/Solar_azimuth_angle>`_\n1255 \n1256     .. [4] `PVCDROM: Azimuth Angle <http://www.pveducation.org/pvcdrom/2-\n1257        properties-sunlight/azimuth-angle>`_\n1258 \n1259     See Also\n1260     --------\n1261     declination_spencer71\n1262     declination_cooper69\n1263     hour_angle\n1264     solar_zenith_analytical\n1265     \"\"\"\n1266 \n1267     numer = (np.cos(zenith) * np.sin(latitude) - np.sin(declination))\n1268     denom = (np.sin(zenith) * np.cos(latitude))\n1269 \n1270     # cases that would generate new NaN values are safely ignored here\n1271     # since they are dealt with further below\n1272     with np.errstate(invalid='ignore', divide='ignore'):\n1273         cos_azi = numer / denom\n1274 \n1275     # when zero division occurs, use the limit value of the analytical\n1276     # expression\n1277     cos_azi = \\\n1278         np.where(np.isclose(denom,    0.0, rtol=0.0, atol=1e-8),  1.0, cos_azi)\n1279 \n1280     # when too many round-ups in floating point math take cos_azi beyond\n1281     # 1.0, use 1.0\n1282     cos_azi = \\\n1283         np.where(np.isclose(cos_azi,  1.0, rtol=0.0, atol=1e-8),  1.0, cos_azi)\n1284     cos_azi = \\\n1285         np.where(np.isclose(cos_azi, -1.0, rtol=0.0, atol=1e-8), -1.0, cos_azi)\n1286 \n1287     # when NaN values occur in input, ignore and pass to output\n1288     with np.errstate(invalid='ignore'):\n1289         sign_ha = np.sign(hourangle)\n1290 \n1291     return sign_ha * np.arccos(cos_azi) + np.pi\n1292 \n1293 \n1294 def solar_zenith_analytical(latitude, hourangle, declination):\n1295     \"\"\"\n1296     Analytical expression of solar zenith angle based on spherical\n1297     trigonometry.\n1298 \n1299     .. warning:: The analytic form neglects the effect of atmospheric\n1300         refraction.\n1301 \n1302     Parameters\n1303     ----------\n1304     latitude : numeric\n1305         Latitude of location in radians.\n1306     hourangle : numeric\n1307         Hour angle in the local solar time in radians.\n1308     declination : numeric\n1309         Declination of the sun in radians.\n1310 \n1311     Returns\n1312     -------\n1313     zenith : numeric\n1314         Solar zenith angle in radians.\n1315 \n1316     References\n1317     ----------\n1318     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1319        Processes, 3rd Edition\" pp. 14, J. Wiley and Sons, New York (2006)\n1320 \n1321     .. [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and\n1322        Physics\" p. 132, J. Wiley (1998)\n1323 \n1324     .. [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for\n1325        Renewable Energy Applications\", p. 5 CRC Press (2013)\n1326 \n1327     .. [4] `Wikipedia: Solar Zenith Angle\n1328        <https://en.wikipedia.org/wiki/Solar_zenith_angle>`_\n1329 \n1330     .. [5] `PVCDROM: Sun's Position\n1331        <http://www.pveducation.org/pvcdrom/2-properties-sunlight/\n1332        suns-position>`_\n1333 \n1334     See Also\n1335     --------\n1336     declination_spencer71\n1337     declination_cooper69\n1338     hour_angle\n1339     \"\"\"\n1340     return np.arccos(\n1341         np.cos(declination) * np.cos(latitude) * np.cos(hourangle) +\n1342         np.sin(declination) * np.sin(latitude)\n1343     )\n1344 \n1345 \n1346 def hour_angle(times, longitude, equation_of_time):\n1347     \"\"\"\n1348     Hour angle in local solar time. Zero at local solar noon.\n1349 \n1350     Parameters\n1351     ----------\n1352     times : :class:`pandas.DatetimeIndex`\n1353         Corresponding timestamps, must be localized to the timezone for the\n1354         ``longitude``.\n1355     longitude : numeric\n1356         Longitude in degrees\n1357     equation_of_time : numeric\n1358         Equation of time in minutes.\n1359 \n1360     Returns\n1361     -------\n1362     hour_angle : numeric\n1363         Hour angle in local solar time in degrees.\n1364 \n1365     References\n1366     ----------\n1367     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1368        Processes, 3rd Edition\" pp. 13, J. Wiley and Sons, New York (2006)\n1369 \n1370     .. [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and Physics\"\n1371        p. 132, J. Wiley (1998)\n1372 \n1373     .. [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n1374        Energy Applications\", p. 5 CRC Press (2013)\n1375 \n1376     See Also\n1377     --------\n1378     equation_of_time_spencer71\n1379     equation_of_time_pvcdrom\n1380     \"\"\"\n1381     naive_times = times.tz_localize(None)  # naive but still localized\n1382     # hours - timezone = (times - normalized_times) - (naive_times - times)\n1383     hrs_minus_tzs = 1 / NS_PER_HR * (\n1384         2 * times.view(np.int64) - times.normalize().view(np.int64) -\n1385         naive_times.view(np.int64))\n1386     # ensure array return instead of a version-dependent pandas <T>Index\n1387     return np.asarray(\n1388         15. * (hrs_minus_tzs - 12.) + longitude + equation_of_time / 4.)\n1389 \n1390 \n1391 def _hour_angle_to_hours(times, hourangle, longitude, equation_of_time):\n1392     \"\"\"converts hour angles in degrees to hours as a numpy array\"\"\"\n1393     naive_times = times.tz_localize(None)  # naive but still localized\n1394     tzs = 1 / NS_PER_HR * (\n1395         naive_times.view(np.int64) - times.view(np.int64))\n1396     hours = (hourangle - longitude - equation_of_time / 4.) / 15. + 12. + tzs\n1397     return np.asarray(hours)\n1398 \n1399 \n1400 def _local_times_from_hours_since_midnight(times, hours):\n1401     \"\"\"\n1402     converts hours since midnight from an array of floats to localized times\n1403     \"\"\"\n1404     tz_info = times.tz  # pytz timezone info\n1405     naive_times = times.tz_localize(None)  # naive but still localized\n1406     # normalize local, naive times to previous midnight and add the hours until\n1407     # sunrise, sunset, and transit\n1408     return pd.DatetimeIndex(\n1409         (naive_times.normalize().view(np.int64) +\n1410          (hours * NS_PER_HR).astype(np.int64)).astype('datetime64[ns]'),\n1411         tz=tz_info)\n1412 \n1413 \n1414 def _times_to_hours_after_local_midnight(times):\n1415     \"\"\"convert local pandas datetime indices to array of hours as floats\"\"\"\n1416     times = times.tz_localize(None)\n1417     hrs = 1 / NS_PER_HR * (\n1418         times.view(np.int64) - times.normalize().view(np.int64))\n1419     return np.array(hrs)\n1420 \n1421 \n1422 def sun_rise_set_transit_geometric(times, latitude, longitude, declination,\n1423                                    equation_of_time):\n1424     \"\"\"\n1425     Geometric calculation of solar sunrise, sunset, and transit.\n1426 \n1427     .. warning:: The geometric calculation assumes a circular earth orbit with\n1428         the sun as a point source at its center, and neglects the effect of\n1429         atmospheric refraction on zenith. The error depends on location and\n1430         time of year but is of order 10 minutes.\n1431 \n1432     Parameters\n1433     ----------\n1434     times : pandas.DatetimeIndex\n1435         Corresponding timestamps, must be localized to the timezone for the\n1436         ``latitude`` and ``longitude``.\n1437     latitude : float\n1438         Latitude in degrees, positive north of equator, negative to south\n1439     longitude : float\n1440         Longitude in degrees, positive east of prime meridian, negative to west\n1441     declination : numeric\n1442         declination angle in radians at ``times``\n1443     equation_of_time : numeric\n1444         difference in time between solar time and mean solar time in minutes\n1445 \n1446     Returns\n1447     -------\n1448     sunrise : datetime\n1449         localized sunrise time\n1450     sunset : datetime\n1451         localized sunset time\n1452     transit : datetime\n1453         localized sun transit time\n1454 \n1455     References\n1456     ----------\n1457     .. [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n1458        Processes, 3rd Edition,\" J. Wiley and Sons, New York (2006)\n1459 \n1460     .. [2] Frank Vignola et al., \"Solar And Infrared Radiation Measurements,\"\n1461        CRC Press (2012)\n1462 \n1463     \"\"\"\n1464     latitude_rad = np.radians(latitude)  # radians\n1465     sunset_angle_rad = np.arccos(-np.tan(declination) * np.tan(latitude_rad))\n1466     sunset_angle = np.degrees(sunset_angle_rad)  # degrees\n1467     # solar noon is at hour angle zero\n1468     # so sunrise is just negative of sunset\n1469     sunrise_angle = -sunset_angle\n1470     sunrise_hour = _hour_angle_to_hours(\n1471         times, sunrise_angle, longitude, equation_of_time)\n1472     sunset_hour = _hour_angle_to_hours(\n1473         times, sunset_angle, longitude, equation_of_time)\n1474     transit_hour = _hour_angle_to_hours(times, 0, longitude, equation_of_time)\n1475     sunrise = _local_times_from_hours_since_midnight(times, sunrise_hour)\n1476     sunset = _local_times_from_hours_since_midnight(times, sunset_hour)\n1477     transit = _local_times_from_hours_since_midnight(times, transit_hour)\n1478     return sunrise, sunset, transit\n1479 \n[end of pvlib/solarposition.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/singlediode.py\n+++ b/pvlib/singlediode.py\n@@ -13,7 +13,7 @@\n # set keyword arguments for all uses of newton in this module\n-newton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\n+newton = partial(newton, fprime2=None)\n\n \n def bishop88(diode_voltage, photocurrent, saturation_current,\n@@ -382,7 +382,10 @@\n                  breakdown_exp=3.28, method='newton', tol=1e-6, maxiter=100):\n     \"\"\"\n     Find max power point.\n \n@@ -429,7 +432,8 @@\n \n     Returns\n     -------\n-    tuple\n+    tuple\n+        max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n+        max power ``p_mp`` [W]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n@@ -434,6 +438,7 @@\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fmpp(x, *a):\n         return bishop88(x, *a, gradients=True)[6]\n \n     if method.lower() == 'brentq':\n         vec_fun = np.vectorize(\n             lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi, vbr_a, vbr,\n             vbr_exp: brentq(fmpp, 0.0, voc,\n@@ -451,7 +456\n             args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                   vbr_a, vbr, vbr_exp))\n         )\n-        vd = vec_fun(voc_est, *args)\n+        vd = vec_fun(voc_est, *args, tol=tol, maxiter=maxiter)\n     elif method.lower() == 'newton':\n         args, v0 = _prepare_newton_inputs((), args, voc_est)\n         vd = newton(\n@@ -458,7 +464,8 @@\n             func=fmpp, x0=v0,\n             fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n         )\n+        vd = newton(func=fmpp, x0=v0, fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args, tol=tol, maxiter=maxiter)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n     return bishop88(vd, *args)\n@@ -460,7 +467,8 @@\n \n def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n                        resistance_series, resistance_shunt, nNsVth,\n                        d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                        breakdown_voltage=-5.5, breakdown_exp=3.28,\n                        method='newton', tol=1e-6, maxiter=100):\n     \"\"\"\n     Find current given any voltage.\n \n@@ -493,7 +501,8 @@\n \n     Returns\n     -------\n-    current : numeric\n+    current : numeric\n+        current (I) at the specified voltage (V). [A]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n@@ -498,7 +507,8 @@\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fv(x, v, *a):\n         return bishop88(x, *a)[1] - v\n \n     if method.lower() == 'brentq':\n         def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                           breakdown_factor, breakdown_voltage, breakdown_exp):\n             return brentq(fv, 0.0, voc,\n@@ -516,7 +526,8 @@\n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n     elif method.lower() == 'newton':\n         args, v0 = _prepare_newton_inputs((voltage,), args, voc_est)\n         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                     args=args, tol=tol, maxiter=maxiter)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n     return bishop88(vd, *args)[0]\n```", "model_patch": "--- a/pvlib/singlediode.py\n+++ b/pvlib/singlediode.py\n@@ -13,7 +13,7 @@\n # set keyword arguments for all uses of newton in this module\n-newton = partial(newton, tol=1e-6, maxiter=100, fprime2=None)\n+newton = partial(newton, fprime2=None)\n\n \n def bishop88(diode_voltage, photocurrent, saturation_current,\n@@ -382,7 +382,10 @@\n                  breakdown_exp=3.28, method='newton', tol=1e-6, maxiter=100):\n     \"\"\"\n     Find max power point.\n \n@@ -429,7 +432,8 @@\n \n     Returns\n     -------\n-    tuple\n+    tuple\n+        max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n+        max power ``p_mp`` [W]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n@@ -434,6 +438,7 @@\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fmpp(x, *a):\n         return bishop88(x, *a, gradients=True)[6]\n \n     if method.lower() == 'brentq':\n         vec_fun = np.vectorize(\n             lambda voc, iph, isat, rs, rsh, gamma, d2mutau, NsVbi, vbr_a, vbr,\n             vbr_exp: brentq(fmpp, 0.0, voc,\n@@ -451,7 +456\n             args=(iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                   vbr_a, vbr, vbr_exp))\n         )\n-        vd = vec_fun(voc_est, *args)\n+        vd = vec_fun(voc_est, *args, tol=tol, maxiter=maxiter)\n     elif method.lower() == 'newton':\n         args, v0 = _prepare_newton_inputs((), args, voc_est)\n         vd = newton(\n@@ -458,7 +464,8 @@\n             func=fmpp, x0=v0,\n             fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n         )\n+        vd = newton(func=fmpp, x0=v0, fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args, tol=tol, maxiter=maxiter)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n     return bishop88(vd, *args)\n@@ -460,7 +467,8 @@\n \n def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n                        resistance_series, resistance_shunt, nNsVth,\n                        d2mutau=0, NsVbi=np.Inf, breakdown_factor=0.,\n                        breakdown_voltage=-5.5, breakdown_exp=3.28,\n                        method='newton', tol=1e-6, maxiter=100):\n     \"\"\"\n     Find current given any voltage.\n \n@@ -493,7 +501,8 @@\n \n     Returns\n     -------\n-    current : numeric\n+    current : numeric\n+        current (I) at the specified voltage (V). [A]\n     \"\"\"\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n@@ -498,7 +507,8 @@\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n \n     def fv(x, v, *a):\n         return bishop88(x, *a)[1] - v\n \n     if method.lower() == 'brentq':\n         def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma, d2mutau, NsVbi,\n                           breakdown_factor, breakdown_voltage, breakdown_exp):\n             return brentq(fv, 0.0, voc,\n@@ -516,7 +526,8 @@\n         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n     elif method.lower() == 'newton':\n         args, v0 = _prepare_newton_inputs((voltage,), args, voc_est)\n         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                     args=args, tol=tol, maxiter=maxiter)\n     else:\n         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n     return bishop88(vd, *args)[0]\n"}
{"instance_id": "pvlib__pvlib-python-1480", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nConsider extracting the surface orientation calculation in pvlib.tracking.singleaxis() to its own function\n**Is your feature request related to a problem? Please describe.**\r\nThe usual workflow for modeling single-axis tracking in pvlib is to treat tracker rotation (`tracker_theta`) as an unknown to be calculated from solar position and array geometry.  However, sometimes a user might have their own tracker rotations but not have the corresponding `surface_tilt` and `surface_azimuth` values.  Here are a few motivating examples:\r\n- Using measured rotation angles\r\n- Post-processing the output of `tracking.singleaxis()` to include wind stow events or tracker stalls\r\n- Other tracking algorithms that determine rotation differently from the astronomical method\r\n\r\nAssuming I have my tracker rotations already in hand, getting the corresponding `surface_tilt` and `surface_azimuth` angles is not as easy as it should be.  For the specific case of horizontal N-S axis the math isn't so bad, but either way it's annoying to have to DIY when pvlib already has code to calculate those angles from tracker rotation.\r\n\r\n**Describe the solution you'd like**\r\nA function `pvlib.tracking.rotation_to_orientation` that implements the same math in `pvlib.tracking.singleaxis` to go from `tracker_theta` to `surface_tilt` and `surface_azimuth`.  Basically extract out the second half of `tracking.singleaxis` into a new function.  Suggestions for the function name are welcome.  To be explicit, this is more or less what I'm imagining:\r\n\r\n```python\r\ndef rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\r\n    # insert math from second half of tracking.singleaxis() here\r\n    out = {'tracker_theta': tracker_theta, 'aoi': aoi,\r\n           'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\r\n    return pandas_if_needed(out)\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nContinue suffering\r\n\r\n**Additional context**\r\nThis is one step towards a broader goal I have for `pvlib.tracking` to house other methods to determine tracker rotation in addition to the current astronomical method, the same way we have multiple temperature and transposition models.  These functions would be responsible for determining tracker rotations, and they'd all use this `rotation_to_orientation` function to convert rotation to module orientation.\r\n\r\nSeparately, I wonder if the code could be simplified using the tilt and azimuth equations in Bill's technical report (https://www.nrel.gov/docs/fy13osti/58891.pdf) -- seems like what we're doing is overly complicated, although maybe I've just not studied it closely enough.\r\n\r\ncc @williamhobbs @spaneja \n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n32       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n33     </a>\n34   </td>\n35 </tr>\n36 <tr>\n37   <td>Code Quality</td>\n38   <td>\n39     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n40     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n41     </a>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n43     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Coverage</td>\n49   <td>\n50     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n51     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n52     </a>\n53     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n54     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Benchmarks</td>\n60   <td>\n61     <a href=\"https://pvlib-benchmarker.github.io/pvlib-benchmarks/\">\n62     <img src=\"https://img.shields.io/badge/benchmarks-asv-lightgrey\" />\n63     </a>\n64   </td>\n65 </tr>\n66 <tr>\n67   <td>Publications</td>\n68   <td>\n69     <a href=\"https://doi.org/10.5281/zenodo.593284\">\n70     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg\" alt=\"zenodo reference\">\n71     </a>\n72     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n73     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n74     </a>\n75   </td>\n76 </tr>\n77 <tr>\n78   <td>Downloads</td>\n79   <td>\n80     <a href=\"https://pypi.org/project/pvlib/\">\n81     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n82     </a>\n83     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n84     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n85     </a>\n86   </td>\n87 </tr>\n88 </table>\n89 \n90 \n91 pvlib python is a community supported tool that provides a set of\n92 functions and classes for simulating the performance of photovoltaic\n93 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n94 toolbox developed at Sandia National Laboratories and it implements many\n95 of the models and methods developed at the Labs. More information on\n96 Sandia Labs PV performance modeling programs can be found at\n97 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n98 but operate independently of it.\n99 \n100 \n101 Documentation\n102 =============\n103 \n104 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n105 \n106 \n107 Installation\n108 ============\n109 \n110 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n111 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.\n112 \n113 \n114 Contributing\n115 ============\n116 \n117 We need your help to make pvlib-python a great tool!\n118 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n119 The long-term success of pvlib-python requires substantial community support.\n120 \n121 \n122 Citing\n123 ======\n124 \n125 If you use pvlib-python in a published work, please cite:\n126 \n127   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n128   \"pvlib python: a python package for modeling solar energy systems.\"\n129   Journal of Open Source Software, 3(29), 884, (2018).\n130   https://doi.org/10.21105/joss.00884\n131 \n132 Please also cite the DOI corresponding to the specific version of\n133 pvlib-python that you used. pvlib-python DOIs are listed at\n134 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n135 \n136 If you use pvlib-python in a commercial or publicly-available application, please\n137 consider displaying one of the \"powered by pvlib\" logos:\n138 \n139 <img src=\"docs/sphinx/source/_images/pvlib_powered_logo_vert.png\" width=\"300\"><img src=\"docs/sphinx/source/_images/pvlib_powered_logo_horiz.png\" width=\"300\">\n140 \n141 Getting support\n142 ===============\n143 \n144 pvlib usage questions can be asked on\n145 [Stack Overflow](http://stackoverflow.com) and tagged with\n146 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n147 \n148 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n149 is used for discussing various topics of interest to the pvlib-python\n150 community. We also make new version announcements on the google group.\n151 \n152 If you suspect that you may have discovered a bug or if you'd like to\n153 change something about pvlib, then please make an issue on our\n154 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n155 \n156 \n157 \n158 License\n159 =======\n160 \n161 BSD 3-clause.\n162 \n163 \n164 NumFOCUS\n165 ========\n166 \n167 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n168 \n169 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n170 \n[end of README.md]\n[start of docs/examples/irradiance-transposition/plot_transposition_gain.py]\n1 \"\"\"\n2 Modeling Transposition Gain\n3 ===========================\n4 \n5 Calculating the gain in insolation of a tilted module over a flat module.\n6 \"\"\"\n7 \n8 # %%\n9 # This example shows how to evaluate the transposition gain of a racking\n10 # strategy.  The transposition gain is the additional insolation collected\n11 # by orienting at a tilt instead of horizontal; using PV modeling lingo, it's\n12 # the increase in POA (plane of array) insolation over GHI (global horizontal\n13 # irradiance) insolation.\n14 #\n15 # This example uses a TMY dataset and the\n16 # :py:meth:`pvlib.irradiance.get_total_irradiance` function to transpose\n17 # irradiance components to POA irradiance for various fixed tilts.  It also\n18 # models a single-axis tracking system for comparison. The monthly POA\n19 # insolation is calculated for each strategy to show how orientation affects\n20 # seasonal irradiance collection.\n21 \n22 import pvlib\n23 from pvlib import location\n24 from pvlib import irradiance\n25 from pvlib import tracking\n26 from pvlib.iotools import read_tmy3\n27 import pandas as pd\n28 from matplotlib import pyplot as plt\n29 import pathlib\n30 \n31 # get full path to the data directory\n32 DATA_DIR = pathlib.Path(pvlib.__file__).parent / 'data'\n33 \n34 # get TMY3 dataset\n35 tmy, metadata = read_tmy3(DATA_DIR / '723170TYA.CSV', coerce_year=1990)\n36 # TMY3 datasets are right-labeled (AKA \"end of interval\") which means the last\n37 # interval of Dec 31, 23:00 to Jan 1 00:00 is labeled Jan 1 00:00. When rolling\n38 # up hourly irradiance to monthly insolation, a spurious January value is\n39 # calculated from that last row, so we'll just go ahead and drop it here:\n40 tmy = tmy.iloc[:-1, :]\n41 \n42 # create location object to store lat, lon, timezone\n43 location = location.Location.from_tmy(metadata)\n44 \n45 # calculate the necessary variables to do transposition.  Note that solar\n46 # position doesn't depend on array orientation, so we just calculate it once.\n47 # Note also that TMY datasets are right-labeled hourly intervals, e.g. the\n48 # 10AM to 11AM interval is labeled 11.  We should calculate solar position in\n49 # the middle of the interval (10:30), so we subtract 30 minutes:\n50 times = tmy.index - pd.Timedelta('30min')\n51 solar_position = location.get_solarposition(times)\n52 # but remember to shift the index back to line up with the TMY data:\n53 solar_position.index += pd.Timedelta('30min')\n54 \n55 \n56 # create a helper function to do the transposition for us\n57 def calculate_poa(tmy, solar_position, surface_tilt, surface_azimuth):\n58     # Use the get_total_irradiance function to transpose the irradiance\n59     # components to POA irradiance\n60     poa = irradiance.get_total_irradiance(\n61         surface_tilt=surface_tilt,\n62         surface_azimuth=surface_azimuth,\n63         dni=tmy['DNI'],\n64         ghi=tmy['GHI'],\n65         dhi=tmy['DHI'],\n66         solar_zenith=solar_position['apparent_zenith'],\n67         solar_azimuth=solar_position['azimuth'],\n68         model='isotropic')\n69     return poa['poa_global']  # just return the total in-plane irradiance\n70 \n71 \n72 # create a dataframe to keep track of our monthly insolations\n73 df_monthly = pd.DataFrame()\n74 \n75 # fixed-tilt:\n76 for tilt in range(0, 50, 10):\n77     # we will hardcode azimuth=180 (south) for all fixed-tilt cases\n78     poa_irradiance = calculate_poa(tmy, solar_position, tilt, 180)\n79     column_name = f\"FT-{tilt}\"\n80     # TMYs are hourly, so we can just sum up irradiance [W/m^2] to get\n81     # insolation [Wh/m^2]:\n82     df_monthly[column_name] = poa_irradiance.resample('m').sum()\n83 \n84 # single-axis tracking:\n85 orientation = tracking.singleaxis(solar_position['apparent_zenith'],\n86                                   solar_position['azimuth'],\n87                                   axis_tilt=0,  # flat array\n88                                   axis_azimuth=180,  # south-facing azimuth\n89                                   max_angle=60,  # a common maximum rotation\n90                                   backtrack=True,  # backtrack for a c-Si array\n91                                   gcr=0.4)  # a common ground coverage ratio\n92 \n93 poa_irradiance = calculate_poa(tmy,\n94                                solar_position,\n95                                orientation['surface_tilt'],\n96                                orientation['surface_azimuth'])\n97 df_monthly['SAT-0.4'] = poa_irradiance.resample('m').sum()\n98 \n99 # calculate the percent difference from GHI\n100 ghi_monthly = tmy['GHI'].resample('m').sum()\n101 df_monthly = 100 * (df_monthly.divide(ghi_monthly, axis=0) - 1)\n102 \n103 df_monthly.plot()\n104 plt.xlabel('Month of Year')\n105 plt.ylabel('Monthly Transposition Gain [%]')\n106 plt.show()\n107 \n108 \n109 # %%\n110 # Note that in summer, steeper tilts actually collect less insolation than\n111 # flatter tilts because the sun is so high in the sky at solar noon.  However,\n112 # the steeper tilts significantly increase insolation capture in winter when\n113 # the sun is lower in the sky.  In contrast to the highly seasonal gain shown\n114 # by fixed tilts, the tracker system shows a much more consistent gain\n115 # year-round.\n116 #\n117 # Because the seasonality of the fixed-tilt transposition gain is driven by\n118 # solar position angles, the relative behavior of different orientations will\n119 # be different for different locations.  For example, a common rule of thumb\n120 # (considered somewhat outdated today) used to be to set tilt equal to the\n121 # latitude of the system location.  At higher latitudes, the sun doesn't get\n122 # as high in the sky, so steeper tilts make more sense.\n123 \n[end of docs/examples/irradiance-transposition/plot_transposition_gain.py]\n[start of docs/examples/solar-tracking/plot_discontinuous_tracking.py]\n1 \"\"\"\n2 Discontinuous Tracking\n3 ======================\n4 \n5 Example of a custom Mount class.\n6 \"\"\"\n7 \n8 # %%\n9 # Many real-world tracking arrays adjust their position in discrete steps\n10 # rather than through continuous movement. This example shows how to model\n11 # this discontinuous tracking by implementing a custom Mount class.\n12 \n13 from pvlib import tracking, pvsystem, location, modelchain\n14 from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n15 import matplotlib.pyplot as plt\n16 import pandas as pd\n17 \n18 \n19 # %%\n20 # We'll define our custom Mount by extending\n21 # :py:class:`~pvlib.pvsystem.SingleAxisTrackerMount` for convenience.\n22 # Another approach would be to extend ``AbstractMount`` directly; see\n23 # the source code of :py:class:`~pvlib.pvsystem.SingleAxisTrackerMount`\n24 # and :py:class:`~pvlib.pvsystem.FixedMount` for how that is done.\n25 \n26 \n27 class DiscontinuousTrackerMount(pvsystem.SingleAxisTrackerMount):\n28     # inherit from SingleAxisTrackerMount so that we get the\n29     # constructor and tracking attributes (axis_tilt etc) automatically\n30 \n31     def get_orientation(self, solar_zenith, solar_azimuth):\n32         # Different trackers update at different rates; in this example we'll\n33         # assume a relatively slow update interval of 15 minutes to make the\n34         # effect more visually apparent.\n35         zenith_subset = solar_zenith.resample('15min').first()\n36         azimuth_subset = solar_azimuth.resample('15min').first()\n37 \n38         tracking_data_15min = tracking.singleaxis(\n39             zenith_subset, azimuth_subset,\n40             self.axis_tilt, self.axis_azimuth,\n41             self.max_angle, self.backtrack,\n42             self.gcr, self.cross_axis_tilt\n43         )\n44         # propagate the 15-minute positions to 1-minute stair-stepped values:\n45         tracking_data_1min = tracking_data_15min.reindex(solar_zenith.index,\n46                                                          method='ffill')\n47         return tracking_data_1min\n48 \n49 \n50 # %%\n51 # Let's take a look at the tracker rotation curve it produces:\n52 \n53 times = pd.date_range('2019-06-01', '2019-06-02', freq='1min', tz='US/Eastern')\n54 loc = location.Location(40, -80)\n55 solpos = loc.get_solarposition(times)\n56 mount = DiscontinuousTrackerMount(axis_azimuth=180, gcr=0.4)\n57 tracker_data = mount.get_orientation(solpos.apparent_zenith, solpos.azimuth)\n58 tracker_data['tracker_theta'].plot()\n59 plt.ylabel('Tracker Rotation [degree]')\n60 plt.show()\n61 \n62 # %%\n63 # With our custom tracking logic defined, we can create the corresponding\n64 # Array and PVSystem, and then run a ModelChain as usual:\n65 \n66 module_parameters = {'pdc0': 1, 'gamma_pdc': -0.004, 'b': 0.05}\n67 temp_params = TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer']\n68 array = pvsystem.Array(mount=mount, module_parameters=module_parameters,\n69                        temperature_model_parameters=temp_params)\n70 system = pvsystem.PVSystem(arrays=[array], inverter_parameters={'pdc0': 1})\n71 mc = modelchain.ModelChain(system, loc, spectral_model='no_loss')\n72 \n73 # simple simulated weather, just to show the effect of discrete tracking\n74 weather = loc.get_clearsky(times)\n75 weather['temp_air'] = 25\n76 weather['wind_speed'] = 1\n77 mc.run_model(weather)\n78 \n79 fig, axes = plt.subplots(2, 1, sharex=True)\n80 mc.results.effective_irradiance.plot(ax=axes[0])\n81 axes[0].set_ylabel('Effective Irradiance [W/m^2]')\n82 mc.results.ac.plot(ax=axes[1])\n83 axes[1].set_ylabel('AC Power')\n84 fig.show()\n85 \n86 # %%\n87 # The effect of discontinuous tracking creates a \"jagged\" effect in the\n88 # simulated plane-of-array irradiance, which then propagates through to\n89 # the AC power output.\n90 \n[end of docs/examples/solar-tracking/plot_discontinuous_tracking.py]\n[start of docs/examples/solar-tracking/plot_single_axis_tracking.py]\n1 \"\"\"\n2 Single-axis tracking\n3 ====================\n4 \n5 Examples of modeling tilt angles for single-axis tracker arrays.\n6 \"\"\"\n7 \n8 #%%\n9 # This example shows basic usage of pvlib's tracker position calculations with\n10 # :py:meth:`pvlib.tracking.singleaxis`.  The examples shown here demonstrate\n11 # how the tracker parameters affect the generated tilt angles.\n12 #\n13 # Because tracker angle is based on where the sun is in the sky, calculating\n14 # solar position is always the first step.\n15 #\n16 # True-tracking\n17 # -------------\n18 #\n19 # The basic tracking algorithm is called \"true-tracking\". It orients the panels\n20 # towards the sun as much as possible in order to maximize the cross section\n21 # presented towards incoming beam irradiance.\n22 \n23 from pvlib import solarposition, tracking\n24 import pandas as pd\n25 import matplotlib.pyplot as plt\n26 \n27 tz = 'US/Eastern'\n28 lat, lon = 40, -80\n29 \n30 times = pd.date_range('2019-01-01', '2019-01-02', closed='left', freq='5min',\n31                       tz=tz)\n32 solpos = solarposition.get_solarposition(times, lat, lon)\n33 \n34 truetracking_angles = tracking.singleaxis(\n35     apparent_zenith=solpos['apparent_zenith'],\n36     apparent_azimuth=solpos['azimuth'],\n37     axis_tilt=0,\n38     axis_azimuth=180,\n39     max_angle=90,\n40     backtrack=False,  # for true-tracking\n41     gcr=0.5)  # irrelevant for true-tracking\n42 \n43 truetracking_position = truetracking_angles['tracker_theta'].fillna(0)\n44 truetracking_position.plot(title='Truetracking Curve')\n45 \n46 plt.show()\n47 \n48 #%%\n49 # Backtracking\n50 # -------------\n51 #\n52 # Because truetracking yields steep tilt angle in morning and afternoon, it\n53 # will cause row to row shading as the shadows from adjacent rows fall on each\n54 # other. To prevent this, the trackers can rotate backwards when the sun is\n55 # near the horizon -- \"backtracking\".  The shading angle depends on row\n56 # geometry, so the gcr parameter must be specified.  The greater the gcr, the\n57 # tighter the row spacing and the more aggressively the array must backtrack.\n58 \n59 fig, ax = plt.subplots()\n60 \n61 for gcr in [0.2, 0.4, 0.6]:\n62     backtracking_angles = tracking.singleaxis(\n63         apparent_zenith=solpos['apparent_zenith'],\n64         apparent_azimuth=solpos['azimuth'],\n65         axis_tilt=0,\n66         axis_azimuth=180,\n67         max_angle=90,\n68         backtrack=True,\n69         gcr=gcr)\n70 \n71     backtracking_position = backtracking_angles['tracker_theta'].fillna(0)\n72     backtracking_position.plot(title='Backtracking Curve',\n73                                label=f'GCR:{gcr:0.01f}',\n74                                ax=ax)\n75 \n76 plt.legend()\n77 plt.show()\n78 \n[end of docs/examples/solar-tracking/plot_single_axis_tracking.py]\n[start of docs/examples/solar-tracking/plot_single_axis_tracking_on_sloped_terrain.py]\n1 \"\"\"\n2 Backtracking on sloped terrain\n3 ==============================\n4 \n5 Modeling backtracking for single-axis tracker arrays on sloped terrain.\n6 \"\"\"\n7 \n8 # %%\n9 # Tracker systems use backtracking to avoid row-to-row shading when the\n10 # sun is low in the sky. The backtracking strategy orients the modules exactly\n11 # on the boundary between shaded and unshaded so that the modules are oriented\n12 # as much towards the sun as possible while still remaining unshaded.\n13 # Unlike the true-tracking calculation (which only depends on solar position),\n14 # calculating the backtracking angle requires knowledge of the relative spacing\n15 # of adjacent tracker rows. This example shows how the backtracking angle\n16 # changes based on a vertical offset between rows caused by sloped terrain.\n17 # It uses :py:func:`pvlib.tracking.calc_axis_tilt` and\n18 # :py:func:`pvlib.tracking.calc_cross_axis_tilt` to calculate the necessary\n19 # array geometry parameters and :py:func:`pvlib.tracking.singleaxis` to\n20 # calculate the backtracking angles.\n21 #\n22 # Angle conventions\n23 # -----------------\n24 #\n25 # First let's go over the sign conventions used for angles. In contrast to\n26 # fixed-tilt arrays where the azimuth is that of the normal to the panels, the\n27 # convention for the azimuth of a single-axis tracker is along the tracker\n28 # axis. Note that the axis azimuth is a property of the array and is distinct\n29 # from the azimuth of the panel orientation, which changes based on tracker\n30 # rotation angle. Because the tracker axis points in two directions, there are\n31 # two choices for the axis azimuth angle, and by convention (at least in the\n32 # northern hemisphere), the more southward angle is chosen:\n33 #\n34 # .. image:: ../../_images/tracker_azimuth_angle_convention.png\n35 #   :alt: Image showing the azimuth convention for single-axis tracker arrays.\n36 #   :width: 500\n37 #   :align: center\n38 #\n39 # Note that, as with fixed-tilt arrays, the axis azimuth is determined as the\n40 # angle clockwise from north. The azimuth of the terrain's slope is also\n41 # determined as an angle clockwise from north, pointing in the direction\n42 # of falling slope. So for example, a hillside that slopes down to the east\n43 # has an azimuth of 90 degrees.\n44 #\n45 # Using the axis azimuth convention above, the sign convention for tracker\n46 # rotations is given by the\n47 # `right-hand rule <https://en.wikipedia.org/wiki/Right-hand_rule>`_.\n48 # Point the right hand thumb along the axis in the direction of the axis\n49 # azimuth and the fingers curl in the direction of positive rotation angle:\n50 #\n51 # .. image:: ../../_images/tracker_rotation_angle_convention.png\n52 #   :alt: Image showing the rotation sign convention for single-axis trackers.\n53 #   :width: 500\n54 #   :align: center\n55 #\n56 # So for an array with ``axis_azimuth=180`` (tracker axis aligned perfectly\n57 # north-south), pointing the right-hand thumb along the axis azimuth has the\n58 # fingers curling towards the west, meaning rotations towards the west are\n59 # positive and rotations towards the east are negative.\n60 #\n61 # The ground slope itself is always positive, but the component of the slope\n62 # perpendicular to the tracker axes can be positive or negative. The convention\n63 # for the cross-axis slope angle follows the right-hand rule: align\n64 # the right-hand thumb along the tracker axis in the direction of the axis\n65 # azimuth and the fingers curl towards positive angles. So in this example,\n66 # with the axis azimuth coming out of the page, an east-facing, downward slope\n67 # is a negative rotation from horizontal:\n68 #\n69 # .. image:: ../../_images/ground_slope_angle_convention.png\n70 #   :alt: Image showing the ground slope sign convention.\n71 #   :width: 500\n72 #   :align: center\n73 #\n74 \n75 # %%\n76 # Rotation curves\n77 # ---------------\n78 #\n79 # Now, let's plot the simple case where the tracker axes are at right angles\n80 # to the direction of the slope.  In this case, the cross-axis tilt angle\n81 # is the same as the slope of the terrain and the tracker axis itself is\n82 # horizontal.\n83 \n84 from pvlib import solarposition, tracking\n85 import pandas as pd\n86 import matplotlib.pyplot as plt\n87 \n88 # PV system parameters\n89 tz = 'US/Eastern'\n90 lat, lon = 40, -80\n91 gcr = 0.4\n92 \n93 # calculate the solar position\n94 times = pd.date_range('2019-01-01 06:00', '2019-01-01 18:00', closed='left',\n95                       freq='1min', tz=tz)\n96 solpos = solarposition.get_solarposition(times, lat, lon)\n97 \n98 # compare the backtracking angle at various terrain slopes\n99 fig, ax = plt.subplots()\n100 for cross_axis_tilt in [0, 5, 10]:\n101     tracker_data = tracking.singleaxis(\n102         apparent_zenith=solpos['apparent_zenith'],\n103         apparent_azimuth=solpos['azimuth'],\n104         axis_tilt=0,  # flat because the axis is perpendicular to the slope\n105         axis_azimuth=180,  # N-S axis, azimuth facing south\n106         max_angle=90,\n107         backtrack=True,\n108         gcr=gcr,\n109         cross_axis_tilt=cross_axis_tilt)\n110 \n111     # tracker rotation is undefined at night\n112     backtracking_position = tracker_data['tracker_theta'].fillna(0)\n113     label = 'cross-axis tilt: {}\u00b0'.format(cross_axis_tilt)\n114     backtracking_position.plot(label=label, ax=ax)\n115 \n116 plt.legend()\n117 plt.title('Backtracking Curves')\n118 plt.show()\n119 \n120 # %%\n121 # This plot shows how backtracking changes based on the slope between rows.\n122 # For example, unlike the flat-terrain backtracking curve, the sloped-terrain\n123 # curves do not approach zero at the end of the day. Because of the vertical\n124 # offset between rows introduced by the sloped terrain, the trackers can be\n125 # slightly tilted without shading each other.\n126 #\n127 # Now let's examine the general case where the terrain slope makes an\n128 # inconvenient angle to the tracker axes. For example, consider an array\n129 # with north-south axes on terrain that slopes down to the south-south-east.\n130 # Assuming the axes are installed parallel to the ground, the northern ends\n131 # of the axes will be higher than the southern ends. But because the slope\n132 # isn't purely parallel or perpendicular to the axes, the axis tilt and\n133 # cross-axis tilt angles are not immediately obvious. We can use pvlib\n134 # to calculate them for us:\n135 \n136 # terrain slopes 10 degrees downward to the south-south-east. note: because\n137 # slope_azimuth is defined in the direction of falling slope, slope_tilt is\n138 # always positive.\n139 slope_azimuth = 155\n140 slope_tilt = 10\n141 axis_azimuth = 180  # tracker axis is still N-S\n142 \n143 # calculate the tracker axis tilt, assuming that the axis follows the terrain:\n144 axis_tilt = tracking.calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth)\n145 \n146 # calculate the cross-axis tilt:\n147 cross_axis_tilt = tracking.calc_cross_axis_tilt(slope_azimuth, slope_tilt,\n148                                                 axis_azimuth, axis_tilt)\n149 \n150 print('Axis tilt:', '{:0.01f}\u00b0'.format(axis_tilt))\n151 print('Cross-axis tilt:', '{:0.01f}\u00b0'.format(cross_axis_tilt))\n152 \n153 # %%\n154 # And now we can pass use these values to generate the tracker curve as\n155 # before:\n156 \n157 tracker_data = tracking.singleaxis(\n158     apparent_zenith=solpos['apparent_zenith'],\n159     apparent_azimuth=solpos['azimuth'],\n160     axis_tilt=axis_tilt,  # no longer flat because the terrain imparts a tilt\n161     axis_azimuth=axis_azimuth,\n162     max_angle=90,\n163     backtrack=True,\n164     gcr=gcr,\n165     cross_axis_tilt=cross_axis_tilt)\n166 \n167 backtracking_position = tracker_data['tracker_theta'].fillna(0)\n168 backtracking_position.plot()\n169 \n170 title_template = 'Axis tilt: {:0.01f}\u00b0   Cross-axis tilt: {:0.01f}\u00b0'\n171 plt.title(title_template.format(axis_tilt, cross_axis_tilt))\n172 plt.show()\n173 \n174 # %%\n175 # Note that the backtracking curve is roughly mirrored compared with the\n176 # earlier example -- it is because the terrain is now sloped somewhat to the\n177 # east instead of west.\n178 \n[end of docs/examples/solar-tracking/plot_single_axis_tracking_on_sloped_terrain.py]\n[start of pvlib/bifacial/infinite_sheds.py]\n1 r\"\"\"\n2 Functions for the infinite sheds bifacial irradiance model.\n3 \"\"\"\n4 \n5 import numpy as np\n6 import pandas as pd\n7 from pvlib.tools import cosd, sind, tand\n8 from pvlib.bifacial import utils\n9 from pvlib.shading import masking_angle\n10 from pvlib.irradiance import beam_component, aoi\n11 \n12 \n13 def _vf_ground_sky_integ(surface_tilt, surface_azimuth, gcr, height,\n14                          pitch, max_rows=10, npoints=100):\n15     \"\"\"\n16     Integrated and per-point view factors from the ground to the sky at points\n17     between interior rows of the array.\n18 \n19     Parameters\n20     ----------\n21     surface_tilt : numeric\n22         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n23         = 0, surface facing horizon = 90. [degree]\n24     surface_azimuth : numeric\n25         Surface azimuth angles in decimal degrees east of north\n26         (e.g. North = 0, South = 180, East = 90, West = 270).\n27         ``surface_azimuth`` must be >=0 and <=360.\n28     gcr : float\n29         Ratio of row slant length to row spacing (pitch). [unitless]\n30     height : float\n31         Height of the center point of the row above the ground; must be in the\n32         same units as ``pitch``.\n33     pitch : float\n34         Distance between two rows. Must be in the same units as ``height``.\n35     max_rows : int, default 10\n36         Maximum number of rows to consider in front and behind the current row.\n37     npoints : int, default 100\n38         Number of points used to discretize distance along the ground.\n39 \n40     Returns\n41     -------\n42     fgnd_sky : float\n43         Integration of view factor over the length between adjacent, interior\n44         rows. [unitless]\n45     fz : ndarray\n46         Fraction of distance from the previous row to the next row. [unitless]\n47     fz_sky : ndarray\n48         View factors at discrete points between adjacent, interior rows.\n49         [unitless]\n50 \n51     \"\"\"\n52     # TODO: vectorize over surface_tilt\n53     # Abuse utils._vf_ground_sky_2d by supplying surface_tilt in place\n54     # of a signed rotation. This is OK because\n55     # 1) z span the full distance between 2 rows, and\n56     # 2) max_rows is set to be large upstream, and\n57     # 3) _vf_ground_sky_2d considers [-max_rows, +max_rows]\n58     # The VFs to the sky will thus be symmetric around z=0.5\n59     z = np.linspace(0, 1, npoints)\n60     rotation = np.atleast_1d(surface_tilt)\n61     fz_sky = np.zeros((len(rotation), npoints))\n62     for k, r in enumerate(rotation):\n63         vf, _ = utils._vf_ground_sky_2d(z, r, gcr, pitch, height, max_rows)\n64         fz_sky[k, :] = vf\n65     # calculate the integrated view factor for all of the ground between rows\n66     return np.trapz(fz_sky, z, axis=1)\n67 \n68 \n69 def _poa_ground_shadows(poa_ground, f_gnd_beam, df, vf_gnd_sky):\n70     \"\"\"\n71     Reduce ground-reflected irradiance to the tilted plane (poa_ground) to\n72     account for shadows on the ground.\n73 \n74     Parameters\n75     ----------\n76     poa_ground : numeric\n77         Ground reflected irradiance on the tilted surface, assuming full GHI\n78         illumination on all of the ground. [W/m^2]\n79     f_gnd_beam : numeric\n80         Fraction of the distance between rows that is illuminated (unshaded).\n81         [unitless]\n82     df : numeric\n83         Diffuse fraction, the ratio of DHI to GHI. [unitless]\n84     vf_gnd_sky : numeric\n85         View factor from the ground to the sky, integrated along the distance\n86         between rows. [unitless]\n87 \n88     Returns\n89     -------\n90     poa_gnd_sky : numeric\n91         Adjusted ground-reflected irradiance accounting for shadows on the\n92         ground. [W/m^2]\n93 \n94     \"\"\"\n95     return poa_ground * (f_gnd_beam*(1 - df) + df*vf_gnd_sky)\n96 \n97 \n98 def _vf_row_sky_integ(f_x, surface_tilt, gcr, npoints=100):\n99     \"\"\"\n100     Integrated view factors from the shaded and unshaded parts of\n101     the row slant height to the sky.\n102 \n103     Parameters\n104     ----------\n105     f_x : numeric\n106         Fraction of row slant height from the bottom that is shaded. [unitless]\n107     surface_tilt : numeric\n108         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n109         = 0, surface facing horizon = 90. [degree]\n110     gcr : float\n111         Ratio of row slant length to row spacing (pitch). [unitless]\n112     npoints : int, default 100\n113         Number of points for integration. [unitless]\n114 \n115     Returns\n116     -------\n117     vf_shade_sky_integ : numeric\n118         Integrated view factor from the shaded part of the row to the sky.\n119         [unitless]\n120     vf_noshade_sky_integ : numeric\n121         Integrated view factor from the unshaded part of the row to the sky.\n122         [unitless]\n123 \n124     Notes\n125     -----\n126     The view factor to the sky at a point x along the row slant height is\n127     given by\n128 \n129     .. math ::\n130         \\\\large{f_{sky} = \\frac{1}{2} \\\\left(\\\\cos\\\\left(\\\\psi_t\\\\right) +\n131         \\\\cos \\\\left(\\\\beta\\\\right) \\\\right)\n132 \n133     where :math:`\\\\psi_t` is the angle from horizontal of the line from point\n134     x to the top of the facing row, and :math:`\\\\beta` is the surface tilt.\n135 \n136     View factors are integrated separately over shaded and unshaded portions\n137     of the row slant height.\n138 \n139     \"\"\"\n140     # handle Series inputs\n141     surface_tilt = np.array(surface_tilt)\n142     cst = cosd(surface_tilt)\n143     # shaded portion\n144     x = np.linspace(0, f_x, num=npoints)\n145     psi_t_shaded = masking_angle(surface_tilt, gcr, x)\n146     y = 0.5 * (cosd(psi_t_shaded) + cst)\n147     # integrate view factors from each point in the discretization. This is an\n148     # improvement over the algorithm described in [2]\n149     vf_shade_sky_integ = np.trapz(y, x, axis=0)\n150     # unshaded portion\n151     x = np.linspace(f_x, 1., num=npoints)\n152     psi_t_unshaded = masking_angle(surface_tilt, gcr, x)\n153     y = 0.5 * (cosd(psi_t_unshaded) + cst)\n154     vf_noshade_sky_integ = np.trapz(y, x, axis=0)\n155     return vf_shade_sky_integ, vf_noshade_sky_integ\n156 \n157 \n158 def _poa_sky_diffuse_pv(f_x, dhi, vf_shade_sky_integ, vf_noshade_sky_integ):\n159     \"\"\"\n160     Sky diffuse POA from integrated view factors combined for both shaded and\n161     unshaded parts of the surface.\n162 \n163     Parameters\n164     ----------\n165     f_x : numeric\n166         Fraction of row slant height from the bottom that is shaded. [unitless]\n167     dhi : numeric\n168         Diffuse horizontal irradiance (DHI). [W/m^2]\n169     vf_shade_sky_integ : numeric\n170         Integrated view factor from the shaded part of the row to the sky.\n171         [unitless]\n172     vf_noshade_sky_integ : numeric\n173         Integrated view factor from the unshaded part of the row to the sky.\n174         [unitless]\n175 \n176     Returns\n177     -------\n178     poa_sky_diffuse_pv : numeric\n179         Total sky diffuse irradiance incident on the PV surface. [W/m^2]\n180     \"\"\"\n181     return dhi * (f_x * vf_shade_sky_integ + (1 - f_x) * vf_noshade_sky_integ)\n182 \n183 \n184 def _ground_angle(x, surface_tilt, gcr):\n185     \"\"\"\n186     Angle from horizontal of the line from a point x on the row slant length\n187     to the bottom of the facing row.\n188 \n189     The angles are clockwise from horizontal, rather than the usual\n190     counterclockwise direction.\n191 \n192     Parameters\n193     ----------\n194     x : numeric\n195         fraction of row slant length from bottom, ``x = 0`` is at the row\n196         bottom, ``x = 1`` is at the top of the row.\n197     surface_tilt : numeric\n198         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n199         = 0, surface facing horizon = 90. [degree]\n200     gcr : float\n201         ground coverage ratio, ratio of row slant length to row spacing.\n202         [unitless]\n203 \n204     Returns\n205     -------\n206     psi : numeric\n207         Angle [degree].\n208     \"\"\"\n209     #  : \\\\            \\\n210     #  :  \\\\            \\\n211     #  :   \\\\            \\\n212     #  :    \\\\            \\  facing row\n213     #  :     \\\\.___________\\\n214     #  :       \\  ^*-.  psi \\\n215     #  :        \\  x   *-.   \\\n216     #  :         \\  v      *-.\\\n217     #  :          \\<-----P---->\\\n218 \n219     x1 = x * sind(surface_tilt)\n220     x2 = (x * cosd(surface_tilt) + 1 / gcr)\n221     psi = np.arctan2(x1, x2)  # do this first because it handles 0 / 0\n222     return np.rad2deg(psi)\n223 \n224 \n225 def _vf_row_ground(x, surface_tilt, gcr):\n226     \"\"\"\n227     View factor from a point x on the row to the ground.\n228 \n229     Parameters\n230     ----------\n231     x : numeric\n232         Fraction of row slant height from the bottom. [unitless]\n233     surface_tilt : numeric\n234         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n235         = 0, surface facing horizon = 90. [degree]\n236     gcr : float\n237         Ground coverage ratio, ratio of row slant length to row spacing.\n238         [unitless]\n239 \n240     Returns\n241     -------\n242     vf : numeric\n243         View factor from the point at x to the ground. [unitless]\n244 \n245     \"\"\"\n246     cst = cosd(surface_tilt)\n247     # angle from horizontal at the point x on the row slant height to the\n248     # bottom of the facing row\n249     psi_t_shaded = _ground_angle(x, surface_tilt, gcr)\n250     # view factor from the point on the row to the ground\n251     return 0.5 * (cosd(psi_t_shaded) - cst)\n252 \n253 \n254 def _vf_row_ground_integ(f_x, surface_tilt, gcr, npoints=100):\n255     \"\"\"\n256     View factors to the ground from shaded and unshaded parts of a row.\n257 \n258     Parameters\n259     ----------\n260     f_x : numeric\n261         Fraction of row slant height from the bottom that is shaded. [unitless]\n262     surface_tilt : numeric\n263         Surface tilt angle in degrees from horizontal, e.g., surface facing up\n264         = 0, surface facing horizon = 90. [degree]\n265     gcr : float\n266         Ground coverage ratio, ratio of row slant length to row spacing.\n267         [unitless]\n268     npoints : int, default 100\n269         Number of points for integration. [unitless]\n270 \n271     Returns\n272     -------\n273     vf_shade_ground_integ : numeric\n274         View factor from the shaded portion of the row to the ground.\n275         [unitless]\n276     vf_noshade_ground_integ : numeric\n277         View factor from the unshaded portion of the row to the ground.\n278         [unitless]\n279 \n280     Notes\n281     -----\n282     The view factor to the ground at a point x along the row slant height is\n283     given by\n284 \n285     .. math ::\n286         \\\\large{f_{gr} = \\frac{1}{2} \\\\left(\\\\cos\\\\left(\\\\psi_t\\\\right) -\n287         \\\\cos \\\\left(\\\\beta\\\\right) \\\\right)\n288 \n289     where :math:`\\\\psi_t` is the angle from horizontal of the line from point\n290     x to the bottom of the facing row, and :math:`\\\\beta` is the surface tilt.\n291 \n292     Each view factor is integrated over the relevant portion of the row\n293     slant height.\n294     \"\"\"\n295     # handle Series inputs\n296     surface_tilt = np.array(surface_tilt)\n297     # shaded portion of row slant height\n298     x = np.linspace(0, f_x, num=npoints)\n299     # view factor from the point on the row to the ground\n300     y = _vf_row_ground(x, surface_tilt, gcr)\n301     # integrate view factors along the shaded portion of the row slant height.\n302     # This is an improvement over the algorithm described in [2]\n303     vf_shade_ground_integ = np.trapz(y, x, axis=0)\n304 \n305     # unshaded portion of row slant height\n306     x = np.linspace(f_x, 1., num=npoints)\n307     # view factor from the point on the row to the ground\n308     y = _vf_row_ground(x, surface_tilt, gcr)\n309     # integrate view factors along the unshaded portion.\n310     # This is an improvement over the algorithm described in [2]\n311     vf_noshade_ground_integ = np.trapz(y, x, axis=0)\n312 \n313     return vf_shade_ground_integ, vf_noshade_ground_integ\n314 \n315 \n316 def _poa_ground_pv(f_x, poa_ground, f_gnd_pv_shade, f_gnd_pv_noshade):\n317     \"\"\"\n318     Reduce ground-reflected irradiance to account for limited view of the\n319     ground from the row surface.\n320 \n321     Parameters\n322     ----------\n323     f_x : numeric\n324         Fraction of row slant height from the bottom that is shaded. [unitless]\n325     poa_ground : numeric\n326         Ground-reflected irradiance that would reach the row surface if the\n327         full ground was visible. poa_gnd_sky accounts for limited view of the\n328         sky from the ground. [W/m^2]\n329     f_gnd_pv_shade : numeric\n330         fraction of ground visible from shaded part of PV surface. [unitless]\n331     f_gnd_pv_noshade : numeric\n332         fraction of ground visible from unshaded part of PV surface. [unitless]\n333 \n334     Returns\n335     -------\n336     numeric\n337         Ground diffuse irradiance on the row plane. [W/m^2]\n338     \"\"\"\n339     return poa_ground * (f_x * f_gnd_pv_shade + (1 - f_x) * f_gnd_pv_noshade)\n340 \n341 \n342 def _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n343                      surface_azimuth, gcr):\n344     \"\"\"\n345     Calculate fraction (from the bottom) of row slant height that is shaded\n346     from direct irradiance by the row in front toward the sun.\n347 \n348     See [1], Eq. 14 and also [2], Eq. 32.\n349 \n350     .. math::\n351         F_x = \\\\max \\\\left( 0, \\\\min \\\\left(\\\\frac{\\\\text{GCR} \\\\cos \\\\theta\n352         + \\\\left( \\\\text{GCR} \\\\sin \\\\theta - \\\\tan \\\\beta_{c} \\\\right)\n353         \\\\tan Z - 1}\n354         {\\\\text{GCR} \\\\left( \\\\cos \\\\theta + \\\\sin \\\\theta \\\\tan Z \\\\right)},\n355         1 \\\\right) \\\\right)\n356 \n357     Parameters\n358     ----------\n359     solar_zenith : numeric\n360         Apparent (refraction-corrected) solar zenith. [degrees]\n361     solar_azimuth : numeric\n362         Solar azimuth. [degrees]\n363     surface_tilt : numeric\n364         Row tilt from horizontal, e.g. surface facing up = 0, surface facing\n365         horizon = 90. [degrees]\n366     surface_azimuth : numeric\n367         Azimuth angle of the row surface. North=0, East=90, South=180,\n368         West=270. [degrees]\n369     gcr : numeric\n370         Ground coverage ratio, which is the ratio of row slant length to row\n371         spacing (pitch). [unitless]\n372 \n373     Returns\n374     -------\n375     f_x : numeric\n376         Fraction of row slant height from the bottom that is shaded from\n377         direct irradiance.\n378 \n379     References\n380     ----------\n381     .. [1] Mikofski, M., Darawali, R., Hamer, M., Neubert, A., and Newmiller,\n382        J. \"Bifacial Performance Modeling in Large Arrays\". 2019 IEEE 46th\n383        Photovoltaic Specialists Conference (PVSC), 2019, pp. 1282-1287.\n384        :doi:`10.1109/PVSC40753.2019.8980572`.\n385     .. [2] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n386        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n387        https://www.nrel.gov/docs/fy20osti/76626.pdf\n388     \"\"\"\n389     tan_phi = utils._solar_projection_tangent(\n390         solar_zenith, solar_azimuth, surface_azimuth)\n391     # length of shadow behind a row as a fraction of pitch\n392     x = gcr * (sind(surface_tilt) * tan_phi + cosd(surface_tilt))\n393     f_x = 1 - 1. / x\n394     # set f_x to be 1 when sun is behind the array\n395     ao = aoi(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth)\n396     f_x = np.where(ao < 90, f_x, 1.)\n397     # when x < 1, the shadow is not long enough to fall on the row surface\n398     f_x = np.where(x > 1., f_x, 0.)\n399     return f_x\n400 \n401 \n402 def get_irradiance_poa(surface_tilt, surface_azimuth, solar_zenith,\n403                        solar_azimuth, gcr, height, pitch, ghi, dhi, dni,\n404                        albedo, iam=1.0, npoints=100):\n405     r\"\"\"\n406     Calculate plane-of-array (POA) irradiance on one side of a row of modules.\n407 \n408     The infinite sheds model [1] assumes the PV system comprises parallel,\n409     evenly spaced rows on a level, horizontal surface. Rows can be on fixed\n410     racking or single axis trackers. The model calculates irradiance at a\n411     location far from the ends of any rows, in effect, assuming that the\n412     rows (sheds) are infinitely long.\n413 \n414     POA irradiance components include direct, diffuse and global (total).\n415     Irradiance values are reduced to account for reflection of direct light,\n416     but are not adjusted for solar spectrum or reduced by a module's\n417     bifaciality factor.\n418 \n419     Parameters\n420     ----------\n421     surface_tilt : numeric\n422         Tilt of the surface from horizontal. Must be between 0 and 180. For\n423         example, for a fixed tilt module mounted at 30 degrees from\n424         horizontal, use ``surface_tilt=30`` to get front-side irradiance and\n425         ``surface_tilt=150`` to get rear-side irradiance. [degree]\n426 \n427     surface_azimuth : numeric\n428         Surface azimuth in decimal degrees east of north\n429         (e.g. North = 0, South = 180, East = 90, West = 270). [degree]\n430 \n431     solar_zenith : numeric\n432         Refraction-corrected solar zenith. [degree]\n433 \n434     solar_azimuth : numeric\n435         Solar azimuth. [degree]\n436 \n437     gcr : float\n438         Ground coverage ratio, ratio of row slant length to row spacing.\n439         [unitless]\n440 \n441     height : float\n442         Height of the center point of the row above the ground; must be in the\n443         same units as ``pitch``.\n444 \n445     pitch : float\n446         Distance between two rows; must be in the same units as ``height``.\n447 \n448     ghi : numeric\n449         Global horizontal irradiance. [W/m2]\n450 \n451     dhi : numeric\n452         Diffuse horizontal irradiance. [W/m2]\n453 \n454     dni : numeric\n455         Direct normal irradiance. [W/m2]\n456 \n457     albedo : numeric\n458         Surface albedo. [unitless]\n459 \n460     iam : numeric, default 1.0\n461         Incidence angle modifier, the fraction of direct irradiance incident\n462         on the surface that is not reflected away. [unitless]\n463 \n464     npoints : int, default 100\n465         Number of points used to discretize distance along the ground.\n466 \n467     Returns\n468     -------\n469     output : dict or DataFrame\n470         Output is a DataFrame when input ghi is a Series. See Notes for\n471         descriptions of content.\n472 \n473     Notes\n474     -----\n475     Input parameters ``height`` and ``pitch`` must have the same unit.\n476 \n477     ``output`` always includes:\n478 \n479     - ``poa_global`` : total POA irradiance. [W/m^2]\n480     - ``poa_diffuse`` : total diffuse POA irradiance from all sources. [W/m^2]\n481     - ``poa_direct`` : total direct POA irradiance. [W/m^2]\n482     - ``poa_sky_diffuse`` : total sky diffuse irradiance on the plane of array.\n483       [W/m^2]\n484     - ``poa_ground_diffuse`` : total ground-reflected diffuse irradiance on the\n485       plane of array. [W/m^2]\n486 \n487     References\n488     ----------\n489     .. [1] Mikofski, M., Darawali, R., Hamer, M., Neubert, A., and Newmiller,\n490        J. \"Bifacial Performance Modeling in Large Arrays\". 2019 IEEE 46th\n491        Photovoltaic Specialists Conference (PVSC), 2019, pp. 1282-1287.\n492        :doi:`10.1109/PVSC40753.2019.8980572`.\n493 \n494     See also\n495     --------\n496     get_irradiance\n497     \"\"\"\n498     # Calculate some geometric quantities\n499     # rows to consider in front and behind current row\n500     # ensures that view factors to the sky are computed to within 5 degrees\n501     # of the horizon\n502     max_rows = np.ceil(height / (pitch * tand(5)))\n503     # fraction of ground between rows that is illuminated accounting for\n504     # shade from panels. [1], Eq. 4\n505     f_gnd_beam = utils._unshaded_ground_fraction(\n506         surface_tilt, surface_azimuth, solar_zenith, solar_azimuth, gcr)\n507     # integrated view factor from the ground to the sky, integrated between\n508     # adjacent rows interior to the array\n509     # method differs from [1], Eq. 7 and Eq. 8; height is defined at row\n510     # center rather than at row lower edge as in [1].\n511     vf_gnd_sky = _vf_ground_sky_integ(\n512         surface_tilt, surface_azimuth, gcr, height, pitch, max_rows, npoints)\n513     # fraction of row slant height that is shaded from direct irradiance\n514     f_x = _shaded_fraction(solar_zenith, solar_azimuth, surface_tilt,\n515                            surface_azimuth, gcr)\n516 \n517     # Integrated view factors to the sky from the shaded and unshaded parts of\n518     # the row slant height\n519     # Differs from [1] Eq. 15 and Eq. 16. Here, we integrate over each\n520     # interval (shaded or unshaded) rather than averaging values at each\n521     # interval's end points.\n522     vf_shade_sky, vf_noshade_sky = _vf_row_sky_integ(\n523         f_x, surface_tilt, gcr, npoints)\n524 \n525     # view factors from the ground to shaded and unshaded portions of the row\n526     # slant height\n527     # Differs from [1] Eq. 17 and Eq. 18. Here, we integrate over each\n528     # interval (shaded or unshaded) rather than averaging values at each\n529     # interval's end points.\n530     f_gnd_pv_shade, f_gnd_pv_noshade = _vf_row_ground_integ(\n531         f_x, surface_tilt, gcr, npoints)\n532 \n533     # Total sky diffuse received by both shaded and unshaded portions\n534     poa_sky_pv = _poa_sky_diffuse_pv(\n535         f_x, dhi, vf_shade_sky, vf_noshade_sky)\n536 \n537     # irradiance reflected from the ground before accounting for shadows\n538     # and restricted views\n539     # this is a deviation from [1], because the row to ground view factor\n540     # is accounted for in a different manner\n541     ground_diffuse = ghi * albedo\n542 \n543     # diffuse fraction\n544     diffuse_fraction = np.clip(dhi / ghi, 0., 1.)\n545     # make diffuse fraction 0 when ghi is small\n546     diffuse_fraction = np.where(ghi < 0.0001, 0., diffuse_fraction)\n547 \n548     # Reduce ground-reflected irradiance because other rows in the array\n549     # block irradiance from reaching the ground.\n550     # [2], Eq. 9\n551     ground_diffuse = _poa_ground_shadows(\n552         ground_diffuse, f_gnd_beam, diffuse_fraction, vf_gnd_sky)\n553 \n554     # Ground-reflected irradiance on the row surface accounting for\n555     # the view to the ground. This deviates from [1], Eq. 10, 11 and\n556     # subsequent. Here, the row to ground view factor is computed. In [1],\n557     # the usual ground-reflected irradiance includes the single row to ground\n558     # view factor (1 - cos(tilt))/2, and Eq. 10, 11 and later multiply\n559     # this quantity by a ratio of view factors.\n560     poa_gnd_pv = _poa_ground_pv(\n561         f_x, ground_diffuse, f_gnd_pv_shade, f_gnd_pv_noshade)\n562 \n563     # add sky and ground-reflected irradiance on the row by irradiance\n564     # component\n565     poa_diffuse = poa_gnd_pv + poa_sky_pv\n566     # beam on plane, make an array for consistency with poa_diffuse\n567     poa_beam = np.atleast_1d(beam_component(\n568         surface_tilt, surface_azimuth, solar_zenith, solar_azimuth, dni))\n569     poa_direct = poa_beam * (1 - f_x) * iam  # direct only on the unshaded part\n570     poa_global = poa_direct + poa_diffuse\n571 \n572     output = {\n573         'poa_global': poa_global, 'poa_direct': poa_direct,\n574         'poa_diffuse': poa_diffuse, 'poa_ground_diffuse': poa_gnd_pv,\n575         'poa_sky_diffuse': poa_sky_pv}\n576     if isinstance(poa_global, pd.Series):\n577         output = pd.DataFrame(output)\n578     return output\n579 \n580 \n581 def get_irradiance(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,\n582                    gcr, height, pitch, ghi, dhi, dni,\n583                    albedo, iam_front=1.0, iam_back=1.0,\n584                    bifaciality=0.8, shade_factor=-0.02,\n585                    transmission_factor=0, npoints=100):\n586     \"\"\"\n587     Get front and rear irradiance using the infinite sheds model.\n588 \n589     The infinite sheds model [1] assumes the PV system comprises parallel,\n590     evenly spaced rows on a level, horizontal surface. Rows can be on fixed\n591     racking or single axis trackers. The model calculates irradiance at a\n592     location far from the ends of any rows, in effect, assuming that the\n593     rows (sheds) are infinitely long.\n594 \n595     The model accounts for the following effects:\n596 \n597     - restricted view of the sky from module surfaces due to the nearby rows.\n598     - restricted view of the ground from module surfaces due to nearby rows.\n599     - restricted view of the sky from the ground due to rows.\n600     - shading of module surfaces by nearby rows.\n601     - shading of rear cells of a module by mounting structure and by\n602       module features.\n603 \n604     The model implicitly assumes that diffuse irradiance from the sky is\n605     isotropic, and that module surfaces do not allow irradiance to transmit\n606     through the module to the ground through gaps between cells.\n607 \n608     Parameters\n609     ----------\n610     surface_tilt : numeric\n611         Tilt from horizontal of the front-side surface. [degree]\n612 \n613     surface_azimuth : numeric\n614         Surface azimuth in decimal degrees east of north\n615         (e.g. North = 0, South = 180, East = 90, West = 270). [degree]\n616 \n617     solar_zenith : numeric\n618         Refraction-corrected solar zenith. [degree]\n619 \n620     solar_azimuth : numeric\n621         Solar azimuth. [degree]\n622 \n623     gcr : float\n624         Ground coverage ratio, ratio of row slant length to row spacing.\n625         [unitless]\n626 \n627     height : float\n628         Height of the center point of the row above the ground; must be in the\n629         same units as ``pitch``.\n630 \n631     pitch : float\n632         Distance between two rows; must be in the same units as ``height``.\n633 \n634     ghi : numeric\n635         Global horizontal irradiance. [W/m2]\n636 \n637     dhi : numeric\n638         Diffuse horizontal irradiance. [W/m2]\n639 \n640     dni : numeric\n641         Direct normal irradiance. [W/m2]\n642 \n643     albedo : numeric\n644         Surface albedo. [unitless]\n645 \n646     iam_front : numeric, default 1.0\n647         Incidence angle modifier, the fraction of direct irradiance incident\n648         on the front surface that is not reflected away. [unitless]\n649 \n650     iam_back : numeric, default 1.0\n651         Incidence angle modifier, the fraction of direct irradiance incident\n652         on the back surface that is not reflected away. [unitless]\n653 \n654     bifaciality : numeric, default 0.8\n655         Ratio of the efficiency of the module's rear surface to the efficiency\n656         of the front surface. [unitless]\n657 \n658     shade_factor : numeric, default -0.02\n659         Fraction of back surface irradiance that is blocked by array mounting\n660         structures. Negative value is a reduction in back irradiance.\n661         [unitless]\n662 \n663     transmission_factor : numeric, default 0.0\n664         Fraction of irradiance on the back surface that does not reach the\n665         module's cells due to module features such as busbars, junction box,\n666         etc. A negative value is a reduction in back irradiance. [unitless]\n667 \n668     npoints : int, default 100\n669         Number of points used to discretize distance along the ground.\n670 \n671     Returns\n672     -------\n673     output : dict or DataFrame\n674         Output is a DataFrame when input ghi is a Series. See Notes for\n675         descriptions of content.\n676 \n677     Notes\n678     -----\n679 \n680     ``output`` includes:\n681 \n682     - ``poa_global`` : total irradiance reaching the module cells from both\n683       front and back surfaces. [W/m^2]\n684     - ``poa_front`` : total irradiance reaching the module cells from the front\n685       surface. [W/m^2]\n686     - ``poa_back`` : total irradiance reaching the module cells from the back\n687       surface. [W/m^2]\n688     - ``poa_front_direct`` : direct irradiance reaching the module cells from\n689       the front surface. [W/m^2]\n690     - ``poa_front_diffuse`` : total diffuse irradiance reaching the module\n691       cells from the front surface. [W/m^2]\n692     - ``poa_front_sky_diffuse`` : sky diffuse irradiance reaching the module\n693       cells from the front surface. [W/m^2]\n694     - ``poa_front_ground_diffuse`` : ground-reflected diffuse irradiance\n695       reaching the module cells from the front surface. [W/m^2]\n696     - ``poa_back_direct`` : direct irradiance reaching the module cells from\n697       the back surface. [W/m^2]\n698     - ``poa_back_diffuse`` : total diffuse irradiance reaching the module\n699       cells from the back surface. [W/m^2]\n700     - ``poa_back_sky_diffuse`` : sky diffuse irradiance reaching the module\n701       cells from the back surface. [W/m^2]\n702     - ``poa_back_ground_diffuse`` : ground-reflected diffuse irradiance\n703       reaching the module cells from the back surface. [W/m^2]\n704 \n705     References\n706     ----------\n707     .. [1] Mikofski, M., Darawali, R., Hamer, M., Neubert, A., and Newmiller,\n708        J. \"Bifacial Performance Modeling in Large Arrays\". 2019 IEEE 46th\n709        Photovoltaic Specialists Conference (PVSC), 2019, pp. 1282-1287.\n710        :doi:`10.1109/PVSC40753.2019.8980572`.\n711 \n712     See also\n713     --------\n714     get_irradiance_poa\n715     \"\"\"\n716     # backside is rotated and flipped relative to front\n717     backside_tilt, backside_sysaz = _backside(surface_tilt, surface_azimuth)\n718     # front side POA irradiance\n719     irrad_front = get_irradiance_poa(\n720         surface_tilt=surface_tilt, surface_azimuth=surface_azimuth,\n721         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n722         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n723         albedo=albedo, iam=iam_front, npoints=npoints)\n724     # back side POA irradiance\n725     irrad_back = get_irradiance_poa(\n726         surface_tilt=backside_tilt, surface_azimuth=backside_sysaz,\n727         solar_zenith=solar_zenith, solar_azimuth=solar_azimuth,\n728         gcr=gcr, height=height, pitch=pitch, ghi=ghi, dhi=dhi, dni=dni,\n729         albedo=albedo, iam=iam_back, npoints=npoints)\n730 \n731     colmap_front = {\n732         'poa_global': 'poa_front',\n733         'poa_direct': 'poa_front_direct',\n734         'poa_diffuse': 'poa_front_diffuse',\n735         'poa_sky_diffuse': 'poa_front_sky_diffuse',\n736         'poa_ground_diffuse': 'poa_front_ground_diffuse',\n737     }\n738     colmap_back = {\n739         'poa_global': 'poa_back',\n740         'poa_direct': 'poa_back_direct',\n741         'poa_diffuse': 'poa_back_diffuse',\n742         'poa_sky_diffuse': 'poa_back_sky_diffuse',\n743         'poa_ground_diffuse': 'poa_back_ground_diffuse',\n744     }\n745 \n746     if isinstance(ghi, pd.Series):\n747         irrad_front = irrad_front.rename(columns=colmap_front)\n748         irrad_back = irrad_back.rename(columns=colmap_back)\n749         output = pd.concat([irrad_front, irrad_back], axis=1)\n750     else:\n751         for old_key, new_key in colmap_front.items():\n752             irrad_front[new_key] = irrad_front.pop(old_key)\n753         for old_key, new_key in colmap_back.items():\n754             irrad_back[new_key] = irrad_back.pop(old_key)\n755         irrad_front.update(irrad_back)\n756         output = irrad_front\n757 \n758     effects = (1 + shade_factor) * (1 + transmission_factor)\n759     output['poa_global'] = output['poa_front'] + \\\n760         output['poa_back'] * bifaciality * effects\n761     return output\n762 \n763 \n764 def _backside(tilt, surface_azimuth):\n765     backside_tilt = 180. - tilt\n766     backside_sysaz = (180. + surface_azimuth) % 360.\n767     return backside_tilt, backside_sysaz\n768 \n[end of pvlib/bifacial/infinite_sheds.py]\n[start of pvlib/bifacial/pvfactors.py]\n1 \"\"\"\n2 The ``bifacial.pvfactors`` module contains functions for modeling back surface\n3 plane-of-array irradiance using the pvfactors package.\n4 \"\"\"\n5 \n6 import pandas as pd\n7 import numpy as np\n8 \n9 \n10 def pvfactors_timeseries(\n11         solar_azimuth, solar_zenith, surface_azimuth, surface_tilt,\n12         axis_azimuth, timestamps, dni, dhi, gcr, pvrow_height, pvrow_width,\n13         albedo, n_pvrows=3, index_observed_pvrow=1,\n14         rho_front_pvrow=0.03, rho_back_pvrow=0.05,\n15         horizon_band_angle=15.):\n16     \"\"\"\n17     Calculate front and back surface plane-of-array irradiance on\n18     a fixed tilt or single-axis tracker PV array configuration, and using\n19     the open-source \"pvfactors\" package.  pvfactors implements the model\n20     described in [1]_.\n21     Please refer to pvfactors online documentation for more details:\n22     https://sunpower.github.io/pvfactors/\n23 \n24     Parameters\n25     ----------\n26     solar_azimuth: numeric\n27         Sun's azimuth angles using pvlib's azimuth convention (deg)\n28     solar_zenith: numeric\n29         Sun's zenith angles (deg)\n30     surface_azimuth: numeric\n31         Azimuth angle of the front surface of the PV modules, using pvlib's\n32         convention (deg)\n33     surface_tilt: numeric\n34         Tilt angle of the PV modules, going from 0 to 180 (deg)\n35     axis_azimuth: float\n36         Azimuth angle of the rotation axis of the PV modules, using pvlib's\n37         convention (deg). This is supposed to be fixed for all timestamps.\n38     timestamps: datetime or DatetimeIndex\n39         List of simulation timestamps\n40     dni: numeric\n41         Direct normal irradiance (W/m2)\n42     dhi: numeric\n43         Diffuse horizontal irradiance (W/m2)\n44     gcr: float\n45         Ground coverage ratio of the pv array\n46     pvrow_height: float\n47         Height of the pv rows, measured at their center (m)\n48     pvrow_width: float\n49         Width of the pv rows in the considered 2D plane (m)\n50     albedo: float\n51         Ground albedo\n52     n_pvrows: int, default 3\n53         Number of PV rows to consider in the PV array\n54     index_observed_pvrow: int, default 1\n55         Index of the PV row whose incident irradiance will be returned. Indices\n56         of PV rows go from 0 to n_pvrows-1.\n57     rho_front_pvrow: float, default 0.03\n58         Front surface reflectivity of PV rows\n59     rho_back_pvrow: float, default 0.05\n60         Back surface reflectivity of PV rows\n61     horizon_band_angle: float, default 15\n62         Elevation angle of the sky dome's diffuse horizon band (deg)\n63 \n64     Returns\n65     -------\n66     poa_front: numeric\n67         Calculated incident irradiance on the front surface of the PV modules\n68         (W/m2)\n69     poa_back: numeric\n70         Calculated incident irradiance on the back surface of the PV modules\n71         (W/m2)\n72     poa_front_absorbed: numeric\n73         Calculated absorbed irradiance on the front surface of the PV modules\n74         (W/m2), after AOI losses\n75     poa_back_absorbed: numeric\n76         Calculated absorbed irradiance on the back surface of the PV modules\n77         (W/m2), after AOI losses\n78 \n79     References\n80     ----------\n81     .. [1] Anoma, Marc Abou, et al. \"View Factor Model and Validation for\n82         Bifacial PV and Diffuse Shade on Single-Axis Trackers.\" 44th IEEE\n83         Photovoltaic Specialist Conference. 2017.\n84     \"\"\"\n85     # Convert Series, list, float inputs to numpy arrays\n86     solar_azimuth = np.array(solar_azimuth)\n87     solar_zenith = np.array(solar_zenith)\n88     dni = np.array(dni)\n89     dhi = np.array(dhi)\n90     # GH 1127, GH 1332\n91     surface_tilt = np.full_like(solar_zenith, surface_tilt)\n92     surface_azimuth = np.full_like(solar_zenith, surface_azimuth)\n93 \n94     # Import pvfactors functions for timeseries calculations.\n95     from pvfactors.run import run_timeseries_engine\n96 \n97     # Build up pv array configuration parameters\n98     pvarray_parameters = {\n99         'n_pvrows': n_pvrows,\n100         'axis_azimuth': axis_azimuth,\n101         'pvrow_height': pvrow_height,\n102         'pvrow_width': pvrow_width,\n103         'gcr': gcr\n104     }\n105 \n106     irradiance_model_params = {\n107         'rho_front': rho_front_pvrow,\n108         'rho_back': rho_back_pvrow,\n109         'horizon_band_angle': horizon_band_angle\n110     }\n111 \n112     # Create report function\n113     def fn_build_report(pvarray):\n114         return {'total_inc_back': pvarray.ts_pvrows[index_observed_pvrow]\n115                 .back.get_param_weighted('qinc'),\n116                 'total_inc_front': pvarray.ts_pvrows[index_observed_pvrow]\n117                 .front.get_param_weighted('qinc'),\n118                 'total_abs_back': pvarray.ts_pvrows[index_observed_pvrow]\n119                 .back.get_param_weighted('qabs'),\n120                 'total_abs_front': pvarray.ts_pvrows[index_observed_pvrow]\n121                 .front.get_param_weighted('qabs')}\n122 \n123     # Run pvfactors calculations\n124     report = run_timeseries_engine(\n125         fn_build_report, pvarray_parameters,\n126         timestamps, dni, dhi, solar_zenith, solar_azimuth,\n127         surface_tilt, surface_azimuth, albedo,\n128         irradiance_model_params=irradiance_model_params)\n129 \n130     # Turn report into dataframe\n131     df_report = pd.DataFrame(report, index=timestamps)\n132 \n133     return (df_report.total_inc_front, df_report.total_inc_back,\n134             df_report.total_abs_front, df_report.total_abs_back)\n135 \n[end of pvlib/bifacial/pvfactors.py]\n[start of pvlib/tracking.py]\n1 import numpy as np\n2 import pandas as pd\n3 \n4 from pvlib.tools import cosd, sind, tand\n5 from pvlib.pvsystem import (\n6     PVSystem, Array, SingleAxisTrackerMount, _unwrap_single_value\n7 )\n8 from pvlib import irradiance, atmosphere\n9 from pvlib._deprecation import deprecated\n10 \n11 \n12 @deprecated('0.9.0', alternative='PVSystem with SingleAxisTrackerMount')\n13 class SingleAxisTracker(PVSystem):\n14     \"\"\"\n15     A class for single-axis trackers that inherits the PV modeling methods from\n16     :py:class:`~pvlib.pvsystem.PVSystem`. For details on calculating tracker\n17     rotation see :py:func:`pvlib.tracking.singleaxis`.\n18 \n19     Parameters\n20     ----------\n21     axis_tilt : float, default 0\n22         The tilt of the axis of rotation (i.e, the y-axis defined by\n23         axis_azimuth) with respect to horizontal, in decimal degrees.\n24 \n25     axis_azimuth : float, default 0\n26         A value denoting the compass direction along which the axis of\n27         rotation lies. Measured in decimal degrees east of north.\n28 \n29     max_angle : float, default 90\n30         A value denoting the maximum rotation angle, in decimal degrees,\n31         of the one-axis tracker from its horizontal position (horizontal\n32         if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n33         to rotate to a vertical position to point the panel towards a\n34         horizon. max_angle of 180 degrees allows for full rotation.\n35 \n36     backtrack : bool, default True\n37         Controls whether the tracker has the capability to \"backtrack\"\n38         to avoid row-to-row shading. False denotes no backtrack\n39         capability. True denotes backtrack capability.\n40 \n41     gcr : float, default 2.0/7.0\n42         A value denoting the ground coverage ratio of a tracker system\n43         which utilizes backtracking; i.e. the ratio between the PV array\n44         surface area to total ground area. A tracker system with modules\n45         2 meters wide, centered on the tracking axis, with 6 meters\n46         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n47         provided, a gcr of 2/7 is default. gcr must be <=1.\n48 \n49     cross_axis_tilt : float, default 0.0\n50         The angle, relative to horizontal, of the line formed by the\n51         intersection between the slope containing the tracker axes and a plane\n52         perpendicular to the tracker axes. Cross-axis tilt should be specified\n53         using a right-handed convention. For example, trackers with axis\n54         azimuth of 180 degrees (heading south) will have a negative cross-axis\n55         tilt if the tracker axes plane slopes down to the east and positive\n56         cross-axis tilt if the tracker axes plane slopes up to the east. Use\n57         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n58         `cross_axis_tilt`. [degrees]\n59 \n60     **kwargs\n61         Passed to :py:class:`~pvlib.pvsystem.PVSystem`. If the `arrays`\n62         parameter is specified it must have only a single Array. Furthermore\n63         if a :py:class:`~pvlib.pvsystem.Array` is provided it must have\n64         ``surface_tilt`` and ``surface_azimuth`` equal to None.\n65 \n66     Raises\n67     ------\n68     ValueError\n69         If more than one Array is specified.\n70     ValueError\n71         If an Array is provided with a surface tilt or azimuth not None.\n72 \n73     See also\n74     --------\n75     pvlib.tracking.singleaxis\n76     pvlib.tracking.calc_axis_tilt\n77     pvlib.tracking.calc_cross_axis_tilt\n78     \"\"\"\n79 \n80     def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n81                  backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n82 \n83         mount_kwargs = {\n84             k: kwargs.pop(k) for k in ['racking_model', 'module_height']\n85             if k in kwargs\n86         }\n87         mount = SingleAxisTrackerMount(axis_tilt, axis_azimuth, max_angle,\n88                                        backtrack, gcr, cross_axis_tilt,\n89                                        **mount_kwargs)\n90 \n91         array_defaults = {\n92             'albedo': None, 'surface_type': None, 'module': None,\n93             'module_type': None, 'module_parameters': None,\n94             'temperature_model_parameters': None,\n95             'modules_per_string': 1,\n96         }\n97         array_kwargs = {\n98             key: kwargs.get(key, array_defaults[key]) for key in array_defaults\n99         }\n100         # strings/strings_per_inverter is a special case\n101         array_kwargs['strings'] = kwargs.get('strings_per_inverter', 1)\n102 \n103         array = Array(mount=mount, **array_kwargs)\n104         pass_through_kwargs = {  # other args to pass to PVSystem()\n105             k: v for k, v in kwargs.items() if k not in array_defaults\n106         }\n107         # leave these in case someone is using them\n108         self.axis_tilt = axis_tilt\n109         self.axis_azimuth = axis_azimuth\n110         self.max_angle = max_angle\n111         self.backtrack = backtrack\n112         self.gcr = gcr\n113         self.cross_axis_tilt = cross_axis_tilt\n114 \n115         pass_through_kwargs['surface_tilt'] = None\n116         pass_through_kwargs['surface_azimuth'] = None\n117 \n118         super().__init__(arrays=[array], **pass_through_kwargs)\n119 \n120     def __repr__(self):\n121         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr',\n122                  'cross_axis_tilt']\n123         sat_repr = ('SingleAxisTracker:\\n  ' + '\\n  '.join(\n124             f'{attr}: {getattr(self, attr)}' for attr in attrs))\n125         # get the parent PVSystem info\n126         pvsystem_repr = super().__repr__()\n127         # remove the first line (contains 'PVSystem: \\n')\n128         pvsystem_repr = '\\n'.join(pvsystem_repr.split('\\n')[1:])\n129         return sat_repr + '\\n' + pvsystem_repr\n130 \n131     def singleaxis(self, apparent_zenith, apparent_azimuth):\n132         \"\"\"\n133         Get tracking data. See :py:func:`pvlib.tracking.singleaxis` more\n134         detail.\n135 \n136         Parameters\n137         ----------\n138         apparent_zenith : float, 1d array, or Series\n139             Solar apparent zenith angles in decimal degrees.\n140 \n141         apparent_azimuth : float, 1d array, or Series\n142             Solar apparent azimuth angles in decimal degrees.\n143 \n144         Returns\n145         -------\n146         tracking data\n147         \"\"\"\n148         tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n149                                    self.axis_tilt, self.axis_azimuth,\n150                                    self.max_angle, self.backtrack,\n151                                    self.gcr, self.cross_axis_tilt)\n152 \n153         return tracking_data\n154 \n155     def get_aoi(self, surface_tilt, surface_azimuth, solar_zenith,\n156                 solar_azimuth):\n157         \"\"\"Get the angle of incidence on the system.\n158 \n159         For a given set of solar zenith and azimuth angles, the\n160         surface tilt and azimuth parameters are typically determined\n161         by :py:meth:`~SingleAxisTracker.singleaxis`. The\n162         :py:meth:`~SingleAxisTracker.singleaxis` method also returns\n163         the angle of incidence, so this method is only needed\n164         if using a different tracking algorithm.\n165 \n166         Parameters\n167         ----------\n168         surface_tilt : numeric\n169             Panel tilt from horizontal.\n170         surface_azimuth : numeric\n171             Panel azimuth from north\n172         solar_zenith : float or Series.\n173             Solar zenith angle.\n174         solar_azimuth : float or Series.\n175             Solar azimuth angle.\n176 \n177         Returns\n178         -------\n179         aoi : Series\n180             The angle of incidence in degrees from normal.\n181         \"\"\"\n182 \n183         aoi = irradiance.aoi(surface_tilt, surface_azimuth,\n184                              solar_zenith, solar_azimuth)\n185         return aoi\n186 \n187     @_unwrap_single_value\n188     def get_irradiance(self, surface_tilt, surface_azimuth,\n189                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n190                        dni_extra=None, airmass=None, model='haydavies',\n191                        **kwargs):\n192         \"\"\"\n193         Uses the :func:`irradiance.get_total_irradiance` function to\n194         calculate the plane of array irradiance components on a tilted\n195         surface defined by the input data and ``self.albedo``.\n196 \n197         For a given set of solar zenith and azimuth angles, the\n198         surface tilt and azimuth parameters are typically determined\n199         by :py:meth:`~SingleAxisTracker.singleaxis`.\n200 \n201         Parameters\n202         ----------\n203         surface_tilt : numeric\n204             Panel tilt from horizontal.\n205         surface_azimuth : numeric\n206             Panel azimuth from north\n207         solar_zenith : numeric\n208             Solar zenith angle.\n209         solar_azimuth : numeric\n210             Solar azimuth angle.\n211         dni : float or Series\n212             Direct Normal Irradiance\n213         ghi : float or Series\n214             Global horizontal irradiance\n215         dhi : float or Series\n216             Diffuse horizontal irradiance\n217         dni_extra : float or Series, default None\n218             Extraterrestrial direct normal irradiance\n219         airmass : float or Series, default None\n220             Airmass\n221         model : String, default 'haydavies'\n222             Irradiance model.\n223 \n224         **kwargs\n225             Passed to :func:`irradiance.get_total_irradiance`.\n226 \n227         Returns\n228         -------\n229         poa_irradiance : DataFrame\n230             Column names are: ``total, beam, sky, ground``.\n231         \"\"\"\n232 \n233         # not needed for all models, but this is easier\n234         if dni_extra is None:\n235             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n236 \n237         if airmass is None:\n238             airmass = atmosphere.get_relative_airmass(solar_zenith)\n239 \n240         # SingleAxisTracker only supports a single Array, but we need the\n241         # validate/iterate machinery so that single length tuple input/output\n242         # is handled the same as PVSystem.get_irradiance. GH 1159\n243         dni = self._validate_per_array(dni, system_wide=True)\n244         ghi = self._validate_per_array(ghi, system_wide=True)\n245         dhi = self._validate_per_array(dhi, system_wide=True)\n246 \n247         return tuple(\n248             irradiance.get_total_irradiance(\n249                 surface_tilt,\n250                 surface_azimuth,\n251                 solar_zenith,\n252                 solar_azimuth,\n253                 dni, ghi, dhi,\n254                 dni_extra=dni_extra,\n255                 airmass=airmass,\n256                 model=model,\n257                 albedo=self.arrays[0].albedo,\n258                 **kwargs)\n259             for array, dni, ghi, dhi in zip(\n260                 self.arrays, dni, ghi, dhi\n261             )\n262         )\n263 \n264 \n265 def singleaxis(apparent_zenith, apparent_azimuth,\n266                axis_tilt=0, axis_azimuth=0, max_angle=90,\n267                backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0):\n268     \"\"\"\n269     Determine the rotation angle of a single-axis tracker when given particular\n270     solar zenith and azimuth angles.\n271 \n272     See [1]_ for details about the equations. Backtracking may be specified,\n273     and if so, a ground coverage ratio is required.\n274 \n275     Rotation angle is determined in a right-handed coordinate system. The\n276     tracker `axis_azimuth` defines the positive y-axis, the positive x-axis is\n277     90 degrees clockwise from the y-axis and parallel to the Earth's surface,\n278     and the positive z-axis is normal to both x & y-axes and oriented skyward.\n279     Rotation angle `tracker_theta` is a right-handed rotation around the y-axis\n280     in the x, y, z coordinate system and indicates tracker position relative to\n281     horizontal. For example, if tracker `axis_azimuth` is 180 (oriented south)\n282     and `axis_tilt` is zero, then a `tracker_theta` of zero is horizontal, a\n283     `tracker_theta` of 30 degrees is a rotation of 30 degrees towards the west,\n284     and a `tracker_theta` of -90 degrees is a rotation to the vertical plane\n285     facing east.\n286 \n287     Parameters\n288     ----------\n289     apparent_zenith : float, 1d array, or Series\n290         Solar apparent zenith angles in decimal degrees.\n291 \n292     apparent_azimuth : float, 1d array, or Series\n293         Solar apparent azimuth angles in decimal degrees.\n294 \n295     axis_tilt : float, default 0\n296         The tilt of the axis of rotation (i.e, the y-axis defined by\n297         axis_azimuth) with respect to horizontal, in decimal degrees.\n298 \n299     axis_azimuth : float, default 0\n300         A value denoting the compass direction along which the axis of\n301         rotation lies. Measured in decimal degrees east of north.\n302 \n303     max_angle : float, default 90\n304         A value denoting the maximum rotation angle, in decimal degrees,\n305         of the one-axis tracker from its horizontal position (horizontal\n306         if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n307         to rotate to a vertical position to point the panel towards a\n308         horizon. max_angle of 180 degrees allows for full rotation.\n309 \n310     backtrack : bool, default True\n311         Controls whether the tracker has the capability to \"backtrack\"\n312         to avoid row-to-row shading. False denotes no backtrack\n313         capability. True denotes backtrack capability.\n314 \n315     gcr : float, default 2.0/7.0\n316         A value denoting the ground coverage ratio of a tracker system\n317         which utilizes backtracking; i.e. the ratio between the PV array\n318         surface area to total ground area. A tracker system with modules\n319         2 meters wide, centered on the tracking axis, with 6 meters\n320         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n321         provided, a gcr of 2/7 is default. gcr must be <=1.\n322 \n323     cross_axis_tilt : float, default 0.0\n324         The angle, relative to horizontal, of the line formed by the\n325         intersection between the slope containing the tracker axes and a plane\n326         perpendicular to the tracker axes. Cross-axis tilt should be specified\n327         using a right-handed convention. For example, trackers with axis\n328         azimuth of 180 degrees (heading south) will have a negative cross-axis\n329         tilt if the tracker axes plane slopes down to the east and positive\n330         cross-axis tilt if the tracker axes plane slopes up to the east. Use\n331         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n332         `cross_axis_tilt`. [degrees]\n333 \n334     Returns\n335     -------\n336     dict or DataFrame with the following columns:\n337         * `tracker_theta`: The rotation angle of the tracker.\n338           tracker_theta = 0 is horizontal, and positive rotation angles are\n339           clockwise. [degrees]\n340         * `aoi`: The angle-of-incidence of direct irradiance onto the\n341           rotated panel surface. [degrees]\n342         * `surface_tilt`: The angle between the panel surface and the earth\n343           surface, accounting for panel rotation. [degrees]\n344         * `surface_azimuth`: The azimuth of the rotated panel, determined by\n345           projecting the vector normal to the panel's surface to the earth's\n346           surface. [degrees]\n347 \n348     See also\n349     --------\n350     pvlib.tracking.calc_axis_tilt\n351     pvlib.tracking.calc_cross_axis_tilt\n352 \n353     References\n354     ----------\n355     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n356        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n357        https://www.nrel.gov/docs/fy20osti/76626.pdf\n358     \"\"\"\n359 \n360     # MATLAB to Python conversion by\n361     # Will Holmgren (@wholmgren), U. Arizona. March, 2015.\n362 \n363     if isinstance(apparent_zenith, pd.Series):\n364         index = apparent_zenith.index\n365     else:\n366         index = None\n367 \n368     # convert scalars to arrays\n369     apparent_azimuth = np.atleast_1d(apparent_azimuth)\n370     apparent_zenith = np.atleast_1d(apparent_zenith)\n371 \n372     if apparent_azimuth.ndim > 1 or apparent_zenith.ndim > 1:\n373         raise ValueError('Input dimensions must not exceed 1')\n374 \n375     # Calculate sun position x, y, z using coordinate system as in [1], Eq 1.\n376 \n377     # NOTE: solar elevation = 90 - solar zenith, then use trig identities:\n378     # sin(90-x) = cos(x) & cos(90-x) = sin(x)\n379     sin_zenith = sind(apparent_zenith)\n380     x = sin_zenith * sind(apparent_azimuth)\n381     y = sin_zenith * cosd(apparent_azimuth)\n382     z = cosd(apparent_zenith)\n383 \n384     # Assume the tracker reference frame is right-handed. Positive y-axis is\n385     # oriented along tracking axis; from north, the y-axis is rotated clockwise\n386     # by the axis azimuth and tilted from horizontal by the axis tilt. The\n387     # positive x-axis is 90 deg clockwise from the y-axis and parallel to\n388     # horizontal (e.g., if the y-axis is south, the x-axis is west); the\n389     # positive z-axis is normal to the x and y axes, pointed upward.\n390 \n391     # Calculate sun position (xp, yp, zp) in tracker coordinate system using\n392     # [1] Eq 4.\n393 \n394     cos_axis_azimuth = cosd(axis_azimuth)\n395     sin_axis_azimuth = sind(axis_azimuth)\n396     cos_axis_tilt = cosd(axis_tilt)\n397     sin_axis_tilt = sind(axis_tilt)\n398     xp = x*cos_axis_azimuth - y*sin_axis_azimuth\n399     yp = (x*cos_axis_tilt*sin_axis_azimuth\n400           + y*cos_axis_tilt*cos_axis_azimuth\n401           - z*sin_axis_tilt)\n402     zp = (x*sin_axis_tilt*sin_axis_azimuth\n403           + y*sin_axis_tilt*cos_axis_azimuth\n404           + z*cos_axis_tilt)\n405 \n406     # The ideal tracking angle wid is the rotation to place the sun position\n407     # vector (xp, yp, zp) in the (y, z) plane, which is normal to the panel and\n408     # contains the axis of rotation.  wid = 0 indicates that the panel is\n409     # horizontal. Here, our convention is that a clockwise rotation is\n410     # positive, to view rotation angles in the same frame of reference as\n411     # azimuth. For example, for a system with tracking axis oriented south, a\n412     # rotation toward the east is negative, and a rotation to the west is\n413     # positive. This is a right-handed rotation around the tracker y-axis.\n414 \n415     # Calculate angle from x-y plane to projection of sun vector onto x-z plane\n416     # using [1] Eq. 5.\n417 \n418     wid = np.degrees(np.arctan2(xp, zp))\n419 \n420     # filter for sun above panel horizon\n421     zen_gt_90 = apparent_zenith > 90\n422     wid[zen_gt_90] = np.nan\n423 \n424     # Account for backtracking\n425     if backtrack:\n426         # distance between rows in terms of rack lengths relative to cross-axis\n427         # tilt\n428         axes_distance = 1/(gcr * cosd(cross_axis_tilt))\n429 \n430         # NOTE: account for rare angles below array, see GH 824\n431         temp = np.abs(axes_distance * cosd(wid - cross_axis_tilt))\n432 \n433         # backtrack angle using [1], Eq. 14\n434         with np.errstate(invalid='ignore'):\n435             wc = np.degrees(-np.sign(wid)*np.arccos(temp))\n436 \n437         # NOTE: in the middle of the day, arccos(temp) is out of range because\n438         # there's no row-to-row shade to avoid, & backtracking is unnecessary\n439         # [1], Eqs. 15-16\n440         with np.errstate(invalid='ignore'):\n441             tracker_theta = wid + np.where(temp < 1, wc, 0)\n442     else:\n443         tracker_theta = wid\n444 \n445     # NOTE: max_angle defined relative to zero-point rotation, not the\n446     # system-plane normal\n447     tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n448 \n449     # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n450     # y-axis is axis of tracker rotation. tracker_theta is a compass angle\n451     # (clockwise is positive) rather than a trigonometric angle.\n452     # NOTE: the *0 is a trick to preserve NaN values.\n453     panel_norm = np.array([sind(tracker_theta),\n454                            tracker_theta*0,\n455                            cosd(tracker_theta)])\n456 \n457     # sun position in vector format in panel-oriented x, y, z coordinates\n458     sun_vec = np.array([xp, yp, zp])\n459 \n460     # calculate angle-of-incidence on panel\n461     # TODO: use irradiance.aoi\n462     projection = np.clip(np.sum(sun_vec*panel_norm, axis=0), -1, 1)\n463     aoi = np.degrees(np.arccos(projection))\n464 \n465     # Calculate panel tilt and azimuth in a coordinate system where the panel\n466     # tilt is the angle from horizontal, and the panel azimuth is the compass\n467     # angle (clockwise from north) to the projection of the panel's normal to\n468     # the earth's surface. These outputs are provided for convenience and\n469     # comparison with other PV software which use these angle conventions.\n470 \n471     # Project normal vector to earth surface. First rotate about x-axis by\n472     # angle -axis_tilt so that y-axis is also parallel to earth surface, then\n473     # project.\n474 \n475     # Calculate standard rotation matrix\n476     rot_x = np.array([[1, 0, 0],\n477                       [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n478                       [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n479 \n480     # panel_norm_earth contains the normal vector expressed in earth-surface\n481     # coordinates (z normal to surface, y aligned with tracker axis parallel to\n482     # earth)\n483     panel_norm_earth = np.dot(rot_x, panel_norm).T\n484 \n485     # projection to plane tangent to earth surface, in earth surface\n486     # coordinates\n487     projected_normal = np.array([panel_norm_earth[:, 0],\n488                                  panel_norm_earth[:, 1],\n489                                  panel_norm_earth[:, 2]*0]).T\n490 \n491     # calculate vector magnitudes\n492     projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n493 \n494     # renormalize the projected vector, avoid creating nan values.\n495     non_zeros = projected_normal_mag != 0\n496     projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n497                                    projected_normal_mag[non_zeros]).T\n498 \n499     # calculation of surface_azimuth\n500     surface_azimuth = \\\n501         np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n502 \n503     # Rotate 0 reference from panel's x-axis to its y-axis and then back to\n504     # north.\n505     surface_azimuth = 90 - surface_azimuth + axis_azimuth\n506 \n507     # Map azimuth into [0,360) domain.\n508     with np.errstate(invalid='ignore'):\n509         surface_azimuth = surface_azimuth % 360\n510 \n511     # Calculate surface_tilt\n512     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n513     # for edge cases like axis_tilt=90, numpy's SIMD can produce values like\n514     # dotproduct = (1 + 2e-16). Clip off the excess so that arccos works:\n515     dotproduct = np.clip(dotproduct, -1, 1)\n516     surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n517 \n518     # Bundle DataFrame for return values and filter for sun below horizon.\n519     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n520            'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n521     if index is not None:\n522         out = pd.DataFrame(out, index=index)\n523         out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n524         out[zen_gt_90] = np.nan\n525     else:\n526         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n527 \n528     return out\n529 \n530 \n531 def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n532     \"\"\"\n533     Calculate tracker axis tilt in the global reference frame when on a sloped\n534     plane.\n535 \n536     Parameters\n537     ----------\n538     slope_azimuth : float\n539         direction of normal to slope on horizontal [degrees]\n540     slope_tilt : float\n541         tilt of normal to slope relative to vertical [degrees]\n542     axis_azimuth : float\n543         direction of tracker axes on horizontal [degrees]\n544 \n545     Returns\n546     -------\n547     axis_tilt : float\n548         tilt of tracker [degrees]\n549 \n550     See also\n551     --------\n552     pvlib.tracking.singleaxis\n553     pvlib.tracking.calc_cross_axis_tilt\n554 \n555     Notes\n556     -----\n557     See [1]_ for derivation of equations.\n558 \n559     References\n560     ----------\n561     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n562        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n563        https://www.nrel.gov/docs/fy20osti/76626.pdf\n564     \"\"\"\n565     delta_gamma = axis_azimuth - slope_azimuth\n566     # equations 18-19\n567     tan_axis_tilt = cosd(delta_gamma) * tand(slope_tilt)\n568     return np.degrees(np.arctan(tan_axis_tilt))\n569 \n570 \n571 def _calc_tracker_norm(ba, bg, dg):\n572     \"\"\"\n573     Calculate tracker normal, v, cross product of tracker axis and unit normal,\n574     N, to the system slope plane.\n575 \n576     Parameters\n577     ----------\n578     ba : float\n579         axis tilt [degrees]\n580     bg : float\n581         ground tilt [degrees]\n582     dg : float\n583         delta gamma, difference between axis and ground azimuths [degrees]\n584 \n585     Returns\n586     -------\n587     vector : tuple\n588         vx, vy, vz\n589     \"\"\"\n590     cos_ba = cosd(ba)\n591     cos_bg = cosd(bg)\n592     sin_bg = sind(bg)\n593     sin_dg = sind(dg)\n594     vx = sin_dg * cos_ba * cos_bg\n595     vy = sind(ba)*sin_bg + cosd(dg)*cos_ba*cos_bg\n596     vz = -sin_dg*sin_bg*cos_ba\n597     return vx, vy, vz\n598 \n599 \n600 def _calc_beta_c(v, dg, ba):\n601     \"\"\"\n602     Calculate the cross-axis tilt angle.\n603 \n604     Parameters\n605     ----------\n606     v : tuple\n607         tracker normal\n608     dg : float\n609         delta gamma, difference between axis and ground azimuths [degrees]\n610     ba : float\n611         axis tilt [degrees]\n612 \n613     Returns\n614     -------\n615     beta_c : float\n616         cross-axis tilt angle [radians]\n617     \"\"\"\n618     vnorm = np.sqrt(np.dot(v, v))\n619     beta_c = np.arcsin(\n620         ((v[0]*cosd(dg) - v[1]*sind(dg)) * sind(ba) + v[2]*cosd(ba)) / vnorm)\n621     return beta_c\n622 \n623 \n624 def calc_cross_axis_tilt(\n625         slope_azimuth, slope_tilt, axis_azimuth, axis_tilt):\n626     \"\"\"\n627     Calculate the angle, relative to horizontal, of the line formed by the\n628     intersection between the slope containing the tracker axes and a plane\n629     perpendicular to the tracker axes.\n630 \n631     Use the cross-axis tilt to avoid row-to-row shade when backtracking on a\n632     slope not parallel with the axis azimuth. Cross-axis tilt should be\n633     specified using a right-handed convention. For example, trackers with axis\n634     azimuth of 180 degrees (heading south) will have a negative cross-axis tilt\n635     if the tracker axes plane slopes down to the east and positive cross-axis\n636     tilt if the tracker axes plane slopes up to the east.\n637 \n638     Parameters\n639     ----------\n640     slope_azimuth : float\n641         direction of the normal to the slope containing the tracker axes, when\n642         projected on the horizontal [degrees]\n643     slope_tilt : float\n644         angle of the slope containing the tracker axes, relative to horizontal\n645         [degrees]\n646     axis_azimuth : float\n647         direction of tracker axes projected on the horizontal [degrees]\n648     axis_tilt : float\n649         tilt of trackers relative to horizontal [degrees]\n650 \n651     Returns\n652     -------\n653     cross_axis_tilt : float\n654         angle, relative to horizontal, of the line formed by the intersection\n655         between the slope containing the tracker axes and a plane perpendicular\n656         to the tracker axes [degrees]\n657 \n658     See also\n659     --------\n660     pvlib.tracking.singleaxis\n661     pvlib.tracking.calc_axis_tilt\n662 \n663     Notes\n664     -----\n665     See [1]_ for derivation of equations.\n666 \n667     References\n668     ----------\n669     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n670        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n671        https://www.nrel.gov/docs/fy20osti/76626.pdf\n672     \"\"\"\n673     # delta-gamma, difference between axis and slope azimuths\n674     delta_gamma = axis_azimuth - slope_azimuth\n675     # equation 22\n676     v = _calc_tracker_norm(axis_tilt, slope_tilt, delta_gamma)\n677     # equation 26\n678     beta_c = _calc_beta_c(v, delta_gamma, axis_tilt)\n679     return np.degrees(beta_c)\n680 \n[end of pvlib/tracking.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/tracking.py\n+++ b/pvlib/tracking.py\n@@ -265,6 +265,35 @@\n \n     return out\n+    \n+def rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\n+    \"\"\"\n+    Convert tracker rotation angle to surface tilt and azimuth.\n+\n+    Parameters\n+    ----------\n+    tracker_theta : float, 1d array, or Series\n+        The rotation angle of the tracker.\n+\n+    axis_tilt : float, default 0\n+        The tilt of the axis of rotation with respect to horizontal, in decimal degrees.\n+\n+    axis_azimuth : float, default 0\n+        A value denoting the compass direction along which the axis of rotation lies.\n+\n+    max_angle : float, default 90\n+        A value denoting the maximum rotation angle of the one-axis tracker.\n+\n+    Returns\n+    -------\n+    dict\n+        Contains `surface_tilt` and `surface_azimuth` calculated from the tracker rotation.\n+    \"\"\"\n+\n+    # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n+    panel_norm = np.array([sind(tracker_theta), 0, cosd(tracker_theta)])\n+\n+    # Calculate surface tilt and azimuth\n+    surface_tilt = 90 - np.degrees(np.arccos(panel_norm[2]))\n+    surface_azimuth = (90 - np.degrees(np.arctan2(panel_norm[0], panel_norm[1]))) + axis_azimuth\n+    surface_azimuth = surface_azimuth % 360  # Ensure azimuth is in [0, 360)\n+\n+    return {'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n \n def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n     \"\"\"\n```", "model_patch": "--- a/pvlib/tracking.py\n+++ b/pvlib/tracking.py\n@@ -265,6 +265,35 @@\n \n     return out\n+    \n+def rotation_to_orientation(tracker_theta, axis_tilt=0, axis_azimuth=0, max_angle=90):\n+    \"\"\"\n+    Convert tracker rotation angle to surface tilt and azimuth.\n+\n+    Parameters\n+    ----------\n+    tracker_theta : float, 1d array, or Series\n+        The rotation angle of the tracker.\n+\n+    axis_tilt : float, default 0\n+        The tilt of the axis of rotation with respect to horizontal, in decimal degrees.\n+\n+    axis_azimuth : float, default 0\n+        A value denoting the compass direction along which the axis of rotation lies.\n+\n+    max_angle : float, default 90\n+        A value denoting the maximum rotation angle of the one-axis tracker.\n+\n+    Returns\n+    -------\n+    dict\n+        Contains `surface_tilt` and `surface_azimuth` calculated from the tracker rotation.\n+    \"\"\"\n+\n+    # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n+    panel_norm = np.array([sind(tracker_theta), 0, cosd(tracker_theta)])\n+\n+    # Calculate surface tilt and azimuth\n+    surface_tilt = 90 - np.degrees(np.arccos(panel_norm[2]))\n+    surface_azimuth = (90 - np.degrees(np.arctan2(panel_norm[0], panel_norm[1]))) + axis_azimuth\n+    surface_azimuth = surface_azimuth % 360  # Ensure azimuth is in [0, 360)\n+\n+    return {'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n \n def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n     \"\"\"\n"}
{"instance_id": "pvlib__pvlib-python-1653", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nCorrections to Townsend snow model\nPrivate communications with the model's author have turned up some issues with the pvlib implementation. Chief among the issues is  this part of the calculation:\r\n\r\n```\r\n    lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\r\n    gamma = (\r\n        slant_height\r\n        * effective_snow_weighted_m\r\n        * cosd(surface_tilt)\r\n        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\r\n        * 2\r\n        * tand(angle_of_repose)\r\n    )\r\n\r\n    ground_interference_term = 1 - C2 * np.exp(-gamma)\r\n```\r\n\r\nWhen `lower_edge_height_clipped` < `effective_snow_weighted_m`, `gamma` < 0 and the `ground_interference_term` can become negative. In contrast, the author's intent is that C2 < `ground_interference_terms` < 1. The author recommends clipping the squared difference (lower bound being worked out but will be something like 0.01.).\r\n\r\nOther issues appear to arise from the unit conversions. The published model uses inches for distance and snow depth. The pvlib code uses cm for snow depth (convenience for working with external snow data) and m for distances (for consistency with the rest of pvlib). After several steps, including the `ground_interference_term` calculation, the code converts from cm or m to inches to apply the final formula for loss (since the formula involves some coefficients determined by a regression). It would be easier to trace the pvlib code back to the paper if the internal unit conversions (from cm / m to inches) were done earlier.\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/main/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amain\">\n32       <img src=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=main\" alt=\"GitHub Actions Testing Status\" />\n33     </a>\n34     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n35     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/main/graph/badge.svg\" alt=\"codecov coverage\" />\n36     </a>\n37   </td>\n38 </tr>\n39 <tr>\n40   <td>Benchmarks</td>\n41   <td>\n42     <a href=\"https://pvlib.github.io/pvlib-benchmarks/\">\n43     <img src=\"https://img.shields.io/badge/benchmarks-asv-lightgrey\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Publications</td>\n49   <td>\n50     <a href=\"https://doi.org/10.5281/zenodo.593284\">\n51     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg\" alt=\"zenodo reference\">\n52     </a>\n53     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n54     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Downloads</td>\n60   <td>\n61     <a href=\"https://pypi.org/project/pvlib/\">\n62     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n63     </a>\n64     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n65     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n66     </a>\n67   </td>\n68 </tr>\n69 </table>\n70 \n71 \n72 pvlib python is a community supported tool that provides a set of\n73 functions and classes for simulating the performance of photovoltaic\n74 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n75 toolbox developed at Sandia National Laboratories and it implements many\n76 of the models and methods developed at the Labs. More information on\n77 Sandia Labs PV performance modeling programs can be found at\n78 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n79 but operate independently of it.\n80 \n81 \n82 Documentation\n83 =============\n84 \n85 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),\n86 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.\n87 \n88 Installation\n89 ============\n90 \n91 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n92 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.\n93 \n94 \n95 Contributing\n96 ============\n97 \n98 We need your help to make pvlib-python a great tool!\n99 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n100 The long-term success of pvlib-python requires substantial community support.\n101 \n102 \n103 Citing\n104 ======\n105 \n106 If you use pvlib-python in a published work, please cite:\n107 \n108   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n109   \"pvlib python: a python package for modeling solar energy systems.\"\n110   Journal of Open Source Software, 3(29), 884, (2018).\n111   https://doi.org/10.21105/joss.00884\n112 \n113 Please also cite the DOI corresponding to the specific version of\n114 pvlib-python that you used. pvlib-python DOIs are listed at\n115 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n116 \n117 If you use pvlib-python in a commercial or publicly-available application, please\n118 consider displaying one of the \"powered by pvlib\" logos:\n119 \n120 <img src=\"docs/sphinx/source/_images/pvlib_powered_logo_vert.png\" width=\"300\"><img src=\"docs/sphinx/source/_images/pvlib_powered_logo_horiz.png\" width=\"300\">\n121 \n122 Getting support\n123 ===============\n124 \n125 pvlib usage questions can be asked on\n126 [Stack Overflow](http://stackoverflow.com) and tagged with\n127 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n128 \n129 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n130 is used for discussing various topics of interest to the pvlib-python\n131 community. We also make new version announcements on the google group.\n132 \n133 If you suspect that you may have discovered a bug or if you'd like to\n134 change something about pvlib, then please make an issue on our\n135 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n136 \n137 \n138 \n139 License\n140 =======\n141 \n142 BSD 3-clause.\n143 \n144 \n145 NumFOCUS\n146 ========\n147 \n148 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n149 \n150 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n151 \n[end of README.md]\n[start of pvlib/iotools/epw.py]\n1 \"\"\"\n2 Import functions for EPW data files.\n3 \"\"\"\n4 \n5 import io\n6 from urllib.request import urlopen, Request\n7 import pandas as pd\n8 \n9 \n10 def read_epw(filename, coerce_year=None):\n11     r'''\n12     Read an EPW file in to a pandas dataframe.\n13 \n14     Note that values contained in the metadata dictionary are unchanged\n15     from the EPW file.\n16 \n17     EPW files are commonly used by building simulation professionals\n18     and are widely available on the web. For example via:\n19     https://energyplus.net/weather , http://climate.onebuilding.org or\n20     http://www.ladybug.tools/epwmap/\n21 \n22 \n23     Parameters\n24     ----------\n25     filename : String\n26         Can be a relative file path, absolute file path, or url.\n27 \n28     coerce_year : None or int, default None\n29         If supplied, the year of the data will be set to this value. This can\n30         be a useful feature because EPW data is composed of data from\n31         different years.\n32         Warning: EPW files always have 365*24 = 8760 data rows;\n33         be careful with the use of leap years.\n34 \n35 \n36     Returns\n37     -------\n38     data : DataFrame\n39         A pandas dataframe with the columns described in the table\n40         below. For more detailed descriptions of each component, please\n41         consult the EnergyPlus Auxiliary Programs documentation [1]_\n42 \n43     metadata : dict\n44         The site metadata available in the file.\n45 \n46     See Also\n47     --------\n48     pvlib.iotools.parse_epw\n49 \n50     Notes\n51     -----\n52 \n53     The returned structures have the following fields.\n54 \n55     ===============   ======  =========================================\n56     key               format  description\n57     ===============   ======  =========================================\n58     loc               String  default identifier, not used\n59     city              String  site loccation\n60     state-prov        String  state, province or region (if available)\n61     country           String  site country code\n62     data_type         String  type of original data source\n63     WMO_code          String  WMO identifier\n64     latitude          Float   site latitude\n65     longitude         Float   site longitude\n66     TZ                Float   UTC offset\n67     altitude          Float   site elevation\n68     ===============   ======  =========================================\n69 \n70 \n71     +-------------------------------+-----------------------------------------+\n72     | EPWData field                 | description                             |\n73     +===============================+=========================================+\n74     | index                         | A pandas datetime index. NOTE, times are|\n75     |                               | set to local standard time (daylight    |\n76     |                               | savings is not included). Days run from |\n77     |                               | 0-23h to comply with PVLIB's convention.|\n78     +-------------------------------+-----------------------------------------+\n79     | year                          | Year, from original EPW file. Can be    |\n80     |                               | overwritten using coerce function.      |\n81     +-------------------------------+-----------------------------------------+\n82     | month                         | Month, from original EPW file.          |\n83     +-------------------------------+-----------------------------------------+\n84     | day                           | Day of the month, from original EPW     |\n85     |                               | file.                                   |\n86     +-------------------------------+-----------------------------------------+\n87     | hour                          | Hour of the day from original EPW file. |\n88     |                               | Note that EPW's convention of 1-24h is  |\n89     |                               | not taken over in the index dataframe   |\n90     |                               | used in PVLIB.                          |\n91     +-------------------------------+-----------------------------------------+\n92     | minute                        | Minute, from original EPW file. Not     |\n93     |                               | used.                                   |\n94     +-------------------------------+-----------------------------------------+\n95     | data_source_unct              | Data source and uncertainty flags. See  |\n96     |                               | [1]_, chapter 2.13                      |\n97     +-------------------------------+-----------------------------------------+\n98     | temp_air                      | Dry bulb temperature at the time        |\n99     |                               | indicated, deg C                        |\n100     +-------------------------------+-----------------------------------------+\n101     | temp_dew                      | Dew-point temperature at the time       |\n102     |                               | indicated, deg C                        |\n103     +-------------------------------+-----------------------------------------+\n104     | relative_humidity             | Relative humidity at the time indicated,|\n105     |                               | percent                                 |\n106     +-------------------------------+-----------------------------------------+\n107     | atmospheric_pressure          | Station pressure at the time indicated, |\n108     |                               | Pa                                      |\n109     +-------------------------------+-----------------------------------------+\n110     | etr                           | Extraterrestrial horizontal radiation   |\n111     |                               | recv'd during 60 minutes prior to       |\n112     |                               | timestamp, Wh/m^2                       |\n113     +-------------------------------+-----------------------------------------+\n114     | etrn                          | Extraterrestrial normal radiation recv'd|\n115     |                               | during 60 minutes prior to timestamp,   |\n116     |                               | Wh/m^2                                  |\n117     +-------------------------------+-----------------------------------------+\n118     | ghi_infrared                  | Horizontal infrared radiation recv'd    |\n119     |                               | during 60 minutes prior to timestamp,   |\n120     |                               | Wh/m^2                                  |\n121     +-------------------------------+-----------------------------------------+\n122     | ghi                           | Direct and diffuse horizontal radiation |\n123     |                               | recv'd during 60 minutes prior to       |\n124     |                               | timestamp, Wh/m^2                       |\n125     +-------------------------------+-----------------------------------------+\n126     | dni                           | Amount of direct normal radiation       |\n127     |                               | (modeled) recv'd during 60 minutes prior|\n128     |                               | to timestamp, Wh/m^2                    |\n129     +-------------------------------+-----------------------------------------+\n130     | dhi                           | Amount of diffuse horizontal radiation  |\n131     |                               | recv'd during 60 minutes prior to       |\n132     |                               | timestamp, Wh/m^2                       |\n133     +-------------------------------+-----------------------------------------+\n134     | global_hor_illum              | Avg. total horizontal illuminance recv'd|\n135     |                               | during the 60 minutes prior to          |\n136     |                               | timestamp, lx                           |\n137     +-------------------------------+-----------------------------------------+\n138     | direct_normal_illum           | Avg. direct normal illuminance recv'd   |\n139     |                               | during the 60 minutes prior to          |\n140     |                               | timestamp, lx                           |\n141     +-------------------------------+-----------------------------------------+\n142     | diffuse_horizontal_illum      | Avg. horizontal diffuse illuminance     |\n143     |                               | recv'd during the 60 minutes prior to   |\n144     |                               | timestamp, lx                           |\n145     +-------------------------------+-----------------------------------------+\n146     | zenith_luminance              | Avg. luminance at the sky's zenith      |\n147     |                               | during the 60 minutes prior to          |\n148     |                               | timestamp, cd/m^2                       |\n149     +-------------------------------+-----------------------------------------+\n150     | wind_direction                | Wind direction at time indicated,       |\n151     |                               | degrees from north (360 = north; 0 =    |\n152     |                               | undefined,calm)                         |\n153     +-------------------------------+-----------------------------------------+\n154     | wind_speed                    | Wind speed at the time indicated, m/s   |\n155     +-------------------------------+-----------------------------------------+\n156     | total_sky_cover               | Amount of sky dome covered by clouds or |\n157     |                               | obscuring phenomena at time stamp,      |\n158     |                               | tenths of sky                           |\n159     +-------------------------------+-----------------------------------------+\n160     | opaque_sky_cover              | Amount of sky dome covered by clouds or |\n161     |                               | obscuring phenomena that prevent        |\n162     |                               | observing the sky at time stamp, tenths |\n163     |                               | of sky                                  |\n164     +-------------------------------+-----------------------------------------+\n165     | visibility                    | Horizontal visibility at the time       |\n166     |                               | indicated, km                           |\n167     +-------------------------------+-----------------------------------------+\n168     | ceiling_height                | Height of cloud base above local terrain|\n169     |                               | (7777=unlimited), meter                 |\n170     +-------------------------------+-----------------------------------------+\n171     | present_weather_observation   | Indicator for remaining fields: If 0,   |\n172     |                               | then the observed weather codes are     |\n173     |                               | taken from the following field. If 9,   |\n174     |                               | then missing weather is assumed.        |\n175     +-------------------------------+-----------------------------------------+\n176     | present_weather_codes         | Present weather code, see [1], chapter  |\n177     |                               | 2.9.1.28                                |\n178     +-------------------------------+-----------------------------------------+\n179     | precipitable_water            | Total precipitable water contained in a |\n180     |                               | column of unit cross section from earth |\n181     |                               | to top of atmosphere, cm. Note that some|\n182     |                               | old \\*_TMY3.epw files may have incorrect|\n183     |                               | unit if it was retrieved from           |\n184     |                               | www.energyplus.net.                     |\n185     +-------------------------------+-----------------------------------------+\n186     | aerosol_optical_depth         | The broadband aerosol optical depth per |\n187     |                               | unit of air mass due to extinction by   |\n188     |                               | aerosol component of atmosphere,        |\n189     |                               | unitless                                |\n190     +-------------------------------+-----------------------------------------+\n191     | snow_depth                    | Snow depth in centimeters on the day    |\n192     |                               | indicated, (999 = missing data)         |\n193     +-------------------------------+-----------------------------------------+\n194     | days_since_last_snowfall      | Number of days since last snowfall      |\n195     |                               | (maximum value of 88, where 88 = 88 or  |\n196     |                               | greater days; 99 = missing data)        |\n197     +-------------------------------+-----------------------------------------+\n198     | albedo                        | The ratio of reflected solar irradiance |\n199     |                               | to global horizontal irradiance,        |\n200     |                               | unitless                                |\n201     +-------------------------------+-----------------------------------------+\n202     | liquid_precipitation_depth    | The amount of liquid precipitation      |\n203     |                               | observed at indicated time for the      |\n204     |                               | period indicated in the liquid          |\n205     |                               | precipitation quantity field,           |\n206     |                               | millimeter                              |\n207     +-------------------------------+-----------------------------------------+\n208     | liquid_precipitation_quantity | The period of accumulation for the      |\n209     |                               | liquid precipitation depth field, hour  |\n210     +-------------------------------+-----------------------------------------+\n211 \n212 \n213     References\n214     ----------\n215 \n216     .. [1] `EnergyPlus documentation, Auxiliary Programs\n217        <https://energyplus.net/documentation>`_\n218     '''\n219 \n220     if str(filename).startswith('http'):\n221         # Attempts to download online EPW file\n222         # See comments above for possible online sources\n223         request = Request(filename, headers={'User-Agent': (\n224             'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) '\n225             'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 '\n226             'Safari/537.36')})\n227         response = urlopen(request)\n228         with io.StringIO(response.read().decode(errors='ignore')) as csvdata:\n229             data, meta = parse_epw(csvdata, coerce_year)\n230 \n231     else:\n232         # Assume it's accessible via the file system\n233         with open(str(filename), 'r') as csvdata:\n234             data, meta = parse_epw(csvdata, coerce_year)\n235 \n236 \n237     return data, meta\n238 \n239 \n240 def parse_epw(csvdata, coerce_year=None):\n241     \"\"\"\n242     Given a file-like buffer with data in Energy Plus Weather (EPW) format,\n243     parse the data into a dataframe.\n244 \n245     Parameters\n246     ----------\n247     csvdata : file-like buffer\n248         a file-like buffer containing data in the EPW format\n249 \n250     coerce_year : None or int, default None\n251         If supplied, the year of the data will be set to this value. This can\n252         be a useful feature because EPW data is composed of data from\n253         different years.\n254         Warning: EPW files always have 365*24 = 8760 data rows;\n255         be careful with the use of leap years.\n256 \n257     Returns\n258     -------\n259     data : DataFrame\n260         A pandas dataframe with the columns described in the table\n261         below. For more detailed descriptions of each component, please\n262         consult the EnergyPlus Auxiliary Programs documentation\n263         available at: https://energyplus.net/documentation.\n264 \n265     metadata : dict\n266         The site metadata available in the file.\n267 \n268     See Also\n269     --------\n270     pvlib.iotools.read_epw\n271     \"\"\"\n272     # Read line with metadata\n273     firstline = csvdata.readline()\n274 \n275     head = ['loc', 'city', 'state-prov', 'country', 'data_type', 'WMO_code',\n276             'latitude', 'longitude', 'TZ', 'altitude']\n277     meta = dict(zip(head, firstline.rstrip('\\n').split(\",\")))\n278 \n279     meta['altitude'] = float(meta['altitude'])\n280     meta['latitude'] = float(meta['latitude'])\n281     meta['longitude'] = float(meta['longitude'])\n282     meta['TZ'] = float(meta['TZ'])\n283 \n284     colnames = ['year', 'month', 'day', 'hour', 'minute', 'data_source_unct',\n285                 'temp_air', 'temp_dew', 'relative_humidity',\n286                 'atmospheric_pressure', 'etr', 'etrn', 'ghi_infrared', 'ghi',\n287                 'dni', 'dhi', 'global_hor_illum', 'direct_normal_illum',\n288                 'diffuse_horizontal_illum', 'zenith_luminance',\n289                 'wind_direction', 'wind_speed', 'total_sky_cover',\n290                 'opaque_sky_cover', 'visibility', 'ceiling_height',\n291                 'present_weather_observation', 'present_weather_codes',\n292                 'precipitable_water', 'aerosol_optical_depth', 'snow_depth',\n293                 'days_since_last_snowfall', 'albedo',\n294                 'liquid_precipitation_depth', 'liquid_precipitation_quantity']\n295 \n296     # We only have to skip 6 rows instead of 7 because we have already used\n297     # the realine call above.\n298     data = pd.read_csv(csvdata, skiprows=6, header=0, names=colnames)\n299 \n300     # Change to single year if requested\n301     if coerce_year is not None:\n302         data[\"year\"] = coerce_year\n303 \n304     # create index that supplies correct date and time zone information\n305     dts = data[['month', 'day']].astype(str).apply(lambda x: x.str.zfill(2))\n306     hrs = (data['hour'] - 1).astype(str).str.zfill(2)\n307     dtscat = data['year'].astype(str) + dts['month'] + dts['day'] + hrs\n308     idx = pd.to_datetime(dtscat, format='%Y%m%d%H')\n309     idx = idx.dt.tz_localize(int(meta['TZ'] * 3600))\n310     data.index = idx\n311 \n312     return data, meta\n313 \n[end of pvlib/iotools/epw.py]\n[start of pvlib/snow.py]\n1 \"\"\"\n2 The ``snow`` module contains functions that model module snow cover and the\n3 associated effects on PV module output\n4 \"\"\"\n5 \n6 import numpy as np\n7 import pandas as pd\n8 from pvlib.tools import sind, cosd, tand\n9 \n10 \n11 def _time_delta_in_hours(times):\n12     delta = times.to_series().diff()\n13     return delta.dt.total_seconds().div(3600)\n14 \n15 \n16 def fully_covered_nrel(snowfall, threshold_snowfall=1.):\n17     '''\n18     Calculates the timesteps when the row's slant height is fully covered\n19     by snow.\n20 \n21     Parameters\n22     ----------\n23     snowfall : Series\n24         Accumulated snowfall in each time period [cm]\n25 \n26     threshold_snowfall : float, default 1.0\n27         Hourly snowfall above which snow coverage is set to the row's slant\n28         height. [cm/hr]\n29 \n30     Returns\n31     ----------\n32     boolean: Series\n33         True where the snowfall exceeds the defined threshold to fully cover\n34         the panel.\n35 \n36     Notes\n37     -----\n38     Implements the model described in [1]_ with minor improvements in [2]_.\n39 \n40     References\n41     ----------\n42     .. [1] Marion, B.; Schaefer, R.; Caine, H.; Sanchez, G. (2013).\n43        \"Measured and modeled photovoltaic system energy losses from snow for\n44        Colorado and Wisconsin locations.\" Solar Energy 97; pp.112-121.\n45     .. [2] Ryberg, D; Freeman, J. \"Integration, Validation, and Application\n46        of a PV Snow Coverage Model in SAM\" (2017) NREL Technical Report\n47        NREL/TP-6A20-68705\n48     '''\n49     timestep = _time_delta_in_hours(snowfall.index)\n50     hourly_snow_rate = snowfall / timestep\n51     # if we can infer a time frequency, use first snowfall value\n52     # otherwise the first snowfall value is ignored\n53     freq = pd.infer_freq(snowfall.index)\n54     if freq is not None:\n55         timedelta = pd.tseries.frequencies.to_offset(freq) / pd.Timedelta('1h')\n56         hourly_snow_rate.iloc[0] = snowfall[0] / timedelta\n57     else:  # can't infer frequency from index\n58         hourly_snow_rate[0] = 0  # replaces NaN\n59     return hourly_snow_rate > threshold_snowfall\n60 \n61 \n62 def coverage_nrel(snowfall, poa_irradiance, temp_air, surface_tilt,\n63                   initial_coverage=0, threshold_snowfall=1.,\n64                   can_slide_coefficient=-80., slide_amount_coefficient=0.197):\n65     '''\n66     Calculates the fraction of the slant height of a row of modules covered by\n67     snow at every time step.\n68 \n69     Implements the model described in [1]_ with minor improvements in [2]_,\n70     with the change that the output is in fraction of the row's slant height\n71     rather than in tenths of the row slant height. As described in [1]_, model\n72     validation focused on fixed tilt systems.\n73 \n74     Parameters\n75     ----------\n76     snowfall : Series\n77         Accumulated snowfall within each time period. [cm]\n78     poa_irradiance : Series\n79         Total in-plane irradiance [W/m^2]\n80     temp_air : Series\n81         Ambient air temperature [C]\n82     surface_tilt : numeric\n83         Tilt of module's from horizontal, e.g. surface facing up = 0,\n84         surface facing horizon = 90. [degrees]\n85     initial_coverage : float, default 0\n86         Fraction of row's slant height that is covered with snow at the\n87         beginning of the simulation. [unitless]\n88     threshold_snowfall : float, default 1.0\n89         Hourly snowfall above which snow coverage is set to the row's slant\n90         height. [cm/hr]\n91     can_slide_coefficient : float, default -80.\n92         Coefficient to determine if snow can slide given irradiance and air\n93         temperature. [W/(m^2 C)]\n94     slide_amount_coefficient : float, default 0.197\n95         Coefficient to determine fraction of snow that slides off in one hour.\n96         [unitless]\n97 \n98     Returns\n99     -------\n100     snow_coverage : Series\n101         The fraction of the slant height of a row of modules that is covered\n102         by snow at each time step.\n103 \n104     Notes\n105     -----\n106     In [1]_, `can_slide_coefficient` is termed `m`, and the value of\n107     `slide_amount_coefficient` is given in tenths of a module's slant height.\n108 \n109     References\n110     ----------\n111     .. [1] Marion, B.; Schaefer, R.; Caine, H.; Sanchez, G. (2013).\n112        \"Measured and modeled photovoltaic system energy losses from snow for\n113        Colorado and Wisconsin locations.\" Solar Energy 97; pp.112-121.\n114     .. [2] Ryberg, D; Freeman, J. (2017). \"Integration, Validation, and\n115        Application of a PV Snow Coverage Model in SAM\" NREL Technical Report\n116        NREL/TP-6A20-68705\n117     '''\n118 \n119     # find times with new snowfall\n120     new_snowfall = fully_covered_nrel(snowfall, threshold_snowfall)\n121 \n122     # set up output Series\n123     snow_coverage = pd.Series(np.nan, index=poa_irradiance.index)\n124 \n125     # determine amount that snow can slide in each timestep\n126     can_slide = temp_air > poa_irradiance / can_slide_coefficient\n127     slide_amt = slide_amount_coefficient * sind(surface_tilt) * \\\n128         _time_delta_in_hours(poa_irradiance.index)\n129     slide_amt[~can_slide] = 0.\n130     # don't slide during snow events\n131     slide_amt[new_snowfall] = 0.\n132     # don't slide in the interval preceding the snowfall data\n133     slide_amt.iloc[0] = 0\n134 \n135     # build time series of cumulative slide amounts\n136     sliding_period_ID = new_snowfall.cumsum()\n137     cumulative_sliding = slide_amt.groupby(sliding_period_ID).cumsum()\n138 \n139     # set up time series of snow coverage without any sliding applied\n140     snow_coverage[new_snowfall] = 1.0\n141     if np.isnan(snow_coverage.iloc[0]):\n142         snow_coverage.iloc[0] = initial_coverage\n143     snow_coverage.ffill(inplace=True)\n144     snow_coverage -= cumulative_sliding\n145 \n146     # clean up periods where row is completely uncovered\n147     return snow_coverage.clip(lower=0)\n148 \n149 \n150 def dc_loss_nrel(snow_coverage, num_strings):\n151     '''\n152     Calculates the fraction of DC capacity lost due to snow coverage.\n153 \n154     DC capacity loss assumes that if a string is partially covered by snow,\n155     the string's capacity is lost; see [1]_, Eq. 11.8.\n156 \n157     Module orientation is accounted for by specifying the number of cell\n158     strings in parallel along the slant height.\n159     For example, a typical 60-cell module has 3 parallel strings, each\n160     comprising 20 cells in series, with the cells arranged in 6 columns of 10\n161     cells each. For a row consisting of single modules, if the module is\n162     mounted in portrait orientation, i.e., the row slant height is along a\n163     column of 10 cells, there is 1 string in parallel along the row slant\n164     height, so `num_strings=1`. In contrast, if the module is mounted in\n165     landscape orientation with the row slant height comprising 6 cells, there\n166     are 3 parallel strings along the row slant height, so `num_strings=3`.\n167 \n168     Parameters\n169     ----------\n170     snow_coverage : numeric\n171         The fraction of row slant height covered by snow at each time step.\n172 \n173     num_strings: int\n174         The number of parallel-connected strings along a row slant height.\n175 \n176     Returns\n177     -------\n178     loss : numeric\n179         fraction of DC capacity loss due to snow coverage at each time step.\n180 \n181     References\n182     ----------\n183     .. [1] Gilman, P. et al., (2018). \"SAM Photovoltaic Model Technical\n184        Reference Update\", NREL Technical Report NREL/TP-6A20-67399.\n185        Available at https://www.nrel.gov/docs/fy18osti/67399.pdf\n186     '''\n187     return np.ceil(snow_coverage * num_strings) / num_strings\n188 \n189 \n190 def _townsend_effective_snow(snow_total, snow_events):\n191     '''\n192     Calculates effective snow using the total snowfall received each month and\n193     the number of snowfall events each month.\n194 \n195     Parameters\n196     ----------\n197     snow_total : array-like\n198         Snow received each month. Referred to as S in [1]_. [cm]\n199 \n200     snow_events : array-like\n201         Number of snowfall events each month. Referred to as N in [1]_. [-]\n202 \n203     Returns\n204     -------\n205     effective_snowfall : array-like\n206         Effective snowfall as defined in the Townsend model. [cm]\n207 \n208     References\n209     ----------\n210     .. [1] Townsend, Tim & Powers, Loren. (2011). Photovoltaics and snow: An\n211        update from two winters of measurements in the SIERRA. 37th IEEE\n212        Photovoltaic Specialists Conference, Seattle, WA, USA.\n213        :doi:`10.1109/PVSC.2011.6186627`\n214     '''\n215     snow_events_no_zeros = np.maximum(snow_events, 1)\n216     effective_snow = 0.5 * snow_total * (1 + 1 / snow_events_no_zeros)\n217     return np.where(snow_events > 0, effective_snow, 0)\n218 \n219 \n220 def loss_townsend(snow_total, snow_events, surface_tilt, relative_humidity,\n221                   temp_air, poa_global, slant_height, lower_edge_height,\n222                   angle_of_repose=40):\n223     '''\n224     Calculates monthly snow loss based on the Townsend monthly snow loss\n225     model [1]_.\n226 \n227     Parameters\n228     ----------\n229     snow_total : array-like\n230         Snow received each month. Referred to as S in [1]_. [cm]\n231 \n232     snow_events : array-like\n233         Number of snowfall events each month. Referred to as N in [1]_. [-]\n234 \n235     surface_tilt : float\n236         Tilt angle of the array. [deg]\n237 \n238     relative_humidity : array-like\n239         Monthly average relative humidity. [%]\n240 \n241     temp_air : array-like\n242         Monthly average ambient temperature. [C]\n243 \n244     poa_global : array-like\n245         Monthly plane of array insolation. [Wh/m2]\n246 \n247     slant_height : float\n248         Row length in the slanted plane of array dimension. [m]\n249 \n250     lower_edge_height : float\n251         Distance from array lower edge to the ground. [m]\n252 \n253     angle_of_repose : float, default 40\n254         Piled snow angle, assumed to stabilize at 40\u00b0, the midpoint of\n255         25\u00b0-55\u00b0 avalanching slope angles. [deg]\n256 \n257     Returns\n258     -------\n259     loss : array-like\n260         Monthly average DC capacity loss fraction due to snow coverage.\n261 \n262     Notes\n263     -----\n264     This model has not been validated for tracking arrays; however, for\n265     tracking arrays [1]_ suggests using the maximum rotation angle in place\n266     of ``surface_tilt``.\n267 \n268     References\n269     ----------\n270     .. [1] Townsend, Tim & Powers, Loren. (2011). Photovoltaics and snow: An\n271        update from two winters of measurements in the SIERRA. 37th IEEE\n272        Photovoltaic Specialists Conference, Seattle, WA, USA.\n273        :doi:`10.1109/PVSC.2011.6186627`\n274     '''\n275 \n276     C1 = 5.7e04\n277     C2 = 0.51\n278 \n279     snow_total_prev = np.roll(snow_total, 1)\n280     snow_events_prev = np.roll(snow_events, 1)\n281 \n282     effective_snow = _townsend_effective_snow(snow_total, snow_events)\n283     effective_snow_prev = _townsend_effective_snow(\n284         snow_total_prev,\n285         snow_events_prev\n286     )\n287     effective_snow_weighted = (\n288         1 / 3 * effective_snow_prev\n289         + 2 / 3 * effective_snow\n290     )\n291     effective_snow_weighted_m = effective_snow_weighted / 100\n292 \n293     lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\n294     gamma = (\n295         slant_height\n296         * effective_snow_weighted_m\n297         * cosd(surface_tilt)\n298         / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\n299         * 2\n300         * tand(angle_of_repose)\n301     )\n302 \n303     ground_interference_term = 1 - C2 * np.exp(-gamma)\n304     relative_humidity_fraction = relative_humidity / 100\n305     temp_air_kelvin = temp_air + 273.15\n306     effective_snow_weighted_in = effective_snow_weighted / 2.54\n307     poa_global_kWh = poa_global / 1000\n308 \n309     # Calculate Eqn. 3 in the reference.\n310     # Although the reference says Eqn. 3 calculates percentage loss, the y-axis\n311     # of Figure 7 indicates Eqn. 3 calculates fractional loss. Since the slope\n312     # of the line in Figure 7 is the same as C1 in Eqn. 3, it is assumed that\n313     # Eqn. 3 calculates fractional loss.\n314     loss_fraction = (\n315         C1\n316         * effective_snow_weighted_in\n317         * cosd(surface_tilt)**2\n318         * ground_interference_term\n319         * relative_humidity_fraction\n320         / temp_air_kelvin**2\n321         / poa_global_kWh**0.67\n322     )\n323 \n324     return np.clip(loss_fraction, 0, 1)\n325 \n[end of pvlib/snow.py]\n[start of pvlib/temperature.py]\n1 \"\"\"\n2 The ``temperature`` module contains functions for modeling temperature of\n3 PV modules and cells.\n4 \"\"\"\n5 \n6 import numpy as np\n7 import pandas as pd\n8 from pvlib.tools import sind\n9 from pvlib._deprecation import warn_deprecated\n10 from pvlib.tools import _get_sample_intervals\n11 import scipy\n12 import scipy.constants\n13 import warnings\n14 \n15 \n16 TEMPERATURE_MODEL_PARAMETERS = {\n17     'sapm': {\n18         'open_rack_glass_glass': {'a': -3.47, 'b': -.0594, 'deltaT': 3},\n19         'close_mount_glass_glass': {'a': -2.98, 'b': -.0471, 'deltaT': 1},\n20         'open_rack_glass_polymer': {'a': -3.56, 'b': -.0750, 'deltaT': 3},\n21         'insulated_back_glass_polymer': {'a': -2.81, 'b': -.0455, 'deltaT': 0},\n22     },\n23     'pvsyst': {'freestanding': {'u_c': 29.0, 'u_v': 0},\n24                'insulated': {'u_c': 15.0, 'u_v': 0}}\n25 }\n26 \"\"\"Dictionary of temperature parameters organized by model.\n27 \n28 There are keys for each model at the top level. Currently there are two models,\n29 ``'sapm'`` for the Sandia Array Performance Model, and ``'pvsyst'``. Each model\n30 has a dictionary of configurations; a value is itself a dictionary containing\n31 model parameters. Retrieve parameters by indexing the model and configuration\n32 by name. Note: the keys are lower-cased and case sensitive.\n33 \n34 Example\n35 -------\n36 Retrieve the open rack glass-polymer configuration for SAPM::\n37 \n38     from pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\n39     temperature_model_parameters = (\n40         TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_polymer'])\n41     # {'a': -3.56, 'b': -0.075, 'deltaT': 3}\n42 \"\"\"\n43 \n44 \n45 def _temperature_model_params(model, parameter_set):\n46     try:\n47         params = TEMPERATURE_MODEL_PARAMETERS[model]\n48         return params[parameter_set]\n49     except KeyError:\n50         msg = ('{} is not a named set of parameters for the {} cell'\n51                ' temperature model.'\n52                ' See pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS'\n53                ' for names'.format(parameter_set, model))\n54         raise KeyError(msg)\n55 \n56 \n57 def sapm_cell(poa_global, temp_air, wind_speed, a, b, deltaT,\n58               irrad_ref=1000.):\n59     r'''\n60     Calculate cell temperature per the Sandia Array Performance Model.\n61 \n62     See [1]_ for details on the Sandia Array Performance Model.\n63 \n64     Parameters\n65     ----------\n66     poa_global : numeric\n67         Total incident irradiance [W/m^2].\n68 \n69     temp_air : numeric\n70         Ambient dry bulb temperature [C].\n71 \n72     wind_speed : numeric\n73         Wind speed at a height of 10 meters [m/s].\n74 \n75     a : float\n76         Parameter :math:`a` in :eq:`sapm1`.\n77 \n78     b : float\n79         Parameter :math:`b` in :eq:`sapm1`.\n80 \n81     deltaT : float\n82         Parameter :math:`\\Delta T` in :eq:`sapm2` [C].\n83 \n84     irrad_ref : float, default 1000\n85         Reference irradiance, parameter :math:`E_{0}` in\n86         :eq:`sapm2` [W/m^2].\n87 \n88     Returns\n89     -------\n90     numeric, values in degrees C.\n91 \n92     Notes\n93     -----\n94     The model for cell temperature :math:`T_{C}` is given by a pair of\n95     equations (Eq. 11 and 12 in [1]_).\n96 \n97     .. math::\n98        :label: sapm1\n99 \n100        T_{m} = E \\times \\exp (a + b \\times WS) + T_{a}\n101 \n102     .. math::\n103        :label: sapm2\n104 \n105        T_{C} = T_{m} + \\frac{E}{E_{0}} \\Delta T\n106 \n107     The module back surface temperature :math:`T_{m}` is implemented in\n108     :py:func:`~pvlib.temperature.sapm_module`.\n109 \n110     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2) and\n111     ambient air temperature :math:`T_{a}` (C). Model parameters depend both on\n112     the module construction and its mounting. Parameter sets are provided in\n113     [1]_ for representative modules and mounting, and are coded for convenience\n114     in :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.\n115 \n116     +---------------+----------------+-------+---------+---------------------+\n117     | Module        | Mounting       | a     | b       | :math:`\\Delta T [C]`|\n118     +===============+================+=======+=========+=====================+\n119     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |\n120     +---------------+----------------+-------+---------+---------------------+\n121     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |\n122     +---------------+----------------+-------+---------+---------------------+\n123     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |\n124     +---------------+----------------+-------+---------+---------------------+\n125     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |\n126     +---------------+----------------+-------+---------+---------------------+\n127 \n128     References\n129     ----------\n130     .. [1] King, D. et al, 2004, \"Sandia Photovoltaic Array Performance\n131        Model\", SAND Report 3535, Sandia National Laboratories, Albuquerque,\n132        NM.\n133 \n134     See also\n135     --------\n136     sapm_cell_from_module\n137     sapm_module\n138 \n139     Examples\n140     --------\n141     >>> from pvlib.temperature import sapm_cell, TEMPERATURE_MODEL_PARAMETERS\n142     >>> params = TEMPERATURE_MODEL_PARAMETERS['sapm']['open_rack_glass_glass']\n143     >>> sapm_cell(1000, 10, 0, **params)\n144     44.11703066106086\n145     '''\n146     module_temperature = sapm_module(poa_global, temp_air, wind_speed,\n147                                      a, b)\n148     return sapm_cell_from_module(module_temperature, poa_global, deltaT,\n149                                  irrad_ref)\n150 \n151 \n152 def sapm_module(poa_global, temp_air, wind_speed, a, b):\n153     r'''\n154     Calculate module back surface temperature per the Sandia Array\n155     Performance Model.\n156 \n157     See [1]_ for details on the Sandia Array Performance Model.\n158 \n159     Parameters\n160     ----------\n161     poa_global : numeric\n162         Total incident irradiance [W/m^2].\n163 \n164     temp_air : numeric\n165         Ambient dry bulb temperature [C].\n166 \n167     wind_speed : numeric\n168         Wind speed at a height of 10 meters [m/s].\n169 \n170     a : float\n171         Parameter :math:`a` in :eq:`sapm1mod`.\n172 \n173     b : float\n174         Parameter :math:`b` in :eq:`sapm1mod`.\n175 \n176     Returns\n177     -------\n178     numeric, values in degrees C.\n179 \n180     Notes\n181     -----\n182     The model for module temperature :math:`T_{m}` is given by Eq. 11 in [1]_.\n183 \n184     .. math::\n185        :label: sapm1mod\n186 \n187        T_{m} = E \\times \\exp (a + b \\times WS) + T_{a}\n188 \n189     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2) and\n190     ambient air temperature :math:`T_{a}` (C). Model outputs are surface\n191     temperature at the back of the module :math:`T_{m}` and cell temperature\n192     :math:`T_{C}`. Model parameters depend both on the module construction and\n193     its mounting. Parameter sets are provided in [1]_ for representative\n194     modules and mounting, and are coded for convenience in\n195     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.\n196 \n197     +---------------+----------------+-------+---------+---------------------+\n198     | Module        | Mounting       | a     | b       | :math:`\\Delta T [C]`|\n199     +===============+================+=======+=========+=====================+\n200     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |\n201     +---------------+----------------+-------+---------+---------------------+\n202     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |\n203     +---------------+----------------+-------+---------+---------------------+\n204     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |\n205     +---------------+----------------+-------+---------+---------------------+\n206     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |\n207     +---------------+----------------+-------+---------+---------------------+\n208 \n209     References\n210     ----------\n211     .. [1] King, D. et al, 2004, \"Sandia Photovoltaic Array Performance\n212        Model\", SAND Report 3535, Sandia National Laboratories, Albuquerque,\n213        NM.\n214 \n215     See also\n216     --------\n217     sapm_cell\n218     sapm_cell_from_module\n219     '''\n220     return poa_global * np.exp(a + b * wind_speed) + temp_air\n221 \n222 \n223 def sapm_cell_from_module(module_temperature, poa_global, deltaT,\n224                           irrad_ref=1000.):\n225     r'''\n226     Calculate cell temperature from module temperature using the Sandia Array\n227     Performance Model.\n228 \n229     See [1]_ for details on the Sandia Array Performance Model.\n230 \n231     Parameters\n232     ----------\n233     module_temperature : numeric\n234         Temperature of back of module surface [C].\n235 \n236     poa_global : numeric\n237         Total incident irradiance [W/m^2].\n238 \n239     deltaT : float\n240         Parameter :math:`\\Delta T` in :eq:`sapm2_cell_from_mod` [C].\n241 \n242     irrad_ref : float, default 1000\n243         Reference irradiance, parameter :math:`E_{0}` in\n244         :eq:`sapm2` [W/m^2].\n245 \n246     Returns\n247     -------\n248     numeric, values in degrees C.\n249 \n250     Notes\n251     -----\n252     The model for cell temperature :math:`T_{C}` is given by Eq. 12 in [1]_.\n253 \n254     .. math::\n255        :label: sapm2_cell_from_mod\n256 \n257        T_{C} = T_{m} + \\frac{E}{E_{0}} \\Delta T\n258 \n259     The module back surface temperature :math:`T_{m}` is implemented in\n260     :py:func:`~pvlib.temperature.sapm_module`.\n261 \n262     Model parameters depend both on the module construction and its mounting.\n263     Parameter sets are provided in [1]_ for representative modules and\n264     mounting, and are coded for convenience in\n265     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`.\n266 \n267     +---------------+----------------+-------+---------+---------------------+\n268     | Module        | Mounting       | a     | b       | :math:`\\Delta T [C]`|\n269     +===============+================+=======+=========+=====================+\n270     | glass/glass   | open rack      | -3.47 | -0.0594 | 3                   |\n271     +---------------+----------------+-------+---------+---------------------+\n272     | glass/glass   | close roof     | -2.98 | -0.0471 | 1                   |\n273     +---------------+----------------+-------+---------+---------------------+\n274     | glass/polymer | open rack      | -3.56 | -0.075  | 3                   |\n275     +---------------+----------------+-------+---------+---------------------+\n276     | glass/polymer | insulated back | -2.81 | -0.0455 | 0                   |\n277     +---------------+----------------+-------+---------+---------------------+\n278 \n279     References\n280     ----------\n281     .. [1] King, D. et al, 2004, \"Sandia Photovoltaic Array Performance\n282        Model\", SAND Report 3535, Sandia National Laboratories, Albuquerque,\n283        NM.\n284 \n285     See also\n286     --------\n287     sapm_cell\n288     sapm_module\n289     '''\n290     return module_temperature + (poa_global / irrad_ref) * deltaT\n291 \n292 \n293 def pvsyst_cell(poa_global, temp_air, wind_speed=1.0, u_c=29.0, u_v=0.0,\n294                 eta_m=None, module_efficiency=0.1, alpha_absorption=0.9):\n295     r\"\"\"\n296     Calculate cell temperature using an empirical heat loss factor model\n297     as implemented in PVsyst.\n298 \n299     Parameters\n300     ----------\n301     poa_global : numeric\n302         Total incident irradiance [W/m^2].\n303 \n304     temp_air : numeric\n305         Ambient dry bulb temperature [C].\n306 \n307     wind_speed : numeric, default 1.0\n308         Wind speed in m/s measured at the same height for which the wind loss\n309         factor was determined.  The default value 1.0 m/s is the wind\n310         speed at module height used to determine NOCT. [m/s]\n311 \n312     u_c : float, default 29.0\n313         Combined heat loss factor coefficient. The default value is\n314         representative of freestanding modules with the rear surfaces exposed\n315         to open air (e.g., rack mounted). Parameter :math:`U_{c}` in\n316         :eq:`pvsyst`.\n317         :math:`\\left[\\frac{\\text{W}/{\\text{m}^2}}{\\text{C}}\\right]`\n318 \n319     u_v : float, default 0.0\n320         Combined heat loss factor influenced by wind. Parameter :math:`U_{v}`\n321         in :eq:`pvsyst`.\n322         :math:`\\left[ \\frac{\\text{W}/\\text{m}^2}{\\text{C}\\ \\left( \\text{m/s} \\right)} \\right]`\n323 \n324     eta_m : numeric, default None (deprecated, use module_efficiency instead)\n325 \n326     module_efficiency : numeric, default 0.1\n327         Module external efficiency as a fraction. Parameter :math:`\\eta_{m}`\n328         in :eq:`pvsyst`. Calculate as\n329         :math:`\\eta_{m} = DC\\ power / (POA\\ irradiance \\times module\\ area)`.\n330 \n331     alpha_absorption : numeric, default 0.9\n332         Absorption coefficient. Parameter :math:`\\alpha` in :eq:`pvsyst`.\n333 \n334     Returns\n335     -------\n336     numeric, values in degrees Celsius\n337 \n338     Notes\n339     -----\n340     The Pvsyst model for cell temperature :math:`T_{C}` is given by\n341 \n342     .. math::\n343        :label: pvsyst\n344 \n345         T_{C} = T_{a} + \\frac{\\alpha E (1 - \\eta_{m})}{U_{c} + U_{v} \\times WS}\n346 \n347     Inputs to the model are plane-of-array irradiance :math:`E` (W/m2), ambient\n348     air temperature :math:`T_{a}` (C) and wind speed :math:`WS` (m/s). Model\n349     output is cell temperature :math:`T_{C}`. Model parameters depend both on\n350     the module construction and its mounting. Parameters are provided in\n351     [1]_ for open (freestanding) and close (insulated) mounting configurations,\n352     , and are coded for convenience in\n353     :data:`~pvlib.temperature.TEMPERATURE_MODEL_PARAMETERS`. The heat loss\n354     factors provided represent the combined effect of convection, radiation and\n355     conduction, and their values are experimentally determined.\n356 \n357     +--------------+---------------+---------------+\n358     | Mounting     | :math:`U_{c}` | :math:`U_{v}` |\n359     +==============+===============+===============+\n360     | freestanding | 29.0          | 0.0           |\n361     +--------------+---------------+---------------+\n362     | insulated    | 15.0          | 0.0           |\n363     +--------------+---------------+---------------+\n364 \n365     References\n366     ----------\n367     .. [1] \"PVsyst 6 Help\", Files.pvsyst.com, 2018. [Online]. Available:\n368        http://files.pvsyst.com/help/index.html. [Accessed: 10- Dec- 2018].\n369 \n370     .. [2] Faiman, D. (2008). \"Assessing the outdoor operating temperature of\n371        photovoltaic modules.\" Progress in Photovoltaics 16(4): 307-315.\n372 \n373     Examples\n374     --------\n375     >>> from pvlib.temperature import pvsyst_cell, TEMPERATURE_MODEL_PARAMETERS\n376     >>> params = TEMPERATURE_MODEL_PARAMETERS['pvsyst']['freestanding']\n377     >>> pvsyst_cell(1000, 10, **params)\n378     37.93103448275862\n379     \"\"\"  # noQA: E501\n380 \n381     if eta_m:\n382         warn_deprecated(\n383             since='v0.9', message='eta_m overwriting module_efficiency',\n384             name='eta_m', alternative='module_efficiency', removal='v0.10')\n385         module_efficiency = eta_m\n386     total_loss_factor = u_c + u_v * wind_speed\n387     heat_input = poa_global * alpha_absorption * (1 - module_efficiency)\n388     temp_difference = heat_input / total_loss_factor\n389     return temp_air + temp_difference\n390 \n391 \n392 def faiman(poa_global, temp_air, wind_speed=1.0, u0=25.0, u1=6.84):\n393     r'''\n394     Calculate cell or module temperature using the Faiman model.\n395 \n396     The Faiman model uses an empirical heat loss factor model [1]_ and is\n397     adopted in the IEC 61853 standards [2]_ and [3]_.\n398 \n399     Usage of this model in the IEC 61853 standard does not distinguish\n400     between cell and module temperature.\n401 \n402     Parameters\n403     ----------\n404     poa_global : numeric\n405         Total incident irradiance [W/m^2].\n406 \n407     temp_air : numeric\n408         Ambient dry bulb temperature [C].\n409 \n410     wind_speed : numeric, default 1.0\n411         Wind speed in m/s measured at the same height for which the wind loss\n412         factor was determined.  The default value 1.0 m/s is the wind\n413         speed at module height used to determine NOCT. [m/s]\n414 \n415     u0 : numeric, default 25.0\n416         Combined heat loss factor coefficient. The default value is one\n417         determined by Faiman for 7 silicon modules\n418         in the Negev desert on an open rack at 30.9\u00b0 tilt.\n419         :math:`\\left[\\frac{\\text{W}/{\\text{m}^2}}{\\text{C}}\\right]`\n420 \n421     u1 : numeric, default 6.84\n422         Combined heat loss factor influenced by wind. The default value is one\n423         determined by Faiman for 7 silicon modules\n424         in the Negev desert on an open rack at 30.9\u00b0 tilt.\n425         :math:`\\left[ \\frac{\\text{W}/\\text{m}^2}{\\text{C}\\ \\left( \\text{m/s} \\right)} \\right]`\n426 \n427     Returns\n428     -------\n429     numeric, values in degrees Celsius\n430 \n431     Notes\n432     -----\n433     All arguments may be scalars or vectors. If multiple arguments\n434     are vectors they must be the same length.\n435 \n436     References\n437     ----------\n438     .. [1] Faiman, D. (2008). \"Assessing the outdoor operating temperature of\n439        photovoltaic modules.\" Progress in Photovoltaics 16(4): 307-315.\n440        :doi:`10.1002/pip.813`\n441 \n442     .. [2] \"IEC 61853-2 Photovoltaic (PV) module performance testing and energy\n443        rating - Part 2: Spectral responsivity, incidence angle and module\n444        operating temperature measurements\". IEC, Geneva, 2018.\n445 \n446     .. [3] \"IEC 61853-3 Photovoltaic (PV) module performance testing and energy\n447        rating - Part 3: Energy rating of PV modules\". IEC, Geneva, 2018.\n448 \n449     See also\n450     --------\n451     pvlib.temperature.faiman_rad\n452 \n453     '''  # noQA: E501\n454 \n455     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. Dec., 2019\n456 \n457     # The following lines may seem odd since u0 & u1 are probably scalar,\n458     # but it serves an indirect and easy way of allowing lists and\n459     # tuples for the other function arguments.\n460     u0 = np.asanyarray(u0)\n461     u1 = np.asanyarray(u1)\n462 \n463     total_loss_factor = u0 + u1 * wind_speed\n464     heat_input = poa_global\n465     temp_difference = heat_input / total_loss_factor\n466     return temp_air + temp_difference\n467 \n468 \n469 def faiman_rad(poa_global, temp_air, wind_speed=1.0, ir_down=None,\n470                u0=25.0, u1=6.84, sky_view=1.0, emissivity=0.88):\n471     r'''\n472     Calculate cell or module temperature using the Faiman model augmented\n473     with a radiative loss term.\n474 \n475     The Faiman model uses an empirical heat loss factor model [1]_ and is\n476     adopted in the IEC 61853 standards [2]_ and [3]_.  The radiative loss\n477     term was proposed and developed by Driesse [4]_.\n478 \n479     The model can be used to represent cell or module temperature.\n480 \n481     Parameters\n482     ----------\n483     poa_global : numeric\n484         Total incident irradiance [W/m^2].\n485 \n486     temp_air : numeric\n487         Ambient dry bulb temperature [C].\n488 \n489     wind_speed : numeric, default 1.0\n490         Wind speed measured at the same height for which the wind loss\n491         factor was determined.  The default value 1.0 m/s is the wind\n492         speed at module height used to determine NOCT. [m/s]\n493 \n494     ir_down : numeric, default 0.0\n495         Downwelling infrared radiation from the sky, measured on a horizontal\n496         surface. [W/m^2]\n497 \n498     u0 : numeric, default 25.0\n499         Combined heat loss factor coefficient. The default value is one\n500         determined by Faiman for 7 silicon modules\n501         in the Negev desert on an open rack at 30.9\u00b0 tilt.\n502         :math:`\\left[\\frac{\\text{W}/{\\text{m}^2}}{\\text{C}}\\right]`\n503 \n504     u1 : numeric, default 6.84\n505         Combined heat loss factor influenced by wind. The default value is one\n506         determined by Faiman for 7 silicon modules\n507         in the Negev desert on an open rack at 30.9\u00b0 tilt.\n508         :math:`\\left[ \\frac{\\text{W}/\\text{m}^2}{\\text{C}\\ \\left( \\text{m/s} \\right)} \\right]`\n509 \n510     sky_view : numeric, default 1.0\n511         Effective view factor limiting the radiative exchange between the\n512         module and the sky. For a tilted array the expressions\n513         (1 + 3*cos(tilt)) / 4 can be used as a first estimate for sky_view\n514         as discussed in [4]_. The default value is for a horizontal module.\n515         [unitless]\n516 \n517     emissivity : numeric, default 0.88\n518         Infrared emissivity of the module surface facing the sky. The default\n519         value represents the middle of a range of values found in the\n520         literature. [unitless]\n521 \n522     Returns\n523     -------\n524     numeric, values in degrees Celsius\n525 \n526     Notes\n527     -----\n528     All arguments may be scalars or vectors. If multiple arguments\n529     are vectors they must be the same length.\n530 \n531     When only irradiance, air temperature and wind speed inputs are provided\n532     (`ir_down` is `None`) this function calculates the same device temperature\n533     as the original faiman model. When down-welling long-wave radiation data\n534     are provided as well (`ir_down` is not None) the default u0 and u1 values\n535     from the original model should not be used because a portion of the\n536     radiative losses would be double-counted.\n537 \n538     References\n539     ----------\n540     .. [1] Faiman, D. (2008). \"Assessing the outdoor operating temperature of\n541        photovoltaic modules.\" Progress in Photovoltaics 16(4): 307-315.\n542        :doi:`10.1002/pip.813`\n543 \n544     .. [2] \"IEC 61853-2 Photovoltaic (PV) module performance testing and energy\n545        rating - Part 2: Spectral responsivity, incidence angle and module\n546        operating temperature measurements\". IEC, Geneva, 2018.\n547 \n548     .. [3] \"IEC 61853-3 Photovoltaic (PV) module performance testing and energy\n549        rating - Part 3: Energy rating of PV modules\". IEC, Geneva, 2018.\n550 \n551     .. [4] Driesse, A. et al (2022) \"Improving Common PV Module Temperature\n552        Models by Incorporating Radiative Losses to the Sky\". SAND2022-11604.\n553        :doi:`10.2172/1884890`\n554 \n555     See also\n556     --------\n557     pvlib.temperature.faiman\n558 \n559     '''  # noQA: E501\n560 \n561     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. Nov., 2022\n562 \n563     abs_zero = -273.15\n564     sigma = scipy.constants.Stefan_Boltzmann\n565 \n566     if ir_down is None:\n567         qrad_sky = 0.0\n568     else:\n569         ir_up = sigma * ((temp_air - abs_zero)**4)\n570         qrad_sky = emissivity * sky_view * (ir_up - ir_down)\n571 \n572     heat_input = poa_global - qrad_sky\n573     total_loss_factor = u0 + u1 * wind_speed\n574     temp_difference = heat_input / total_loss_factor\n575     return temp_air + temp_difference\n576 \n577 \n578 def ross(poa_global, temp_air, noct):\n579     r'''\n580     Calculate cell temperature using the Ross model.\n581 \n582     The Ross model [1]_ assumes the difference between cell temperature\n583     and ambient temperature is proportional to the plane of array irradiance,\n584     and assumes wind speed of 1 m/s. The model implicitly assumes steady or\n585     slowly changing irradiance conditions.\n586 \n587     Parameters\n588     ----------\n589     poa_global : numeric\n590         Total incident irradiance. [W/m^2]\n591 \n592     temp_air : numeric\n593         Ambient dry bulb temperature. [C]\n594 \n595     noct : numeric\n596         Nominal operating cell temperature [C], determined at conditions of\n597         800 W/m^2 irradiance, 20 C ambient air temperature and 1 m/s wind.\n598 \n599     Returns\n600     -------\n601     cell_temperature : numeric\n602         Cell temperature. [C]\n603 \n604     Notes\n605     -----\n606     The Ross model for cell temperature :math:`T_{C}` is given in [1]_ as\n607 \n608     .. math::\n609 \n610         T_{C} = T_{a} + \\frac{NOCT - 20}{80} S\n611 \n612     where :math:`S` is the plane of array irradiance in :math:`mW/{cm}^2`.\n613     This function expects irradiance in :math:`W/m^2`.\n614 \n615     References\n616     ----------\n617     .. [1] Ross, R. G. Jr., (1981). \"Design Techniques for Flat-Plate\n618        Photovoltaic Arrays\". 15th IEEE Photovoltaic Specialist Conference,\n619        Orlando, FL.\n620     '''\n621     # factor of 0.1 converts irradiance from W/m2 to mW/cm2\n622     return temp_air + (noct - 20.) / 80. * poa_global * 0.1\n623 \n624 \n625 def _fuentes_hconv(tave, windmod, tinoct, temp_delta, xlen, tilt,\n626                    check_reynold):\n627     # Calculate the convective coefficient as in Fuentes 1987 -- a mixture of\n628     # free, laminar, and turbulent convection.\n629     densair = 0.003484 * 101325.0 / tave  # density\n630     visair = 0.24237e-6 * tave**0.76 / densair  # kinematic viscosity\n631     condair = 2.1695e-4 * tave**0.84  # thermal conductivity\n632     reynold = windmod * xlen / visair\n633     # the boundary between laminar and turbulent is modeled as an abrupt\n634     # change at Re = 1.2e5:\n635     if check_reynold and reynold > 1.2e5:\n636         # turbulent convection\n637         hforce = 0.0282 / reynold**0.2 * densair * windmod * 1007 / 0.71**0.4\n638     else:\n639         # laminar convection\n640         hforce = 0.8600 / reynold**0.5 * densair * windmod * 1007 / 0.71**0.67\n641     # free convection via Grashof number\n642     # NB: Fuentes hardwires sind(tilt) as 0.5 for tilt=30\n643     grashof = 9.8 / tave * temp_delta * xlen**3 / visair**2 * sind(tilt)\n644     # product of Nusselt number and (k/l)\n645     hfree = 0.21 * (grashof * 0.71)**0.32 * condair / xlen\n646     # combine free and forced components\n647     hconv = (hfree**3 + hforce**3)**(1/3)\n648     return hconv\n649 \n650 \n651 def _hydraulic_diameter(width, height):\n652     # calculate the hydraulic diameter of a rectangle\n653     return 2 * (width * height) / (width + height)\n654 \n655 \n656 def fuentes(poa_global, temp_air, wind_speed, noct_installed, module_height=5,\n657             wind_height=9.144, emissivity=0.84, absorption=0.83,\n658             surface_tilt=30, module_width=0.31579, module_length=1.2):\n659     \"\"\"\n660     Calculate cell or module temperature using the Fuentes model.\n661 \n662     The Fuentes model is a first-principles heat transfer energy balance\n663     model [1]_ that is used in PVWatts for cell temperature modeling [2]_.\n664 \n665     Parameters\n666     ----------\n667     poa_global : pandas Series\n668         Total incident irradiance [W/m^2]\n669 \n670     temp_air : pandas Series\n671         Ambient dry bulb temperature [C]\n672 \n673     wind_speed : pandas Series\n674         Wind speed [m/s]\n675 \n676     noct_installed : float\n677         The \"installed\" nominal operating cell temperature as defined in [1]_.\n678         PVWatts assumes this value to be 45 C for rack-mounted arrays and\n679         49 C for roof mount systems with restricted air flow around the\n680         module.  [C]\n681 \n682     module_height : float, default 5.0\n683         The height above ground of the center of the module. The PVWatts\n684         default is 5.0 [m]\n685 \n686     wind_height : float, default 9.144\n687         The height above ground at which ``wind_speed`` is measured. The\n688         PVWatts defauls is 9.144 [m]\n689 \n690     emissivity : float, default 0.84\n691         The effectiveness of the module at radiating thermal energy. [unitless]\n692 \n693     absorption : float, default 0.83\n694         The fraction of incident irradiance that is converted to thermal\n695         energy in the module. [unitless]\n696 \n697     surface_tilt : float, default 30\n698         Module tilt from horizontal. If not provided, the default value\n699         of 30 degrees from [1]_ and [2]_ is used. [degrees]\n700 \n701     module_width : float, default 0.31579\n702         Module width. The default value of 0.31579 meters in combination with\n703         the default `module_length` gives a hydraulic diameter of 0.5 as\n704         assumed in [1]_ and [2]_. [m]\n705 \n706     module_length : float, default 1.2\n707         Module length. The default value of 1.2 meters in combination with\n708         the default `module_width` gives a hydraulic diameter of 0.5 as\n709         assumed in [1]_ and [2]_. [m]\n710 \n711     Returns\n712     -------\n713     temperature_cell : pandas Series\n714         The modeled cell temperature [C]\n715 \n716     Notes\n717     -----\n718     This function returns slightly different values from PVWatts at night\n719     and just after dawn. This is because the SAM SSC assumes that module\n720     temperature equals ambient temperature when irradiance is zero so it can\n721     skip the heat balance calculation at night.\n722 \n723     References\n724     ----------\n725     .. [1] Fuentes, M. K., 1987, \"A Simplifed Thermal Model for Flat-Plate\n726            Photovoltaic Arrays\", SAND85-0330, Sandia National Laboratories,\n727            Albuquerque NM.\n728            http://prod.sandia.gov/techlib/access-control.cgi/1985/850330.pdf\n729     .. [2] Dobos, A. P., 2014, \"PVWatts Version 5 Manual\", NREL/TP-6A20-62641,\n730            National Renewable Energy Laboratory, Golden CO.\n731            doi:10.2172/1158421.\n732     \"\"\"\n733     # ported from the FORTRAN77 code provided in Appendix A of Fuentes 1987;\n734     # nearly all variable names are kept the same for ease of comparison.\n735 \n736     boltz = 5.669e-8\n737     emiss = emissivity\n738     absorp = absorption\n739     xlen = _hydraulic_diameter(module_width, module_length)\n740     # cap0 has units of [J / (m^2 K)], equal to mass per unit area times\n741     # specific heat of the module.\n742     cap0 = 11000\n743     tinoct = noct_installed + 273.15\n744 \n745     # convective coefficient of top surface of module at NOCT\n746     windmod = 1.0\n747     tave = (tinoct + 293.15) / 2\n748     hconv = _fuentes_hconv(tave, windmod, tinoct, tinoct - 293.15, xlen,\n749                            surface_tilt, False)\n750 \n751     # determine the ground temperature ratio and the ratio of the total\n752     # convection to the top side convection\n753     hground = emiss * boltz * (tinoct**2 + 293.15**2) * (tinoct + 293.15)\n754     backrat = (\n755         absorp * 800.0\n756         - emiss * boltz * (tinoct**4 - 282.21**4)\n757         - hconv * (tinoct - 293.15)\n758     ) / ((hground + hconv) * (tinoct - 293.15))\n759     tground = (tinoct**4 - backrat * (tinoct**4 - 293.15**4))**0.25\n760     tground = np.clip(tground, 293.15, tinoct)\n761 \n762     tgrat = (tground - 293.15) / (tinoct - 293.15)\n763     convrat = (absorp * 800 - emiss * boltz * (\n764         2 * tinoct**4 - 282.21**4 - tground**4)) / (hconv * (tinoct - 293.15))\n765 \n766     # adjust the capacitance (thermal mass) of the module based on the INOCT.\n767     # It is a function of INOCT because high INOCT implies thermal coupling\n768     # with the racking (e.g. roofmount), so the thermal mass is increased.\n769     # `cap` has units J/(m^2 C) -- see Table 3, Equations 26 & 27\n770     cap = cap0\n771     if tinoct > 321.15:\n772         cap = cap * (1 + (tinoct - 321.15) / 12)\n773 \n774     # iterate through timeseries inputs\n775     sun0 = 0\n776 \n777     # n.b. the way Fuentes calculates the first timedelta makes it seem like\n778     # the value doesn't matter -- rather than recreate it here, just assume\n779     # it's the same as the second timedelta:\n780     timedelta_seconds = poa_global.index.to_series().diff().dt.total_seconds()\n781     timedelta_hours = timedelta_seconds / 3600\n782     timedelta_hours.iloc[0] = timedelta_hours.iloc[1]\n783 \n784     tamb_array = temp_air + 273.15\n785     sun_array = poa_global * absorp\n786 \n787     # Two of the calculations are easily vectorized, so precalculate them:\n788     # sky temperature -- Equation 24\n789     tsky_array = 0.68 * (0.0552 * tamb_array**1.5) + 0.32 * tamb_array\n790     # wind speed at module height -- Equation 22\n791     # not sure why the 1e-4 factor is included -- maybe the equations don't\n792     # behave well if wind == 0?\n793     windmod_array = wind_speed * (module_height/wind_height)**0.2 + 1e-4\n794 \n795     tmod0 = 293.15\n796     tmod_array = np.zeros_like(poa_global)\n797 \n798     iterator = zip(tamb_array, sun_array, windmod_array, tsky_array,\n799                    timedelta_hours)\n800     for i, (tamb, sun, windmod, tsky, dtime) in enumerate(iterator):\n801         # solve the heat transfer equation, iterating because the heat loss\n802         # terms depend on tmod. NB Fuentes doesn't show that 10 iterations is\n803         # sufficient for convergence.\n804         tmod = tmod0\n805         for j in range(10):\n806             # overall convective coefficient\n807             tave = (tmod + tamb) / 2\n808             hconv = convrat * _fuentes_hconv(tave, windmod, tinoct,\n809                                              abs(tmod-tamb), xlen,\n810                                              surface_tilt, True)\n811             # sky radiation coefficient (Equation 3)\n812             hsky = emiss * boltz * (tmod**2 + tsky**2) * (tmod + tsky)\n813             # ground radiation coeffieicient (Equation 4)\n814             tground = tamb + tgrat * (tmod - tamb)\n815             hground = emiss * boltz * (tmod**2 + tground**2) * (tmod + tground)\n816             # thermal lag -- Equation 8\n817             eigen = - (hconv + hsky + hground) / cap * dtime * 3600\n818             # not sure why this check is done, maybe as a speed optimization?\n819             if eigen > -10:\n820                 ex = np.exp(eigen)\n821             else:\n822                 ex = 0\n823             # Equation 7 -- note that `sun` and `sun0` already account for\n824             # absorption (alpha)\n825             tmod = tmod0 * ex + (\n826                 (1 - ex) * (\n827                     hconv * tamb\n828                     + hsky * tsky\n829                     + hground * tground\n830                     + sun0\n831                     + (sun - sun0) / eigen\n832                 ) + sun - sun0\n833             ) / (hconv + hsky + hground)\n834         tmod_array[i] = tmod\n835         tmod0 = tmod\n836         sun0 = sun\n837 \n838     return pd.Series(tmod_array - 273.15, index=poa_global.index, name='tmod')\n839 \n840 \n841 def _adj_for_mounting_standoff(x):\n842     # supports noct cell temperature function. Except for x > 3.5, the SAM code\n843     # and documentation aren't clear on the precise intervals. The choice of\n844     # < or <= here is pvlib's.\n845     return np.piecewise(x, [x <= 0, (x > 0) & (x < 0.5),\n846                             (x >= 0.5) & (x < 1.5), (x >= 1.5) & (x < 2.5),\n847                             (x >= 2.5) & (x <= 3.5), x > 3.5],\n848                         [0., 18., 11., 6., 2., 0.])\n849 \n850 \n851 def noct_sam(poa_global, temp_air, wind_speed, noct, module_efficiency,\n852              effective_irradiance=None, transmittance_absorptance=0.9,\n853              array_height=1, mount_standoff=4):\n854     r'''\n855     Cell temperature model from the System Advisor Model (SAM).\n856 \n857     The model is described in [1]_, Section 10.6.\n858 \n859     Parameters\n860     ----------\n861     poa_global : numeric\n862         Total incident irradiance. [W/m^2]\n863 \n864     temp_air : numeric\n865         Ambient dry bulb temperature. [C]\n866 \n867     wind_speed : numeric\n868         Wind speed in m/s measured at the same height for which the wind loss\n869         factor was determined.  The default value 1.0 m/s is the wind\n870         speed at module height used to determine NOCT. [m/s]\n871 \n872     noct : float\n873         Nominal operating cell temperature [C], determined at conditions of\n874         800 W/m^2 irradiance, 20 C ambient air temperature and 1 m/s wind.\n875 \n876     module_efficiency : float\n877         Module external efficiency [unitless] at reference conditions of\n878         1000 W/m^2 and 20C. Denoted as :math:`eta_{m}` in [1]_. Calculate as\n879         :math:`\\eta_{m} = \\frac{V_{mp} I_{mp}}{A \\times 1000 W/m^2}`\n880         where A is module area [m^2].\n881 \n882     effective_irradiance : numeric, default None.\n883         The irradiance that is converted to photocurrent. If None,\n884         assumed equal to poa_global. [W/m^2]\n885 \n886     transmittance_absorptance : numeric, default 0.9\n887         Coefficient for combined transmittance and absorptance effects.\n888         [unitless]\n889 \n890     array_height : int, default 1\n891         Height of array above ground in stories (one story is about 3m). Must\n892         be either 1 or 2. For systems elevated less than one story, use 1.\n893         If system is elevated more than two stories, use 2.\n894 \n895     mount_standoff : numeric, default 4\n896         Distance between array mounting and mounting surface. Use default\n897         if system is ground-mounted. [inches]\n898 \n899     Returns\n900     -------\n901     cell_temperature : numeric\n902         Cell temperature. [C]\n903 \n904     Raises\n905     ------\n906     ValueError\n907         If array_height is an invalid value (must be 1 or 2).\n908 \n909     References\n910     ----------\n911     .. [1] Gilman, P., Dobos, A., DiOrio, N., Freeman, J., Janzou, S.,\n912            Ryberg, D., 2018, \"SAM Photovoltaic Model Technical Reference\n913            Update\", National Renewable Energy Laboratory Report\n914            NREL/TP-6A20-67399.\n915     '''\n916     # in [1] the denominator for irr_ratio isn't precisely clear. From\n917     # reproducing output of the SAM function noct_celltemp_t, we determined\n918     # that:\n919     #  - G_total (SAM) is broadband plane-of-array irradiance before\n920     #    reflections. Equivalent to pvlib variable poa_global\n921     #  - Geff_total (SAM) is POA irradiance after reflections and\n922     #    adjustment for spectrum. Equivalent to effective_irradiance\n923     if effective_irradiance is None:\n924         irr_ratio = 1.\n925     else:\n926         irr_ratio = effective_irradiance / poa_global\n927 \n928     if array_height == 1:\n929         wind_adj = 0.51 * wind_speed\n930     elif array_height == 2:\n931         wind_adj = 0.61 * wind_speed\n932     else:\n933         raise ValueError(\n934             f'array_height must be 1 or 2, {array_height} was given')\n935 \n936     noct_adj = noct + _adj_for_mounting_standoff(mount_standoff)\n937     tau_alpha = transmittance_absorptance * irr_ratio\n938 \n939     # [1] Eq. 10.37 isn't clear on exactly what \"G\" is. SAM SSC code uses\n940     # poa_global where G appears\n941     cell_temp_init = poa_global / 800. * (noct_adj - 20.)\n942     heat_loss = 1 - module_efficiency / tau_alpha\n943     wind_loss = 9.5 / (5.7 + 3.8 * wind_adj)\n944     return temp_air + cell_temp_init * heat_loss * wind_loss\n945 \n946 \n947 def prilliman(temp_cell, wind_speed, unit_mass=11.1, coefficients=None):\n948     \"\"\"\n949     Smooth short-term cell temperature transients using the Prilliman model.\n950 \n951     The Prilliman et al. model [1]_ applies a weighted moving average to\n952     the output of a steady-state cell temperature model to account for\n953     a module's thermal inertia by smoothing the cell temperature's\n954     response to changing weather conditions.\n955 \n956     .. warning::\n957         This implementation requires the time series inputs to be regularly\n958         sampled in time with frequency less than 20 minutes.  Data with\n959         irregular time steps (including from data gaps, missing leap days,\n960         etc) should be resampled prior to using this function.\n961 \n962     Parameters\n963     ----------\n964     temp_cell : pandas.Series with DatetimeIndex\n965         Cell temperature modeled with steady-state assumptions. [C]\n966 \n967     wind_speed : pandas.Series\n968         Wind speed, adjusted to correspond to array height [m/s]\n969 \n970     unit_mass : float, default 11.1\n971         Total mass of module divided by its one-sided surface area [kg/m^2]\n972 \n973     coefficients : 4-element list-like, optional\n974         Values for coefficients a_0 through a_3, see Eq. 9 of [1]_\n975 \n976     Returns\n977     -------\n978     temp_cell : pandas.Series\n979         Smoothed version of the input cell temperature. Input temperature\n980         with sampling interval >= 20 minutes is returned unchanged. [C]\n981 \n982     Notes\n983     -----\n984     This smoothing model was developed and validated using the SAPM\n985     cell temperature model for the steady-state input.\n986 \n987     Smoothing is done using the 20 minute window behind each temperature\n988     value. At the beginning of the series where a full 20 minute window is not\n989     possible, partial windows are used instead.\n990 \n991     Output ``temp_cell[k]`` is NaN when input ``wind_speed[k]`` is NaN, or\n992     when no non-NaN data are in the input temperature for the 20 minute window\n993     preceding index ``k``.\n994 \n995     References\n996     ----------\n997     .. [1] M. Prilliman, J. S. Stein, D. Riley and G. Tamizhmani,\n998        \"Transient Weighted Moving-Average Model of Photovoltaic Module\n999        Back-Surface Temperature,\" IEEE Journal of Photovoltaics, 2020.\n1000        :doi:`10.1109/JPHOTOV.2020.2992351`\n1001     \"\"\"\n1002 \n1003     # `sample_interval` in minutes:\n1004     sample_interval, samples_per_window = \\\n1005         _get_sample_intervals(times=temp_cell.index, win_length=20)\n1006 \n1007     if sample_interval >= 20:\n1008         warnings.warn(\"temperature.prilliman only applies smoothing when \"\n1009                       \"the sampling interval is shorter than 20 minutes \"\n1010                       f\"(input sampling interval: {sample_interval} minutes);\"\n1011                       \" returning input temperature series unchanged\")\n1012         # too coarsely sampled for smoothing to be relevant\n1013         return temp_cell\n1014 \n1015     # handle cases where the time series is shorter than 20 minutes total\n1016     samples_per_window = min(samples_per_window, len(temp_cell))\n1017 \n1018     # prefix with NaNs so that the rolling window is \"full\",\n1019     # even for the first actual value:\n1020     prefix = np.full(samples_per_window, np.nan)\n1021     temp_cell_prefixed = np.append(prefix, temp_cell.values)\n1022 \n1023     # generate matrix of integers for creating windows with indexing\n1024     H = scipy.linalg.hankel(np.arange(samples_per_window),\n1025                             np.arange(samples_per_window - 1,\n1026                                       len(temp_cell_prefixed) - 1))\n1027     # each row of `subsets` is the values in one window\n1028     subsets = temp_cell_prefixed[H].T\n1029 \n1030     # `subsets` now looks like this (for 5-minute data, so 4 samples/window)\n1031     # where \"1.\" is a stand-in for the actual temperature values\n1032     # [[nan, nan, nan, nan],\n1033     #  [nan, nan, nan,  1.],\n1034     #  [nan, nan,  1.,  1.],\n1035     #  [nan,  1.,  1.,  1.],\n1036     #  [ 1.,  1.,  1.,  1.],\n1037     #  [ 1.,  1.,  1.,  1.],\n1038     #  [ 1.,  1.,  1.,  1.],\n1039     #  ...\n1040 \n1041     # calculate weights for the values in each window\n1042     if coefficients is not None:\n1043         a = coefficients\n1044     else:\n1045         # values from [1], Table II\n1046         a = [0.0046, 0.00046, -0.00023, -1.6e-5]\n1047 \n1048     wind_speed = wind_speed.values\n1049     p = a[0] + a[1]*wind_speed + a[2]*unit_mass + a[3]*wind_speed*unit_mass\n1050     # calculate the time lag for each sample in the window, paying attention\n1051     # to units (seconds for `timedeltas`, minutes for `sample_interval`)\n1052     timedeltas = np.arange(samples_per_window, 0, -1) * sample_interval * 60\n1053     weights = np.exp(-p[:, np.newaxis] * timedeltas)\n1054 \n1055     # Set weights corresponding to the prefix values to zero; otherwise the\n1056     # denominator of the weighted average below would be wrong.\n1057     # Weights corresponding to (non-prefix) NaN values must be zero too\n1058     # for the same reason.\n1059 \n1060     # Right now `weights` is something like this\n1061     # (using 5-minute inputs, so 4 samples per window -> 4 values per row):\n1062     # [[0.0611, 0.1229, 0.2472, 0.4972],\n1063     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1064     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1065     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1066     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1067     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1068     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1069     #  ...\n1070 \n1071     # After the next line, the NaNs in `subsets` will be zeros in `weights`,\n1072     # like this (with more zeros for any NaNs in the input temperature):\n1073 \n1074     # [[0.    , 0.    , 0.    , 0.    ],\n1075     #  [0.    , 0.    , 0.    , 0.4972],\n1076     #  [0.    , 0.    , 0.2472, 0.4972],\n1077     #  [0.    , 0.1229, 0.2472, 0.4972],\n1078     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1079     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1080     #  [0.0611, 0.1229, 0.2472, 0.4972],\n1081     #  ...\n1082 \n1083     weights[np.isnan(subsets)] = 0\n1084 \n1085     # change the first row of weights from zero to nan -- this is a\n1086     # trick to prevent div by zero warning when dividing by summed weights\n1087     weights[0, :] = np.nan\n1088 \n1089     # finally, take the weighted average of each window:\n1090     # use np.nansum for numerator to ignore nans in input temperature, but\n1091     # np.sum for denominator to propagate nans in input wind speed.\n1092     numerator = np.nansum(subsets * weights, axis=1)\n1093     denominator = np.sum(weights, axis=1)\n1094     smoothed = numerator / denominator\n1095     smoothed[0] = temp_cell.values[0]\n1096     smoothed = pd.Series(smoothed, index=temp_cell.index)\n1097     return smoothed\n1098 \n1099 \n1100 def generic_linear(poa_global, temp_air, wind_speed, u_const, du_wind,\n1101                    module_efficiency, absorptance):\n1102     \"\"\"\n1103     Calculate cell temperature using a generic linear heat loss factor model.\n1104 \n1105     The parameters for this model can be obtained from other model\n1106     parameters using :py:class:`GenericLinearModel`.  A description of this\n1107     model and its relationship to other temperature models is found in [1]_.\n1108 \n1109     Parameters\n1110     ----------\n1111     poa_global : numeric\n1112         Total incident irradiance [W/m^2].\n1113 \n1114     temp_air : numeric\n1115         Ambient dry bulb temperature [C].\n1116 \n1117     wind_speed : numeric\n1118         Wind speed at a height of 10 meters [m/s].\n1119 \n1120     u_const : float\n1121         Combined heat transfer coefficient at zero wind speed [(W/m^2)/C]\n1122 \n1123     du_wind : float\n1124         Influence of wind speed on combined heat transfer coefficient\n1125         [(W/m^2)/C/(m/s)]\n1126 \n1127     module_efficiency : float\n1128         The electrical efficiency of the module. [-]\n1129 \n1130     absorptance : float\n1131         The light absorptance of the module. [-]\n1132 \n1133     Returns\n1134     -------\n1135     numeric, values in degrees C.\n1136 \n1137     References\n1138     ----------\n1139     .. [1] A. Driesse et al, \"PV Module Operating Temperature\n1140        Model Equivalence and Parameter Translation\". 2022 IEEE\n1141        Photovoltaic Specialists Conference (PVSC), 2022.\n1142 \n1143     See also\n1144     --------\n1145     pvlib.temperature.GenericLinearModel\n1146     \"\"\"\n1147     # Contributed by Anton Driesse (@adriesse), PV Performance Labs, Sept. 2022\n1148 \n1149     heat_input = poa_global * (absorptance - module_efficiency)\n1150     total_loss_factor = u_const + du_wind * wind_speed\n1151     temp_difference = heat_input / total_loss_factor\n1152 \n1153     return temp_air + temp_difference\n1154 \n1155 \n1156 class GenericLinearModel():\n1157     '''\n1158     A class that can both use and convert parameters of linear module\n1159     temperature models: faiman, pvsyst, noct_sam, sapm_module\n1160     and generic_linear.\n1161 \n1162     Parameters are converted between models by first converting\n1163     to the generic linear heat transfer model [1]_ by the ``use_``\n1164     methods. The equivalent parameters for the target temperature\n1165     model are then obtained by the ``to_`` method.\n1166     Parameters are returned as a dictionary that is compatible with the\n1167     target model function to use in simulations.\n1168 \n1169     An instance of the class represents a specific module type and\n1170     the parameters ``module_efficiency`` and ``absorptance`` are required.\n1171     Although some temperature models do not use these properties, they\n1172     nevertheless exist and affect operating temperature. Values\n1173     should be representative of the conditions at which the input\n1174     model parameters were determined (usually high irradiance).\n1175 \n1176     Parameters\n1177     ----------\n1178     module_efficiency : float\n1179         The electrical efficiency of the module. [-]\n1180 \n1181     absorptance : float\n1182         The light absorptance of the module. [-]\n1183 \n1184     Notes\n1185     -----\n1186     After creating a GenericLinearModel object using the module properties,\n1187     one of the ``use_`` methods must be called to provide thermal model\n1188     parameters.  If this is not done, the ``to_`` methods will return ``nan``\n1189     values.\n1190 \n1191     References\n1192     ----------\n1193     .. [1] A. Driesse et al, \"PV Module Operating Temperature\n1194        Model Equivalence and Parameter Translation\". 2022 IEEE\n1195        Photovoltaic Specialists Conference (PVSC), 2022.\n1196 \n1197     Examples\n1198     --------\n1199     >>> glm = GenericLinearModel(module_efficiency=0.19, absorptance=0.88)\n1200 \n1201     >>> glm.use_faiman(16, 8)\n1202     GenericLinearModel: {'u_const': 11.04, 'du_wind': 5.52,\n1203                          'eta': 0.19, 'alpha': 0.88}\n1204 \n1205     >>> glm.to_pvsyst()\n1206     {'u_c': 11.404800000000002, 'u_v': 5.702400000000001,\n1207      'module_efficiency': 0.19, 'alpha_absorption': 0.88}\n1208 \n1209     >>> parmdict = glm.to_pvsyst()\n1210     >>> pvsyst_cell(800, 20, 1, **parmdict)\n1211     53.33333333333333\n1212 \n1213     See also\n1214     --------\n1215     pvlib.temperature.generic_linear\n1216     '''\n1217     # Contributed by Anton Driesse (@adriesse), PV Performance Labs, Sept. 2022\n1218 \n1219     def __init__(self, module_efficiency, absorptance):\n1220 \n1221         self.u_const = np.nan\n1222         self.du_wind = np.nan\n1223         self.eta = module_efficiency\n1224         self.alpha = absorptance\n1225 \n1226         return None\n1227 \n1228     def __repr__(self):\n1229 \n1230         return self.__class__.__name__ + ': ' + vars(self).__repr__()\n1231 \n1232     def __call__(self, poa_global, temp_air, wind_speed,\n1233                  module_efficiency=None):\n1234         '''\n1235         Calculate module temperature using the generic_linear model and\n1236         previously initialized parameters.\n1237 \n1238         Parameters\n1239         ----------\n1240         poa_global : numeric\n1241             Total incident irradiance [W/m^2].\n1242 \n1243         temp_air : numeric\n1244             Ambient dry bulb temperature [C].\n1245 \n1246         wind_speed : numeric\n1247             Wind speed in m/s measured at the same height for which the wind\n1248             loss factor was determined.  [m/s]\n1249 \n1250         module_efficiency : numeric, optional\n1251             Module electrical efficiency.  The default value is the one\n1252             that was specified initially. [-]\n1253 \n1254         Returns\n1255         -------\n1256         numeric, values in degrees Celsius\n1257 \n1258         See also\n1259         --------\n1260         get_generic\n1261         pvlib.temperature.generic_linear\n1262         '''\n1263         if module_efficiency is None:\n1264             module_efficiency = self.eta\n1265 \n1266         return generic_linear(poa_global, temp_air, wind_speed,\n1267                               self.u_const, self.du_wind,\n1268                               module_efficiency, self.alpha)\n1269 \n1270     def get_generic_linear(self):\n1271         '''\n1272         Get the generic linear model parameters to use with the separate\n1273         generic linear module temperature calculation function.\n1274 \n1275         Returns\n1276         -------\n1277         model_parameters : dict\n1278 \n1279         See also\n1280         --------\n1281         pvlib.temperature.generic_linear\n1282         '''\n1283         return dict(u_const=self.u_const,\n1284                     du_wind=self.du_wind,\n1285                     module_efficiency=self.eta,\n1286                     absorptance=self.alpha)\n1287 \n1288     def use_faiman(self, u0, u1):\n1289         '''\n1290         Use the Faiman model parameters to set the generic_model equivalents.\n1291 \n1292         Parameters\n1293         ----------\n1294         u0, u1 : float\n1295             See :py:func:`pvlib.temperature.faiman` for details.\n1296         '''\n1297         net_absorptance = self.alpha - self.eta\n1298         self.u_const = u0 * net_absorptance\n1299         self.du_wind = u1 * net_absorptance\n1300 \n1301         return self\n1302 \n1303     def to_faiman(self):\n1304         '''\n1305         Convert the generic model parameters to Faiman equivalents.\n1306 \n1307         Returns\n1308         ----------\n1309         model_parameters : dict\n1310             See :py:func:`pvlib.temperature.faiman` for\n1311             model parameter details.\n1312         '''\n1313         net_absorptance = self.alpha - self.eta\n1314         u0 = self.u_const / net_absorptance\n1315         u1 = self.du_wind / net_absorptance\n1316 \n1317         return dict(u0=u0, u1=u1)\n1318 \n1319     def use_pvsyst(self, u_c, u_v, module_efficiency=None,\n1320                    alpha_absorption=None):\n1321         '''\n1322         Use the PVsyst model parameters to set the generic_model equivalents.\n1323 \n1324         Parameters\n1325         ----------\n1326         u_c, u_v : float\n1327             See :py:func:`pvlib.temperature.pvsyst_cell` for details.\n1328 \n1329         module_efficiency, alpha_absorption : float, optional\n1330             See :py:func:`pvlib.temperature.pvsyst_cell` for details.\n1331 \n1332         Notes\n1333         -----\n1334         The optional parameters are primarily for convenient compatibility\n1335         with existing function signatures.\n1336         '''\n1337         if module_efficiency is not None:\n1338             self.eta = module_efficiency\n1339 \n1340         if alpha_absorption is not None:\n1341             self.alpha = alpha_absorption\n1342 \n1343         net_absorptance_glm = self.alpha - self.eta\n1344         net_absorptance_pvsyst = self.alpha * (1.0 - self.eta)\n1345         absorptance_ratio = net_absorptance_glm / net_absorptance_pvsyst\n1346 \n1347         self.u_const = u_c * absorptance_ratio\n1348         self.du_wind = u_v * absorptance_ratio\n1349 \n1350         return self\n1351 \n1352     def to_pvsyst(self):\n1353         '''\n1354         Convert the generic model parameters to PVsyst model equivalents.\n1355 \n1356         Returns\n1357         ----------\n1358         model_parameters : dict\n1359             See :py:func:`pvlib.temperature.pvsyst_cell` for\n1360             model parameter details.\n1361         '''\n1362         net_absorptance_glm = self.alpha - self.eta\n1363         net_absorptance_pvsyst = self.alpha * (1.0 - self.eta)\n1364         absorptance_ratio = net_absorptance_glm / net_absorptance_pvsyst\n1365 \n1366         u_c = self.u_const / absorptance_ratio\n1367         u_v = self.du_wind / absorptance_ratio\n1368 \n1369         return dict(u_c=u_c,\n1370                     u_v=u_v,\n1371                     module_efficiency=self.eta,\n1372                     alpha_absorption=self.alpha)\n1373 \n1374     def use_noct_sam(self, noct, module_efficiency=None,\n1375                      transmittance_absorptance=None):\n1376         '''\n1377         Use the NOCT SAM model parameters to set the generic_model equivalents.\n1378 \n1379         Parameters\n1380         ----------\n1381         noct : float\n1382             See :py:func:`pvlib.temperature.noct_sam` for details.\n1383 \n1384         module_efficiency, transmittance_absorptance : float, optional\n1385             See :py:func:`pvlib.temperature.noct_sam` for details.\n1386 \n1387         Notes\n1388         -----\n1389         The optional parameters are primarily for convenient compatibility\n1390         with existing function signatures.\n1391         '''\n1392         if module_efficiency is not None:\n1393             self.eta = module_efficiency\n1394 \n1395         if transmittance_absorptance is not None:\n1396             self.alpha = transmittance_absorptance\n1397 \n1398         # NOCT is determined with wind speed near module height\n1399         # the adjustment reduces the wind coefficient for use with 10m wind\n1400         wind_adj = 0.51\n1401         u_noct = 800.0 * self.alpha / (noct - 20.0)\n1402         self.u_const = u_noct * 0.6\n1403         self.du_wind = u_noct * 0.4 * wind_adj\n1404 \n1405         return self\n1406 \n1407     def to_noct_sam(self):\n1408         '''\n1409         Convert the generic model parameters to NOCT SAM model equivalents.\n1410 \n1411         Returns\n1412         ----------\n1413         model_parameters : dict\n1414             See :py:func:`pvlib.temperature.noct_sam` for\n1415             model parameter details.\n1416         '''\n1417         # NOCT is determined with wind speed near module height\n1418         # the adjustment reduces the wind coefficient for use with 10m wind\n1419         wind_adj = 0.51\n1420         u_noct = self.u_const + self.du_wind / wind_adj\n1421         noct = 20.0 + (800.0 * self.alpha) / u_noct\n1422 \n1423         return dict(noct=noct,\n1424                     module_efficiency=self.eta,\n1425                     transmittance_absorptance=self.alpha)\n1426 \n1427     def use_sapm(self, a, b, wind_fit_low=1.4, wind_fit_high=5.4):\n1428         '''\n1429         Use the SAPM model parameters to set the generic_model equivalents.\n1430 \n1431         In the SAPM the heat transfer coefficient increases exponentially\n1432         with windspeed, whereas in the other models the increase is linear.\n1433         This function equates the generic linear model to SAPM at two\n1434         specified winds speeds, thereby defining a linear approximation\n1435         for the exponential behavior.\n1436 \n1437         Parameters\n1438         ----------\n1439         a, b : float\n1440             See :py:func:`pvlib.temperature.sapm_module` for details.\n1441 \n1442         wind_fit_low : float, optional\n1443             First wind speed value at which the generic linear model\n1444             must be equal to the SAPM model. [m/s]\n1445 \n1446         wind_fit_high : float, optional\n1447             Second wind speed value at which the generic linear model\n1448             must be equal to the SAPM model. [m/s]\n1449 \n1450         Notes\n1451         -----\n1452         The two default wind speed values are based on measurements\n1453         at 10 m height.  Both the SAPM model and the conversion\n1454         functions can work with wind speed data at different heights as\n1455         long as the same height is used consistently throughout.\n1456         '''\n1457         u_low = 1.0 / np.exp(a + b * wind_fit_low)\n1458         u_high = 1.0 / np.exp(a + b * wind_fit_high)\n1459 \n1460         du_wind = (u_high - u_low) / (wind_fit_high - wind_fit_low)\n1461         u_const = u_low - du_wind * wind_fit_low\n1462 \n1463         net_absorptance = self.alpha - self.eta\n1464         self.u_const = u_const * net_absorptance\n1465         self.du_wind = du_wind * net_absorptance\n1466 \n1467         return self\n1468 \n1469     def to_sapm(self, wind_fit_low=1.4, wind_fit_high=5.4):\n1470         '''\n1471         Convert the generic model parameters to SAPM model equivalents.\n1472 \n1473         In the SAPM the heat transfer coefficient increases exponentially\n1474         with windspeed, whereas in the other models the increase is linear.\n1475         This function equates SAPM to the generic linear model at two\n1476         specified winds speeds, thereby defining an exponential approximation\n1477         for the linear behavior.\n1478 \n1479         Parameters\n1480         ----------\n1481         wind_fit_low : float, optional\n1482             First wind speed value at which the generic linear model\n1483             must be equal to the SAPM model. [m/s]\n1484 \n1485         wind_fit_high : float, optional\n1486             Second wind speed value at which the generic linear model\n1487             must be equal to the SAPM model. [m/s]\n1488 \n1489         Returns\n1490         ----------\n1491         model_parameters : dict\n1492             See :py:func:`pvlib.temperature.sapm_module` for\n1493             model parameter details.\n1494 \n1495         Notes\n1496         -----\n1497         The two default wind speed values are based on measurements\n1498         at 10 m height.  Both the SAPM model and the conversion\n1499         functions can work with wind speed data at different heights as\n1500         long as the same height is used consistently throughout.\n1501         '''\n1502         net_absorptance = self.alpha - self.eta\n1503         u_const = self.u_const / net_absorptance\n1504         du_wind = self.du_wind / net_absorptance\n1505 \n1506         u_low = u_const + du_wind * wind_fit_low\n1507         u_high = u_const + du_wind * wind_fit_high\n1508 \n1509         b = - ((np.log(u_high) - np.log(u_low)) /\n1510                (wind_fit_high - wind_fit_low))\n1511         a = - (np.log(u_low) + b * wind_fit_low)\n1512 \n1513         return dict(a=a, b=b)\n1514 \n[end of pvlib/temperature.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/snow.py\n+++ b/pvlib/snow.py\n@@ -293,14 +293,20 @@\n     lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\n     gamma = (\n         slant_height\n         * effective_snow_weighted_m\n         * cosd(surface_tilt)\n-        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\n+        / np.maximum(lower_edge_height_clipped**2 - effective_snow_weighted_m**2, 0.01)\n         * 2\n         * tand(angle_of_repose)\n     )\n \n     ground_interference_term = 1 - C2 * np.exp(-gamma)\n \n     relative_humidity_fraction = relative_humidity / 100\n     temp_air_kelvin = temp_air + 273.15\n     effective_snow_weighted_in = effective_snow_weighted / 2.54\n     poa_global_kWh = poa_global / 1000\n```\n\nThis patch modifies the calculation of `gamma` in the `loss_townsend` function to ensure that the denominator does not become negative, thus preventing `ground_interference_term` from being negative. It clips the squared difference to a minimum value of 0.01 as recommended.", "model_patch": "--- a/pvlib/snow.py\n+++ b/pvlib/snow.py\n@@ -293,14 +293,20 @@\n     lower_edge_height_clipped = np.maximum(lower_edge_height, 0.01)\n     gamma = (\n         slant_height\n         * effective_snow_weighted_m\n         * cosd(surface_tilt)\n-        / (lower_edge_height_clipped**2 - effective_snow_weighted_m**2)\n+        / np.maximum(lower_edge_height_clipped**2 - effective_snow_weighted_m**2, 0.01)\n         * 2\n         * tand(angle_of_repose)\n     )\n \n     ground_interference_term = 1 - C2 * np.exp(-gamma)\n \n     relative_humidity_fraction = relative_humidity / 100\n     temp_air_kelvin = temp_air + 273.15\n     effective_snow_weighted_in = effective_snow_weighted / 2.54\n     poa_global_kWh = poa_global / 1000\n"}
{"instance_id": "pvlib__pvlib-python-763", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nAdd recombination current params to all bishop88 functions\nThe changes made in #163 incorporate recombination current into the `bishop88()` function.  Functions that build on the `bishop88()` function should likewise accept these parameters.\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 [![TravisCI](https://travis-ci.org/pvlib/pvlib-python.svg?branch=master)](https://travis-ci.org/pvlib/pvlib-python)\n4 [![Coverage Status](https://img.shields.io/coveralls/pvlib/pvlib-python.svg)](https://coveralls.io/r/pvlib/pvlib-python)\n5 [![codecov](https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg)](https://codecov.io/gh/pvlib/pvlib-python)\n6 [![Documentation Status](https://readthedocs.org/projects/pvlib-python/badge/?version=latest)](http://pvlib-python.readthedocs.org/en/latest/)\n7 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.2554311.svg)](https://doi.org/10.5281/zenodo.2554311)\n8 [![status](http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg)](http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1)\n9 [![Code Quality: Python](https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pvlib/pvlib-python/context:python)\n10 [![Total Alerts](https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/pvlib/pvlib-python/alerts)\n11 \n12 \n13 pvlib python is a community supported tool that provides a set of\n14 functions and classes for simulating the performance of photovoltaic\n15 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n16 toolbox developed at Sandia National Laboratories and it implements many\n17 of the models and methods developed at the Labs. More information on\n18 Sandia Labs PV performance modeling programs can be found at\n19 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n20 but operate independently of it.\n21 \n22 \n23 Documentation\n24 =============\n25 \n26 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/latest/).\n27 \n28 \n29 Installation\n30 ============\n31 \n32 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n33 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/latest/installation.html) of the documentation for complete instructions.\n34 \n35 pvlib-python is compatible with Python 3.5 and above.\n36 \n37 **Python 2.7 support ended on June 1, 2019, with pvlib-python 0.6.3.**\n38 \n39 \n40 Contributing\n41 ============\n42 \n43 We need your help to make pvlib-python a great tool!\n44 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/latest/contributing.html) for more on how you can contribute.\n45 The long-term success of pvlib-python requires substantial community support.\n46 \n47 \n48 License\n49 =======\n50 \n51 BSD 3-clause\n52 \n53 \n54 Getting support\n55 ===============\n56 \n57 pvlib usage questions can be asked on\n58 [Stack Overflow](http://stackoverflow.com) and tagged with\n59 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n60 \n61 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n62 is used for discussing various topics of interest to the pvlib-python\n63 community. We also make new version announcements on the google group.\n64 \n65 If you suspect that you may have discovered a bug or if you'd like to\n66 change something about pvlib, then please make an issue on our\n67 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n68 \n69 \n70 Citing\n71 ======\n72 \n73 If you use pvlib-python in a published work, please cite:\n74 \n75   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n76   \"pvlib python: a python package for modeling solar energy systems.\"\n77   Journal of Open Source Software, 3(29), 884, (2018).\n78   https://doi.org/10.21105/joss.00884\n79 \n80 Please also cite the DOI corresponding to the specific version of\n81 pvlib-python that you used. pvlib-python DOIs are listed at\n82 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n83 \n84 NumFOCUS\n85 ========\n86 \n87 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n88 \n89 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n90 \n[end of README.md]\n[start of docs/sphinx/source/conf.py]\n1 # -*- coding: utf-8 -*-\n2 #\n3 # PVLIB_Python documentation build configuration file, created by\n4 # sphinx-quickstart on Fri Nov  7 15:56:33 2014.\n5 #\n6 # This file is execfile()d with the current directory set to its\n7 # containing dir.\n8 #\n9 # Note that not all possible configuration values are present in this\n10 # autogenerated file.\n11 #\n12 # All configuration values have a default; values that are commented out\n13 # serve to show the default.\n14 \n15 import sys\n16 import os\n17 \n18 # Mock modules so RTD works\n19 from unittest.mock import MagicMock\n20 \n21 \n22 class Mock(MagicMock):\n23     @classmethod\n24     def __getattr__(cls, name):\n25         return Mock()\n26 \n27 MOCK_MODULES = []\n28 sys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n29 \n30 import pandas as pd\n31 pd.show_versions()\n32 \n33 # If extensions (or modules to document with autodoc) are in another directory,\n34 # add these directories to sys.path here. If the directory is relative to the\n35 # documentation root, use os.path.abspath to make it absolute, like shown here.\n36 sys.path.insert(0, os.path.abspath('../sphinxext'))\n37 sys.path.insert(0, os.path.abspath('../../../'))\n38 \n39 # -- General configuration ------------------------------------------------\n40 \n41 # use napoleon in lieu of numpydoc 2019-04-23\n42 \n43 # If your documentation needs a minimal Sphinx version, state it here.\n44 #needs_sphinx = '1.0'\n45 \n46 # Add any Sphinx extension module names here, as strings. They can be\n47 # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n48 # ones.\n49 extensions = [\n50     'sphinx.ext.autodoc',\n51     'sphinx.ext.mathjax',\n52     'sphinx.ext.viewcode',\n53     'sphinx.ext.intersphinx',\n54     'sphinx.ext.extlinks',\n55     'sphinx.ext.napoleon',\n56     'sphinx.ext.autosummary',\n57     'IPython.sphinxext.ipython_directive',\n58     'IPython.sphinxext.ipython_console_highlighting'\n59 ]\n60 \n61 napoleon_use_rtype = False  # group rtype on same line together with return\n62 \n63 # Add any paths that contain templates here, relative to this directory.\n64 templates_path = ['_templates']\n65 \n66 # The suffix of source filenames.\n67 source_suffix = '.rst'\n68 \n69 # The encoding of source files.\n70 #source_encoding = 'utf-8-sig'\n71 \n72 # The master toctree document.\n73 master_doc = 'index'\n74 \n75 # General information about the project.\n76 project = u'pvlib-python'\n77 copyright = u'2015, Sandia National Labs, Rob Andrews, University of Arizona, github contributors'\n78 \n79 # The version info for the project you're documenting, acts as replacement for\n80 # |version| and |release|, also used in various other places throughout the\n81 # built documents.\n82 \n83 import pvlib\n84 \n85 # The short X.Y version.\n86 version = '%s' % (pvlib.__version__)\n87 # The full version, including alpha/beta/rc tags.\n88 release = version\n89 \n90 # The language for content autogenerated by Sphinx. Refer to documentation\n91 # for a list of supported languages.\n92 #language = None\n93 \n94 # There are two options for replacing |today|: either, you set today to some\n95 # non-false value, then it is used:\n96 #today = ''\n97 # Else, today_fmt is used as the format for a strftime call.\n98 #today_fmt = '%B %d, %Y'\n99 \n100 # List of patterns, relative to source directory, that match files and\n101 # directories to ignore when looking for source files.\n102 exclude_patterns = ['whatsnew/*', '**.ipynb_checkpoints']\n103 \n104 # The reST default role (used for this markup: `text`) to use for all\n105 # documents.\n106 #default_role = None\n107 \n108 # If true, '()' will be appended to :func: etc. cross-reference text.\n109 #add_function_parentheses = True\n110 \n111 # If true, the current module name will be prepended to all description\n112 # unit titles (such as .. function::).\n113 #add_module_names = True\n114 \n115 # If true, sectionauthor and moduleauthor directives will be shown in the\n116 # output. They are ignored by default.\n117 #show_authors = False\n118 \n119 # The name of the Pygments (syntax highlighting) style to use.\n120 pygments_style = 'sphinx'\n121 \n122 # A list of ignored prefixes for module index sorting.\n123 #modindex_common_prefix = []\n124 \n125 # If true, keep warnings as \"system message\" paragraphs in the built documents.\n126 #keep_warnings = False\n127 \n128 autosummary_generate = True\n129 \n130 # -- Options for HTML output ----------------------------------------------\n131 \n132 # The theme to use for HTML and HTML Help pages.  See the documentation for\n133 # a list of builtin themes.\n134 # on_rtd is whether we are on readthedocs.org\n135 on_rtd = os.environ.get('READTHEDOCS', None) == 'True'\n136 \n137 if not on_rtd:  # only import and set the theme if we're building docs locally\n138     import sphinx_rtd_theme\n139     html_theme = 'sphinx_rtd_theme'\n140     html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n141 else:\n142     html_theme = 'default'\n143 \n144 # Theme options are theme-specific and customize the look and feel of a theme\n145 # further.  For a list of options available for each theme, see the\n146 # documentation.\n147 #html_theme_options = {}\n148 \n149 # Add any paths that contain custom themes here, relative to this directory.\n150 #html_theme_path = []\n151 \n152 # The name for this set of Sphinx documents.  If None, it defaults to\n153 # \"<project> v<release> documentation\".\n154 #html_title = None\n155 \n156 # A shorter title for the navigation bar.  Default is the same as html_title.\n157 #html_short_title = None\n158 \n159 # The name of an image file (relative to this directory) to place at the top\n160 # of the sidebar.\n161 #html_logo = None\n162 \n163 # The name of an image file (within the static path) to use as favicon of the\n164 # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n165 # pixels large.\n166 #html_favicon = None\n167 \n168 # Add any paths that contain custom static files (such as style sheets) here,\n169 # relative to this directory. They are copied after the builtin static files,\n170 # so a file named \"default.css\" will overwrite the builtin \"default.css\".\n171 html_static_path = ['_static']\n172 \n173 # Add any extra paths that contain custom files (such as robots.txt or\n174 # .htaccess) here, relative to this directory. These files are copied\n175 # directly to the root of the documentation.\n176 #html_extra_path = []\n177 \n178 # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n179 # using the given strftime format.\n180 #html_last_updated_fmt = '%b %d, %Y'\n181 \n182 # If true, SmartyPants will be used to convert quotes and dashes to\n183 # typographically correct entities.\n184 #html_use_smartypants = True\n185 \n186 # Custom sidebar templates, maps document names to template names.\n187 #html_sidebars = {}\n188 \n189 # Additional templates that should be rendered to pages, maps page names to\n190 # template names.\n191 #html_additional_pages = {}\n192 \n193 # If false, no module index is generated.\n194 #html_domain_indices = True\n195 \n196 # If false, no index is generated.\n197 #html_use_index = True\n198 \n199 # If true, the index is split into individual pages for each letter.\n200 #html_split_index = False\n201 \n202 # If true, links to the reST sources are added to the pages.\n203 #html_show_sourcelink = True\n204 \n205 # If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n206 #html_show_sphinx = True\n207 \n208 # If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n209 html_show_copyright = False\n210 \n211 # If true, an OpenSearch description file will be output, and all pages will\n212 # contain a <link> tag referring to it.  The value of this option must be the\n213 # base URL from which the finished HTML is served.\n214 #html_use_opensearch = ''\n215 \n216 # This is the file name suffix for HTML files (e.g. \".xhtml\").\n217 #html_file_suffix = None\n218 \n219 # Output file base name for HTML help builder.\n220 htmlhelp_basename = 'PVLIB_Pythondoc'\n221 \n222 # A workaround for the responsive tables always having annoying scrollbars.\n223 def setup(app):\n224     app.add_stylesheet(\"no_scrollbars.css\")\n225 \n226 \n227 # -- Options for LaTeX output ---------------------------------------------\n228 \n229 latex_elements = {\n230 # The paper size ('letterpaper' or 'a4paper').\n231 #'papersize': 'letterpaper',\n232 \n233 # The font size ('10pt', '11pt' or '12pt').\n234 #'pointsize': '10pt',\n235 \n236 # Additional stuff for the LaTeX preamble.\n237 #'preamble': '',\n238 }\n239 \n240 # Grouping the document tree into LaTeX files. List of tuples\n241 # (source start file, target name, title,\n242 #  author, documentclass [howto, manual, or own class]).\n243 latex_documents = [\n244   ('index', 'PVLIB_Python.tex', u'PVLIB\\\\_Python Documentation',\n245    u'Sandia National Labs, Rob Andrews, University of Arizona, github contributors', 'manual'),\n246 ]\n247 \n248 # The name of an image file (relative to this directory) to place at the top of\n249 # the title page.\n250 #latex_logo = None\n251 \n252 # For \"manual\" documents, if this is true, then toplevel headings are parts,\n253 # not chapters.\n254 #latex_use_parts = False\n255 \n256 # If true, show page references after internal links.\n257 #latex_show_pagerefs = False\n258 \n259 # If true, show URL addresses after external links.\n260 #latex_show_urls = False\n261 \n262 # Documents to append as an appendix to all manuals.\n263 #latex_appendices = []\n264 \n265 # If false, no module index is generated.\n266 #latex_domain_indices = True\n267 \n268 # extlinks alias\n269 extlinks = {'issue': ('https://github.com/pvlib/pvlib-python/issues/%s',\n270                       'GH'),\n271             'wiki': ('https://github.com/pvlib/pvlib-python/wiki/%s',\n272                      'wiki '),\n273             'doi': ('http://dx.doi.org/%s', 'DOI: '),\n274             'ghuser': ('https://github.com/%s', '@')}\n275 \n276 # -- Options for manual page output ---------------------------------------\n277 \n278 # One entry per manual page. List of tuples\n279 # (source start file, name, description, authors, manual section).\n280 man_pages = [\n281     ('index', 'pvlib_python', u'PVLIB_Python Documentation',\n282      [u'Sandia National Labs, Rob Andrews, University of Arizona, github contributors'], 1)\n283 ]\n284 \n285 # If true, show URL addresses after external links.\n286 #man_show_urls = False\n287 \n288 \n289 # -- Options for Texinfo output -------------------------------------------\n290 \n291 # Grouping the document tree into Texinfo files. List of tuples\n292 # (source start file, target name, title, author,\n293 #  dir menu entry, description, category)\n294 texinfo_documents = [\n295   ('index', 'PVLIB_Python', u'PVLIB_Python Documentation',\n296    u'Sandia National Labs, Rob Andrews, University of Arizona, github contributors', 'PVLIB_Python', 'One line description of project.',\n297    'Miscellaneous'),\n298 ]\n299 \n300 # Documents to append as an appendix to all manuals.\n301 #texinfo_appendices = []\n302 \n303 # If false, no module index is generated.\n304 #texinfo_domain_indices = True\n305 \n306 # How to display URL addresses: 'footnote', 'no', or 'inline'.\n307 #texinfo_show_urls = 'footnote'\n308 \n309 # If true, do not generate a @detailmenu in the \"Top\" node's menu.\n310 #texinfo_no_detailmenu = False\n311 \n312 # Example configuration for intersphinx: refer to the Python standard library.\n313 intersphinx_mapping = {\n314     'python': ('https://docs.python.org/3.7/', None),\n315     'pandas': ('https://pandas.pydata.org/pandas-docs/stable/', None),\n316     'numpy': ('https://docs.scipy.org/doc/numpy/', None),\n317 }\n318 \n319 nbsphinx_allow_errors = True\n320 \n321 ipython_warning_is_error = False\n322 \n[end of docs/sphinx/source/conf.py]\n[start of pvlib/_deprecation.py]\n1 \"\"\"Matplotlib license for the deprecation module.\n2 \n3 License agreement for matplotlib versions 1.3.0 and later\n4 =========================================================\n5 \n6 1. This LICENSE AGREEMENT is between the Matplotlib Development Team\n7 (\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and\n8 otherwise using matplotlib software in source or binary form and its\n9 associated documentation.\n10 \n11 2. Subject to the terms and conditions of this License Agreement, MDT\n12 hereby grants Licensee a nonexclusive, royalty-free, world-wide license\n13 to reproduce, analyze, test, perform and/or display publicly, prepare\n14 derivative works, distribute, and otherwise use matplotlib\n15 alone or in any derivative version, provided, however, that MDT's\n16 License Agreement and MDT's notice of copyright, i.e., \"Copyright (c)\n17 2012- Matplotlib Development Team; All Rights Reserved\" are retained in\n18 matplotlib  alone or in any derivative version prepared by\n19 Licensee.\n20 \n21 3. In the event Licensee prepares a derivative work that is based on or\n22 incorporates matplotlib or any part thereof, and wants to\n23 make the derivative work available to others as provided herein, then\n24 Licensee hereby agrees to include in any such work a brief summary of\n25 the changes made to matplotlib .\n26 \n27 4. MDT is making matplotlib available to Licensee on an \"AS\n28 IS\" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n29 IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND\n30 DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n31 FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\n32 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n33 \n34 5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n35  FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\n36 LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\n37 MATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\n38 THE POSSIBILITY THEREOF.\n39 \n40 6. This License Agreement will automatically terminate upon a material\n41 breach of its terms and conditions.\n42 \n43 7. Nothing in this License Agreement shall be deemed to create any\n44 relationship of agency, partnership, or joint venture between MDT and\n45 Licensee.  This License Agreement does not grant permission to use MDT\n46 trademarks or trade name in a trademark sense to endorse or promote\n47 products or services of Licensee, or any third party.\n48 \n49 8. By copying, installing or otherwise using matplotlib ,\n50 Licensee agrees to be bound by the terms and conditions of this License\n51 Agreement.\n52 \n53 License agreement for matplotlib versions prior to 1.3.0\n54 ========================================================\n55 \n56 1. This LICENSE AGREEMENT is between John D. Hunter (\"JDH\"), and the\n57 Individual or Organization (\"Licensee\") accessing and otherwise using\n58 matplotlib software in source or binary form and its associated\n59 documentation.\n60 \n61 2. Subject to the terms and conditions of this License Agreement, JDH\n62 hereby grants Licensee a nonexclusive, royalty-free, world-wide license\n63 to reproduce, analyze, test, perform and/or display publicly, prepare\n64 derivative works, distribute, and otherwise use matplotlib\n65 alone or in any derivative version, provided, however, that JDH's\n66 License Agreement and JDH's notice of copyright, i.e., \"Copyright (c)\n67 2002-2011 John D. Hunter; All Rights Reserved\" are retained in\n68 matplotlib  alone or in any derivative version prepared by\n69 Licensee.\n70 \n71 3. In the event Licensee prepares a derivative work that is based on or\n72 incorporates matplotlib  or any part thereof, and wants to\n73 make the derivative work available to others as provided herein, then\n74 Licensee hereby agrees to include in any such work a brief summary of\n75 the changes made to matplotlib.\n76 \n77 4. JDH is making matplotlib  available to Licensee on an \"AS\n78 IS\" basis.  JDH MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\n79 IMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, JDH MAKES NO AND\n80 DISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\n81 FOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\n82 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n83 \n84 5. JDH SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n85  FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\n86 LOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\n87 MATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\n88 THE POSSIBILITY THEREOF.\n89 \n90 6. This License Agreement will automatically terminate upon a material\n91 breach of its terms and conditions.\n92 \n93 7. Nothing in this License Agreement shall be deemed to create any\n94 relationship of agency, partnership, or joint venture between JDH and\n95 Licensee.  This License Agreement does not grant permission to use JDH\n96 trademarks or trade name in a trademark sense to endorse or promote\n97 products or services of Licensee, or any third party.\n98 \n99 8. By copying, installing or otherwise using matplotlib,\n100 Licensee agrees to be bound by the terms and conditions of this License\n101 Agreement.\n102 \"\"\"\n103 \n104 # modified from Matplotlib b97cd2d (post 2.2.2) in the following ways:\n105 # 1. use module-level _projectName = 'pvlib' and\n106 #    _projectWarning = 'pvlibDeprecationWarning' in place of MPL specific\n107 #    string/Class.\n108 # 2. remove keyword only argument requirement for removal\n109 # 3. remove deprecated obj_type from deprecated function\n110 # 4. if removal is empty, say 'soon' instead of assuming two minor releases\n111 #    later.\n112 \n113 import functools\n114 import textwrap\n115 import warnings\n116 \n117 \n118 class pvlibDeprecationWarning(UserWarning):\n119     \"\"\"A class for issuing deprecation warnings for pvlib users.\n120 \n121     In light of the fact that Python builtin DeprecationWarnings are ignored\n122     by default as of Python 2.7 (see link below), this class was put in to\n123     allow for the signaling of deprecation, but via UserWarnings which are not\n124     ignored by default.\n125 \n126     https://docs.python.org/dev/whatsnew/2.7.html#the-future-for-python-2-x\n127     \"\"\"\n128 \n129     pass\n130 \n131 \n132 # make it easier for others to copy paste this code into their projects\n133 _projectName = 'pvlib'\n134 _projectWarning = pvlibDeprecationWarning\n135 \n136 \n137 def _generate_deprecation_message(\n138         since, message='', name='', alternative='', pending=False,\n139         obj_type='attribute', addendum='', removal=''):\n140 \n141     if removal == \"\":\n142         removal = \"soon\"\n143     elif removal:\n144         if pending:\n145             raise ValueError(\n146                 \"A pending deprecation cannot have a scheduled removal\")\n147         removal = \"in {}\".format(removal)\n148 \n149     if not message:\n150         message = (\n151             \"The %(name)s %(obj_type)s\"\n152             + (\" will be deprecated in a future version\"\n153                if pending else\n154                (\" was deprecated in %(projectName)s %(since)s\"\n155                 + (\" and will be removed %(removal)s\"\n156                    if removal else\n157                    \"\")))\n158             + \".\"\n159             + (\" Use %(alternative)s instead.\" if alternative else \"\")\n160             + (\" %(addendum)s\" if addendum else \"\"))\n161 \n162     return message % dict(\n163         func=name, name=name, obj_type=obj_type, since=since, removal=removal,\n164         alternative=alternative, addendum=addendum, projectName=_projectName)\n165 \n166 \n167 def warn_deprecated(\n168         since, message='', name='', alternative='', pending=False,\n169         obj_type='attribute', addendum='', removal=''):\n170     \"\"\"\n171     Used to display deprecation in a standard way.\n172     Parameters\n173     ----------\n174     since : str\n175         The release at which this API became deprecated.\n176     message : str, optional\n177         Override the default deprecation message.  The format\n178         specifier `%(name)s` may be used for the name of the function,\n179         and `%(alternative)s` may be used in the deprecation message\n180         to insert the name of an alternative to the deprecated\n181         function.  `%(obj_type)s` may be used to insert a friendly name\n182         for the type of object being deprecated.\n183     name : str, optional\n184         The name of the deprecated object.\n185     alternative : str, optional\n186         An alternative API that the user may use in place of the deprecated\n187         API.  The deprecation warning will tell the user about this alternative\n188         if provided.\n189     pending : bool, optional\n190         If True, uses a PendingDeprecationWarning instead of a\n191         DeprecationWarning.  Cannot be used together with *removal*.\n192     removal : str, optional\n193         The expected removal version.  With the default (an empty string), a\n194         removal version is automatically computed from *since*.  Set to other\n195         Falsy values to not schedule a removal date.  Cannot be used together\n196         with *pending*.\n197     obj_type : str, optional\n198         The object type being deprecated.\n199     addendum : str, optional\n200         Additional text appended directly to the final message.\n201     Examples\n202     --------\n203         Basic example::\n204             # To warn of the deprecation of \"matplotlib.name_of_module\"\n205             warn_deprecated('1.4.0', name='matplotlib.name_of_module',\n206                             obj_type='module')\n207     \"\"\"\n208     message = '\\n' + _generate_deprecation_message(\n209         since, message, name, alternative, pending, obj_type, addendum,\n210         removal=removal)\n211     category = (PendingDeprecationWarning if pending\n212                 else _projectWarning)\n213     warnings.warn(message, category, stacklevel=2)\n214 \n215 \n216 def deprecated(since, message='', name='', alternative='', pending=False,\n217                addendum='', removal=''):\n218     \"\"\"\n219     Decorator to mark a function or a class as deprecated.\n220     Parameters\n221     ----------\n222     since : str\n223         The release at which this API became deprecated.  This is\n224         required.\n225     message : str, optional\n226         Override the default deprecation message.  The format\n227         specifier `%(name)s` may be used for the name of the object,\n228         and `%(alternative)s` may be used in the deprecation message\n229         to insert the name of an alternative to the deprecated\n230         object.\n231     name : str, optional\n232         The name of the deprecated object; if not provided the name\n233         is automatically determined from the passed in object,\n234         though this is useful in the case of renamed functions, where\n235         the new function is just assigned to the name of the\n236         deprecated function.  For example::\n237             def new_function():\n238                 ...\n239             oldFunction = new_function\n240     alternative : str, optional\n241         An alternative API that the user may use in place of the deprecated\n242         API.  The deprecation warning will tell the user about this alternative\n243         if provided.\n244     pending : bool, optional\n245         If True, uses a PendingDeprecationWarning instead of a\n246         DeprecationWarning.  Cannot be used together with *removal*.\n247     removal : str, optional\n248         The expected removal version.  With the default (an empty string), a\n249         removal version is automatically computed from *since*.  Set to other\n250         Falsy values to not schedule a removal date.  Cannot be used together\n251         with *pending*.\n252     addendum : str, optional\n253         Additional text appended directly to the final message.\n254     Examples\n255     --------\n256         Basic example::\n257             @deprecated('1.4.0')\n258             def the_function_to_deprecate():\n259                 pass\n260     \"\"\"\n261 \n262     def deprecate(obj, message=message, name=name, alternative=alternative,\n263                   pending=pending, addendum=addendum):\n264 \n265         if not name:\n266             name = obj.__name__\n267 \n268         if isinstance(obj, type):\n269             obj_type = \"class\"\n270             old_doc = obj.__doc__\n271             func = obj.__init__\n272 \n273             def finalize(wrapper, new_doc):\n274                 obj.__doc__ = new_doc\n275                 obj.__init__ = wrapper\n276                 return obj\n277         else:\n278             obj_type = \"function\"\n279             if isinstance(obj, classmethod):\n280                 func = obj.__func__\n281                 old_doc = func.__doc__\n282 \n283                 def finalize(wrapper, new_doc):\n284                     wrapper = functools.wraps(func)(wrapper)\n285                     wrapper.__doc__ = new_doc\n286                     return classmethod(wrapper)\n287             else:\n288                 func = obj\n289                 old_doc = func.__doc__\n290 \n291                 def finalize(wrapper, new_doc):\n292                     wrapper = functools.wraps(func)(wrapper)\n293                     wrapper.__doc__ = new_doc\n294                     return wrapper\n295 \n296         message = _generate_deprecation_message(\n297             since, message, name, alternative, pending, obj_type, addendum,\n298             removal=removal)\n299         category = (PendingDeprecationWarning if pending\n300                     else _projectWarning)\n301 \n302         def wrapper(*args, **kwargs):\n303             warnings.warn(message, category, stacklevel=2)\n304             return func(*args, **kwargs)\n305 \n306         old_doc = textwrap.dedent(old_doc or '').strip('\\n')\n307         message = message.strip()\n308         new_doc = (('\\n.. deprecated:: %(since)s'\n309                     '\\n    %(message)s\\n\\n' %\n310                     {'since': since, 'message': message}) + old_doc)\n311         if not old_doc:\n312             # This is to prevent a spurious 'unexected unindent' warning from\n313             # docutils when the original docstring was blank.\n314             new_doc += r'\\ '\n315 \n316         return finalize(wrapper, new_doc)\n317 \n318     return deprecate\n319 \n[end of pvlib/_deprecation.py]\n[start of pvlib/atmosphere.py]\n1 \"\"\"\n2 The ``atmosphere`` module contains methods to calculate relative and\n3 absolute airmass and to determine pressure from altitude or vice versa.\n4 \"\"\"\n5 \n6 from warnings import warn\n7 \n8 import numpy as np\n9 import pandas as pd\n10 \n11 from pvlib._deprecation import deprecated\n12 \n13 APPARENT_ZENITH_MODELS = ('simple', 'kasten1966', 'kastenyoung1989',\n14                           'gueymard1993', 'pickering2002')\n15 TRUE_ZENITH_MODELS = ('youngirvine1967', 'young1994')\n16 AIRMASS_MODELS = APPARENT_ZENITH_MODELS + TRUE_ZENITH_MODELS\n17 \n18 \n19 def pres2alt(pressure):\n20     '''\n21     Determine altitude from site pressure.\n22 \n23     Parameters\n24     ----------\n25     pressure : numeric\n26         Atmospheric pressure (Pascals)\n27 \n28     Returns\n29     -------\n30     altitude : numeric\n31         Altitude in meters above sea level\n32 \n33     Notes\n34     ------\n35     The following assumptions are made\n36 \n37     ============================   ================\n38     Parameter                      Value\n39     ============================   ================\n40     Base pressure                  101325 Pa\n41     Temperature at zero altitude   288.15 K\n42     Gravitational acceleration     9.80665 m/s^2\n43     Lapse rate                     -6.5E-3 K/m\n44     Gas constant for air           287.053 J/(kgK)\n45     Relative Humidity              0%\n46     ============================   ================\n47 \n48     References\n49     -----------\n50     [1] \"A Quick Derivation relating altitude to air pressure\" from\n51     Portland State Aerospace Society, Version 1.03, 12/22/2004.\n52     '''\n53 \n54     alt = 44331.5 - 4946.62 * pressure ** (0.190263)\n55 \n56     return alt\n57 \n58 \n59 def alt2pres(altitude):\n60     '''\n61     Determine site pressure from altitude.\n62 \n63     Parameters\n64     ----------\n65     altitude : numeric\n66         Altitude in meters above sea level\n67 \n68     Returns\n69     -------\n70     pressure : numeric\n71         Atmospheric pressure (Pascals)\n72 \n73     Notes\n74     ------\n75     The following assumptions are made\n76 \n77     ============================   ================\n78     Parameter                      Value\n79     ============================   ================\n80     Base pressure                  101325 Pa\n81     Temperature at zero altitude   288.15 K\n82     Gravitational acceleration     9.80665 m/s^2\n83     Lapse rate                     -6.5E-3 K/m\n84     Gas constant for air           287.053 J/(kgK)\n85     Relative Humidity              0%\n86     ============================   ================\n87 \n88     References\n89     -----------\n90     [1] \"A Quick Derivation relating altitude to air pressure\" from\n91     Portland State Aerospace Society, Version 1.03, 12/22/2004.\n92     '''\n93 \n94     press = 100 * ((44331.514 - altitude) / 11880.516) ** (1 / 0.1902632)\n95 \n96     return press\n97 \n98 \n99 def get_absolute_airmass(airmass_relative, pressure=101325.):\n100     '''\n101     Determine absolute (pressure corrected) airmass from relative\n102     airmass and pressure\n103 \n104     Gives the airmass for locations not at sea-level (i.e. not at\n105     standard pressure). The input argument \"AMrelative\" is the relative\n106     airmass. The input argument \"pressure\" is the pressure (in Pascals)\n107     at the location of interest and must be greater than 0. The\n108     calculation for absolute airmass is\n109 \n110     .. math::\n111         absolute airmass = (relative airmass)*pressure/101325\n112 \n113     Parameters\n114     ----------\n115     airmass_relative : numeric\n116         The airmass at sea-level.\n117 \n118     pressure : numeric, default 101325\n119         The site pressure in Pascal.\n120 \n121     Returns\n122     -------\n123     airmass_absolute : numeric\n124         Absolute (pressure corrected) airmass\n125 \n126     References\n127     ----------\n128     [1] C. Gueymard, \"Critical analysis and performance assessment of\n129     clear sky solar irradiance models using theoretical and measured\n130     data,\" Solar Energy, vol. 51, pp. 121-138, 1993.\n131     '''\n132 \n133     airmass_absolute = airmass_relative * pressure / 101325.\n134 \n135     return airmass_absolute\n136 \n137 \n138 absoluteairmass = deprecated('0.6', alternative='get_absolute_airmass',\n139                              name='absoluteairmass', removal='0.7')(\n140                              get_absolute_airmass)\n141 \n142 \n143 def get_relative_airmass(zenith, model='kastenyoung1989'):\n144     '''\n145     Gives the relative (not pressure-corrected) airmass.\n146 \n147     Gives the airmass at sea-level when given a sun zenith angle (in\n148     degrees). The ``model`` variable allows selection of different\n149     airmass models (described below). If ``model`` is not included or is\n150     not valid, the default model is 'kastenyoung1989'.\n151 \n152     Parameters\n153     ----------\n154     zenith : numeric\n155         Zenith angle of the sun in degrees. Note that some models use\n156         the apparent (refraction corrected) zenith angle, and some\n157         models use the true (not refraction-corrected) zenith angle. See\n158         model descriptions to determine which type of zenith angle is\n159         required. Apparent zenith angles must be calculated at sea level.\n160 \n161     model : string, default 'kastenyoung1989'\n162         Available models include the following:\n163 \n164         * 'simple' - secant(apparent zenith angle) -\n165           Note that this gives -inf at zenith=90\n166         * 'kasten1966' - See reference [1] -\n167           requires apparent sun zenith\n168         * 'youngirvine1967' - See reference [2] -\n169           requires true sun zenith\n170         * 'kastenyoung1989' - See reference [3] -\n171           requires apparent sun zenith\n172         * 'gueymard1993' - See reference [4] -\n173           requires apparent sun zenith\n174         * 'young1994' - See reference [5] -\n175           requries true sun zenith\n176         * 'pickering2002' - See reference [6] -\n177           requires apparent sun zenith\n178 \n179     Returns\n180     -------\n181     airmass_relative : numeric\n182         Relative airmass at sea level. Will return NaN values for any\n183         zenith angle greater than 90 degrees.\n184 \n185     References\n186     ----------\n187     [1] Fritz Kasten. \"A New Table and Approximation Formula for the\n188     Relative Optical Air Mass\". Technical Report 136, Hanover, N.H.:\n189     U.S. Army Material Command, CRREL.\n190 \n191     [2] A. T. Young and W. M. Irvine, \"Multicolor Photoelectric\n192     Photometry of the Brighter Planets,\" The Astronomical Journal, vol.\n193     72, pp. 945-950, 1967.\n194 \n195     [3] Fritz Kasten and Andrew Young. \"Revised optical air mass tables\n196     and approximation formula\". Applied Optics 28:4735-4738\n197 \n198     [4] C. Gueymard, \"Critical analysis and performance assessment of\n199     clear sky solar irradiance models using theoretical and measured\n200     data,\" Solar Energy, vol. 51, pp. 121-138, 1993.\n201 \n202     [5] A. T. Young, \"AIR-MASS AND REFRACTION,\" Applied Optics, vol. 33,\n203     pp. 1108-1110, Feb 1994.\n204 \n205     [6] Keith A. Pickering. \"The Ancient Star Catalog\". DIO 12:1, 20,\n206 \n207     [7] Matthew J. Reno, Clifford W. Hansen and Joshua S. Stein, \"Global\n208     Horizontal Irradiance Clear Sky Models: Implementation and Analysis\"\n209     Sandia Report, (2012).\n210     '''\n211 \n212     # set zenith values greater than 90 to nans\n213     z = np.where(zenith > 90, np.nan, zenith)\n214     zenith_rad = np.radians(z)\n215 \n216     model = model.lower()\n217 \n218     if 'kastenyoung1989' == model:\n219         am = (1.0 / (np.cos(zenith_rad) +\n220               0.50572*(((6.07995 + (90 - z)) ** - 1.6364))))\n221     elif 'kasten1966' == model:\n222         am = 1.0 / (np.cos(zenith_rad) + 0.15*((93.885 - z) ** - 1.253))\n223     elif 'simple' == model:\n224         am = 1.0 / np.cos(zenith_rad)\n225     elif 'pickering2002' == model:\n226         am = (1.0 / (np.sin(np.radians(90 - z +\n227               244.0 / (165 + 47.0 * (90 - z) ** 1.1)))))\n228     elif 'youngirvine1967' == model:\n229         sec_zen = 1.0 / np.cos(zenith_rad)\n230         am = sec_zen * (1 - 0.0012 * (sec_zen * sec_zen - 1))\n231     elif 'young1994' == model:\n232         am = ((1.002432*((np.cos(zenith_rad)) ** 2) +\n233               0.148386*(np.cos(zenith_rad)) + 0.0096467) /\n234               (np.cos(zenith_rad) ** 3 +\n235               0.149864*(np.cos(zenith_rad) ** 2) +\n236               0.0102963*(np.cos(zenith_rad)) + 0.000303978))\n237     elif 'gueymard1993' == model:\n238         am = (1.0 / (np.cos(zenith_rad) +\n239               0.00176759*(z)*((94.37515 - z) ** - 1.21563)))\n240     else:\n241         raise ValueError('%s is not a valid model for relativeairmass', model)\n242 \n243     if isinstance(zenith, pd.Series):\n244         am = pd.Series(am, index=zenith.index)\n245 \n246     return am\n247 \n248 \n249 relativeairmass = deprecated('0.6', alternative='get_relative_airmass',\n250                              name='relativeairmass', removal='0.7')(\n251                              get_relative_airmass)\n252 \n253 \n254 def gueymard94_pw(temp_air, relative_humidity):\n255     r\"\"\"\n256     Calculates precipitable water (cm) from ambient air temperature (C)\n257     and relatively humidity (%) using an empirical model. The\n258     accuracy of this method is approximately 20% for moderate PW (1-3\n259     cm) and less accurate otherwise.\n260 \n261     The model was developed by expanding Eq. 1 in [2]_:\n262 \n263     .. math::\n264 \n265            w = 0.1 H_v \\rho_v\n266 \n267     using Eq. 2 in [2]_\n268 \n269     .. math::\n270 \n271            \\rho_v = 216.7 R_H e_s /T\n272 \n273     :math:`H_v` is the apparant water vapor scale height (km). The\n274     expression for :math:`H_v` is Eq. 4 in [2]_:\n275 \n276     .. math::\n277 \n278            H_v = 0.4976 + 1.5265*T/273.15 + \\exp(13.6897*T/273.15 - 14.9188*(T/273.15)^3)\n279 \n280     :math:`\\rho_v` is the surface water vapor density (g/m^3). In the\n281     expression :math:`\\rho_v`, :math:`e_s` is the saturation water vapor\n282     pressure (millibar). The\n283     expression for :math:`e_s` is Eq. 1 in [3]_\n284 \n285     .. math::\n286 \n287           e_s = \\exp(22.330 - 49.140*(100/T) - 10.922*(100/T)^2 - 0.39015*T/100)\n288 \n289     Parameters\n290     ----------\n291     temp_air : numeric\n292         ambient air temperature at the surface (C)\n293     relative_humidity : numeric\n294         relative humidity at the surface (%)\n295 \n296     Returns\n297     -------\n298     pw : numeric\n299         precipitable water (cm)\n300 \n301     References\n302     ----------\n303     .. [1] W. M. Keogh and A. W. Blakers, Accurate Measurement, Using Natural\n304        Sunlight, of Silicon Solar Cells, Prog. in Photovoltaics: Res.\n305        and Appl. 2004, vol 12, pp. 1-19 (:doi:`10.1002/pip.517`)\n306 \n307     .. [2] C. Gueymard, Analysis of Monthly Average Atmospheric Precipitable\n308        Water and Turbidity in Canada and Northern United States,\n309        Solar Energy vol 53(1), pp. 57-71, 1994.\n310 \n311     .. [3] C. Gueymard, Assessment of the Accuracy and Computing Speed of\n312        simplified saturation vapor equations using a new reference\n313        dataset, J. of Applied Meteorology 1993, vol. 32(7), pp.\n314        1294-1300.\n315     \"\"\"\n316 \n317     T = temp_air + 273.15  # Convert to Kelvin                  # noqa: N806\n318     RH = relative_humidity                                      # noqa: N806\n319 \n320     theta = T / 273.15\n321 \n322     # Eq. 1 from Keogh and Blakers\n323     pw = (\n324         0.1 *\n325         (0.4976 + 1.5265*theta + np.exp(13.6897*theta - 14.9188*(theta)**3)) *\n326         (216.7*RH/(100*T)*np.exp(22.330 - 49.140*(100/T) -\n327          10.922*(100/T)**2 - 0.39015*T/100)))\n328 \n329     pw = np.maximum(pw, 0.1)\n330 \n331     return pw\n332 \n333 \n334 def first_solar_spectral_correction(pw, airmass_absolute, module_type=None,\n335                                     coefficients=None):\n336     r\"\"\"\n337     Spectral mismatch modifier based on precipitable water and absolute\n338     (pressure corrected) airmass.\n339 \n340     Estimates a spectral mismatch modifier M representing the effect on\n341     module short circuit current of variation in the spectral\n342     irradiance. M is estimated from absolute (pressure currected) air\n343     mass, AMa, and precipitable water, Pwat, using the following\n344     function:\n345 \n346     .. math::\n347 \n348         M = c_1 + c_2*AMa  + c_3*Pwat  + c_4*AMa^.5\n349             + c_5*Pwat^.5 + c_6*AMa/Pwat^.5\n350 \n351     Default coefficients are determined for several cell types with\n352     known quantum efficiency curves, by using the Simple Model of the\n353     Atmospheric Radiative Transfer of Sunshine (SMARTS) [1]_. Using\n354     SMARTS, spectrums are simulated with all combinations of AMa and\n355     Pwat where:\n356 \n357        * 0.5 cm <= Pwat <= 5 cm\n358        * 1.0 <= AMa <= 5.0\n359        * Spectral range is limited to that of CMP11 (280 nm to 2800 nm)\n360        * spectrum simulated on a plane normal to the sun\n361        * All other parameters fixed at G173 standard\n362 \n363     From these simulated spectra, M is calculated using the known\n364     quantum efficiency curves. Multiple linear regression is then\n365     applied to fit Eq. 1 to determine the coefficients for each module.\n366 \n367     Based on the PVLIB Matlab function ``pvl_FSspeccorr`` by Mitchell\n368     Lee and Alex Panchula, at First Solar, 2016 [2]_.\n369 \n370     Parameters\n371     ----------\n372     pw : array-like\n373         atmospheric precipitable water (cm).\n374 \n375     airmass_absolute : array-like\n376         absolute (pressure corrected) airmass.\n377 \n378     module_type : None or string, default None\n379         a string specifying a cell type. Can be lower or upper case\n380         letters. Admits values of 'cdte', 'monosi', 'xsi', 'multisi',\n381         'polysi'. If provided, this input selects coefficients for the\n382         following default modules:\n383 \n384             * 'cdte' - First Solar Series 4-2 CdTe modules.\n385             * 'monosi', 'xsi' - First Solar TetraSun modules.\n386             * 'multisi', 'polysi' - multi-crystalline silicon modules.\n387             * 'cigs' - anonymous copper indium gallium selenide PV module\n388             * 'asi' - anonymous amorphous silicon PV module\n389 \n390         The module used to calculate the spectral correction\n391         coefficients corresponds to the Mult-crystalline silicon\n392         Manufacturer 2 Model C from [3]_. Spectral Response (SR) of CIGS\n393         and a-Si modules used to derive coefficients can be found in [4]_\n394 \n395     coefficients : None or array-like, default None\n396         allows for entry of user defined spectral correction\n397         coefficients. Coefficients must be of length 6. Derivation of\n398         coefficients requires use of SMARTS and PV module quantum\n399         efficiency curve. Useful for modeling PV module types which are\n400         not included as defaults, or to fine tune the spectral\n401         correction to a particular mono-Si, multi-Si, or CdTe PV module.\n402         Note that the parameters for modules with very similar QE should\n403         be similar, in most cases limiting the need for module specific\n404         coefficients.\n405 \n406     Returns\n407     -------\n408     modifier: array-like\n409         spectral mismatch factor (unitless) which is can be multiplied\n410         with broadband irradiance reaching a module's cells to estimate\n411         effective irradiance, i.e., the irradiance that is converted to\n412         electrical current.\n413 \n414     References\n415     ----------\n416     .. [1] Gueymard, Christian. SMARTS2: a simple model of the atmospheric\n417        radiative transfer of sunshine: algorithms and performance\n418        assessment. Cocoa, FL: Florida Solar Energy Center, 1995.\n419     .. [2] Lee, Mitchell, and Panchula, Alex. \"Spectral Correction for\n420        Photovoltaic Module Performance Based on Air Mass and Precipitable\n421        Water.\" IEEE Photovoltaic Specialists Conference, Portland, 2016\n422     .. [3] Marion, William F., et al. User's Manual for Data for Validating\n423        Models for PV Module Performance. National Renewable Energy\n424        Laboratory, 2014. http://www.nrel.gov/docs/fy14osti/61610.pdf\n425     .. [4] Schweiger, M. and Hermann, W, Influence of Spectral Effects\n426         on Energy Yield of Different PV Modules: Comparison of Pwat and\n427         MMF Approach, TUV Rheinland Energy GmbH report 21237296.003,\n428         January 2017\n429     \"\"\"\n430 \n431     # --- Screen Input Data ---\n432 \n433     # *** Pwat ***\n434     # Replace Pwat Values below 0.1 cm with 0.1 cm to prevent model from\n435     # diverging\"\n436 \n437     if np.min(pw) < 0.1:\n438         pw = np.maximum(pw, 0.1)\n439         warn('Exceptionally low Pwat values replaced with 0.1 cm to prevent' +\n440              ' model divergence')\n441 \n442     # Warn user about Pwat data that is exceptionally high\n443     if np.max(pw) > 8:\n444         warn('Exceptionally high Pwat values. Check input data:' +\n445              ' model may diverge in this range')\n446 \n447     # *** AMa ***\n448     # Replace Extremely High AM with AM 10 to prevent model divergence\n449     # AM > 10 will only occur very close to sunset\n450     if np.max(airmass_absolute) > 10:\n451         airmass_absolute = np.minimum(airmass_absolute, 10)\n452 \n453     # Warn user about AMa data that is exceptionally low\n454     if np.min(airmass_absolute) < 0.58:\n455         warn('Exceptionally low air mass: ' +\n456              'model not intended for extra-terrestrial use')\n457         # pvl_absoluteairmass(1,pvl_alt2pres(4340)) = 0.58 Elevation of\n458         # Mina Pirquita, Argentian = 4340 m. Highest elevation city with\n459         # population over 50,000.\n460 \n461     _coefficients = {}\n462     _coefficients['cdte'] = (\n463         0.86273, -0.038948, -0.012506, 0.098871, 0.084658, -0.0042948)\n464     _coefficients['monosi'] = (\n465         0.85914, -0.020880, -0.0058853, 0.12029, 0.026814, -0.0017810)\n466     _coefficients['xsi'] = _coefficients['monosi']\n467     _coefficients['polysi'] = (\n468         0.84090, -0.027539, -0.0079224, 0.13570, 0.038024, -0.0021218)\n469     _coefficients['multisi'] = _coefficients['polysi']\n470     _coefficients['cigs'] = (\n471         0.85252, -0.022314, -0.0047216, 0.13666, 0.013342, -0.0008945)\n472     _coefficients['asi'] = (\n473         1.12094, -0.047620, -0.0083627, -0.10443, 0.098382, -0.0033818)\n474 \n475     if module_type is not None and coefficients is None:\n476         coefficients = _coefficients[module_type.lower()]\n477     elif module_type is None and coefficients is not None:\n478         pass\n479     elif module_type is None and coefficients is None:\n480         raise TypeError('No valid input provided, both module_type and ' +\n481                         'coefficients are None')\n482     else:\n483         raise TypeError('Cannot resolve input, must supply only one of ' +\n484                         'module_type and coefficients')\n485 \n486     # Evaluate Spectral Shift\n487     coeff = coefficients\n488     ama = airmass_absolute\n489     modifier = (\n490         coeff[0] + coeff[1]*ama + coeff[2]*pw + coeff[3]*np.sqrt(ama) +\n491         coeff[4]*np.sqrt(pw) + coeff[5]*ama/np.sqrt(pw))\n492 \n493     return modifier\n494 \n495 \n496 def bird_hulstrom80_aod_bb(aod380, aod500):\n497     \"\"\"\n498     Approximate broadband aerosol optical depth.\n499 \n500     Bird and Hulstrom developed a correlation for broadband aerosol optical\n501     depth (AOD) using two wavelengths, 380 nm and 500 nm.\n502 \n503     Parameters\n504     ----------\n505     aod380 : numeric\n506         AOD measured at 380 nm\n507     aod500 : numeric\n508         AOD measured at 500 nm\n509 \n510     Returns\n511     -------\n512     aod_bb : numeric\n513         broadband AOD\n514 \n515     See also\n516     --------\n517     kasten96_lt\n518 \n519     References\n520     ----------\n521     [1] Bird and Hulstrom, \"Direct Insolation Models\" (1980)\n522     `SERI/TR-335-344 <http://www.nrel.gov/docs/legosti/old/344.pdf>`_\n523 \n524     [2] R. E. Bird and R. L. Hulstrom, \"Review, Evaluation, and Improvement of\n525     Direct Irradiance Models\", Journal of Solar Energy Engineering 103(3),\n526     pp. 182-192 (1981)\n527     :doi:`10.1115/1.3266239`\n528     \"\"\"\n529     # approximate broadband AOD using (Bird-Hulstrom 1980)\n530     return 0.27583 * aod380 + 0.35 * aod500\n531 \n532 \n533 def kasten96_lt(airmass_absolute, precipitable_water, aod_bb):\n534     \"\"\"\n535     Calculate Linke turbidity factor using Kasten pyrheliometric formula.\n536 \n537     Note that broadband aerosol optical depth (AOD) can be approximated by AOD\n538     measured at 700 nm according to Molineaux [4] . Bird and Hulstrom offer an\n539     alternate approximation using AOD measured at 380 nm and 500 nm.\n540 \n541     Based on original implementation by Armel Oumbe.\n542 \n543     .. warning::\n544         These calculations are only valid for air mass less than 5 atm and\n545         precipitable water less than 5 cm.\n546 \n547     Parameters\n548     ----------\n549     airmass_absolute : numeric\n550         airmass, pressure corrected in atmospheres\n551     precipitable_water : numeric\n552         precipitable water or total column water vapor in centimeters\n553     aod_bb : numeric\n554         broadband AOD\n555 \n556     Returns\n557     -------\n558     lt : numeric\n559         Linke turbidity\n560 \n561     See also\n562     --------\n563     bird_hulstrom80_aod_bb\n564     angstrom_aod_at_lambda\n565 \n566     References\n567     ----------\n568     [1] F. Linke, \"Transmissions-Koeffizient und Trubungsfaktor\", Beitrage\n569     zur Physik der Atmosphare, Vol 10, pp. 91-103 (1922)\n570 \n571     [2] F. Kasten, \"A simple parameterization of the pyrheliometric formula for\n572     determining the Linke turbidity factor\", Meteorologische Rundschau 33,\n573     pp. 124-127 (1980)\n574 \n575     [3] Kasten, \"The Linke turbidity factor based on improved values of the\n576     integral Rayleigh optical thickness\", Solar Energy, Vol. 56, No. 3,\n577     pp. 239-244 (1996)\n578     :doi:`10.1016/0038-092X(95)00114-7`\n579 \n580     [4] B. Molineaux, P. Ineichen, N. O'Neill, \"Equivalence of pyrheliometric\n581     and monochromatic aerosol optical depths at a single key wavelength\",\n582     Applied Optics Vol. 37, issue 10, 7008-7018 (1998)\n583     :doi:`10.1364/AO.37.007008`\n584 \n585     [5] P. Ineichen, \"Conversion function between the Linke turbidity and the\n586     atmospheric water vapor and aerosol content\", Solar Energy 82,\n587     pp. 1095-1097 (2008)\n588     :doi:`10.1016/j.solener.2008.04.010`\n589 \n590     [6] P. Ineichen and R. Perez, \"A new airmass independent formulation for\n591     the Linke Turbidity coefficient\", Solar Energy, Vol. 73, no. 3, pp. 151-157\n592     (2002)\n593     :doi:`10.1016/S0038-092X(02)00045-2`\n594     \"\"\"\n595     # \"From numerically integrated spectral simulations done with Modtran\n596     # (Berk, 1989), Molineaux (1998) obtained for the broadband optical depth\n597     # of a clean and dry atmospshere (fictitious atmosphere that comprises only\n598     # the effects of Rayleigh scattering and absorption by the atmosphere gases\n599     # other than the water vapor) the following expression\"\n600     # - P. Ineichen (2008)\n601     delta_cda = -0.101 + 0.235 * airmass_absolute ** (-0.16)\n602     # \"and the broadband water vapor optical depth where pwat is the integrated\n603     # precipitable water vapor content of the atmosphere expressed in cm and am\n604     # the optical air mass. The precision of these fits is better than 1% when\n605     # compared with Modtran simulations in the range 1 < am < 5 and\n606     # 0 < pwat < 5 cm at sea level\" - P. Ineichen (2008)\n607     delta_w = 0.112 * airmass_absolute ** (-0.55) * precipitable_water ** 0.34\n608     # broadband AOD\n609     delta_a = aod_bb\n610     # \"Then using the Kasten pyrheliometric formula (1980, 1996), the Linke\n611     # turbidity at am = 2 can be written. The extension of the Linke turbidity\n612     # coefficient to other values of air mass was published by Ineichen and\n613     # Perez (2002)\" - P. Ineichen (2008)\n614     lt = -(9.4 + 0.9 * airmass_absolute) * np.log(\n615         np.exp(-airmass_absolute * (delta_cda + delta_w + delta_a))\n616     ) / airmass_absolute\n617     # filter out of extrapolated values\n618     return lt\n619 \n620 \n621 def angstrom_aod_at_lambda(aod0, lambda0, alpha=1.14, lambda1=700.0):\n622     r\"\"\"\n623     Get AOD at specified wavelength using Angstrom turbidity model.\n624 \n625     Parameters\n626     ----------\n627     aod0 : numeric\n628         aerosol optical depth (AOD) measured at known wavelength\n629     lambda0 : numeric\n630         wavelength in nanometers corresponding to ``aod0``\n631     alpha : numeric, default 1.14\n632         Angstrom :math:`\\alpha` exponent corresponding to ``aod0``\n633     lambda1 : numeric, default 700\n634         desired wavelength in nanometers\n635 \n636     Returns\n637     -------\n638     aod1 : numeric\n639         AOD at desired wavelength, ``lambda1``\n640 \n641     See also\n642     --------\n643     angstrom_alpha\n644 \n645     References\n646     ----------\n647     [1] Anders Angstrom, \"On the Atmospheric Transmission of Sun Radiation and\n648     On Dust in the Air\", Geografiska Annaler Vol. 11, pp. 156-166 (1929) JSTOR\n649     :doi:`10.2307/519399`\n650 \n651     [2] Anders Angstrom, \"Techniques of Determining the Turbidity of the\n652     Atmosphere\", Tellus 13:2, pp. 214-223 (1961) Taylor & Francis\n653     :doi:`10.3402/tellusa.v13i2.9493` and Co-Action Publishing\n654     :doi:`10.1111/j.2153-3490.1961.tb00078.x`\n655     \"\"\"\n656     return aod0 * ((lambda1 / lambda0) ** (-alpha))\n657 \n658 \n659 def angstrom_alpha(aod1, lambda1, aod2, lambda2):\n660     r\"\"\"\n661     Calculate Angstrom alpha exponent.\n662 \n663     Parameters\n664     ----------\n665     aod1 : numeric\n666         first aerosol optical depth\n667     lambda1 : numeric\n668         wavelength in nanometers corresponding to ``aod1``\n669     aod2 : numeric\n670         second aerosol optical depth\n671     lambda2 : numeric\n672         wavelength in nanometers corresponding to ``aod2``\n673 \n674     Returns\n675     -------\n676     alpha : numeric\n677         Angstrom :math:`\\alpha` exponent for AOD in ``(lambda1, lambda2)``\n678 \n679     See also\n680     --------\n681     angstrom_aod_at_lambda\n682     \"\"\"\n683     return - np.log(aod1 / aod2) / np.log(lambda1 / lambda2)\n684 \n[end of pvlib/atmosphere.py]\n[start of pvlib/singlediode.py]\n1 # -*- coding: utf-8 -*-\n2 \"\"\"\n3 Low-level functions for solving the single diode equation.\n4 \"\"\"\n5 \n6 from functools import partial\n7 import numpy as np\n8 from pvlib.tools import _golden_sect_DataFrame\n9 \n10 # Try to import brentq from scipy to use when specified in bishop88_i_from_v,\n11 # bishop88_v_from_i, and bishop88_mpp methods below. If not imported, raises\n12 # ImportError when 'brentq' method is specified for those methods.\n13 try:\n14     from scipy.optimize import brentq\n15 except ImportError:\n16     def brentq(*a, **kw):\n17         raise ImportError(\n18             \"brentq couldn't be imported. Is SciPy installed?\")\n19 \n20 # FIXME: change this to newton when scipy-1.2 is released\n21 try:\n22     from scipy.optimize import _array_newton\n23 except ImportError:\n24     from pvlib.tools import _array_newton\n25 # rename newton and set keyword arguments\n26 newton = partial(_array_newton, tol=1e-6, maxiter=100, fprime2=None)\n27 \n28 # intrinsic voltage per cell junction for a:Si, CdTe, Mertens et al.\n29 VOLTAGE_BUILTIN = 0.9  # [V]\n30 \n31 \n32 def estimate_voc(photocurrent, saturation_current, nNsVth):\n33     \"\"\"\n34     Rough estimate of open circuit voltage useful for bounding searches for\n35     ``i`` of ``v`` when using :func:`~pvlib.pvsystem.singlediode`.\n36 \n37     Parameters\n38     ----------\n39     photocurrent : numeric\n40         photo-generated current [A]\n41     saturation_current : numeric\n42         diode reverse saturation current [A]\n43     nNsVth : numeric\n44         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n45         and number of series cells ``Ns``\n46 \n47     Returns\n48     -------\n49     numeric\n50         rough estimate of open circuit voltage [V]\n51 \n52     Notes\n53     -----\n54     Calculating the open circuit voltage, :math:`V_{oc}`, of an ideal device\n55     with infinite shunt resistance, :math:`R_{sh} \\\\to \\\\infty`, and zero\n56     series resistance, :math:`R_s = 0`, yields the following equation [1]. As\n57     an estimate of :math:`V_{oc}` it is useful as an upper bound for the\n58     bisection method.\n59 \n60     .. math::\n61 \n62         V_{oc, est}=n Ns V_{th} \\\\log \\\\left( \\\\frac{I_L}{I_0} + 1 \\\\right)\n63 \n64     [1] http://www.pveducation.org/pvcdrom/open-circuit-voltage\n65     \"\"\"\n66 \n67     return nNsVth * np.log(np.asarray(photocurrent) / saturation_current + 1.0)\n68 \n69 \n70 def bishop88(diode_voltage, photocurrent, saturation_current,\n71              resistance_series, resistance_shunt, nNsVth, d2mutau=0,\n72              NsVbi=np.Inf, gradients=False):\n73     \"\"\"\n74     Explicit calculation of points on the IV curve described by the single\n75     diode equation [1]_.\n76 \n77     .. warning::\n78        * Do not use ``d2mutau`` with CEC coefficients.\n79        * Usage of ``d2mutau`` with PVSyst coefficients is required for cadmium-\n80          telluride (CdTe) and amorphous-silicon (a:Si) PV modules only.\n81 \n82     Parameters\n83     ----------\n84     diode_voltage : numeric\n85         diode voltages [V]\n86     photocurrent : numeric\n87         photo-generated current [A]\n88     saturation_current : numeric\n89         diode reverse saturation current [A]\n90     resistance_series : numeric\n91         series resistance [ohms]\n92     resistance_shunt: numeric\n93         shunt resistance [ohms]\n94     nNsVth : numeric\n95         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n96         and number of series cells ``Ns``\n97     d2mutau : numeric\n98         PVSyst thin-film recombination parameter that is the ratio of thickness\n99         of the intrinsic layer squared :math:`d^2` and the diffusion length of\n100         charge carriers :math:`\\\\mu \\\\tau`, in volts [V], defaults to 0[V]\n101     NsVbi : numeric\n102         PVSyst thin-film recombination parameter that is the product of the PV\n103         module number of series cells ``Ns`` and the builtin voltage ``Vbi`` of\n104         the intrinsic layer, in volts [V], defaults to ``np.inf``\n105     gradients : bool\n106         False returns only I, V, and P. True also returns gradients\n107 \n108     Returns\n109     -------\n110     tuple\n111         currents [A], voltages [V], power [W], and optionally\n112         :math:`\\\\frac{dI}{dV_d}`, :math:`\\\\frac{dV}{dV_d}`,\n113         :math:`\\\\frac{dI}{dV}`, :math:`\\\\frac{dP}{dV}`, and\n114         :math:`\\\\frac{d^2 P}{dV dV_d}`\n115 \n116     Notes\n117     -----\n118     The PVSyst thin-film recombination losses parameters ``d2mutau`` and\n119     ``NsVbi`` are only applied to cadmium-telluride (CdTe) and amorphous-\n120     silicon (a:Si) PV modules, [2]_, [3]_. The builtin voltage :math:`V_{bi}`\n121     should account for all junctions. For example: tandem and triple junction\n122     cells would have builtin voltages of 1.8[V] and 2.7[V] respectively, based\n123     on the default of 0.9[V] for a single junction. The parameter ``NsVbi``\n124     should only account for the number of series cells in a single parallel\n125     sub-string if the module has cells in parallel greater than 1.\n126 \n127     References\n128     ----------\n129     .. [1] \"Computer simulation of the effects of electrical mismatches in\n130        photovoltaic cell interconnection circuits\" JW Bishop, Solar Cell (1988)\n131        :doi:`10.1016/0379-6787(88)90059-2`\n132 \n133     .. [2] \"Improved equivalent circuit and Analytical Model for Amorphous\n134        Silicon Solar Cells and Modules.\" J. Mertens, et al., IEEE Transactions\n135        on Electron Devices, Vol 45, No 2, Feb 1998.\n136        :doi:`10.1109/16.658676`\n137 \n138     .. [3] \"Performance assessment of a simulation model for PV modules of any\n139        available technology\", Andr\u00e9 Mermoud and Thibault Lejeune, 25th EUPVSEC,\n140        2010\n141        :doi:`10.4229/25thEUPVSEC2010-4BV.1.114`\n142     \"\"\"\n143     # calculate recombination loss current where d2mutau > 0\n144     is_recomb = d2mutau > 0  # True where there is thin-film recombination loss\n145     v_recomb = np.where(is_recomb, NsVbi - diode_voltage, np.inf)\n146     i_recomb = np.where(is_recomb, photocurrent * d2mutau / v_recomb, 0)\n147     # calculate temporary values to simplify calculations\n148     v_star = diode_voltage / nNsVth  # non-dimensional diode voltage\n149     g_sh = 1.0 / resistance_shunt  # conductance\n150     i = (photocurrent - saturation_current * np.expm1(v_star)\n151          - diode_voltage * g_sh - i_recomb)\n152     v = diode_voltage - i * resistance_series\n153     retval = (i, v, i*v)\n154     if gradients:\n155         # calculate recombination loss current gradients where d2mutau > 0\n156         grad_i_recomb = np.where(is_recomb, i_recomb / v_recomb, 0)\n157         grad_2i_recomb = np.where(is_recomb, 2 * grad_i_recomb / v_recomb, 0)\n158         g_diode = saturation_current * np.exp(v_star) / nNsVth  # conductance\n159         grad_i = -g_diode - g_sh - grad_i_recomb  # di/dvd\n160         grad_v = 1.0 - grad_i * resistance_series  # dv/dvd\n161         # dp/dv = d(iv)/dv = v * di/dv + i\n162         grad = grad_i / grad_v  # di/dv\n163         grad_p = v * grad + i  # dp/dv\n164         grad2i = -g_diode / nNsVth - grad_2i_recomb  # d2i/dvd\n165         grad2v = -grad2i * resistance_series  # d2v/dvd\n166         grad2p = (\n167             grad_v * grad + v * (grad2i/grad_v - grad_i*grad2v/grad_v**2)\n168             + grad_i\n169         )  # d2p/dv/dvd\n170         retval += (grad_i, grad_v, grad, grad_p, grad2p)\n171     return retval\n172 \n173 \n174 def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n175                       resistance_series, resistance_shunt, nNsVth,\n176                       method='newton'):\n177     \"\"\"\n178     Find current given any voltage.\n179 \n180     Parameters\n181     ----------\n182     voltage : numeric\n183         voltage (V) in volts [V]\n184     photocurrent : numeric\n185         photogenerated current (Iph or IL) in amperes [A]\n186     saturation_current : numeric\n187         diode dark or saturation current (Io or Isat) in amperes [A]\n188     resistance_series : numeric\n189         series resistance (Rs) in ohms\n190     resistance_shunt : numeric\n191         shunt resistance (Rsh) in ohms\n192     nNsVth : numeric\n193         product of diode ideality factor (n), number of series cells (Ns), and\n194         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n195     method : str\n196         one of two optional search methods: either ``'brentq'``, a reliable and\n197         bounded method or ``'newton'`` which is the default.\n198 \n199     Returns\n200     -------\n201     current : numeric\n202         current (I) at the specified voltage (V) in amperes [A]\n203     \"\"\"\n204     # collect args\n205     args = (photocurrent, saturation_current, resistance_series,\n206             resistance_shunt, nNsVth)\n207 \n208     def fv(x, v, *a):\n209         # calculate voltage residual given diode voltage \"x\"\n210         return bishop88(x, *a)[1] - v\n211 \n212     if method.lower() == 'brentq':\n213         # first bound the search using voc\n214         voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n215 \n216         # brentq only works with scalar inputs, so we need a set up function\n217         # and np.vectorize to repeatedly call the optimizer with the right\n218         # arguments for possible array input\n219         def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma):\n220             return brentq(fv, 0.0, voc, args=(v, iph, isat, rs, rsh, gamma))\n221 \n222         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n223         vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n224     elif method.lower() == 'newton':\n225         # make sure all args are numpy arrays if max size > 1\n226         # if voltage is an array, then make a copy to use for initial guess, v0\n227         args, v0 = _prepare_newton_inputs((voltage,), args, voltage)\n228         vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n229                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n230                     args=args)\n231     else:\n232         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n233     return bishop88(vd, *args)[0]\n234 \n235 \n236 def bishop88_v_from_i(current, photocurrent, saturation_current,\n237                       resistance_series, resistance_shunt, nNsVth,\n238                       method='newton'):\n239     \"\"\"\n240     Find voltage given any current.\n241 \n242     Parameters\n243     ----------\n244     current : numeric\n245         current (I) in amperes [A]\n246     photocurrent : numeric\n247         photogenerated current (Iph or IL) in amperes [A]\n248     saturation_current : numeric\n249         diode dark or saturation current (Io or Isat) in amperes [A]\n250     resistance_series : numeric\n251         series resistance (Rs) in ohms\n252     resistance_shunt : numeric\n253         shunt resistance (Rsh) in ohms\n254     nNsVth : numeric\n255         product of diode ideality factor (n), number of series cells (Ns), and\n256         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n257     method : str\n258         one of two optional search methods: either ``'brentq'``, a reliable and\n259         bounded method or ``'newton'`` which is the default.\n260 \n261     Returns\n262     -------\n263     voltage : numeric\n264         voltage (V) at the specified current (I) in volts [V]\n265     \"\"\"\n266     # collect args\n267     args = (photocurrent, saturation_current, resistance_series,\n268             resistance_shunt, nNsVth)\n269     # first bound the search using voc\n270     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n271 \n272     def fi(x, i, *a):\n273         # calculate current residual given diode voltage \"x\"\n274         return bishop88(x, *a)[0] - i\n275 \n276     if method.lower() == 'brentq':\n277         # brentq only works with scalar inputs, so we need a set up function\n278         # and np.vectorize to repeatedly call the optimizer with the right\n279         # arguments for possible array input\n280         def vd_from_brent(voc, i, iph, isat, rs, rsh, gamma):\n281             return brentq(fi, 0.0, voc, args=(i, iph, isat, rs, rsh, gamma))\n282 \n283         vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n284         vd = vd_from_brent_vectorized(voc_est, current, *args)\n285     elif method.lower() == 'newton':\n286         # make sure all args are numpy arrays if max size > 1\n287         # if voc_est is an array, then make a copy to use for initial guess, v0\n288         args, v0 = _prepare_newton_inputs((current,), args, voc_est)\n289         vd = newton(func=lambda x, *a: fi(x, current, *a), x0=v0,\n290                     fprime=lambda x, *a: bishop88(x, *a, gradients=True)[3],\n291                     args=args)\n292     else:\n293         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n294     return bishop88(vd, *args)[1]\n295 \n296 \n297 def bishop88_mpp(photocurrent, saturation_current, resistance_series,\n298                  resistance_shunt, nNsVth, method='newton'):\n299     \"\"\"\n300     Find max power point.\n301 \n302     Parameters\n303     ----------\n304     photocurrent : numeric\n305         photogenerated current (Iph or IL) in amperes [A]\n306     saturation_current : numeric\n307         diode dark or saturation current (Io or Isat) in amperes [A]\n308     resistance_series : numeric\n309         series resistance (Rs) in ohms\n310     resistance_shunt : numeric\n311         shunt resistance (Rsh) in ohms\n312     nNsVth : numeric\n313         product of diode ideality factor (n), number of series cells (Ns), and\n314         thermal voltage (Vth = k_b * T / q_e) in volts [V]\n315     method : str\n316         one of two optional search methods: either ``'brentq'``, a reliable and\n317         bounded method or ``'newton'`` which is the default.\n318 \n319     Returns\n320     -------\n321     OrderedDict or pandas.DataFrame\n322         max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n323         max power ``p_mp`` [W]\n324     \"\"\"\n325     # collect args\n326     args = (photocurrent, saturation_current, resistance_series,\n327             resistance_shunt, nNsVth)\n328     # first bound the search using voc\n329     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n330 \n331     def fmpp(x, *a):\n332         return bishop88(x, *a, gradients=True)[6]\n333 \n334     if method.lower() == 'brentq':\n335         # break out arguments for numpy.vectorize to handle broadcasting\n336         vec_fun = np.vectorize(\n337             lambda voc, iph, isat, rs, rsh, gamma:\n338                 brentq(fmpp, 0.0, voc, args=(iph, isat, rs, rsh, gamma))\n339         )\n340         vd = vec_fun(voc_est, *args)\n341     elif method.lower() == 'newton':\n342         # make sure all args are numpy arrays if max size > 1\n343         # if voc_est is an array, then make a copy to use for initial guess, v0\n344         args, v0 = _prepare_newton_inputs((), args, voc_est)\n345         vd = newton(\n346             func=fmpp, x0=v0,\n347             fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n348         )\n349     else:\n350         raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n351     return bishop88(vd, *args)\n352 \n353 \n354 def _get_size_and_shape(args):\n355     # find the right size and shape for returns\n356     size, shape = 0, None  # 0 or None both mean scalar\n357     for arg in args:\n358         try:\n359             this_shape = arg.shape  # try to get shape\n360         except AttributeError:\n361             this_shape = None\n362             try:\n363                 this_size = len(arg)  # try to get the size\n364             except TypeError:\n365                 this_size = 0\n366         else:\n367             this_size = arg.size  # if it has shape then it also has size\n368             if shape is None:\n369                 shape = this_shape  # set the shape if None\n370         # update size and shape\n371         if this_size > size:\n372             size = this_size\n373             if this_shape is not None:\n374                 shape = this_shape\n375     return size, shape\n376 \n377 \n378 def _prepare_newton_inputs(i_or_v_tup, args, v0):\n379     # broadcast arguments for newton method\n380     # the first argument should be a tuple, eg: (i,), (v,) or ()\n381     size, shape = _get_size_and_shape(i_or_v_tup + args)\n382     if size > 1:\n383         args = [np.asarray(arg) for arg in args]\n384     # newton uses initial guess for the output shape\n385     # copy v0 to a new array and broadcast it to the shape of max size\n386     if shape is not None:\n387         v0 = np.broadcast_to(v0, shape).copy()\n388     return args, v0\n389 \n390 \n391 def _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, current,\n392                        saturation_current, photocurrent):\n393     try:\n394         from scipy.special import lambertw\n395     except ImportError:\n396         raise ImportError('This function requires scipy')\n397 \n398     # Record if inputs were all scalar\n399     output_is_scalar = all(map(np.isscalar,\n400                                [resistance_shunt, resistance_series, nNsVth,\n401                                 current, saturation_current, photocurrent]))\n402 \n403     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n404     #  is generally more numerically stable\n405     conductance_shunt = 1. / resistance_shunt\n406 \n407     # Ensure that we are working with read-only views of numpy arrays\n408     # Turns Series into arrays so that we don't have to worry about\n409     #  multidimensional broadcasting failing\n410     Gsh, Rs, a, I, I0, IL = \\\n411         np.broadcast_arrays(conductance_shunt, resistance_series, nNsVth,\n412                             current, saturation_current, photocurrent)\n413 \n414     # Intitalize output V (I might not be float64)\n415     V = np.full_like(I, np.nan, dtype=np.float64)\n416 \n417     # Determine indices where 0 < Gsh requires implicit model solution\n418     idx_p = 0. < Gsh\n419 \n420     # Determine indices where 0 = Gsh allows explicit model solution\n421     idx_z = 0. == Gsh\n422 \n423     # Explicit solutions where Gsh=0\n424     if np.any(idx_z):\n425         V[idx_z] = a[idx_z] * np.log1p((IL[idx_z] - I[idx_z]) / I0[idx_z]) - \\\n426                    I[idx_z] * Rs[idx_z]\n427 \n428     # Only compute using LambertW if there are cases with Gsh>0\n429     if np.any(idx_p):\n430         # LambertW argument, cannot be float128, may overflow to np.inf\n431         # overflow is explicitly handled below, so ignore warnings here\n432         with np.errstate(over='ignore'):\n433             argW = (I0[idx_p] / (Gsh[idx_p] * a[idx_p]) *\n434                     np.exp((-I[idx_p] + IL[idx_p] + I0[idx_p]) /\n435                            (Gsh[idx_p] * a[idx_p])))\n436 \n437         # lambertw typically returns complex value with zero imaginary part\n438         # may overflow to np.inf\n439         lambertwterm = lambertw(argW).real\n440 \n441         # Record indices where lambertw input overflowed output\n442         idx_inf = np.logical_not(np.isfinite(lambertwterm))\n443 \n444         # Only re-compute LambertW if it overflowed\n445         if np.any(idx_inf):\n446             # Calculate using log(argW) in case argW is really big\n447             logargW = (np.log(I0[idx_p]) - np.log(Gsh[idx_p]) -\n448                        np.log(a[idx_p]) +\n449                        (-I[idx_p] + IL[idx_p] + I0[idx_p]) /\n450                        (Gsh[idx_p] * a[idx_p]))[idx_inf]\n451 \n452             # Three iterations of Newton-Raphson method to solve\n453             #  w+log(w)=logargW. The initial guess is w=logargW. Where direct\n454             #  evaluation (above) results in NaN from overflow, 3 iterations\n455             #  of Newton's method gives approximately 8 digits of precision.\n456             w = logargW\n457             for _ in range(0, 3):\n458                 w = w * (1. - np.log(w) + logargW) / (1. + w)\n459             lambertwterm[idx_inf] = w\n460 \n461         # Eqn. 3 in Jain and Kapoor, 2004\n462         #  V = -I*(Rs + Rsh) + IL*Rsh - a*lambertwterm + I0*Rsh\n463         # Recast in terms of Gsh=1/Rsh for better numerical stability.\n464         V[idx_p] = (IL[idx_p] + I0[idx_p] - I[idx_p]) / Gsh[idx_p] - \\\n465             I[idx_p] * Rs[idx_p] - a[idx_p] * lambertwterm\n466 \n467     if output_is_scalar:\n468         return V.item()\n469     else:\n470         return V\n471 \n472 \n473 def _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n474                        saturation_current, photocurrent):\n475     try:\n476         from scipy.special import lambertw\n477     except ImportError:\n478         raise ImportError('This function requires scipy')\n479 \n480     # Record if inputs were all scalar\n481     output_is_scalar = all(map(np.isscalar,\n482                                [resistance_shunt, resistance_series, nNsVth,\n483                                 voltage, saturation_current, photocurrent]))\n484 \n485     # This transforms Gsh=1/Rsh, including ideal Rsh=np.inf into Gsh=0., which\n486     #  is generally more numerically stable\n487     conductance_shunt = 1. / resistance_shunt\n488 \n489     # Ensure that we are working with read-only views of numpy arrays\n490     # Turns Series into arrays so that we don't have to worry about\n491     #  multidimensional broadcasting failing\n492     Gsh, Rs, a, V, I0, IL = \\\n493         np.broadcast_arrays(conductance_shunt, resistance_series, nNsVth,\n494                             voltage, saturation_current, photocurrent)\n495 \n496     # Intitalize output I (V might not be float64)\n497     I = np.full_like(V, np.nan, dtype=np.float64)           # noqa: E741, N806\n498 \n499     # Determine indices where 0 < Rs requires implicit model solution\n500     idx_p = 0. < Rs\n501 \n502     # Determine indices where 0 = Rs allows explicit model solution\n503     idx_z = 0. == Rs\n504 \n505     # Explicit solutions where Rs=0\n506     if np.any(idx_z):\n507         I[idx_z] = IL[idx_z] - I0[idx_z] * np.expm1(V[idx_z] / a[idx_z]) - \\\n508                    Gsh[idx_z] * V[idx_z]\n509 \n510     # Only compute using LambertW if there are cases with Rs>0\n511     # Does NOT handle possibility of overflow, github issue 298\n512     if np.any(idx_p):\n513         # LambertW argument, cannot be float128, may overflow to np.inf\n514         argW = Rs[idx_p] * I0[idx_p] / (\n515                     a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)) * \\\n516                np.exp((Rs[idx_p] * (IL[idx_p] + I0[idx_p]) + V[idx_p]) /\n517                       (a[idx_p] * (Rs[idx_p] * Gsh[idx_p] + 1.)))\n518 \n519         # lambertw typically returns complex value with zero imaginary part\n520         # may overflow to np.inf\n521         lambertwterm = lambertw(argW).real\n522 \n523         # Eqn. 2 in Jain and Kapoor, 2004\n524         #  I = -V/(Rs + Rsh) - (a/Rs)*lambertwterm + Rsh*(IL + I0)/(Rs + Rsh)\n525         # Recast in terms of Gsh=1/Rsh for better numerical stability.\n526         I[idx_p] = (IL[idx_p] + I0[idx_p] - V[idx_p] * Gsh[idx_p]) / \\\n527                    (Rs[idx_p] * Gsh[idx_p] + 1.) - (\n528                                a[idx_p] / Rs[idx_p]) * lambertwterm\n529 \n530     if output_is_scalar:\n531         return I.item()\n532     else:\n533         return I\n534 \n535 \n536 def _lambertw(photocurrent, saturation_current, resistance_series,\n537               resistance_shunt, nNsVth, ivcurve_pnts=None):\n538     # Compute short circuit current\n539     i_sc = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, 0.,\n540                               saturation_current, photocurrent)\n541 \n542     # Compute open circuit voltage\n543     v_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\n544                               saturation_current, photocurrent)\n545 \n546     params = {'r_sh': resistance_shunt,\n547               'r_s': resistance_series,\n548               'nNsVth': nNsVth,\n549               'i_0': saturation_current,\n550               'i_l': photocurrent}\n551 \n552     # Find the voltage, v_mp, where the power is maximized.\n553     # Start the golden section search at v_oc * 1.14\n554     p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\n555                                         _pwr_optfcn)\n556 \n557     # Find Imp using Lambert W\n558     i_mp = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n559                               v_mp, saturation_current, photocurrent)\n560 \n561     # Find Ix and Ixx using Lambert W\n562     i_x = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n563                              0.5 * v_oc, saturation_current, photocurrent)\n564 \n565     i_xx = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth,\n566                               0.5 * (v_oc + v_mp), saturation_current,\n567                               photocurrent)\n568 \n569     out = (i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx)\n570 \n571     # create ivcurve\n572     if ivcurve_pnts:\n573         ivcurve_v = (np.asarray(v_oc)[..., np.newaxis] *\n574                      np.linspace(0, 1, ivcurve_pnts))\n575 \n576         ivcurve_i = _lambertw_i_from_v(resistance_shunt, resistance_series,\n577                                        nNsVth, ivcurve_v.T, saturation_current,\n578                                        photocurrent).T\n579 \n580         out += (ivcurve_i, ivcurve_v)\n581 \n582     return out\n583 \n584 \n585 def _pwr_optfcn(df, loc):\n586     '''\n587     Function to find power from ``i_from_v``.\n588     '''\n589 \n590     I = _lambertw_i_from_v(df['r_sh'], df['r_s'],           # noqa: E741, N806\n591                            df['nNsVth'], df[loc], df['i_0'], df['i_l'])\n592 \n593     return I * df[loc]\n594 \n[end of pvlib/singlediode.py]\n[start of pvlib/tools.py]\n1 \"\"\"\n2 Collection of functions used in pvlib_python\n3 \"\"\"\n4 \n5 from collections import namedtuple\n6 import datetime as dt\n7 import warnings\n8 import numpy as np\n9 import pandas as pd\n10 import pytz\n11 \n12 \n13 def cosd(angle):\n14     \"\"\"\n15     Cosine with angle input in degrees\n16 \n17     Parameters\n18     ----------\n19     angle : float or array-like\n20         Angle in degrees\n21 \n22     Returns\n23     -------\n24     result : float or array-like\n25         Cosine of the angle\n26     \"\"\"\n27 \n28     res = np.cos(np.radians(angle))\n29     return res\n30 \n31 \n32 def sind(angle):\n33     \"\"\"\n34     Sine with angle input in degrees\n35 \n36     Parameters\n37     ----------\n38     angle : float\n39         Angle in degrees\n40 \n41     Returns\n42     -------\n43     result : float\n44         Sin of the angle\n45     \"\"\"\n46 \n47     res = np.sin(np.radians(angle))\n48     return res\n49 \n50 \n51 def tand(angle):\n52     \"\"\"\n53     Tan with angle input in degrees\n54 \n55     Parameters\n56     ----------\n57     angle : float\n58         Angle in degrees\n59 \n60     Returns\n61     -------\n62     result : float\n63         Tan of the angle\n64     \"\"\"\n65 \n66     res = np.tan(np.radians(angle))\n67     return res\n68 \n69 \n70 def asind(number):\n71     \"\"\"\n72     Inverse Sine returning an angle in degrees\n73 \n74     Parameters\n75     ----------\n76     number : float\n77         Input number\n78 \n79     Returns\n80     -------\n81     result : float\n82         arcsin result\n83     \"\"\"\n84 \n85     res = np.degrees(np.arcsin(number))\n86     return res\n87 \n88 \n89 def localize_to_utc(time, location):\n90     \"\"\"\n91     Converts or localizes a time series to UTC.\n92 \n93     Parameters\n94     ----------\n95     time : datetime.datetime, pandas.DatetimeIndex,\n96            or pandas.Series/DataFrame with a DatetimeIndex.\n97     location : pvlib.Location object\n98 \n99     Returns\n100     -------\n101     pandas object localized to UTC.\n102     \"\"\"\n103     if isinstance(time, dt.datetime):\n104         if time.tzinfo is None:\n105             time = pytz.timezone(location.tz).localize(time)\n106         time_utc = time.astimezone(pytz.utc)\n107     else:\n108         try:\n109             time_utc = time.tz_convert('UTC')\n110         except TypeError:\n111             time_utc = time.tz_localize(location.tz).tz_convert('UTC')\n112 \n113     return time_utc\n114 \n115 \n116 def datetime_to_djd(time):\n117     \"\"\"\n118     Converts a datetime to the Dublin Julian Day\n119 \n120     Parameters\n121     ----------\n122     time : datetime.datetime\n123         time to convert\n124 \n125     Returns\n126     -------\n127     float\n128         fractional days since 12/31/1899+0000\n129     \"\"\"\n130 \n131     if time.tzinfo is None:\n132         time_utc = pytz.utc.localize(time)\n133     else:\n134         time_utc = time.astimezone(pytz.utc)\n135 \n136     djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))\n137     djd = (time_utc - djd_start).total_seconds() * 1.0/(60 * 60 * 24)\n138 \n139     return djd\n140 \n141 \n142 def djd_to_datetime(djd, tz='UTC'):\n143     \"\"\"\n144     Converts a Dublin Julian Day float to a datetime.datetime object\n145 \n146     Parameters\n147     ----------\n148     djd : float\n149         fractional days since 12/31/1899+0000\n150     tz : str, default 'UTC'\n151         timezone to localize the result to\n152 \n153     Returns\n154     -------\n155     datetime.datetime\n156        The resultant datetime localized to tz\n157     \"\"\"\n158 \n159     djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))\n160 \n161     utc_time = djd_start + dt.timedelta(days=djd)\n162     return utc_time.astimezone(pytz.timezone(tz))\n163 \n164 \n165 def _pandas_to_doy(pd_object):\n166     \"\"\"\n167     Finds the day of year for a pandas datetime-like object.\n168 \n169     Useful for delayed evaluation of the dayofyear attribute.\n170 \n171     Parameters\n172     ----------\n173     pd_object : DatetimeIndex or Timestamp\n174 \n175     Returns\n176     -------\n177     dayofyear\n178     \"\"\"\n179     return pd_object.dayofyear\n180 \n181 \n182 def _doy_to_datetimeindex(doy, epoch_year=2014):\n183     \"\"\"\n184     Convert a day of year scalar or array to a pd.DatetimeIndex.\n185 \n186     Parameters\n187     ----------\n188     doy : numeric\n189         Contains days of the year\n190 \n191     Returns\n192     -------\n193     pd.DatetimeIndex\n194     \"\"\"\n195     doy = np.atleast_1d(doy).astype('float')\n196     epoch = pd.Timestamp('{}-12-31'.format(epoch_year - 1))\n197     timestamps = [epoch + dt.timedelta(days=adoy) for adoy in doy]\n198     return pd.DatetimeIndex(timestamps)\n199 \n200 \n201 def _datetimelike_scalar_to_doy(time):\n202     return pd.DatetimeIndex([pd.Timestamp(time)]).dayofyear\n203 \n204 \n205 def _datetimelike_scalar_to_datetimeindex(time):\n206     return pd.DatetimeIndex([pd.Timestamp(time)])\n207 \n208 \n209 def _scalar_out(input):\n210     if np.isscalar(input):\n211         output = input\n212     else:  #\n213         # works if it's a 1 length array and\n214         # will throw a ValueError otherwise\n215         output = input.item()\n216 \n217     return output\n218 \n219 \n220 def _array_out(input):\n221     if isinstance(input, pd.Series):\n222         output = input.values\n223     else:\n224         output = input\n225 \n226     return output\n227 \n228 \n229 def _build_kwargs(keys, input_dict):\n230     \"\"\"\n231     Parameters\n232     ----------\n233     keys : iterable\n234         Typically a list of strings.\n235     adict : dict-like\n236         A dictionary from which to attempt to pull each key.\n237 \n238     Returns\n239     -------\n240     kwargs : dict\n241         A dictionary with only the keys that were in input_dict\n242     \"\"\"\n243 \n244     kwargs = {}\n245     for key in keys:\n246         try:\n247             kwargs[key] = input_dict[key]\n248         except KeyError:\n249             pass\n250 \n251     return kwargs\n252 \n253 \n254 # FIXME: remove _array_newton when SciPy-1.2.0 is released\n255 # pvlib.singlediode.bishop88_i_from_v(..., method='newton') and other\n256 # functions in singlediode call scipy.optimize.newton with a vector\n257 # unfortunately wrapping the functions with np.vectorize() was too slow\n258 # a vectorized newton method was merged into SciPy but isn't released yet, so\n259 # in the meantime, we just copied the relevant code: \"_array_newton\" for more\n260 # info see: https://github.com/scipy/scipy/pull/8357\n261 \n262 def _array_newton(func, x0, fprime, args, tol, maxiter, fprime2,\n263                   converged=False):\n264     \"\"\"\n265     A vectorized version of Newton, Halley, and secant methods for arrays. Do\n266     not use this method directly. This method is called from :func:`newton`\n267     when ``np.isscalar(x0)`` is true. For docstring, see :func:`newton`.\n268     \"\"\"\n269     try:\n270         p = np.asarray(x0, dtype=float)\n271     except TypeError:  # can't convert complex to float\n272         p = np.asarray(x0)\n273     failures = np.ones_like(p, dtype=bool)  # at start, nothing converged\n274     nz_der = np.copy(failures)\n275     if fprime is not None:\n276         # Newton-Raphson method\n277         for iteration in range(maxiter):\n278             # first evaluate fval\n279             fval = np.asarray(func(p, *args))\n280             # If all fval are 0, all roots have been found, then terminate\n281             if not fval.any():\n282                 failures = fval.astype(bool)\n283                 break\n284             fder = np.asarray(fprime(p, *args))\n285             nz_der = (fder != 0)\n286             # stop iterating if all derivatives are zero\n287             if not nz_der.any():\n288                 break\n289             # Newton step\n290             dp = fval[nz_der] / fder[nz_der]\n291             if fprime2 is not None:\n292                 fder2 = np.asarray(fprime2(p, *args))\n293                 dp = dp / (1.0 - 0.5 * dp * fder2[nz_der] / fder[nz_der])\n294             # only update nonzero derivatives\n295             p[nz_der] -= dp\n296             failures[nz_der] = np.abs(dp) >= tol  # items not yet converged\n297             # stop iterating if there aren't any failures, not incl zero der\n298             if not failures[nz_der].any():\n299                 break\n300     else:\n301         # Secant method\n302         dx = np.finfo(float).eps**0.33\n303         p1 = p * (1 + dx) + np.where(p >= 0, dx, -dx)\n304         q0 = np.asarray(func(p, *args))\n305         q1 = np.asarray(func(p1, *args))\n306         active = np.ones_like(p, dtype=bool)\n307         for iteration in range(maxiter):\n308             nz_der = (q1 != q0)\n309             # stop iterating if all derivatives are zero\n310             if not nz_der.any():\n311                 p = (p1 + p) / 2.0\n312                 break\n313             # Secant Step\n314             dp = (q1 * (p1 - p))[nz_der] / (q1 - q0)[nz_der]\n315             # only update nonzero derivatives\n316             p[nz_der] = p1[nz_der] - dp\n317             active_zero_der = ~nz_der & active\n318             p[active_zero_der] = (p1 + p)[active_zero_der] / 2.0\n319             active &= nz_der  # don't assign zero derivatives again\n320             failures[nz_der] = np.abs(dp) >= tol  # not yet converged\n321             # stop iterating if there aren't any failures, not incl zero der\n322             if not failures[nz_der].any():\n323                 break\n324             p1, p = p, p1\n325             q0 = q1\n326             q1 = np.asarray(func(p1, *args))\n327     zero_der = ~nz_der & failures  # don't include converged with zero-ders\n328     if zero_der.any():\n329         # secant warnings\n330         if fprime is None:\n331             nonzero_dp = (p1 != p)\n332             # non-zero dp, but infinite newton step\n333             zero_der_nz_dp = (zero_der & nonzero_dp)\n334             if zero_der_nz_dp.any():\n335                 rms = np.sqrt(\n336                     sum((p1[zero_der_nz_dp] - p[zero_der_nz_dp]) ** 2)\n337                 )\n338                 warnings.warn('RMS of {:g} reached'.format(rms),\n339                               RuntimeWarning)\n340         # newton or halley warnings\n341         else:\n342             all_or_some = 'all' if zero_der.all() else 'some'\n343             msg = '{:s} derivatives were zero'.format(all_or_some)\n344             warnings.warn(msg, RuntimeWarning)\n345     elif failures.any():\n346         all_or_some = 'all' if failures.all() else 'some'\n347         msg = '{0:s} failed to converge after {1:d} iterations'.format(\n348             all_or_some, maxiter\n349         )\n350         if failures.all():\n351             raise RuntimeError(msg)\n352         warnings.warn(msg, RuntimeWarning)\n353     if converged:\n354         result = namedtuple('result', ('root', 'converged', 'zero_der'))\n355         p = result(p, ~failures, zero_der)\n356     return p\n357 \n358 \n359 # Created April,2014\n360 # Author: Rob Andrews, Calama Consulting\n361 \n362 def _golden_sect_DataFrame(params, VL, VH, func):\n363     \"\"\"\n364     Vectorized golden section search for finding MPP from a dataframe\n365     timeseries.\n366 \n367     Parameters\n368     ----------\n369     params : dict\n370         Dictionary containing scalars or arrays\n371         of inputs to the function to be optimized.\n372         Each row should represent an independent optimization.\n373 \n374     VL: float\n375         Lower bound of the optimization\n376 \n377     VH: float\n378         Upper bound of the optimization\n379 \n380     func: function\n381         Function to be optimized must be in the form f(array-like, x)\n382 \n383     Returns\n384     -------\n385     func(df,'V1') : DataFrame\n386         function evaluated at the optimal point\n387 \n388     df['V1']: Dataframe\n389         Dataframe of optimal points\n390 \n391     Notes\n392     -----\n393     This function will find the MAXIMUM of a function\n394     \"\"\"\n395 \n396     df = params\n397     df['VH'] = VH\n398     df['VL'] = VL\n399 \n400     errflag = True\n401     iterations = 0\n402 \n403     while errflag:\n404 \n405         phi = (np.sqrt(5)-1)/2*(df['VH']-df['VL'])\n406         df['V1'] = df['VL'] + phi\n407         df['V2'] = df['VH'] - phi\n408 \n409         df['f1'] = func(df, 'V1')\n410         df['f2'] = func(df, 'V2')\n411         df['SW_Flag'] = df['f1'] > df['f2']\n412 \n413         df['VL'] = df['V2']*df['SW_Flag'] + df['VL']*(~df['SW_Flag'])\n414         df['VH'] = df['V1']*~df['SW_Flag'] + df['VH']*(df['SW_Flag'])\n415 \n416         err = df['V1'] - df['V2']\n417         try:\n418             errflag = (abs(err) > .01).any()\n419         except ValueError:\n420             errflag = (abs(err) > .01)\n421 \n422         iterations += 1\n423 \n424         if iterations > 50:\n425             raise Exception(\"EXCEPTION:iterations exceeded maximum (50)\")\n426 \n427     return func(df, 'V1'), df['V1']\n428 \n[end of pvlib/tools.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/singlediode.py\n+++ b/pvlib/singlediode.py\n@@ -71,6 +71,8 @@\n              resistance_shunt, nNsVth, d2mutau=0,\n              NsVbi=np.Inf, gradients=False):\n \n+    recombination_current = 0  # Initialize recombination current\n+\n     \"\"\"\n     Explicit calculation of points on the IV curve described by the single\n     diode equation [1]_.\n@@ -145,6 +147,8 @@\n     # calculate temporary values to simplify calculations\n     v_star = diode_voltage / nNsVth  # non-dimensional diode voltage\n     g_sh = 1.0 / resistance_shunt  # conductance\n+    recombination_current = i_recomb  # Capture recombination current\n     \n     i = (photocurrent - saturation_current * np.expm1(v_star)\n          - diode_voltage * g_sh - i_recomb)\n@@ -176,6 +180,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -204,6 +210,8 @@\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n+    recombination_current = 0  # Initialize recombination current\n     \n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n@@ -236,6 +244,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -298,6 +308,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -350,6 +362,8 @@\n     # Compute short circuit current\n     i_sc = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, 0.,\n                               saturation_current, photocurrent)\n+    recombination_current = 0  # Initialize recombination current\n     \n     # Compute open circuit voltage\n     v_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\n                               saturation_current, photocurrent)\n```\n", "model_patch": "--- a/pvlib/singlediode.py\n+++ b/pvlib/singlediode.py\n@@ -71,6 +71,8 @@\n              resistance_shunt, nNsVth, d2mutau=0,\n              NsVbi=np.Inf, gradients=False):\n \n+    recombination_current = 0  # Initialize recombination current\n+\n     \"\"\"\n     Explicit calculation of points on the IV curve described by the single\n     diode equation [1]_.\n@@ -145,6 +147,8 @@\n     # calculate temporary values to simplify calculations\n     v_star = diode_voltage / nNsVth  # non-dimensional diode voltage\n     g_sh = 1.0 / resistance_shunt  # conductance\n+    recombination_current = i_recomb  # Capture recombination current\n     \n     i = (photocurrent - saturation_current * np.expm1(v_star)\n          - diode_voltage * g_sh - i_recomb)\n@@ -176,6 +180,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -204,6 +210,8 @@\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n+    recombination_current = 0  # Initialize recombination current\n     \n     # first bound the search using voc\n     voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n@@ -236,6 +244,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -298,6 +308,8 @@\n                       method='newton'):\n \n     \"\"\"\n+    recombination_current = 0  # Initialize recombination current\n+\n     # collect args\n     args = (photocurrent, saturation_current, resistance_series,\n             resistance_shunt, nNsVth)\n@@ -350,6 +362,8 @@\n     # Compute short circuit current\n     i_sc = _lambertw_i_from_v(resistance_shunt, resistance_series, nNsVth, 0.,\n                               saturation_current, photocurrent)\n+    recombination_current = 0  # Initialize recombination current\n     \n     # Compute open circuit voltage\n     v_oc = _lambertw_v_from_i(resistance_shunt, resistance_series, nNsVth, 0.,\n                               saturation_current, photocurrent)\n"}
{"instance_id": "pvlib__pvlib-python-1218", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nchange eta_m to module_efficiency\n`temperature.noct_sam` uses `eta_m_ref` to describe the module efficiency at reference conditions and `temperature.pvsyst_cell` uses `eta_m` to describe the module efficiency generically.\r\n\r\nJust calling both of these `module_efficiency` would make the function signatures easily understandable by many more people. I'd be ok with `module_efficiency_ref` but I don't think that precision is very important.\r\n\r\nI skimmed [pvterms](https://duramat.github.io/pv-terms/) and didn't see a suggestion for this quantity.\r\n\r\n`temperature.noct_sam` has not yet been released and it's just a positional argument, so changing the name is trivial. `temperature.pvsyst_cell` would need a deprecation cycle.\r\n\r\nOriginally discussed in https://github.com/pvlib/pvlib-python/pull/1177#discussion_r589081257\r\n\r\nAssignment of milestone indicates that we will act on this or close it forever before 0.9 is released.\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n32       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n33     </a>\n34   </td>\n35 </tr>\n36 <tr>\n37   <td>Code Quality</td>\n38  \u00a0<td>\n39     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n40     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n41     </a>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n43     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Coverage</td>\n49  \u00a0<td>\n50     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n51     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n52     </a>\n53     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n54     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Publications</td>\n60   <td>\n61     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n62     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n63     </a>\n64     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n65     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n66     </a>\n67   </td>\n68 </tr>\n69 <tr>\n70   <td>Downloads</td>\n71   <td>\n72     <a href=\"https://pypi.org/project/pvlib/\">\n73     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n74     </a>\n75     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n76     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n77     </a>\n78   </td>\n79 </tr>\n80 </table>\n81 \n82 \n83 pvlib python is a community supported tool that provides a set of\n84 functions and classes for simulating the performance of photovoltaic\n85 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n86 toolbox developed at Sandia National Laboratories and it implements many\n87 of the models and methods developed at the Labs. More information on\n88 Sandia Labs PV performance modeling programs can be found at\n89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n90 but operate independently of it.\n91 \n92 \n93 Documentation\n94 =============\n95 \n96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n97 \n98 \n99 Installation\n100 ============\n101 \n102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n104 \n105 \n106 Contributing\n107 ============\n108 \n109 We need your help to make pvlib-python a great tool!\n110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n111 The long-term success of pvlib-python requires substantial community support.\n112 \n113 \n114 License\n115 =======\n116 \n117 BSD 3-clause\n118 \n119 \n120 Getting support\n121 ===============\n122 \n123 pvlib usage questions can be asked on\n124 [Stack Overflow](http://stackoverflow.com) and tagged with\n125 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n126 \n127 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n128 is used for discussing various topics of interest to the pvlib-python\n129 community. We also make new version announcements on the google group.\n130 \n131 If you suspect that you may have discovered a bug or if you'd like to\n132 change something about pvlib, then please make an issue on our\n133 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n134 \n135 \n136 Citing\n137 ======\n138 \n139 If you use pvlib-python in a published work, please cite:\n140 \n141   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n142   \"pvlib python: a python package for modeling solar energy systems.\"\n143   Journal of Open Source Software, 3(29), 884, (2018).\n144   https://doi.org/10.21105/joss.00884\n145 \n146 Please also cite the DOI corresponding to the specific version of\n147 pvlib-python that you used. pvlib-python DOIs are listed at\n148 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n149 \n150 NumFOCUS\n151 ========\n152 \n153 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n154 \n155 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n156 \n[end of README.md]\n[start of pvlib/_version.py]\n1 \n2 # This file helps to compute a version number in source trees obtained from\n3 # git-archive tarball (such as those provided by githubs download-from-tag\n4 # feature). Distribution tarballs (built by setup.py sdist) and build\n5 # directories (produced by setup.py build) will contain a much shorter file\n6 # that just contains the computed version number.\n7 \n8 # This file is released into the public domain. Generated by\n9 # versioneer-0.16 (https://github.com/warner/python-versioneer)\n10 \n11 \"\"\"Git implementation of _version.py.\"\"\"\n12 \n13 import errno\n14 import os\n15 import re\n16 import subprocess\n17 import sys\n18 \n19 \n20 def get_keywords():\n21     \"\"\"Get the keywords needed to look up the version information.\"\"\"\n22     # these strings will be replaced by git during git-archive.\n23     # setup.py/versioneer.py will grep for the variable names, so they must\n24     # each be defined on a line of their own. _version.py will just call\n25     # get_keywords().\n26     git_refnames = \"$Format:%d$\"\n27     git_full = \"$Format:%H$\"\n28     keywords = {\"refnames\": git_refnames, \"full\": git_full}\n29     return keywords\n30 \n31 \n32 class VersioneerConfig:\n33     \"\"\"Container for Versioneer configuration parameters.\"\"\"\n34 \n35 \n36 def get_config():\n37     \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n38     # these strings are filled in when 'setup.py versioneer' creates\n39     # _version.py\n40     cfg = VersioneerConfig()\n41     cfg.VCS = \"git\"\n42     cfg.style = \"pep440\"\n43     cfg.tag_prefix = \"v\"\n44     cfg.parentdir_prefix = \"pvlib-python-\"\n45     cfg.versionfile_source = \"pvlib/_version.py\"\n46     cfg.verbose = False\n47     return cfg\n48 \n49 \n50 class NotThisMethod(Exception):\n51     \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n52 \n53 \n54 LONG_VERSION_PY = {}\n55 HANDLERS = {}\n56 \n57 \n58 def register_vcs_handler(vcs, method):  # decorator\n59     \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n60     def decorate(f):\n61         \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n62         if vcs not in HANDLERS:\n63             HANDLERS[vcs] = {}\n64         HANDLERS[vcs][method] = f\n65         return f\n66     return decorate\n67 \n68 \n69 def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n70     \"\"\"Call the given command(s).\"\"\"\n71     assert isinstance(commands, list)\n72     p = None\n73     for c in commands:\n74         try:\n75             dispcmd = str([c] + args)\n76             # remember shell=False, so use git.cmd on windows, not just git\n77             p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n78                                  stderr=(subprocess.PIPE if hide_stderr\n79                                          else None))\n80             break\n81         except EnvironmentError:\n82             e = sys.exc_info()[1]\n83             if e.errno == errno.ENOENT:\n84                 continue\n85             if verbose:\n86                 print(\"unable to run %s\" % dispcmd)\n87                 print(e)\n88             return None\n89     else:\n90         if verbose:\n91             print(\"unable to find command, tried %s\" % (commands,))\n92         return None\n93     stdout = p.communicate()[0].strip()\n94     if sys.version_info[0] >= 3:\n95         stdout = stdout.decode()\n96     if p.returncode != 0:\n97         if verbose:\n98             print(\"unable to run %s (error)\" % dispcmd)\n99         return None\n100     return stdout\n101 \n102 \n103 def versions_from_parentdir(parentdir_prefix, root, verbose):\n104     \"\"\"Try to determine the version from the parent directory name.\n105 \n106     Source tarballs conventionally unpack into a directory that includes\n107     both the project name and a version string.\n108     \"\"\"\n109     dirname = os.path.basename(root)\n110     if not dirname.startswith(parentdir_prefix):\n111         if verbose:\n112             print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n113                   \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n114         raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n115     return {\"version\": dirname[len(parentdir_prefix):],\n116             \"full-revisionid\": None,\n117             \"dirty\": False, \"error\": None}\n118 \n119 \n120 @register_vcs_handler(\"git\", \"get_keywords\")\n121 def git_get_keywords(versionfile_abs):\n122     \"\"\"Extract version information from the given file.\"\"\"\n123     # the code embedded in _version.py can just fetch the value of these\n124     # keywords. When used from setup.py, we don't want to import _version.py,\n125     # so we do it with a regexp instead. This function is not used from\n126     # _version.py.\n127     keywords = {}\n128     try:\n129         f = open(versionfile_abs, \"r\")\n130         for line in f.readlines():\n131             if line.strip().startswith(\"git_refnames =\"):\n132                 mo = re.search(r'=\\s*\"(.*)\"', line)\n133                 if mo:\n134                     keywords[\"refnames\"] = mo.group(1)\n135             if line.strip().startswith(\"git_full =\"):\n136                 mo = re.search(r'=\\s*\"(.*)\"', line)\n137                 if mo:\n138                     keywords[\"full\"] = mo.group(1)\n139         f.close()\n140     except EnvironmentError:\n141         pass\n142     return keywords\n143 \n144 \n145 @register_vcs_handler(\"git\", \"keywords\")\n146 def git_versions_from_keywords(keywords, tag_prefix, verbose):\n147     \"\"\"Get version information from git keywords.\"\"\"\n148     if not keywords:\n149         raise NotThisMethod(\"no keywords at all, weird\")\n150     refnames = keywords[\"refnames\"].strip()\n151     if refnames.startswith(\"$Format\"):\n152         if verbose:\n153             print(\"keywords are unexpanded, not using\")\n154         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n155     refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n156     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n157     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n158     TAG = \"tag: \"\n159     tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n160     if not tags:\n161         # Either we're using git < 1.8.3, or there really are no tags. We use\n162         # a heuristic: assume all version tags have a digit. The old git %d\n163         # expansion behaves like git log --decorate=short and strips out the\n164         # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n165         # between branches and tags. By ignoring refnames without digits, we\n166         # filter out many common branch names like \"release\" and\n167         # \"stabilization\", as well as \"HEAD\" and \"master\".\n168         tags = set([r for r in refs if re.search(r'\\d', r)])\n169         if verbose:\n170             print(\"discarding '%s', no digits\" % \",\".join(refs-tags))\n171     if verbose:\n172         print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n173     for ref in sorted(tags):\n174         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n175         if ref.startswith(tag_prefix):\n176             r = ref[len(tag_prefix):]\n177             if verbose:\n178                 print(\"picking %s\" % r)\n179             return {\"version\": r,\n180                     \"full-revisionid\": keywords[\"full\"].strip(),\n181                     \"dirty\": False, \"error\": None\n182                     }\n183     # no suitable tags, so version is \"0+unknown\", but full hex is still there\n184     if verbose:\n185         print(\"no suitable tags, using unknown + full revision id\")\n186     return {\"version\": \"0+unknown\",\n187             \"full-revisionid\": keywords[\"full\"].strip(),\n188             \"dirty\": False, \"error\": \"no suitable tags\"}\n189 \n190 \n191 @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n192 def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n193     \"\"\"Get version from 'git describe' in the root of the source tree.\n194 \n195     This only gets called if the git-archive 'subst' keywords were *not*\n196     expanded, and _version.py hasn't already been rewritten with a short\n197     version string, meaning we're inside a checked out source tree.\n198     \"\"\"\n199     if not os.path.exists(os.path.join(root, \".git\")):\n200         if verbose:\n201             print(\"no .git in %s\" % root)\n202         raise NotThisMethod(\"no .git directory\")\n203 \n204     GITS = [\"git\"]\n205     if sys.platform == \"win32\":\n206         GITS = [\"git.cmd\", \"git.exe\"]\n207     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n208     # if there isn't one, this yields HEX[-dirty] (no NUM)\n209     describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n210                                       \"--always\", \"--long\",\n211                                       \"--match\", \"%s*\" % tag_prefix],\n212                                cwd=root)\n213     # --long was added in git-1.5.5\n214     if describe_out is None:\n215         raise NotThisMethod(\"'git describe' failed\")\n216     describe_out = describe_out.strip()\n217     full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n218     if full_out is None:\n219         raise NotThisMethod(\"'git rev-parse' failed\")\n220     full_out = full_out.strip()\n221 \n222     pieces = {}\n223     pieces[\"long\"] = full_out\n224     pieces[\"short\"] = full_out[:7]  # maybe improved later\n225     pieces[\"error\"] = None\n226 \n227     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n228     # TAG might have hyphens.\n229     git_describe = describe_out\n230 \n231     # look for -dirty suffix\n232     dirty = git_describe.endswith(\"-dirty\")\n233     pieces[\"dirty\"] = dirty\n234     if dirty:\n235         git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n236 \n237     # now we have TAG-NUM-gHEX or HEX\n238 \n239     if \"-\" in git_describe:\n240         # TAG-NUM-gHEX\n241         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n242         if not mo:\n243             # unparseable. Maybe git-describe is misbehaving?\n244             pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n245                                % describe_out)\n246             return pieces\n247 \n248         # tag\n249         full_tag = mo.group(1)\n250         if not full_tag.startswith(tag_prefix):\n251             if verbose:\n252                 fmt = \"tag '%s' doesn't start with prefix '%s'\"\n253                 print(fmt % (full_tag, tag_prefix))\n254             pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n255                                % (full_tag, tag_prefix))\n256             return pieces\n257         pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n258 \n259         # distance: number of commits since tag\n260         pieces[\"distance\"] = int(mo.group(2))\n261 \n262         # commit: short hex revision ID\n263         pieces[\"short\"] = mo.group(3)\n264 \n265     else:\n266         # HEX: no tags\n267         pieces[\"closest-tag\"] = None\n268         count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n269                                 cwd=root)\n270         pieces[\"distance\"] = int(count_out)  # total number of commits\n271 \n272     return pieces\n273 \n274 \n275 def plus_or_dot(pieces):\n276     \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n277     if \"+\" in pieces.get(\"closest-tag\", \"\"):\n278         return \".\"\n279     return \"+\"\n280 \n281 \n282 def render_pep440(pieces):\n283     \"\"\"Build up version string, with post-release \"local version identifier\".\n284 \n285     Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n286     get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n287 \n288     Exceptions:\n289     1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n290     \"\"\"\n291     if pieces[\"closest-tag\"]:\n292         rendered = pieces[\"closest-tag\"]\n293         if pieces[\"distance\"] or pieces[\"dirty\"]:\n294             rendered += plus_or_dot(pieces)\n295             rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n296             if pieces[\"dirty\"]:\n297                 rendered += \".dirty\"\n298     else:\n299         # exception #1\n300         rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n301                                           pieces[\"short\"])\n302         if pieces[\"dirty\"]:\n303             rendered += \".dirty\"\n304     return rendered\n305 \n306 \n307 def render_pep440_pre(pieces):\n308     \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n309 \n310     Exceptions:\n311     1: no tags. 0.post.devDISTANCE\n312     \"\"\"\n313     if pieces[\"closest-tag\"]:\n314         rendered = pieces[\"closest-tag\"]\n315         if pieces[\"distance\"]:\n316             rendered += \".post.dev%d\" % pieces[\"distance\"]\n317     else:\n318         # exception #1\n319         rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n320     return rendered\n321 \n322 \n323 def render_pep440_post(pieces):\n324     \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n325 \n326     The \".dev0\" means dirty. Note that .dev0 sorts backwards\n327     (a dirty tree will appear \"older\" than the corresponding clean one),\n328     but you shouldn't be releasing software with -dirty anyways.\n329 \n330     Exceptions:\n331     1: no tags. 0.postDISTANCE[.dev0]\n332     \"\"\"\n333     if pieces[\"closest-tag\"]:\n334         rendered = pieces[\"closest-tag\"]\n335         if pieces[\"distance\"] or pieces[\"dirty\"]:\n336             rendered += \".post%d\" % pieces[\"distance\"]\n337             if pieces[\"dirty\"]:\n338                 rendered += \".dev0\"\n339             rendered += plus_or_dot(pieces)\n340             rendered += \"g%s\" % pieces[\"short\"]\n341     else:\n342         # exception #1\n343         rendered = \"0.post%d\" % pieces[\"distance\"]\n344         if pieces[\"dirty\"]:\n345             rendered += \".dev0\"\n346         rendered += \"+g%s\" % pieces[\"short\"]\n347     return rendered\n348 \n349 \n350 def render_pep440_old(pieces):\n351     \"\"\"TAG[.postDISTANCE[.dev0]] .\n352 \n353     The \".dev0\" means dirty.\n354 \n355     Eexceptions:\n356     1: no tags. 0.postDISTANCE[.dev0]\n357     \"\"\"\n358     if pieces[\"closest-tag\"]:\n359         rendered = pieces[\"closest-tag\"]\n360         if pieces[\"distance\"] or pieces[\"dirty\"]:\n361             rendered += \".post%d\" % pieces[\"distance\"]\n362             if pieces[\"dirty\"]:\n363                 rendered += \".dev0\"\n364     else:\n365         # exception #1\n366         rendered = \"0.post%d\" % pieces[\"distance\"]\n367         if pieces[\"dirty\"]:\n368             rendered += \".dev0\"\n369     return rendered\n370 \n371 \n372 def render_git_describe(pieces):\n373     \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n374 \n375     Like 'git describe --tags --dirty --always'.\n376 \n377     Exceptions:\n378     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n379     \"\"\"\n380     if pieces[\"closest-tag\"]:\n381         rendered = pieces[\"closest-tag\"]\n382         if pieces[\"distance\"]:\n383             rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n384     else:\n385         # exception #1\n386         rendered = pieces[\"short\"]\n387     if pieces[\"dirty\"]:\n388         rendered += \"-dirty\"\n389     return rendered\n390 \n391 \n392 def render_git_describe_long(pieces):\n393     \"\"\"TAG-DISTANCE-gHEX[-dirty].\n394 \n395     Like 'git describe --tags --dirty --always -long'.\n396     The distance/hash is unconditional.\n397 \n398     Exceptions:\n399     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n400     \"\"\"\n401     if pieces[\"closest-tag\"]:\n402         rendered = pieces[\"closest-tag\"]\n403         rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n404     else:\n405         # exception #1\n406         rendered = pieces[\"short\"]\n407     if pieces[\"dirty\"]:\n408         rendered += \"-dirty\"\n409     return rendered\n410 \n411 \n412 def render(pieces, style):\n413     \"\"\"Render the given version pieces into the requested style.\"\"\"\n414     if pieces[\"error\"]:\n415         return {\"version\": \"unknown\",\n416                 \"full-revisionid\": pieces.get(\"long\"),\n417                 \"dirty\": None,\n418                 \"error\": pieces[\"error\"]}\n419 \n420     if not style or style == \"default\":\n421         style = \"pep440\"  # the default\n422 \n423     if style == \"pep440\":\n424         rendered = render_pep440(pieces)\n425     elif style == \"pep440-pre\":\n426         rendered = render_pep440_pre(pieces)\n427     elif style == \"pep440-post\":\n428         rendered = render_pep440_post(pieces)\n429     elif style == \"pep440-old\":\n430         rendered = render_pep440_old(pieces)\n431     elif style == \"git-describe\":\n432         rendered = render_git_describe(pieces)\n433     elif style == \"git-describe-long\":\n434         rendered = render_git_describe_long(pieces)\n435     else:\n436         raise ValueError(\"unknown style '%s'\" % style)\n437 \n438     return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n439             \"dirty\": pieces[\"dirty\"], \"error\": None}\n440 \n441 \n442 def get_versions():\n443     \"\"\"Get version information or return default if unable to do so.\"\"\"\n444     # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n445     # __file__, we can work backwards from there to the root. Some\n446     # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n447     # case we can only use expanded keywords.\n448 \n449     cfg = get_config()\n450     verbose = cfg.verbose\n451 \n452     try:\n453         return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n454                                           verbose)\n455     except NotThisMethod:\n456         pass\n457 \n458     try:\n459         root = os.path.realpath(__file__)\n460         # versionfile_source is the relative path from the top of the source\n461         # tree (where the .git directory might live) to this file. Invert\n462         # this to find the root from __file__.\n463         for i in cfg.versionfile_source.split('/'):\n464             root = os.path.dirname(root)\n465     except NameError:\n466         return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n467                 \"dirty\": None,\n468                 \"error\": \"unable to find root of source tree\"}\n469 \n470     try:\n471         pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n472         return render(pieces, cfg.style)\n473     except NotThisMethod:\n474         pass\n475 \n476     try:\n477         if cfg.parentdir_prefix:\n478             return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n479     except NotThisMethod:\n480         pass\n481 \n482     return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n483             \"dirty\": None,\n484             \"error\": \"unable to compute version\"}\n485 \n[end of pvlib/_version.py]\n[start of versioneer.py]\n1 \n2 # Version: 0.16\n3 \n4 \"\"\"The Versioneer - like a rocketeer, but for versions.\n5 \n6 The Versioneer\n7 ==============\n8 \n9 * like a rocketeer, but for versions!\n10 * https://github.com/warner/python-versioneer\n11 * Brian Warner\n12 * License: Public Domain\n13 * Compatible With: python2.6, 2.7, 3.3, 3.4, 3.5, and pypy\n14 * [![Latest Version]\n15 (https://pypip.in/version/versioneer/badge.svg?style=flat)\n16 ](https://pypi.python.org/pypi/versioneer/)\n17 * [![Build Status]\n18 (https://travis-ci.org/warner/python-versioneer.png?branch=master)\n19 ](https://travis-ci.org/warner/python-versioneer)\n20 \n21 This is a tool for managing a recorded version number in distutils-based\n22 python projects. The goal is to remove the tedious and error-prone \"update\n23 the embedded version string\" step from your release process. Making a new\n24 release should be as easy as recording a new tag in your version-control\n25 system, and maybe making new tarballs.\n26 \n27 \n28 ## Quick Install\n29 \n30 * `pip install versioneer` to somewhere to your $PATH\n31 * add a `[versioneer]` section to your setup.cfg (see below)\n32 * run `versioneer install` in your source tree, commit the results\n33 \n34 ## Version Identifiers\n35 \n36 Source trees come from a variety of places:\n37 \n38 * a version-control system checkout (mostly used by developers)\n39 * a nightly tarball, produced by build automation\n40 * a snapshot tarball, produced by a web-based VCS browser, like github's\n41   \"tarball from tag\" feature\n42 * a release tarball, produced by \"setup.py sdist\", distributed through PyPI\n43 \n44 Within each source tree, the version identifier (either a string or a number,\n45 this tool is format-agnostic) can come from a variety of places:\n46 \n47 * ask the VCS tool itself, e.g. \"git describe\" (for checkouts), which knows\n48   about recent \"tags\" and an absolute revision-id\n49 * the name of the directory into which the tarball was unpacked\n50 * an expanded VCS keyword ($Id$, etc)\n51 * a `_version.py` created by some earlier build step\n52 \n53 For released software, the version identifier is closely related to a VCS\n54 tag. Some projects use tag names that include more than just the version\n55 string (e.g. \"myproject-1.2\" instead of just \"1.2\"), in which case the tool\n56 needs to strip the tag prefix to extract the version identifier. For\n57 unreleased software (between tags), the version identifier should provide\n58 enough information to help developers recreate the same tree, while also\n59 giving them an idea of roughly how old the tree is (after version 1.2, before\n60 version 1.3). Many VCS systems can report a description that captures this,\n61 for example `git describe --tags --dirty --always` reports things like\n62 \"0.7-1-g574ab98-dirty\" to indicate that the checkout is one revision past the\n63 0.7 tag, has a unique revision id of \"574ab98\", and is \"dirty\" (it has\n64 uncommitted changes.\n65 \n66 The version identifier is used for multiple purposes:\n67 \n68 * to allow the module to self-identify its version: `myproject.__version__`\n69 * to choose a name and prefix for a 'setup.py sdist' tarball\n70 \n71 ## Theory of Operation\n72 \n73 Versioneer works by adding a special `_version.py` file into your source\n74 tree, where your `__init__.py` can import it. This `_version.py` knows how to\n75 dynamically ask the VCS tool for version information at import time.\n76 \n77 `_version.py` also contains `$Revision$` markers, and the installation\n78 process marks `_version.py` to have this marker rewritten with a tag name\n79 during the `git archive` command. As a result, generated tarballs will\n80 contain enough information to get the proper version.\n81 \n82 To allow `setup.py` to compute a version too, a `versioneer.py` is added to\n83 the top level of your source tree, next to `setup.py` and the `setup.cfg`\n84 that configures it. This overrides several distutils/setuptools commands to\n85 compute the version when invoked, and changes `setup.py build` and `setup.py\n86 sdist` to replace `_version.py` with a small static file that contains just\n87 the generated version data.\n88 \n89 ## Installation\n90 \n91 First, decide on values for the following configuration variables:\n92 \n93 * `VCS`: the version control system you use. Currently accepts \"git\".\n94 \n95 * `style`: the style of version string to be produced. See \"Styles\" below for\n96   details. Defaults to \"pep440\", which looks like\n97   `TAG[+DISTANCE.gSHORTHASH[.dirty]]`.\n98 \n99 * `versionfile_source`:\n100 \n101   A project-relative pathname into which the generated version strings should\n102   be written. This is usually a `_version.py` next to your project's main\n103   `__init__.py` file, so it can be imported at runtime. If your project uses\n104   `src/myproject/__init__.py`, this should be `src/myproject/_version.py`.\n105   This file should be checked in to your VCS as usual: the copy created below\n106   by `setup.py setup_versioneer` will include code that parses expanded VCS\n107   keywords in generated tarballs. The 'build' and 'sdist' commands will\n108   replace it with a copy that has just the calculated version string.\n109 \n110   This must be set even if your project does not have any modules (and will\n111   therefore never import `_version.py`), since \"setup.py sdist\" -based trees\n112   still need somewhere to record the pre-calculated version strings. Anywhere\n113   in the source tree should do. If there is a `__init__.py` next to your\n114   `_version.py`, the `setup.py setup_versioneer` command (described below)\n115   will append some `__version__`-setting assignments, if they aren't already\n116   present.\n117 \n118 * `versionfile_build`:\n119 \n120   Like `versionfile_source`, but relative to the build directory instead of\n121   the source directory. These will differ when your setup.py uses\n122   'package_dir='. If you have `package_dir={'myproject': 'src/myproject'}`,\n123   then you will probably have `versionfile_build='myproject/_version.py'` and\n124   `versionfile_source='src/myproject/_version.py'`.\n125 \n126   If this is set to None, then `setup.py build` will not attempt to rewrite\n127   any `_version.py` in the built tree. If your project does not have any\n128   libraries (e.g. if it only builds a script), then you should use\n129   `versionfile_build = None`. To actually use the computed version string,\n130   your `setup.py` will need to override `distutils.command.build_scripts`\n131   with a subclass that explicitly inserts a copy of\n132   `versioneer.get_version()` into your script file. See\n133   `test/demoapp-script-only/setup.py` for an example.\n134 \n135 * `tag_prefix`:\n136 \n137   a string, like 'PROJECTNAME-', which appears at the start of all VCS tags.\n138   If your tags look like 'myproject-1.2.0', then you should use\n139   tag_prefix='myproject-'. If you use unprefixed tags like '1.2.0', this\n140   should be an empty string, using either `tag_prefix=` or `tag_prefix=''`.\n141 \n142 * `parentdir_prefix`:\n143 \n144   a optional string, frequently the same as tag_prefix, which appears at the\n145   start of all unpacked tarball filenames. If your tarball unpacks into\n146   'myproject-1.2.0', this should be 'myproject-'. To disable this feature,\n147   just omit the field from your `setup.cfg`.\n148 \n149 This tool provides one script, named `versioneer`. That script has one mode,\n150 \"install\", which writes a copy of `versioneer.py` into the current directory\n151 and runs `versioneer.py setup` to finish the installation.\n152 \n153 To versioneer-enable your project:\n154 \n155 * 1: Modify your `setup.cfg`, adding a section named `[versioneer]` and\n156   populating it with the configuration values you decided earlier (note that\n157   the option names are not case-sensitive):\n158 \n159   ````\n160   [versioneer]\n161   VCS = git\n162   style = pep440\n163   versionfile_source = src/myproject/_version.py\n164   versionfile_build = myproject/_version.py\n165   tag_prefix =\n166   parentdir_prefix = myproject-\n167   ````\n168 \n169 * 2: Run `versioneer install`. This will do the following:\n170 \n171   * copy `versioneer.py` into the top of your source tree\n172   * create `_version.py` in the right place (`versionfile_source`)\n173   * modify your `__init__.py` (if one exists next to `_version.py`) to define\n174     `__version__` (by calling a function from `_version.py`)\n175   * modify your `MANIFEST.in` to include both `versioneer.py` and the\n176     generated `_version.py` in sdist tarballs\n177 \n178   `versioneer install` will complain about any problems it finds with your\n179   `setup.py` or `setup.cfg`. Run it multiple times until you have fixed all\n180   the problems.\n181 \n182 * 3: add a `import versioneer` to your setup.py, and add the following\n183   arguments to the setup() call:\n184 \n185         version=versioneer.get_version(),\n186         cmdclass=versioneer.get_cmdclass(),\n187 \n188 * 4: commit these changes to your VCS. To make sure you won't forget,\n189   `versioneer install` will mark everything it touched for addition using\n190   `git add`. Don't forget to add `setup.py` and `setup.cfg` too.\n191 \n192 ## Post-Installation Usage\n193 \n194 Once established, all uses of your tree from a VCS checkout should get the\n195 current version string. All generated tarballs should include an embedded\n196 version string (so users who unpack them will not need a VCS tool installed).\n197 \n198 If you distribute your project through PyPI, then the release process should\n199 boil down to two steps:\n200 \n201 * 1: git tag 1.0\n202 * 2: python setup.py register sdist upload\n203 \n204 If you distribute it through github (i.e. users use github to generate\n205 tarballs with `git archive`), the process is:\n206 \n207 * 1: git tag 1.0\n208 * 2: git push; git push --tags\n209 \n210 Versioneer will report \"0+untagged.NUMCOMMITS.gHASH\" until your tree has at\n211 least one tag in its history.\n212 \n213 ## Version-String Flavors\n214 \n215 Code which uses Versioneer can learn about its version string at runtime by\n216 importing `_version` from your main `__init__.py` file and running the\n217 `get_versions()` function. From the \"outside\" (e.g. in `setup.py`), you can\n218 import the top-level `versioneer.py` and run `get_versions()`.\n219 \n220 Both functions return a dictionary with different flavors of version\n221 information:\n222 \n223 * `['version']`: A condensed version string, rendered using the selected\n224   style. This is the most commonly used value for the project's version\n225   string. The default \"pep440\" style yields strings like `0.11`,\n226   `0.11+2.g1076c97`, or `0.11+2.g1076c97.dirty`. See the \"Styles\" section\n227   below for alternative styles.\n228 \n229 * `['full-revisionid']`: detailed revision identifier. For Git, this is the\n230   full SHA1 commit id, e.g. \"1076c978a8d3cfc70f408fe5974aa6c092c949ac\".\n231 \n232 * `['dirty']`: a boolean, True if the tree has uncommitted changes. Note that\n233   this is only accurate if run in a VCS checkout, otherwise it is likely to\n234   be False or None\n235 \n236 * `['error']`: if the version string could not be computed, this will be set\n237   to a string describing the problem, otherwise it will be None. It may be\n238   useful to throw an exception in setup.py if this is set, to avoid e.g.\n239   creating tarballs with a version string of \"unknown\".\n240 \n241 Some variants are more useful than others. Including `full-revisionid` in a\n242 bug report should allow developers to reconstruct the exact code being tested\n243 (or indicate the presence of local changes that should be shared with the\n244 developers). `version` is suitable for display in an \"about\" box or a CLI\n245 `--version` output: it can be easily compared against release notes and lists\n246 of bugs fixed in various releases.\n247 \n248 The installer adds the following text to your `__init__.py` to place a basic\n249 version in `YOURPROJECT.__version__`:\n250 \n251     from ._version import get_versions\n252     __version__ = get_versions()['version']\n253     del get_versions\n254 \n255 ## Styles\n256 \n257 The setup.cfg `style=` configuration controls how the VCS information is\n258 rendered into a version string.\n259 \n260 The default style, \"pep440\", produces a PEP440-compliant string, equal to the\n261 un-prefixed tag name for actual releases, and containing an additional \"local\n262 version\" section with more detail for in-between builds. For Git, this is\n263 TAG[+DISTANCE.gHEX[.dirty]] , using information from `git describe --tags\n264 --dirty --always`. For example \"0.11+2.g1076c97.dirty\" indicates that the\n265 tree is like the \"1076c97\" commit but has uncommitted changes (\".dirty\"), and\n266 that this commit is two revisions (\"+2\") beyond the \"0.11\" tag. For released\n267 software (exactly equal to a known tag), the identifier will only contain the\n268 stripped tag, e.g. \"0.11\".\n269 \n270 Other styles are available. See details.md in the Versioneer source tree for\n271 descriptions.\n272 \n273 ## Debugging\n274 \n275 Versioneer tries to avoid fatal errors: if something goes wrong, it will tend\n276 to return a version of \"0+unknown\". To investigate the problem, run `setup.py\n277 version`, which will run the version-lookup code in a verbose mode, and will\n278 display the full contents of `get_versions()` (including the `error` string,\n279 which may help identify what went wrong).\n280 \n281 ## Updating Versioneer\n282 \n283 To upgrade your project to a new release of Versioneer, do the following:\n284 \n285 * install the new Versioneer (`pip install -U versioneer` or equivalent)\n286 * edit `setup.cfg`, if necessary, to include any new configuration settings\n287   indicated by the release notes\n288 * re-run `versioneer install` in your source tree, to replace\n289   `SRC/_version.py`\n290 * commit any changed files\n291 \n292 ### Upgrading to 0.16\n293 \n294 Nothing special.\n295 \n296 ### Upgrading to 0.15\n297 \n298 Starting with this version, Versioneer is configured with a `[versioneer]`\n299 section in your `setup.cfg` file. Earlier versions required the `setup.py` to\n300 set attributes on the `versioneer` module immediately after import. The new\n301 version will refuse to run (raising an exception during import) until you\n302 have provided the necessary `setup.cfg` section.\n303 \n304 In addition, the Versioneer package provides an executable named\n305 `versioneer`, and the installation process is driven by running `versioneer\n306 install`. In 0.14 and earlier, the executable was named\n307 `versioneer-installer` and was run without an argument.\n308 \n309 ### Upgrading to 0.14\n310 \n311 0.14 changes the format of the version string. 0.13 and earlier used\n312 hyphen-separated strings like \"0.11-2-g1076c97-dirty\". 0.14 and beyond use a\n313 plus-separated \"local version\" section strings, with dot-separated\n314 components, like \"0.11+2.g1076c97\". PEP440-strict tools did not like the old\n315 format, but should be ok with the new one.\n316 \n317 ### Upgrading from 0.11 to 0.12\n318 \n319 Nothing special.\n320 \n321 ### Upgrading from 0.10 to 0.11\n322 \n323 You must add a `versioneer.VCS = \"git\"` to your `setup.py` before re-running\n324 `setup.py setup_versioneer`. This will enable the use of additional\n325 version-control systems (SVN, etc) in the future.\n326 \n327 ## Future Directions\n328 \n329 This tool is designed to make it easily extended to other version-control\n330 systems: all VCS-specific components are in separate directories like\n331 src/git/ . The top-level `versioneer.py` script is assembled from these\n332 components by running make-versioneer.py . In the future, make-versioneer.py\n333 will take a VCS name as an argument, and will construct a version of\n334 `versioneer.py` that is specific to the given VCS. It might also take the\n335 configuration arguments that are currently provided manually during\n336 installation by editing setup.py . Alternatively, it might go the other\n337 direction and include code from all supported VCS systems, reducing the\n338 number of intermediate scripts.\n339 \n340 \n341 ## License\n342 \n343 To make Versioneer easier to embed, all its code is dedicated to the public\n344 domain. The `_version.py` that it creates is also in the public domain.\n345 Specifically, both are released under the Creative Commons \"Public Domain\n346 Dedication\" license (CC0-1.0), as described in\n347 https://creativecommons.org/publicdomain/zero/1.0/ .\n348 \n349 \"\"\"\n350 \n351 from __future__ import print_function\n352 try:\n353     import configparser\n354 except ImportError:\n355     import ConfigParser as configparser\n356 import errno\n357 import json\n358 import os\n359 import re\n360 import subprocess\n361 import sys\n362 \n363 \n364 class VersioneerConfig:\n365     \"\"\"Container for Versioneer configuration parameters.\"\"\"\n366 \n367 \n368 def get_root():\n369     \"\"\"Get the project root directory.\n370 \n371     We require that all commands are run from the project root, i.e. the\n372     directory that contains setup.py, setup.cfg, and versioneer.py .\n373     \"\"\"\n374     root = os.path.realpath(os.path.abspath(os.getcwd()))\n375     setup_py = os.path.join(root, \"setup.py\")\n376     versioneer_py = os.path.join(root, \"versioneer.py\")\n377     if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n378         # allow 'python path/to/setup.py COMMAND'\n379         root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n380         setup_py = os.path.join(root, \"setup.py\")\n381         versioneer_py = os.path.join(root, \"versioneer.py\")\n382     if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n383         err = (\"Versioneer was unable to run the project root directory. \"\n384                \"Versioneer requires setup.py to be executed from \"\n385                \"its immediate directory (like 'python setup.py COMMAND'), \"\n386                \"or in a way that lets it use sys.argv[0] to find the root \"\n387                \"(like 'python path/to/setup.py COMMAND').\")\n388         raise VersioneerBadRootError(err)\n389     try:\n390         # Certain runtime workflows (setup.py install/develop in a setuptools\n391         # tree) execute all dependencies in a single python process, so\n392         # \"versioneer\" may be imported multiple times, and python's shared\n393         # module-import table will cache the first one. So we can't use\n394         # os.path.dirname(__file__), as that will find whichever\n395         # versioneer.py was first imported, even in later projects.\n396         me = os.path.realpath(os.path.abspath(__file__))\n397         if os.path.splitext(me)[0] != os.path.splitext(versioneer_py)[0]:\n398             print(\"Warning: build in %s is using versioneer.py from %s\"\n399                   % (os.path.dirname(me), versioneer_py))\n400     except NameError:\n401         pass\n402     return root\n403 \n404 \n405 def get_config_from_root(root):\n406     \"\"\"Read the project setup.cfg file to determine Versioneer config.\"\"\"\n407     # This might raise EnvironmentError (if setup.cfg is missing), or\n408     # configparser.NoSectionError (if it lacks a [versioneer] section), or\n409     # configparser.NoOptionError (if it lacks \"VCS=\"). See the docstring at\n410     # the top of versioneer.py for instructions on writing your setup.cfg .\n411     setup_cfg = os.path.join(root, \"setup.cfg\")\n412     parser = configparser.SafeConfigParser()\n413     with open(setup_cfg, \"r\") as f:\n414         parser.readfp(f)\n415     VCS = parser.get(\"versioneer\", \"VCS\")  # mandatory\n416 \n417     def get(parser, name):\n418         if parser.has_option(\"versioneer\", name):\n419             return parser.get(\"versioneer\", name)\n420         return None\n421     cfg = VersioneerConfig()\n422     cfg.VCS = VCS\n423     cfg.style = get(parser, \"style\") or \"\"\n424     cfg.versionfile_source = get(parser, \"versionfile_source\")\n425     cfg.versionfile_build = get(parser, \"versionfile_build\")\n426     cfg.tag_prefix = get(parser, \"tag_prefix\")\n427     if cfg.tag_prefix in (\"''\", '\"\"'):\n428         cfg.tag_prefix = \"\"\n429     cfg.parentdir_prefix = get(parser, \"parentdir_prefix\")\n430     cfg.verbose = get(parser, \"verbose\")\n431     return cfg\n432 \n433 \n434 class NotThisMethod(Exception):\n435     \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n436 \n437 # these dictionaries contain VCS-specific tools\n438 LONG_VERSION_PY = {}\n439 HANDLERS = {}\n440 \n441 \n442 def register_vcs_handler(vcs, method):  # decorator\n443     \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n444     def decorate(f):\n445         \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n446         if vcs not in HANDLERS:\n447             HANDLERS[vcs] = {}\n448         HANDLERS[vcs][method] = f\n449         return f\n450     return decorate\n451 \n452 \n453 def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n454     \"\"\"Call the given command(s).\"\"\"\n455     assert isinstance(commands, list)\n456     p = None\n457     for c in commands:\n458         try:\n459             dispcmd = str([c] + args)\n460             # remember shell=False, so use git.cmd on windows, not just git\n461             p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n462                                  stderr=(subprocess.PIPE if hide_stderr\n463                                          else None))\n464             break\n465         except EnvironmentError:\n466             e = sys.exc_info()[1]\n467             if e.errno == errno.ENOENT:\n468                 continue\n469             if verbose:\n470                 print(\"unable to run %s\" % dispcmd)\n471                 print(e)\n472             return None\n473     else:\n474         if verbose:\n475             print(\"unable to find command, tried %s\" % (commands,))\n476         return None\n477     stdout = p.communicate()[0].strip()\n478     if sys.version_info[0] >= 3:\n479         stdout = stdout.decode()\n480     if p.returncode != 0:\n481         if verbose:\n482             print(\"unable to run %s (error)\" % dispcmd)\n483         return None\n484     return stdout\n485 LONG_VERSION_PY['git'] = '''\n486 # This file helps to compute a version number in source trees obtained from\n487 # git-archive tarball (such as those provided by githubs download-from-tag\n488 # feature). Distribution tarballs (built by setup.py sdist) and build\n489 # directories (produced by setup.py build) will contain a much shorter file\n490 # that just contains the computed version number.\n491 \n492 # This file is released into the public domain. Generated by\n493 # versioneer-0.16 (https://github.com/warner/python-versioneer)\n494 \n495 \"\"\"Git implementation of _version.py.\"\"\"\n496 \n497 import errno\n498 import os\n499 import re\n500 import subprocess\n501 import sys\n502 \n503 \n504 def get_keywords():\n505     \"\"\"Get the keywords needed to look up the version information.\"\"\"\n506     # these strings will be replaced by git during git-archive.\n507     # setup.py/versioneer.py will grep for the variable names, so they must\n508     # each be defined on a line of their own. _version.py will just call\n509     # get_keywords().\n510     git_refnames = \"%(DOLLAR)sFormat:%%d%(DOLLAR)s\"\n511     git_full = \"%(DOLLAR)sFormat:%%H%(DOLLAR)s\"\n512     keywords = {\"refnames\": git_refnames, \"full\": git_full}\n513     return keywords\n514 \n515 \n516 class VersioneerConfig:\n517     \"\"\"Container for Versioneer configuration parameters.\"\"\"\n518 \n519 \n520 def get_config():\n521     \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n522     # these strings are filled in when 'setup.py versioneer' creates\n523     # _version.py\n524     cfg = VersioneerConfig()\n525     cfg.VCS = \"git\"\n526     cfg.style = \"%(STYLE)s\"\n527     cfg.tag_prefix = \"%(TAG_PREFIX)s\"\n528     cfg.parentdir_prefix = \"%(PARENTDIR_PREFIX)s\"\n529     cfg.versionfile_source = \"%(VERSIONFILE_SOURCE)s\"\n530     cfg.verbose = False\n531     return cfg\n532 \n533 \n534 class NotThisMethod(Exception):\n535     \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n536 \n537 \n538 LONG_VERSION_PY = {}\n539 HANDLERS = {}\n540 \n541 \n542 def register_vcs_handler(vcs, method):  # decorator\n543     \"\"\"Decorator to mark a method as the handler for a particular VCS.\"\"\"\n544     def decorate(f):\n545         \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n546         if vcs not in HANDLERS:\n547             HANDLERS[vcs] = {}\n548         HANDLERS[vcs][method] = f\n549         return f\n550     return decorate\n551 \n552 \n553 def run_command(commands, args, cwd=None, verbose=False, hide_stderr=False):\n554     \"\"\"Call the given command(s).\"\"\"\n555     assert isinstance(commands, list)\n556     p = None\n557     for c in commands:\n558         try:\n559             dispcmd = str([c] + args)\n560             # remember shell=False, so use git.cmd on windows, not just git\n561             p = subprocess.Popen([c] + args, cwd=cwd, stdout=subprocess.PIPE,\n562                                  stderr=(subprocess.PIPE if hide_stderr\n563                                          else None))\n564             break\n565         except EnvironmentError:\n566             e = sys.exc_info()[1]\n567             if e.errno == errno.ENOENT:\n568                 continue\n569             if verbose:\n570                 print(\"unable to run %%s\" %% dispcmd)\n571                 print(e)\n572             return None\n573     else:\n574         if verbose:\n575             print(\"unable to find command, tried %%s\" %% (commands,))\n576         return None\n577     stdout = p.communicate()[0].strip()\n578     if sys.version_info[0] >= 3:\n579         stdout = stdout.decode()\n580     if p.returncode != 0:\n581         if verbose:\n582             print(\"unable to run %%s (error)\" %% dispcmd)\n583         return None\n584     return stdout\n585 \n586 \n587 def versions_from_parentdir(parentdir_prefix, root, verbose):\n588     \"\"\"Try to determine the version from the parent directory name.\n589 \n590     Source tarballs conventionally unpack into a directory that includes\n591     both the project name and a version string.\n592     \"\"\"\n593     dirname = os.path.basename(root)\n594     if not dirname.startswith(parentdir_prefix):\n595         if verbose:\n596             print(\"guessing rootdir is '%%s', but '%%s' doesn't start with \"\n597                   \"prefix '%%s'\" %% (root, dirname, parentdir_prefix))\n598         raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n599     return {\"version\": dirname[len(parentdir_prefix):],\n600             \"full-revisionid\": None,\n601             \"dirty\": False, \"error\": None}\n602 \n603 \n604 @register_vcs_handler(\"git\", \"get_keywords\")\n605 def git_get_keywords(versionfile_abs):\n606     \"\"\"Extract version information from the given file.\"\"\"\n607     # the code embedded in _version.py can just fetch the value of these\n608     # keywords. When used from setup.py, we don't want to import _version.py,\n609     # so we do it with a regexp instead. This function is not used from\n610     # _version.py.\n611     keywords = {}\n612     try:\n613         f = open(versionfile_abs, \"r\")\n614         for line in f.readlines():\n615             if line.strip().startswith(\"git_refnames =\"):\n616                 mo = re.search(r'=\\s*\"(.*)\"', line)\n617                 if mo:\n618                     keywords[\"refnames\"] = mo.group(1)\n619             if line.strip().startswith(\"git_full =\"):\n620                 mo = re.search(r'=\\s*\"(.*)\"', line)\n621                 if mo:\n622                     keywords[\"full\"] = mo.group(1)\n623         f.close()\n624     except EnvironmentError:\n625         pass\n626     return keywords\n627 \n628 \n629 @register_vcs_handler(\"git\", \"keywords\")\n630 def git_versions_from_keywords(keywords, tag_prefix, verbose):\n631     \"\"\"Get version information from git keywords.\"\"\"\n632     if not keywords:\n633         raise NotThisMethod(\"no keywords at all, weird\")\n634     refnames = keywords[\"refnames\"].strip()\n635     if refnames.startswith(\"$Format\"):\n636         if verbose:\n637             print(\"keywords are unexpanded, not using\")\n638         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n639     refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n640     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n641     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n642     TAG = \"tag: \"\n643     tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n644     if not tags:\n645         # Either we're using git < 1.8.3, or there really are no tags. We use\n646         # a heuristic: assume all version tags have a digit. The old git %%d\n647         # expansion behaves like git log --decorate=short and strips out the\n648         # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n649         # between branches and tags. By ignoring refnames without digits, we\n650         # filter out many common branch names like \"release\" and\n651         # \"stabilization\", as well as \"HEAD\" and \"master\".\n652         tags = set([r for r in refs if re.search(r'\\d', r)])\n653         if verbose:\n654             print(\"discarding '%%s', no digits\" %% \",\".join(refs-tags))\n655     if verbose:\n656         print(\"likely tags: %%s\" %% \",\".join(sorted(tags)))\n657     for ref in sorted(tags):\n658         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n659         if ref.startswith(tag_prefix):\n660             r = ref[len(tag_prefix):]\n661             if verbose:\n662                 print(\"picking %%s\" %% r)\n663             return {\"version\": r,\n664                     \"full-revisionid\": keywords[\"full\"].strip(),\n665                     \"dirty\": False, \"error\": None\n666                     }\n667     # no suitable tags, so version is \"0+unknown\", but full hex is still there\n668     if verbose:\n669         print(\"no suitable tags, using unknown + full revision id\")\n670     return {\"version\": \"0+unknown\",\n671             \"full-revisionid\": keywords[\"full\"].strip(),\n672             \"dirty\": False, \"error\": \"no suitable tags\"}\n673 \n674 \n675 @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n676 def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n677     \"\"\"Get version from 'git describe' in the root of the source tree.\n678 \n679     This only gets called if the git-archive 'subst' keywords were *not*\n680     expanded, and _version.py hasn't already been rewritten with a short\n681     version string, meaning we're inside a checked out source tree.\n682     \"\"\"\n683     if not os.path.exists(os.path.join(root, \".git\")):\n684         if verbose:\n685             print(\"no .git in %%s\" %% root)\n686         raise NotThisMethod(\"no .git directory\")\n687 \n688     GITS = [\"git\"]\n689     if sys.platform == \"win32\":\n690         GITS = [\"git.cmd\", \"git.exe\"]\n691     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n692     # if there isn't one, this yields HEX[-dirty] (no NUM)\n693     describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n694                                       \"--always\", \"--long\",\n695                                       \"--match\", \"%%s*\" %% tag_prefix],\n696                                cwd=root)\n697     # --long was added in git-1.5.5\n698     if describe_out is None:\n699         raise NotThisMethod(\"'git describe' failed\")\n700     describe_out = describe_out.strip()\n701     full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n702     if full_out is None:\n703         raise NotThisMethod(\"'git rev-parse' failed\")\n704     full_out = full_out.strip()\n705 \n706     pieces = {}\n707     pieces[\"long\"] = full_out\n708     pieces[\"short\"] = full_out[:7]  # maybe improved later\n709     pieces[\"error\"] = None\n710 \n711     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n712     # TAG might have hyphens.\n713     git_describe = describe_out\n714 \n715     # look for -dirty suffix\n716     dirty = git_describe.endswith(\"-dirty\")\n717     pieces[\"dirty\"] = dirty\n718     if dirty:\n719         git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n720 \n721     # now we have TAG-NUM-gHEX or HEX\n722 \n723     if \"-\" in git_describe:\n724         # TAG-NUM-gHEX\n725         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n726         if not mo:\n727             # unparseable. Maybe git-describe is misbehaving?\n728             pieces[\"error\"] = (\"unable to parse git-describe output: '%%s'\"\n729                                %% describe_out)\n730             return pieces\n731 \n732         # tag\n733         full_tag = mo.group(1)\n734         if not full_tag.startswith(tag_prefix):\n735             if verbose:\n736                 fmt = \"tag '%%s' doesn't start with prefix '%%s'\"\n737                 print(fmt %% (full_tag, tag_prefix))\n738             pieces[\"error\"] = (\"tag '%%s' doesn't start with prefix '%%s'\"\n739                                %% (full_tag, tag_prefix))\n740             return pieces\n741         pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n742 \n743         # distance: number of commits since tag\n744         pieces[\"distance\"] = int(mo.group(2))\n745 \n746         # commit: short hex revision ID\n747         pieces[\"short\"] = mo.group(3)\n748 \n749     else:\n750         # HEX: no tags\n751         pieces[\"closest-tag\"] = None\n752         count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n753                                 cwd=root)\n754         pieces[\"distance\"] = int(count_out)  # total number of commits\n755 \n756     return pieces\n757 \n758 \n759 def plus_or_dot(pieces):\n760     \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n761     if \"+\" in pieces.get(\"closest-tag\", \"\"):\n762         return \".\"\n763     return \"+\"\n764 \n765 \n766 def render_pep440(pieces):\n767     \"\"\"Build up version string, with post-release \"local version identifier\".\n768 \n769     Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n770     get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n771 \n772     Exceptions:\n773     1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n774     \"\"\"\n775     if pieces[\"closest-tag\"]:\n776         rendered = pieces[\"closest-tag\"]\n777         if pieces[\"distance\"] or pieces[\"dirty\"]:\n778             rendered += plus_or_dot(pieces)\n779             rendered += \"%%d.g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n780             if pieces[\"dirty\"]:\n781                 rendered += \".dirty\"\n782     else:\n783         # exception #1\n784         rendered = \"0+untagged.%%d.g%%s\" %% (pieces[\"distance\"],\n785                                           pieces[\"short\"])\n786         if pieces[\"dirty\"]:\n787             rendered += \".dirty\"\n788     return rendered\n789 \n790 \n791 def render_pep440_pre(pieces):\n792     \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n793 \n794     Exceptions:\n795     1: no tags. 0.post.devDISTANCE\n796     \"\"\"\n797     if pieces[\"closest-tag\"]:\n798         rendered = pieces[\"closest-tag\"]\n799         if pieces[\"distance\"]:\n800             rendered += \".post.dev%%d\" %% pieces[\"distance\"]\n801     else:\n802         # exception #1\n803         rendered = \"0.post.dev%%d\" %% pieces[\"distance\"]\n804     return rendered\n805 \n806 \n807 def render_pep440_post(pieces):\n808     \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n809 \n810     The \".dev0\" means dirty. Note that .dev0 sorts backwards\n811     (a dirty tree will appear \"older\" than the corresponding clean one),\n812     but you shouldn't be releasing software with -dirty anyways.\n813 \n814     Exceptions:\n815     1: no tags. 0.postDISTANCE[.dev0]\n816     \"\"\"\n817     if pieces[\"closest-tag\"]:\n818         rendered = pieces[\"closest-tag\"]\n819         if pieces[\"distance\"] or pieces[\"dirty\"]:\n820             rendered += \".post%%d\" %% pieces[\"distance\"]\n821             if pieces[\"dirty\"]:\n822                 rendered += \".dev0\"\n823             rendered += plus_or_dot(pieces)\n824             rendered += \"g%%s\" %% pieces[\"short\"]\n825     else:\n826         # exception #1\n827         rendered = \"0.post%%d\" %% pieces[\"distance\"]\n828         if pieces[\"dirty\"]:\n829             rendered += \".dev0\"\n830         rendered += \"+g%%s\" %% pieces[\"short\"]\n831     return rendered\n832 \n833 \n834 def render_pep440_old(pieces):\n835     \"\"\"TAG[.postDISTANCE[.dev0]] .\n836 \n837     The \".dev0\" means dirty.\n838 \n839     Eexceptions:\n840     1: no tags. 0.postDISTANCE[.dev0]\n841     \"\"\"\n842     if pieces[\"closest-tag\"]:\n843         rendered = pieces[\"closest-tag\"]\n844         if pieces[\"distance\"] or pieces[\"dirty\"]:\n845             rendered += \".post%%d\" %% pieces[\"distance\"]\n846             if pieces[\"dirty\"]:\n847                 rendered += \".dev0\"\n848     else:\n849         # exception #1\n850         rendered = \"0.post%%d\" %% pieces[\"distance\"]\n851         if pieces[\"dirty\"]:\n852             rendered += \".dev0\"\n853     return rendered\n854 \n855 \n856 def render_git_describe(pieces):\n857     \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n858 \n859     Like 'git describe --tags --dirty --always'.\n860 \n861     Exceptions:\n862     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n863     \"\"\"\n864     if pieces[\"closest-tag\"]:\n865         rendered = pieces[\"closest-tag\"]\n866         if pieces[\"distance\"]:\n867             rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n868     else:\n869         # exception #1\n870         rendered = pieces[\"short\"]\n871     if pieces[\"dirty\"]:\n872         rendered += \"-dirty\"\n873     return rendered\n874 \n875 \n876 def render_git_describe_long(pieces):\n877     \"\"\"TAG-DISTANCE-gHEX[-dirty].\n878 \n879     Like 'git describe --tags --dirty --always -long'.\n880     The distance/hash is unconditional.\n881 \n882     Exceptions:\n883     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n884     \"\"\"\n885     if pieces[\"closest-tag\"]:\n886         rendered = pieces[\"closest-tag\"]\n887         rendered += \"-%%d-g%%s\" %% (pieces[\"distance\"], pieces[\"short\"])\n888     else:\n889         # exception #1\n890         rendered = pieces[\"short\"]\n891     if pieces[\"dirty\"]:\n892         rendered += \"-dirty\"\n893     return rendered\n894 \n895 \n896 def render(pieces, style):\n897     \"\"\"Render the given version pieces into the requested style.\"\"\"\n898     if pieces[\"error\"]:\n899         return {\"version\": \"unknown\",\n900                 \"full-revisionid\": pieces.get(\"long\"),\n901                 \"dirty\": None,\n902                 \"error\": pieces[\"error\"]}\n903 \n904     if not style or style == \"default\":\n905         style = \"pep440\"  # the default\n906 \n907     if style == \"pep440\":\n908         rendered = render_pep440(pieces)\n909     elif style == \"pep440-pre\":\n910         rendered = render_pep440_pre(pieces)\n911     elif style == \"pep440-post\":\n912         rendered = render_pep440_post(pieces)\n913     elif style == \"pep440-old\":\n914         rendered = render_pep440_old(pieces)\n915     elif style == \"git-describe\":\n916         rendered = render_git_describe(pieces)\n917     elif style == \"git-describe-long\":\n918         rendered = render_git_describe_long(pieces)\n919     else:\n920         raise ValueError(\"unknown style '%%s'\" %% style)\n921 \n922     return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n923             \"dirty\": pieces[\"dirty\"], \"error\": None}\n924 \n925 \n926 def get_versions():\n927     \"\"\"Get version information or return default if unable to do so.\"\"\"\n928     # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n929     # __file__, we can work backwards from there to the root. Some\n930     # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n931     # case we can only use expanded keywords.\n932 \n933     cfg = get_config()\n934     verbose = cfg.verbose\n935 \n936     try:\n937         return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n938                                           verbose)\n939     except NotThisMethod:\n940         pass\n941 \n942     try:\n943         root = os.path.realpath(__file__)\n944         # versionfile_source is the relative path from the top of the source\n945         # tree (where the .git directory might live) to this file. Invert\n946         # this to find the root from __file__.\n947         for i in cfg.versionfile_source.split('/'):\n948             root = os.path.dirname(root)\n949     except NameError:\n950         return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n951                 \"dirty\": None,\n952                 \"error\": \"unable to find root of source tree\"}\n953 \n954     try:\n955         pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n956         return render(pieces, cfg.style)\n957     except NotThisMethod:\n958         pass\n959 \n960     try:\n961         if cfg.parentdir_prefix:\n962             return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n963     except NotThisMethod:\n964         pass\n965 \n966     return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n967             \"dirty\": None,\n968             \"error\": \"unable to compute version\"}\n969 '''\n970 \n971 \n972 @register_vcs_handler(\"git\", \"get_keywords\")\n973 def git_get_keywords(versionfile_abs):\n974     \"\"\"Extract version information from the given file.\"\"\"\n975     # the code embedded in _version.py can just fetch the value of these\n976     # keywords. When used from setup.py, we don't want to import _version.py,\n977     # so we do it with a regexp instead. This function is not used from\n978     # _version.py.\n979     keywords = {}\n980     try:\n981         f = open(versionfile_abs, \"r\")\n982         for line in f.readlines():\n983             if line.strip().startswith(\"git_refnames =\"):\n984                 mo = re.search(r'=\\s*\"(.*)\"', line)\n985                 if mo:\n986                     keywords[\"refnames\"] = mo.group(1)\n987             if line.strip().startswith(\"git_full =\"):\n988                 mo = re.search(r'=\\s*\"(.*)\"', line)\n989                 if mo:\n990                     keywords[\"full\"] = mo.group(1)\n991         f.close()\n992     except EnvironmentError:\n993         pass\n994     return keywords\n995 \n996 \n997 @register_vcs_handler(\"git\", \"keywords\")\n998 def git_versions_from_keywords(keywords, tag_prefix, verbose):\n999     \"\"\"Get version information from git keywords.\"\"\"\n1000     if not keywords:\n1001         raise NotThisMethod(\"no keywords at all, weird\")\n1002     refnames = keywords[\"refnames\"].strip()\n1003     if refnames.startswith(\"$Format\"):\n1004         if verbose:\n1005             print(\"keywords are unexpanded, not using\")\n1006         raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n1007     refs = set([r.strip() for r in refnames.strip(\"()\").split(\",\")])\n1008     # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n1009     # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n1010     TAG = \"tag: \"\n1011     tags = set([r[len(TAG):] for r in refs if r.startswith(TAG)])\n1012     if not tags:\n1013         # Either we're using git < 1.8.3, or there really are no tags. We use\n1014         # a heuristic: assume all version tags have a digit. The old git %d\n1015         # expansion behaves like git log --decorate=short and strips out the\n1016         # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n1017         # between branches and tags. By ignoring refnames without digits, we\n1018         # filter out many common branch names like \"release\" and\n1019         # \"stabilization\", as well as \"HEAD\" and \"master\".\n1020         tags = set([r for r in refs if re.search(r'\\d', r)])\n1021         if verbose:\n1022             print(\"discarding '%s', no digits\" % \",\".join(refs-tags))\n1023     if verbose:\n1024         print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n1025     for ref in sorted(tags):\n1026         # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n1027         if ref.startswith(tag_prefix):\n1028             r = ref[len(tag_prefix):]\n1029             if verbose:\n1030                 print(\"picking %s\" % r)\n1031             return {\"version\": r,\n1032                     \"full-revisionid\": keywords[\"full\"].strip(),\n1033                     \"dirty\": False, \"error\": None\n1034                     }\n1035     # no suitable tags, so version is \"0+unknown\", but full hex is still there\n1036     if verbose:\n1037         print(\"no suitable tags, using unknown + full revision id\")\n1038     return {\"version\": \"0+unknown\",\n1039             \"full-revisionid\": keywords[\"full\"].strip(),\n1040             \"dirty\": False, \"error\": \"no suitable tags\"}\n1041 \n1042 \n1043 @register_vcs_handler(\"git\", \"pieces_from_vcs\")\n1044 def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n1045     \"\"\"Get version from 'git describe' in the root of the source tree.\n1046 \n1047     This only gets called if the git-archive 'subst' keywords were *not*\n1048     expanded, and _version.py hasn't already been rewritten with a short\n1049     version string, meaning we're inside a checked out source tree.\n1050     \"\"\"\n1051     if not os.path.exists(os.path.join(root, \".git\")):\n1052         if verbose:\n1053             print(\"no .git in %s\" % root)\n1054         raise NotThisMethod(\"no .git directory\")\n1055 \n1056     GITS = [\"git\"]\n1057     if sys.platform == \"win32\":\n1058         GITS = [\"git.cmd\", \"git.exe\"]\n1059     # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n1060     # if there isn't one, this yields HEX[-dirty] (no NUM)\n1061     describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n1062                                       \"--always\", \"--long\",\n1063                                       \"--match\", \"%s*\" % tag_prefix],\n1064                                cwd=root)\n1065     # --long was added in git-1.5.5\n1066     if describe_out is None:\n1067         raise NotThisMethod(\"'git describe' failed\")\n1068     describe_out = describe_out.strip()\n1069     full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n1070     if full_out is None:\n1071         raise NotThisMethod(\"'git rev-parse' failed\")\n1072     full_out = full_out.strip()\n1073 \n1074     pieces = {}\n1075     pieces[\"long\"] = full_out\n1076     pieces[\"short\"] = full_out[:7]  # maybe improved later\n1077     pieces[\"error\"] = None\n1078 \n1079     # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n1080     # TAG might have hyphens.\n1081     git_describe = describe_out\n1082 \n1083     # look for -dirty suffix\n1084     dirty = git_describe.endswith(\"-dirty\")\n1085     pieces[\"dirty\"] = dirty\n1086     if dirty:\n1087         git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n1088 \n1089     # now we have TAG-NUM-gHEX or HEX\n1090 \n1091     if \"-\" in git_describe:\n1092         # TAG-NUM-gHEX\n1093         mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n1094         if not mo:\n1095             # unparseable. Maybe git-describe is misbehaving?\n1096             pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n1097                                % describe_out)\n1098             return pieces\n1099 \n1100         # tag\n1101         full_tag = mo.group(1)\n1102         if not full_tag.startswith(tag_prefix):\n1103             if verbose:\n1104                 fmt = \"tag '%s' doesn't start with prefix '%s'\"\n1105                 print(fmt % (full_tag, tag_prefix))\n1106             pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n1107                                % (full_tag, tag_prefix))\n1108             return pieces\n1109         pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n1110 \n1111         # distance: number of commits since tag\n1112         pieces[\"distance\"] = int(mo.group(2))\n1113 \n1114         # commit: short hex revision ID\n1115         pieces[\"short\"] = mo.group(3)\n1116 \n1117     else:\n1118         # HEX: no tags\n1119         pieces[\"closest-tag\"] = None\n1120         count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n1121                                 cwd=root)\n1122         pieces[\"distance\"] = int(count_out)  # total number of commits\n1123 \n1124     return pieces\n1125 \n1126 \n1127 def do_vcs_install(manifest_in, versionfile_source, ipy):\n1128     \"\"\"Git-specific installation logic for Versioneer.\n1129 \n1130     For Git, this means creating/changing .gitattributes to mark _version.py\n1131     for export-time keyword substitution.\n1132     \"\"\"\n1133     GITS = [\"git\"]\n1134     if sys.platform == \"win32\":\n1135         GITS = [\"git.cmd\", \"git.exe\"]\n1136     files = [manifest_in, versionfile_source]\n1137     if ipy:\n1138         files.append(ipy)\n1139     try:\n1140         me = __file__\n1141         if me.endswith(\".pyc\") or me.endswith(\".pyo\"):\n1142             me = os.path.splitext(me)[0] + \".py\"\n1143         versioneer_file = os.path.relpath(me)\n1144     except NameError:\n1145         versioneer_file = \"versioneer.py\"\n1146     files.append(versioneer_file)\n1147     present = False\n1148     try:\n1149         f = open(\".gitattributes\", \"r\")\n1150         for line in f.readlines():\n1151             if line.strip().startswith(versionfile_source):\n1152                 if \"export-subst\" in line.strip().split()[1:]:\n1153                     present = True\n1154         f.close()\n1155     except EnvironmentError:\n1156         pass\n1157     if not present:\n1158         f = open(\".gitattributes\", \"a+\")\n1159         f.write(\"%s export-subst\\n\" % versionfile_source)\n1160         f.close()\n1161         files.append(\".gitattributes\")\n1162     run_command(GITS, [\"add\", \"--\"] + files)\n1163 \n1164 \n1165 def versions_from_parentdir(parentdir_prefix, root, verbose):\n1166     \"\"\"Try to determine the version from the parent directory name.\n1167 \n1168     Source tarballs conventionally unpack into a directory that includes\n1169     both the project name and a version string.\n1170     \"\"\"\n1171     dirname = os.path.basename(root)\n1172     if not dirname.startswith(parentdir_prefix):\n1173         if verbose:\n1174             print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n1175                   \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n1176         raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n1177     return {\"version\": dirname[len(parentdir_prefix):],\n1178             \"full-revisionid\": None,\n1179             \"dirty\": False, \"error\": None}\n1180 \n1181 SHORT_VERSION_PY = \"\"\"\n1182 # This file was generated by 'versioneer.py' (0.16) from\n1183 # revision-control system data, or from the parent directory name of an\n1184 # unpacked source archive. Distribution tarballs contain a pre-generated copy\n1185 # of this file.\n1186 \n1187 import json\n1188 import sys\n1189 \n1190 version_json = '''\n1191 %s\n1192 '''  # END VERSION_JSON\n1193 \n1194 \n1195 def get_versions():\n1196     return json.loads(version_json)\n1197 \"\"\"\n1198 \n1199 \n1200 def versions_from_file(filename):\n1201     \"\"\"Try to determine the version from _version.py if present.\"\"\"\n1202     try:\n1203         with open(filename) as f:\n1204             contents = f.read()\n1205     except EnvironmentError:\n1206         raise NotThisMethod(\"unable to read _version.py\")\n1207     mo = re.search(r\"version_json = '''\\n(.*)'''  # END VERSION_JSON\",\n1208                    contents, re.M | re.S)\n1209     if not mo:\n1210         raise NotThisMethod(\"no version_json in _version.py\")\n1211     return json.loads(mo.group(1))\n1212 \n1213 \n1214 def write_to_version_file(filename, versions):\n1215     \"\"\"Write the given version number to the given _version.py file.\"\"\"\n1216     os.unlink(filename)\n1217     contents = json.dumps(versions, sort_keys=True,\n1218                           indent=1, separators=(\",\", \": \"))\n1219     with open(filename, \"w\") as f:\n1220         f.write(SHORT_VERSION_PY % contents)\n1221 \n1222     print(\"set %s to '%s'\" % (filename, versions[\"version\"]))\n1223 \n1224 \n1225 def plus_or_dot(pieces):\n1226     \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n1227     if \"+\" in pieces.get(\"closest-tag\", \"\"):\n1228         return \".\"\n1229     return \"+\"\n1230 \n1231 \n1232 def render_pep440(pieces):\n1233     \"\"\"Build up version string, with post-release \"local version identifier\".\n1234 \n1235     Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n1236     get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n1237 \n1238     Exceptions:\n1239     1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n1240     \"\"\"\n1241     if pieces[\"closest-tag\"]:\n1242         rendered = pieces[\"closest-tag\"]\n1243         if pieces[\"distance\"] or pieces[\"dirty\"]:\n1244             rendered += plus_or_dot(pieces)\n1245             rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n1246             if pieces[\"dirty\"]:\n1247                 rendered += \".dirty\"\n1248     else:\n1249         # exception #1\n1250         rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"],\n1251                                           pieces[\"short\"])\n1252         if pieces[\"dirty\"]:\n1253             rendered += \".dirty\"\n1254     return rendered\n1255 \n1256 \n1257 def render_pep440_pre(pieces):\n1258     \"\"\"TAG[.post.devDISTANCE] -- No -dirty.\n1259 \n1260     Exceptions:\n1261     1: no tags. 0.post.devDISTANCE\n1262     \"\"\"\n1263     if pieces[\"closest-tag\"]:\n1264         rendered = pieces[\"closest-tag\"]\n1265         if pieces[\"distance\"]:\n1266             rendered += \".post.dev%d\" % pieces[\"distance\"]\n1267     else:\n1268         # exception #1\n1269         rendered = \"0.post.dev%d\" % pieces[\"distance\"]\n1270     return rendered\n1271 \n1272 \n1273 def render_pep440_post(pieces):\n1274     \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n1275 \n1276     The \".dev0\" means dirty. Note that .dev0 sorts backwards\n1277     (a dirty tree will appear \"older\" than the corresponding clean one),\n1278     but you shouldn't be releasing software with -dirty anyways.\n1279 \n1280     Exceptions:\n1281     1: no tags. 0.postDISTANCE[.dev0]\n1282     \"\"\"\n1283     if pieces[\"closest-tag\"]:\n1284         rendered = pieces[\"closest-tag\"]\n1285         if pieces[\"distance\"] or pieces[\"dirty\"]:\n1286             rendered += \".post%d\" % pieces[\"distance\"]\n1287             if pieces[\"dirty\"]:\n1288                 rendered += \".dev0\"\n1289             rendered += plus_or_dot(pieces)\n1290             rendered += \"g%s\" % pieces[\"short\"]\n1291     else:\n1292         # exception #1\n1293         rendered = \"0.post%d\" % pieces[\"distance\"]\n1294         if pieces[\"dirty\"]:\n1295             rendered += \".dev0\"\n1296         rendered += \"+g%s\" % pieces[\"short\"]\n1297     return rendered\n1298 \n1299 \n1300 def render_pep440_old(pieces):\n1301     \"\"\"TAG[.postDISTANCE[.dev0]] .\n1302 \n1303     The \".dev0\" means dirty.\n1304 \n1305     Eexceptions:\n1306     1: no tags. 0.postDISTANCE[.dev0]\n1307     \"\"\"\n1308     if pieces[\"closest-tag\"]:\n1309         rendered = pieces[\"closest-tag\"]\n1310         if pieces[\"distance\"] or pieces[\"dirty\"]:\n1311             rendered += \".post%d\" % pieces[\"distance\"]\n1312             if pieces[\"dirty\"]:\n1313                 rendered += \".dev0\"\n1314     else:\n1315         # exception #1\n1316         rendered = \"0.post%d\" % pieces[\"distance\"]\n1317         if pieces[\"dirty\"]:\n1318             rendered += \".dev0\"\n1319     return rendered\n1320 \n1321 \n1322 def render_git_describe(pieces):\n1323     \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n1324 \n1325     Like 'git describe --tags --dirty --always'.\n1326 \n1327     Exceptions:\n1328     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n1329     \"\"\"\n1330     if pieces[\"closest-tag\"]:\n1331         rendered = pieces[\"closest-tag\"]\n1332         if pieces[\"distance\"]:\n1333             rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n1334     else:\n1335         # exception #1\n1336         rendered = pieces[\"short\"]\n1337     if pieces[\"dirty\"]:\n1338         rendered += \"-dirty\"\n1339     return rendered\n1340 \n1341 \n1342 def render_git_describe_long(pieces):\n1343     \"\"\"TAG-DISTANCE-gHEX[-dirty].\n1344 \n1345     Like 'git describe --tags --dirty --always -long'.\n1346     The distance/hash is unconditional.\n1347 \n1348     Exceptions:\n1349     1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n1350     \"\"\"\n1351     if pieces[\"closest-tag\"]:\n1352         rendered = pieces[\"closest-tag\"]\n1353         rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n1354     else:\n1355         # exception #1\n1356         rendered = pieces[\"short\"]\n1357     if pieces[\"dirty\"]:\n1358         rendered += \"-dirty\"\n1359     return rendered\n1360 \n1361 \n1362 def render(pieces, style):\n1363     \"\"\"Render the given version pieces into the requested style.\"\"\"\n1364     if pieces[\"error\"]:\n1365         return {\"version\": \"unknown\",\n1366                 \"full-revisionid\": pieces.get(\"long\"),\n1367                 \"dirty\": None,\n1368                 \"error\": pieces[\"error\"]}\n1369 \n1370     if not style or style == \"default\":\n1371         style = \"pep440\"  # the default\n1372 \n1373     if style == \"pep440\":\n1374         rendered = render_pep440(pieces)\n1375     elif style == \"pep440-pre\":\n1376         rendered = render_pep440_pre(pieces)\n1377     elif style == \"pep440-post\":\n1378         rendered = render_pep440_post(pieces)\n1379     elif style == \"pep440-old\":\n1380         rendered = render_pep440_old(pieces)\n1381     elif style == \"git-describe\":\n1382         rendered = render_git_describe(pieces)\n1383     elif style == \"git-describe-long\":\n1384         rendered = render_git_describe_long(pieces)\n1385     else:\n1386         raise ValueError(\"unknown style '%s'\" % style)\n1387 \n1388     return {\"version\": rendered, \"full-revisionid\": pieces[\"long\"],\n1389             \"dirty\": pieces[\"dirty\"], \"error\": None}\n1390 \n1391 \n1392 class VersioneerBadRootError(Exception):\n1393     \"\"\"The project root directory is unknown or missing key files.\"\"\"\n1394 \n1395 \n1396 def get_versions(verbose=False):\n1397     \"\"\"Get the project version from whatever source is available.\n1398 \n1399     Returns dict with two keys: 'version' and 'full'.\n1400     \"\"\"\n1401     if \"versioneer\" in sys.modules:\n1402         # see the discussion in cmdclass.py:get_cmdclass()\n1403         del sys.modules[\"versioneer\"]\n1404 \n1405     root = get_root()\n1406     cfg = get_config_from_root(root)\n1407 \n1408     assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n1409     handlers = HANDLERS.get(cfg.VCS)\n1410     assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n1411     verbose = verbose or cfg.verbose\n1412     assert cfg.versionfile_source is not None, \\\n1413         \"please set versioneer.versionfile_source\"\n1414     assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n1415 \n1416     versionfile_abs = os.path.join(root, cfg.versionfile_source)\n1417 \n1418     # extract version from first of: _version.py, VCS command (e.g. 'git\n1419     # describe'), parentdir. This is meant to work for developers using a\n1420     # source checkout, for users of a tarball created by 'setup.py sdist',\n1421     # and for users of a tarball/zipball created by 'git archive' or github's\n1422     # download-from-tag feature or the equivalent in other VCSes.\n1423 \n1424     get_keywords_f = handlers.get(\"get_keywords\")\n1425     from_keywords_f = handlers.get(\"keywords\")\n1426     if get_keywords_f and from_keywords_f:\n1427         try:\n1428             keywords = get_keywords_f(versionfile_abs)\n1429             ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n1430             if verbose:\n1431                 print(\"got version from expanded keyword %s\" % ver)\n1432             return ver\n1433         except NotThisMethod:\n1434             pass\n1435 \n1436     try:\n1437         ver = versions_from_file(versionfile_abs)\n1438         if verbose:\n1439             print(\"got version from file %s %s\" % (versionfile_abs, ver))\n1440         return ver\n1441     except NotThisMethod:\n1442         pass\n1443 \n1444     from_vcs_f = handlers.get(\"pieces_from_vcs\")\n1445     if from_vcs_f:\n1446         try:\n1447             pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n1448             ver = render(pieces, cfg.style)\n1449             if verbose:\n1450                 print(\"got version from VCS %s\" % ver)\n1451             return ver\n1452         except NotThisMethod:\n1453             pass\n1454 \n1455     try:\n1456         if cfg.parentdir_prefix:\n1457             ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n1458             if verbose:\n1459                 print(\"got version from parentdir %s\" % ver)\n1460             return ver\n1461     except NotThisMethod:\n1462         pass\n1463 \n1464     if verbose:\n1465         print(\"unable to compute version\")\n1466 \n1467     return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n1468             \"dirty\": None, \"error\": \"unable to compute version\"}\n1469 \n1470 \n1471 def get_version():\n1472     \"\"\"Get the short version string for this project.\"\"\"\n1473     return get_versions()[\"version\"]\n1474 \n1475 \n1476 def get_cmdclass():\n1477     \"\"\"Get the custom setuptools/distutils subclasses used by Versioneer.\"\"\"\n1478     if \"versioneer\" in sys.modules:\n1479         del sys.modules[\"versioneer\"]\n1480         # this fixes the \"python setup.py develop\" case (also 'install' and\n1481         # 'easy_install .'), in which subdependencies of the main project are\n1482         # built (using setup.py bdist_egg) in the same python process. Assume\n1483         # a main project A and a dependency B, which use different versions\n1484         # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n1485         # sys.modules by the time B's setup.py is executed, causing B to run\n1486         # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n1487         # sandbox that restores sys.modules to it's pre-build state, so the\n1488         # parent is protected against the child's \"import versioneer\". By\n1489         # removing ourselves from sys.modules here, before the child build\n1490         # happens, we protect the child from the parent's versioneer too.\n1491         # Also see https://github.com/warner/python-versioneer/issues/52\n1492 \n1493     cmds = {}\n1494 \n1495     # we add \"version\" to both distutils and setuptools\n1496     from distutils.core import Command\n1497 \n1498     class cmd_version(Command):\n1499         description = \"report generated version string\"\n1500         user_options = []\n1501         boolean_options = []\n1502 \n1503         def initialize_options(self):\n1504             pass\n1505 \n1506         def finalize_options(self):\n1507             pass\n1508 \n1509         def run(self):\n1510             vers = get_versions(verbose=True)\n1511             print(\"Version: %s\" % vers[\"version\"])\n1512             print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n1513             print(\" dirty: %s\" % vers.get(\"dirty\"))\n1514             if vers[\"error\"]:\n1515                 print(\" error: %s\" % vers[\"error\"])\n1516     cmds[\"version\"] = cmd_version\n1517 \n1518     # we override \"build_py\" in both distutils and setuptools\n1519     #\n1520     # most invocation pathways end up running build_py:\n1521     #  distutils/build -> build_py\n1522     #  distutils/install -> distutils/build ->..\n1523     #  setuptools/bdist_wheel -> distutils/install ->..\n1524     #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n1525     #  setuptools/install -> bdist_egg ->..\n1526     #  setuptools/develop -> ?\n1527 \n1528     # we override different \"build_py\" commands for both environments\n1529     if \"setuptools\" in sys.modules:\n1530         from setuptools.command.build_py import build_py as _build_py\n1531     else:\n1532         from distutils.command.build_py import build_py as _build_py\n1533 \n1534     class cmd_build_py(_build_py):\n1535         def run(self):\n1536             root = get_root()\n1537             cfg = get_config_from_root(root)\n1538             versions = get_versions()\n1539             _build_py.run(self)\n1540             # now locate _version.py in the new build/ directory and replace\n1541             # it with an updated value\n1542             if cfg.versionfile_build:\n1543                 target_versionfile = os.path.join(self.build_lib,\n1544                                                   cfg.versionfile_build)\n1545                 print(\"UPDATING %s\" % target_versionfile)\n1546                 write_to_version_file(target_versionfile, versions)\n1547     cmds[\"build_py\"] = cmd_build_py\n1548 \n1549     if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n1550         from cx_Freeze.dist import build_exe as _build_exe\n1551 \n1552         class cmd_build_exe(_build_exe):\n1553             def run(self):\n1554                 root = get_root()\n1555                 cfg = get_config_from_root(root)\n1556                 versions = get_versions()\n1557                 target_versionfile = cfg.versionfile_source\n1558                 print(\"UPDATING %s\" % target_versionfile)\n1559                 write_to_version_file(target_versionfile, versions)\n1560 \n1561                 _build_exe.run(self)\n1562                 os.unlink(target_versionfile)\n1563                 with open(cfg.versionfile_source, \"w\") as f:\n1564                     LONG = LONG_VERSION_PY[cfg.VCS]\n1565                     f.write(LONG %\n1566                             {\"DOLLAR\": \"$\",\n1567                              \"STYLE\": cfg.style,\n1568                              \"TAG_PREFIX\": cfg.tag_prefix,\n1569                              \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n1570                              \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n1571                              })\n1572         cmds[\"build_exe\"] = cmd_build_exe\n1573         del cmds[\"build_py\"]\n1574 \n1575     # we override different \"sdist\" commands for both environments\n1576     if \"setuptools\" in sys.modules:\n1577         from setuptools.command.sdist import sdist as _sdist\n1578     else:\n1579         from distutils.command.sdist import sdist as _sdist\n1580 \n1581     class cmd_sdist(_sdist):\n1582         def run(self):\n1583             versions = get_versions()\n1584             self._versioneer_generated_versions = versions\n1585             # unless we update this, the command will keep using the old\n1586             # version\n1587             self.distribution.metadata.version = versions[\"version\"]\n1588             return _sdist.run(self)\n1589 \n1590         def make_release_tree(self, base_dir, files):\n1591             root = get_root()\n1592             cfg = get_config_from_root(root)\n1593             _sdist.make_release_tree(self, base_dir, files)\n1594             # now locate _version.py in the new base_dir directory\n1595             # (remembering that it may be a hardlink) and replace it with an\n1596             # updated value\n1597             target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n1598             print(\"UPDATING %s\" % target_versionfile)\n1599             write_to_version_file(target_versionfile,\n1600                                   self._versioneer_generated_versions)\n1601     cmds[\"sdist\"] = cmd_sdist\n1602 \n1603     return cmds\n1604 \n1605 \n1606 CONFIG_ERROR = \"\"\"\n1607 setup.cfg is missing the necessary Versioneer configuration. You need\n1608 a section like:\n1609 \n1610  [versioneer]\n1611  VCS = git\n1612  style = pep440\n1613  versionfile_source = src/myproject/_version.py\n1614  versionfile_build = myproject/_version.py\n1615  tag_prefix =\n1616  parentdir_prefix = myproject-\n1617 \n1618 You will also need to edit your setup.py to use the results:\n1619 \n1620  import versioneer\n1621  setup(version=versioneer.get_version(),\n1622        cmdclass=versioneer.get_cmdclass(), ...)\n1623 \n1624 Please read the docstring in ./versioneer.py for configuration instructions,\n1625 edit setup.cfg, and re-run the installer or 'python versioneer.py setup'.\n1626 \"\"\"\n1627 \n1628 SAMPLE_CONFIG = \"\"\"\n1629 # See the docstring in versioneer.py for instructions. Note that you must\n1630 # re-run 'versioneer.py setup' after changing this section, and commit the\n1631 # resulting files.\n1632 \n1633 [versioneer]\n1634 #VCS = git\n1635 #style = pep440\n1636 #versionfile_source =\n1637 #versionfile_build =\n1638 #tag_prefix =\n1639 #parentdir_prefix =\n1640 \n1641 \"\"\"\n1642 \n1643 INIT_PY_SNIPPET = \"\"\"\n1644 from ._version import get_versions\n1645 __version__ = get_versions()['version']\n1646 del get_versions\n1647 \"\"\"\n1648 \n1649 \n1650 def do_setup():\n1651     \"\"\"Main VCS-independent setup function for installing Versioneer.\"\"\"\n1652     root = get_root()\n1653     try:\n1654         cfg = get_config_from_root(root)\n1655     except (EnvironmentError, configparser.NoSectionError,\n1656             configparser.NoOptionError) as e:\n1657         if isinstance(e, (EnvironmentError, configparser.NoSectionError)):\n1658             print(\"Adding sample versioneer config to setup.cfg\",\n1659                   file=sys.stderr)\n1660             with open(os.path.join(root, \"setup.cfg\"), \"a\") as f:\n1661                 f.write(SAMPLE_CONFIG)\n1662         print(CONFIG_ERROR, file=sys.stderr)\n1663         return 1\n1664 \n1665     print(\" creating %s\" % cfg.versionfile_source)\n1666     with open(cfg.versionfile_source, \"w\") as f:\n1667         LONG = LONG_VERSION_PY[cfg.VCS]\n1668         f.write(LONG % {\"DOLLAR\": \"$\",\n1669                         \"STYLE\": cfg.style,\n1670                         \"TAG_PREFIX\": cfg.tag_prefix,\n1671                         \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n1672                         \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n1673                         })\n1674 \n1675     ipy = os.path.join(os.path.dirname(cfg.versionfile_source),\n1676                        \"__init__.py\")\n1677     if os.path.exists(ipy):\n1678         try:\n1679             with open(ipy, \"r\") as f:\n1680                 old = f.read()\n1681         except EnvironmentError:\n1682             old = \"\"\n1683         if INIT_PY_SNIPPET not in old:\n1684             print(\" appending to %s\" % ipy)\n1685             with open(ipy, \"a\") as f:\n1686                 f.write(INIT_PY_SNIPPET)\n1687         else:\n1688             print(\" %s unmodified\" % ipy)\n1689     else:\n1690         print(\" %s doesn't exist, ok\" % ipy)\n1691         ipy = None\n1692 \n1693     # Make sure both the top-level \"versioneer.py\" and versionfile_source\n1694     # (PKG/_version.py, used by runtime code) are in MANIFEST.in, so\n1695     # they'll be copied into source distributions. Pip won't be able to\n1696     # install the package without this.\n1697     manifest_in = os.path.join(root, \"MANIFEST.in\")\n1698     simple_includes = set()\n1699     try:\n1700         with open(manifest_in, \"r\") as f:\n1701             for line in f:\n1702                 if line.startswith(\"include \"):\n1703                     for include in line.split()[1:]:\n1704                         simple_includes.add(include)\n1705     except EnvironmentError:\n1706         pass\n1707     # That doesn't cover everything MANIFEST.in can do\n1708     # (http://docs.python.org/2/distutils/sourcedist.html#commands), so\n1709     # it might give some false negatives. Appending redundant 'include'\n1710     # lines is safe, though.\n1711     if \"versioneer.py\" not in simple_includes:\n1712         print(\" appending 'versioneer.py' to MANIFEST.in\")\n1713         with open(manifest_in, \"a\") as f:\n1714             f.write(\"include versioneer.py\\n\")\n1715     else:\n1716         print(\" 'versioneer.py' already in MANIFEST.in\")\n1717     if cfg.versionfile_source not in simple_includes:\n1718         print(\" appending versionfile_source ('%s') to MANIFEST.in\" %\n1719               cfg.versionfile_source)\n1720         with open(manifest_in, \"a\") as f:\n1721             f.write(\"include %s\\n\" % cfg.versionfile_source)\n1722     else:\n1723         print(\" versionfile_source already in MANIFEST.in\")\n1724 \n1725     # Make VCS-specific changes. For git, this means creating/changing\n1726     # .gitattributes to mark _version.py for export-time keyword\n1727     # substitution.\n1728     do_vcs_install(manifest_in, cfg.versionfile_source, ipy)\n1729     return 0\n1730 \n1731 \n1732 def scan_setup_py():\n1733     \"\"\"Validate the contents of setup.py against Versioneer's expectations.\"\"\"\n1734     found = set()\n1735     setters = False\n1736     errors = 0\n1737     with open(\"setup.py\", \"r\") as f:\n1738         for line in f.readlines():\n1739             if \"import versioneer\" in line:\n1740                 found.add(\"import\")\n1741             if \"versioneer.get_cmdclass()\" in line:\n1742                 found.add(\"cmdclass\")\n1743             if \"versioneer.get_version()\" in line:\n1744                 found.add(\"get_version\")\n1745             if \"versioneer.VCS\" in line:\n1746                 setters = True\n1747             if \"versioneer.versionfile_source\" in line:\n1748                 setters = True\n1749     if len(found) != 3:\n1750         print(\"\")\n1751         print(\"Your setup.py appears to be missing some important items\")\n1752         print(\"(but I might be wrong). Please make sure it has something\")\n1753         print(\"roughly like the following:\")\n1754         print(\"\")\n1755         print(\" import versioneer\")\n1756         print(\" setup( version=versioneer.get_version(),\")\n1757         print(\"        cmdclass=versioneer.get_cmdclass(),  ...)\")\n1758         print(\"\")\n1759         errors += 1\n1760     if setters:\n1761         print(\"You should remove lines like 'versioneer.VCS = ' and\")\n1762         print(\"'versioneer.versionfile_source = ' . This configuration\")\n1763         print(\"now lives in setup.cfg, and should be removed from setup.py\")\n1764         print(\"\")\n1765         errors += 1\n1766     return errors\n1767 \n1768 if __name__ == \"__main__\":\n1769     cmd = sys.argv[1]\n1770     if cmd == \"setup\":\n1771         errors = do_setup()\n1772         errors += scan_setup_py()\n1773         if errors:\n1774             sys.exit(1)\n1775 \n[end of versioneer.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/temperature.py\n+++ b/pvlib/temperature.py\n@@ -1,5 +1,5 @@\n def noct_sam(..., eta_m_ref, ...):\n-    # existing code\n+    # existing code with eta_m_ref renamed to module_efficiency\n     ...\n \n def pvsyst_cell(..., eta_m, ...):\n-    # existing code\n+    # existing code with eta_m renamed to module_efficiency\n     ...\n```\n", "model_patch": "--- a/pvlib/temperature.py\n+++ b/pvlib/temperature.py\n@@ -1,5 +1,5 @@\n def noct_sam(..., eta_m_ref, ...):\n-    # existing code\n+    # existing code with eta_m_ref renamed to module_efficiency\n     ...\n \n def pvsyst_cell(..., eta_m, ...):\n-    # existing code\n+    # existing code with eta_m renamed to module_efficiency\n     ...\n"}
{"instance_id": "pylint-dev__astroid-1962", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nv2.13.x regression: Crash when inspecting `PyQt5.QtWidgets` due to `RuntimeError` during `hasattr`\n### Steps to reproduce\r\n\r\nInstall PyQt5, run `pylint --extension-pkg-whitelist=PyQt5 x.py` over a file containing `from PyQt5 import QtWidgets`\r\n\r\n### Current behavior\r\n\r\nWith astroid 2.12.13 and pylint 2.15.10, this works fine. With astroid 2.13.2, this happens:\r\n\r\n```pytb\r\nException on node <ImportFrom l.1 at 0x7fc5a3c47d00> in file '/home/florian/tmp/pylintbug/x.py'\r\nTraceback (most recent call last):\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 1726, in visit_importfrom\r\n    self._check_module_attrs(node, module, name.split(\".\"))\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/pylint/checkers/variables.py\", line 2701, in _check_module_attrs\r\n    module = next(module.getattr(name)[0].infer())\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 412, in getattr\r\n    result = [self.import_module(name, relative_only=True)]\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 205, in ast_from_module_name\r\n    return self.ast_from_module(named_module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/manager.py\", line 312, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/builder.py\", line 101, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 366, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/home/florian/tmp/pylintbug/.venv/lib/python3.10/site-packages/astroid/raw_building.py\", line 422, in object_build\r\n    elif hasattr(member, \"__all__\"):\r\nRuntimeError: wrapped C/C++ object of type QApplication has been deleted\r\nx.py:1:0: F0002: x.py: Fatal error while checking 'x.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/home/florian/.cache/pylint/pylint-crash-2023-01-10-11-06-17.txt'. (astroid-error)\r\n```\r\n\r\nIt looks like it happens when `member` is `QtWidgets.qApp`, which is a kind of \"magic\" object referring to the QApplication singleton. Since none exists, it looks like PyQt doesn't like trying to access an attribute on that.\r\n\r\nBisected to:\r\n\r\n- #1885 \r\n\r\nIt looks like 974f26f75eb3eccb4bcd8ea143901baf60a685ff is the exact culprit.\r\n\r\ncc @nickdrozd \r\n\r\n(took the freedom to add appropriate labels already, hope that's fine)\r\n\n\n</issue>\n<code>\n[start of README.rst]\n1 Astroid\n2 =======\n3 \n4 .. image:: https://codecov.io/gh/PyCQA/astroid/branch/main/graph/badge.svg?token=Buxy4WptLb\n5     :target: https://codecov.io/gh/PyCQA/astroid\n6     :alt: Coverage badge from codecov\n7 \n8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest\n9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest\n10     :alt: Documentation Status\n11 \n12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n13     :target: https://github.com/ambv/black\n14 \n15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg\n16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main\n17    :alt: pre-commit.ci status\n18 \n19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png\n20    :width: 200\n21    :alt: Tidelift\n22 \n23 .. list-table::\n24    :widths: 10 100\n25 \n26    * - |tidelift_logo|\n27      - Professional support for astroid is available as part of the\n28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for\n29        purchasing and maintaining their software, with professional grade assurances\n30        from the experts who know it best, while seamlessly integrating with existing\n31        tools.\n32 \n33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme\n34 \n35 \n36 \n37 What's this?\n38 ------------\n39 \n40 The aim of this module is to provide a common base representation of\n41 python source code. It is currently the library powering pylint's capabilities.\n42 \n43 It provides a compatible representation which comes from the `_ast`\n44 module.  It rebuilds the tree generated by the builtin _ast module by\n45 recursively walking down the AST and building an extended ast. The new\n46 node classes have additional methods and attributes for different\n47 usages. They include some support for static inference and local name\n48 scopes. Furthermore, astroid can also build partial trees by inspecting living\n49 objects.\n50 \n51 \n52 Installation\n53 ------------\n54 \n55 Extract the tarball, jump into the created directory and run::\n56 \n57     pip install .\n58 \n59 \n60 If you want to do an editable installation, you can run::\n61 \n62     pip install -e .\n63 \n64 \n65 If you have any questions, please mail the code-quality@python.org\n66 mailing list for support. See\n67 http://mail.python.org/mailman/listinfo/code-quality for subscription\n68 information and archives.\n69 \n70 Documentation\n71 -------------\n72 http://astroid.readthedocs.io/en/latest/\n73 \n74 \n75 Python Versions\n76 ---------------\n77 \n78 astroid 2.0 is currently available for Python 3 only. If you want Python 2\n79 support, use an older version of astroid (though note that these versions\n80 are no longer supported).\n81 \n82 Test\n83 ----\n84 \n85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use\n86 either `tox` or `pytest`::\n87 \n88     tox\n89     pytest\n90 \n[end of README.rst]\n[start of astroid/builder.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"The AstroidBuilder makes astroid from living object and / or from _ast.\n6 \n7 The builder is not thread safe and can't be used to parse different sources\n8 at the same time.\n9 \"\"\"\n10 \n11 from __future__ import annotations\n12 \n13 import ast\n14 import os\n15 import textwrap\n16 import types\n17 from collections.abc import Iterator, Sequence\n18 from io import TextIOWrapper\n19 from tokenize import detect_encoding\n20 from typing import TYPE_CHECKING\n21 \n22 from astroid import bases, modutils, nodes, raw_building, rebuilder, util\n23 from astroid._ast import ParserModule, get_parser_module\n24 from astroid.exceptions import AstroidBuildingError, AstroidSyntaxError, InferenceError\n25 from astroid.manager import AstroidManager\n26 \n27 if TYPE_CHECKING:\n28     from astroid import objects\n29 else:\n30     objects = util.lazy_import(\"objects\")\n31 \n32 \n33 # The name of the transient function that is used to\n34 # wrap expressions to be extracted when calling\n35 # extract_node.\n36 _TRANSIENT_FUNCTION = \"__\"\n37 \n38 # The comment used to select a statement to be extracted\n39 # when calling extract_node.\n40 _STATEMENT_SELECTOR = \"#@\"\n41 MISPLACED_TYPE_ANNOTATION_ERROR = \"misplaced type annotation\"\n42 \n43 \n44 def open_source_file(filename: str) -> tuple[TextIOWrapper, str, str]:\n45     # pylint: disable=consider-using-with\n46     with open(filename, \"rb\") as byte_stream:\n47         encoding = detect_encoding(byte_stream.readline)[0]\n48     stream = open(filename, newline=None, encoding=encoding)\n49     data = stream.read()\n50     return stream, encoding, data\n51 \n52 \n53 def _can_assign_attr(node: nodes.ClassDef, attrname: str | None) -> bool:\n54     try:\n55         slots = node.slots()\n56     except NotImplementedError:\n57         pass\n58     else:\n59         if slots and attrname not in {slot.value for slot in slots}:\n60             return False\n61     return node.qname() != \"builtins.object\"\n62 \n63 \n64 class AstroidBuilder(raw_building.InspectBuilder):\n65     \"\"\"Class for building an astroid tree from source code or from a live module.\n66 \n67     The param *manager* specifies the manager class which should be used.\n68     If no manager is given, then the default one will be used. The\n69     param *apply_transforms* determines if the transforms should be\n70     applied after the tree was built from source or from a live object,\n71     by default being True.\n72     \"\"\"\n73 \n74     def __init__(\n75         self, manager: AstroidManager | None = None, apply_transforms: bool = True\n76     ) -> None:\n77         super().__init__(manager)\n78         self._apply_transforms = apply_transforms\n79 \n80     def module_build(\n81         self, module: types.ModuleType, modname: str | None = None\n82     ) -> nodes.Module:\n83         \"\"\"Build an astroid from a living module instance.\"\"\"\n84         node = None\n85         path = getattr(module, \"__file__\", None)\n86         loader = getattr(module, \"__loader__\", None)\n87         # Prefer the loader to get the source rather than assuming we have a\n88         # filesystem to read the source file from ourselves.\n89         if loader:\n90             modname = modname or module.__name__\n91             source = loader.get_source(modname)\n92             if source:\n93                 node = self.string_build(source, modname, path=path)\n94         if node is None and path is not None:\n95             path_, ext = os.path.splitext(modutils._path_from_filename(path))\n96             if ext in {\".py\", \".pyc\", \".pyo\"} and os.path.exists(path_ + \".py\"):\n97                 node = self.file_build(path_ + \".py\", modname)\n98         if node is None:\n99             # this is a built-in module\n100             # get a partial representation by introspection\n101             node = self.inspect_build(module, modname=modname, path=path)\n102             if self._apply_transforms:\n103                 # We have to handle transformation by ourselves since the\n104                 # rebuilder isn't called for builtin nodes\n105                 node = self._manager.visit_transforms(node)\n106         assert isinstance(node, nodes.Module)\n107         return node\n108 \n109     def file_build(self, path: str, modname: str | None = None) -> nodes.Module:\n110         \"\"\"Build astroid from a source code file (i.e. from an ast).\n111 \n112         *path* is expected to be a python source file\n113         \"\"\"\n114         try:\n115             stream, encoding, data = open_source_file(path)\n116         except OSError as exc:\n117             raise AstroidBuildingError(\n118                 \"Unable to load file {path}:\\n{error}\",\n119                 modname=modname,\n120                 path=path,\n121                 error=exc,\n122             ) from exc\n123         except (SyntaxError, LookupError) as exc:\n124             raise AstroidSyntaxError(\n125                 \"Python 3 encoding specification error or unknown encoding:\\n\"\n126                 \"{error}\",\n127                 modname=modname,\n128                 path=path,\n129                 error=exc,\n130             ) from exc\n131         except UnicodeError as exc:  # wrong encoding\n132             # detect_encoding returns utf-8 if no encoding specified\n133             raise AstroidBuildingError(\n134                 \"Wrong or no encoding specified for {filename}.\", filename=path\n135             ) from exc\n136         with stream:\n137             # get module name if necessary\n138             if modname is None:\n139                 try:\n140                     modname = \".\".join(modutils.modpath_from_file(path))\n141                 except ImportError:\n142                     modname = os.path.splitext(os.path.basename(path))[0]\n143             # build astroid representation\n144             module, builder = self._data_build(data, modname, path)\n145             return self._post_build(module, builder, encoding)\n146 \n147     def string_build(\n148         self, data: str, modname: str = \"\", path: str | None = None\n149     ) -> nodes.Module:\n150         \"\"\"Build astroid from source code string.\"\"\"\n151         module, builder = self._data_build(data, modname, path)\n152         module.file_bytes = data.encode(\"utf-8\")\n153         return self._post_build(module, builder, \"utf-8\")\n154 \n155     def _post_build(\n156         self, module: nodes.Module, builder: rebuilder.TreeRebuilder, encoding: str\n157     ) -> nodes.Module:\n158         \"\"\"Handles encoding and delayed nodes after a module has been built.\"\"\"\n159         module.file_encoding = encoding\n160         self._manager.cache_module(module)\n161         # post tree building steps after we stored the module in the cache:\n162         for from_node in builder._import_from_nodes:\n163             if from_node.modname == \"__future__\":\n164                 for symbol, _ in from_node.names:\n165                     module.future_imports.add(symbol)\n166             self.add_from_names_to_locals(from_node)\n167         # handle delayed assattr nodes\n168         for delayed in builder._delayed_assattr:\n169             self.delayed_assattr(delayed)\n170 \n171         # Visit the transforms\n172         if self._apply_transforms:\n173             module = self._manager.visit_transforms(module)\n174         return module\n175 \n176     def _data_build(\n177         self, data: str, modname: str, path: str | None\n178     ) -> tuple[nodes.Module, rebuilder.TreeRebuilder]:\n179         \"\"\"Build tree node from data and add some informations.\"\"\"\n180         try:\n181             node, parser_module = _parse_string(data, type_comments=True)\n182         except (TypeError, ValueError, SyntaxError) as exc:\n183             raise AstroidSyntaxError(\n184                 \"Parsing Python code failed:\\n{error}\",\n185                 source=data,\n186                 modname=modname,\n187                 path=path,\n188                 error=exc,\n189             ) from exc\n190 \n191         if path is not None:\n192             node_file = os.path.abspath(path)\n193         else:\n194             node_file = \"<?>\"\n195         if modname.endswith(\".__init__\"):\n196             modname = modname[:-9]\n197             package = True\n198         else:\n199             package = (\n200                 path is not None\n201                 and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n202             )\n203         builder = rebuilder.TreeRebuilder(self._manager, parser_module, data)\n204         module = builder.visit_module(node, modname, node_file, package)\n205         return module, builder\n206 \n207     def add_from_names_to_locals(self, node: nodes.ImportFrom) -> None:\n208         \"\"\"Store imported names to the locals.\n209 \n210         Resort the locals if coming from a delayed node\n211         \"\"\"\n212 \n213         def _key_func(node: nodes.NodeNG) -> int:\n214             return node.fromlineno or 0\n215 \n216         def sort_locals(my_list: list[nodes.NodeNG]) -> None:\n217             my_list.sort(key=_key_func)\n218 \n219         assert node.parent  # It should always default to the module\n220         for name, asname in node.names:\n221             if name == \"*\":\n222                 try:\n223                     imported = node.do_import_module()\n224                 except AstroidBuildingError:\n225                     continue\n226                 for name in imported.public_names():\n227                     node.parent.set_local(name, node)\n228                     sort_locals(node.parent.scope().locals[name])  # type: ignore[arg-type]\n229             else:\n230                 node.parent.set_local(asname or name, node)\n231                 sort_locals(node.parent.scope().locals[asname or name])  # type: ignore[arg-type]\n232 \n233     def delayed_assattr(self, node: nodes.AssignAttr) -> None:\n234         \"\"\"Visit a AssAttr node.\n235 \n236         This adds name to locals and handle members definition.\n237         \"\"\"\n238         try:\n239             frame = node.frame(future=True)\n240             for inferred in node.expr.infer():\n241                 if inferred is util.Uninferable:\n242                     continue\n243                 try:\n244                     # pylint: disable=unidiomatic-typecheck # We want a narrow check on the\n245                     # parent type, not all of its subclasses\n246                     if (\n247                         type(inferred) == bases.Instance\n248                         or type(inferred) == objects.ExceptionInstance\n249                     ):\n250                         inferred = inferred._proxied\n251                         iattrs = inferred.instance_attrs\n252                         if not _can_assign_attr(inferred, node.attrname):\n253                             continue\n254                     elif isinstance(inferred, bases.Instance):\n255                         # Const, Tuple or other containers that inherit from\n256                         # `Instance`\n257                         continue\n258                     elif (\n259                         isinstance(inferred, bases.Proxy)\n260                         or inferred is util.Uninferable\n261                     ):\n262                         continue\n263                     elif inferred.is_function:\n264                         iattrs = inferred.instance_attrs\n265                     else:\n266                         iattrs = inferred.locals\n267                 except AttributeError:\n268                     # XXX log error\n269                     continue\n270                 values = iattrs.setdefault(node.attrname, [])\n271                 if node in values:\n272                     continue\n273                 # get assign in __init__ first XXX useful ?\n274                 if (\n275                     frame.name == \"__init__\"\n276                     and values\n277                     and values[0].frame(future=True).name != \"__init__\"\n278                 ):\n279                     values.insert(0, node)\n280                 else:\n281                     values.append(node)\n282         except InferenceError:\n283             pass\n284 \n285 \n286 def build_namespace_package_module(name: str, path: Sequence[str]) -> nodes.Module:\n287     # TODO: Typing: Remove the cast to list and just update typing to accept Sequence\n288     return nodes.Module(name, path=list(path), package=True)\n289 \n290 \n291 def parse(\n292     code: str,\n293     module_name: str = \"\",\n294     path: str | None = None,\n295     apply_transforms: bool = True,\n296 ) -> nodes.Module:\n297     \"\"\"Parses a source string in order to obtain an astroid AST from it.\n298 \n299     :param str code: The code for the module.\n300     :param str module_name: The name for the module, if any\n301     :param str path: The path for the module\n302     :param bool apply_transforms:\n303         Apply the transforms for the give code. Use it if you\n304         don't want the default transforms to be applied.\n305     \"\"\"\n306     code = textwrap.dedent(code)\n307     builder = AstroidBuilder(\n308         manager=AstroidManager(), apply_transforms=apply_transforms\n309     )\n310     return builder.string_build(code, modname=module_name, path=path)\n311 \n312 \n313 def _extract_expressions(node: nodes.NodeNG) -> Iterator[nodes.NodeNG]:\n314     \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n315 \n316     The function walks the AST recursively to search for expressions that\n317     are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n318     expression, it completely removes the function call node from the tree,\n319     replacing it by the wrapped expression inside the parent.\n320 \n321     :param node: An astroid node.\n322     :type node:  astroid.bases.NodeNG\n323     :yields: The sequence of wrapped expressions on the modified tree\n324     expression can be found.\n325     \"\"\"\n326     if (\n327         isinstance(node, nodes.Call)\n328         and isinstance(node.func, nodes.Name)\n329         and node.func.name == _TRANSIENT_FUNCTION\n330     ):\n331         real_expr = node.args[0]\n332         assert node.parent\n333         real_expr.parent = node.parent\n334         # Search for node in all _astng_fields (the fields checked when\n335         # get_children is called) of its parent. Some of those fields may\n336         # be lists or tuples, in which case the elements need to be checked.\n337         # When we find it, replace it by real_expr, so that the AST looks\n338         # like no call to _TRANSIENT_FUNCTION ever took place.\n339         for name in node.parent._astroid_fields:\n340             child = getattr(node.parent, name)\n341             if isinstance(child, list):\n342                 for idx, compound_child in enumerate(child):\n343                     if compound_child is node:\n344                         child[idx] = real_expr\n345             elif child is node:\n346                 setattr(node.parent, name, real_expr)\n347         yield real_expr\n348     else:\n349         for child in node.get_children():\n350             yield from _extract_expressions(child)\n351 \n352 \n353 def _find_statement_by_line(node: nodes.NodeNG, line: int) -> nodes.NodeNG | None:\n354     \"\"\"Extracts the statement on a specific line from an AST.\n355 \n356     If the line number of node matches line, it will be returned;\n357     otherwise its children are iterated and the function is called\n358     recursively.\n359 \n360     :param node: An astroid node.\n361     :type node: astroid.bases.NodeNG\n362     :param line: The line number of the statement to extract.\n363     :type line: int\n364     :returns: The statement on the line, or None if no statement for the line\n365       can be found.\n366     :rtype:  astroid.bases.NodeNG or None\n367     \"\"\"\n368     if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n369         # This is an inaccuracy in the AST: the nodes that can be\n370         # decorated do not carry explicit information on which line\n371         # the actual definition (class/def), but .fromline seems to\n372         # be close enough.\n373         node_line = node.fromlineno\n374     else:\n375         node_line = node.lineno\n376 \n377     if node_line == line:\n378         return node\n379 \n380     for child in node.get_children():\n381         result = _find_statement_by_line(child, line)\n382         if result:\n383             return result\n384 \n385     return None\n386 \n387 \n388 def extract_node(code: str, module_name: str = \"\") -> nodes.NodeNG | list[nodes.NodeNG]:\n389     \"\"\"Parses some Python code as a module and extracts a designated AST node.\n390 \n391     Statements:\n392      To extract one or more statement nodes, append #@ to the end of the line\n393 \n394      Examples:\n395        >>> def x():\n396        >>>   def y():\n397        >>>     return 1 #@\n398 \n399        The return statement will be extracted.\n400 \n401        >>> class X(object):\n402        >>>   def meth(self): #@\n403        >>>     pass\n404 \n405       The function object 'meth' will be extracted.\n406 \n407     Expressions:\n408      To extract arbitrary expressions, surround them with the fake\n409      function call __(...). After parsing, the surrounded expression\n410      will be returned and the whole AST (accessible via the returned\n411      node's parent attribute) will look like the function call was\n412      never there in the first place.\n413 \n414      Examples:\n415        >>> a = __(1)\n416 \n417        The const node will be extracted.\n418 \n419        >>> def x(d=__(foo.bar)): pass\n420 \n421        The node containing the default argument will be extracted.\n422 \n423        >>> def foo(a, b):\n424        >>>   return 0 < __(len(a)) < b\n425 \n426        The node containing the function call 'len' will be extracted.\n427 \n428     If no statements or expressions are selected, the last toplevel\n429     statement will be returned.\n430 \n431     If the selected statement is a discard statement, (i.e. an expression\n432     turned into a statement), the wrapped expression is returned instead.\n433 \n434     For convenience, singleton lists are unpacked.\n435 \n436     :param str code: A piece of Python code that is parsed as\n437     a module. Will be passed through textwrap.dedent first.\n438     :param str module_name: The name of the module.\n439     :returns: The designated node from the parse tree, or a list of nodes.\n440     \"\"\"\n441 \n442     def _extract(node: nodes.NodeNG | None) -> nodes.NodeNG | None:\n443         if isinstance(node, nodes.Expr):\n444             return node.value\n445 \n446         return node\n447 \n448     requested_lines: list[int] = []\n449     for idx, line in enumerate(code.splitlines()):\n450         if line.strip().endswith(_STATEMENT_SELECTOR):\n451             requested_lines.append(idx + 1)\n452 \n453     tree = parse(code, module_name=module_name)\n454     if not tree.body:\n455         raise ValueError(\"Empty tree, cannot extract from it\")\n456 \n457     extracted: list[nodes.NodeNG | None] = []\n458     if requested_lines:\n459         extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n460 \n461     # Modifies the tree.\n462     extracted.extend(_extract_expressions(tree))\n463 \n464     if not extracted:\n465         extracted.append(tree.body[-1])\n466 \n467     extracted = [_extract(node) for node in extracted]\n468     extracted_without_none = [node for node in extracted if node is not None]\n469     if len(extracted_without_none) == 1:\n470         return extracted_without_none[0]\n471     return extracted_without_none\n472 \n473 \n474 def _extract_single_node(code: str, module_name: str = \"\") -> nodes.NodeNG:\n475     \"\"\"Call extract_node while making sure that only one value is returned.\"\"\"\n476     ret = extract_node(code, module_name)\n477     if isinstance(ret, list):\n478         return ret[0]\n479     return ret\n480 \n481 \n482 def _parse_string(\n483     data: str, type_comments: bool = True\n484 ) -> tuple[ast.Module, ParserModule]:\n485     parser_module = get_parser_module(type_comments=type_comments)\n486     try:\n487         parsed = parser_module.parse(data + \"\\n\", type_comments=type_comments)\n488     except SyntaxError as exc:\n489         # If the type annotations are misplaced for some reason, we do not want\n490         # to fail the entire parsing of the file, so we need to retry the parsing without\n491         # type comment support.\n492         if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n493             raise\n494 \n495         parser_module = get_parser_module(type_comments=False)\n496         parsed = parser_module.parse(data + \"\\n\", type_comments=False)\n497     return parsed, parser_module\n498 \n[end of astroid/builder.py]\n[start of astroid/interpreter/_import/spec.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import abc\n8 import enum\n9 import importlib\n10 import importlib.machinery\n11 import importlib.util\n12 import os\n13 import pathlib\n14 import sys\n15 import zipimport\n16 from collections.abc import Iterator, Sequence\n17 from pathlib import Path\n18 from typing import Any, NamedTuple\n19 \n20 from astroid.const import PY310_PLUS\n21 from astroid.modutils import EXT_LIB_DIRS\n22 \n23 from . import util\n24 \n25 if sys.version_info >= (3, 8):\n26     from typing import Literal\n27 else:\n28     from typing_extensions import Literal\n29 \n30 \n31 class ModuleType(enum.Enum):\n32     \"\"\"Python module types used for ModuleSpec.\"\"\"\n33 \n34     C_BUILTIN = enum.auto()\n35     C_EXTENSION = enum.auto()\n36     PKG_DIRECTORY = enum.auto()\n37     PY_CODERESOURCE = enum.auto()\n38     PY_COMPILED = enum.auto()\n39     PY_FROZEN = enum.auto()\n40     PY_RESOURCE = enum.auto()\n41     PY_SOURCE = enum.auto()\n42     PY_ZIPMODULE = enum.auto()\n43     PY_NAMESPACE = enum.auto()\n44 \n45 \n46 class ModuleSpec(NamedTuple):\n47     \"\"\"Defines a class similar to PEP 420's ModuleSpec.\n48 \n49     A module spec defines a name of a module, its type, location\n50     and where submodules can be found, if the module is a package.\n51     \"\"\"\n52 \n53     name: str\n54     type: ModuleType | None\n55     location: str | None = None\n56     origin: str | None = None\n57     submodule_search_locations: Sequence[str] | None = None\n58 \n59 \n60 class Finder:\n61     \"\"\"A finder is a class which knows how to find a particular module.\"\"\"\n62 \n63     def __init__(self, path: Sequence[str] | None = None) -> None:\n64         self._path = path or sys.path\n65 \n66     @abc.abstractmethod\n67     def find_module(\n68         self,\n69         modname: str,\n70         module_parts: Sequence[str],\n71         processed: list[str],\n72         submodule_path: Sequence[str] | None,\n73     ) -> ModuleSpec | None:\n74         \"\"\"Find the given module.\n75 \n76         Each finder is responsible for each protocol of finding, as long as\n77         they all return a ModuleSpec.\n78 \n79         :param modname: The module which needs to be searched.\n80         :param module_parts: It should be a list of strings,\n81                                   where each part contributes to the module's\n82                                   namespace.\n83         :param processed: What parts from the module parts were processed\n84                                so far.\n85         :param submodule_path: A list of paths where the module\n86                                     can be looked into.\n87         :returns: A ModuleSpec, describing how and where the module was found,\n88                   None, otherwise.\n89         \"\"\"\n90 \n91     def contribute_to_path(\n92         self, spec: ModuleSpec, processed: list[str]\n93     ) -> Sequence[str] | None:\n94         \"\"\"Get a list of extra paths where this finder can search.\"\"\"\n95 \n96 \n97 class ImportlibFinder(Finder):\n98     \"\"\"A finder based on the importlib module.\"\"\"\n99 \n100     _SUFFIXES: Sequence[tuple[str, ModuleType]] = (\n101         [(s, ModuleType.C_EXTENSION) for s in importlib.machinery.EXTENSION_SUFFIXES]\n102         + [(s, ModuleType.PY_SOURCE) for s in importlib.machinery.SOURCE_SUFFIXES]\n103         + [(s, ModuleType.PY_COMPILED) for s in importlib.machinery.BYTECODE_SUFFIXES]\n104     )\n105 \n106     def find_module(\n107         self,\n108         modname: str,\n109         module_parts: Sequence[str],\n110         processed: list[str],\n111         submodule_path: Sequence[str] | None,\n112     ) -> ModuleSpec | None:\n113         if submodule_path is not None:\n114             submodule_path = list(submodule_path)\n115         elif modname in sys.builtin_module_names:\n116             return ModuleSpec(\n117                 name=modname,\n118                 location=None,\n119                 type=ModuleType.C_BUILTIN,\n120             )\n121         else:\n122             try:\n123                 spec = importlib.util.find_spec(modname)\n124                 if (\n125                     spec and spec.loader is importlib.machinery.FrozenImporter\n126                 ):  # noqa: E501 # type: ignore[comparison-overlap]\n127                     # No need for BuiltinImporter; builtins handled above\n128                     return ModuleSpec(\n129                         name=modname,\n130                         location=getattr(spec.loader_state, \"filename\", None),\n131                         type=ModuleType.PY_FROZEN,\n132                     )\n133             except ValueError:\n134                 pass\n135             submodule_path = sys.path\n136 \n137         for entry in submodule_path:\n138             package_directory = os.path.join(entry, modname)\n139             for suffix in (\".py\", importlib.machinery.BYTECODE_SUFFIXES[0]):\n140                 package_file_name = \"__init__\" + suffix\n141                 file_path = os.path.join(package_directory, package_file_name)\n142                 if os.path.isfile(file_path):\n143                     return ModuleSpec(\n144                         name=modname,\n145                         location=package_directory,\n146                         type=ModuleType.PKG_DIRECTORY,\n147                     )\n148             for suffix, type_ in ImportlibFinder._SUFFIXES:\n149                 file_name = modname + suffix\n150                 file_path = os.path.join(entry, file_name)\n151                 if os.path.isfile(file_path):\n152                     return ModuleSpec(name=modname, location=file_path, type=type_)\n153         return None\n154 \n155     def contribute_to_path(\n156         self, spec: ModuleSpec, processed: list[str]\n157     ) -> Sequence[str] | None:\n158         if spec.location is None:\n159             # Builtin.\n160             return None\n161 \n162         if _is_setuptools_namespace(Path(spec.location)):\n163             # extend_path is called, search sys.path for module/packages\n164             # of this name see pkgutil.extend_path documentation\n165             path = [\n166                 os.path.join(p, *processed)\n167                 for p in sys.path\n168                 if os.path.isdir(os.path.join(p, *processed))\n169             ]\n170         elif spec.name == \"distutils\" and not any(\n171             spec.location.lower().startswith(ext_lib_dir.lower())\n172             for ext_lib_dir in EXT_LIB_DIRS\n173         ):\n174             # virtualenv below 20.0 patches distutils in an unexpected way\n175             # so we just find the location of distutils that will be\n176             # imported to avoid spurious import-error messages\n177             # https://github.com/PyCQA/pylint/issues/5645\n178             # A regression test to create this scenario exists in release-tests.yml\n179             # and can be triggered manually from GitHub Actions\n180             distutils_spec = importlib.util.find_spec(\"distutils\")\n181             if distutils_spec and distutils_spec.origin:\n182                 origin_path = Path(\n183                     distutils_spec.origin\n184                 )  # e.g. .../distutils/__init__.py\n185                 path = [str(origin_path.parent)]  # e.g. .../distutils\n186             else:\n187                 path = [spec.location]\n188         else:\n189             path = [spec.location]\n190         return path\n191 \n192 \n193 class ExplicitNamespacePackageFinder(ImportlibFinder):\n194     \"\"\"A finder for the explicit namespace packages.\"\"\"\n195 \n196     def find_module(\n197         self,\n198         modname: str,\n199         module_parts: Sequence[str],\n200         processed: list[str],\n201         submodule_path: Sequence[str] | None,\n202     ) -> ModuleSpec | None:\n203         if processed:\n204             modname = \".\".join(processed + [modname])\n205         if util.is_namespace(modname) and modname in sys.modules:\n206             submodule_path = sys.modules[modname].__path__\n207             return ModuleSpec(\n208                 name=modname,\n209                 location=\"\",\n210                 origin=\"namespace\",\n211                 type=ModuleType.PY_NAMESPACE,\n212                 submodule_search_locations=submodule_path,\n213             )\n214         return None\n215 \n216     def contribute_to_path(\n217         self, spec: ModuleSpec, processed: list[str]\n218     ) -> Sequence[str] | None:\n219         return spec.submodule_search_locations\n220 \n221 \n222 class ZipFinder(Finder):\n223     \"\"\"Finder that knows how to find a module inside zip files.\"\"\"\n224 \n225     def __init__(self, path: Sequence[str]) -> None:\n226         super().__init__(path)\n227         for entry_path in path:\n228             if entry_path not in sys.path_importer_cache:\n229                 # pylint: disable=no-member\n230                 try:\n231                     sys.path_importer_cache[entry_path] = zipimport.zipimporter(  # type: ignore[assignment]\n232                         entry_path\n233                     )\n234                 except zipimport.ZipImportError:\n235                     continue\n236 \n237     def find_module(\n238         self,\n239         modname: str,\n240         module_parts: Sequence[str],\n241         processed: list[str],\n242         submodule_path: Sequence[str] | None,\n243     ) -> ModuleSpec | None:\n244         try:\n245             file_type, filename, path = _search_zip(module_parts)\n246         except ImportError:\n247             return None\n248 \n249         return ModuleSpec(\n250             name=modname,\n251             location=filename,\n252             origin=\"egg\",\n253             type=file_type,\n254             submodule_search_locations=path,\n255         )\n256 \n257 \n258 class PathSpecFinder(Finder):\n259     \"\"\"Finder based on importlib.machinery.PathFinder.\"\"\"\n260 \n261     def find_module(\n262         self,\n263         modname: str,\n264         module_parts: Sequence[str],\n265         processed: list[str],\n266         submodule_path: Sequence[str] | None,\n267     ) -> ModuleSpec | None:\n268         spec = importlib.machinery.PathFinder.find_spec(modname, path=submodule_path)\n269         if spec is not None:\n270             is_namespace_pkg = spec.origin is None\n271             location = spec.origin if not is_namespace_pkg else None\n272             module_type = ModuleType.PY_NAMESPACE if is_namespace_pkg else None\n273             return ModuleSpec(\n274                 name=spec.name,\n275                 location=location,\n276                 origin=spec.origin,\n277                 type=module_type,\n278                 submodule_search_locations=list(spec.submodule_search_locations or []),\n279             )\n280         return spec\n281 \n282     def contribute_to_path(\n283         self, spec: ModuleSpec, processed: list[str]\n284     ) -> Sequence[str] | None:\n285         if spec.type == ModuleType.PY_NAMESPACE:\n286             return spec.submodule_search_locations\n287         return None\n288 \n289 \n290 _SPEC_FINDERS = (\n291     ImportlibFinder,\n292     ZipFinder,\n293     PathSpecFinder,\n294     ExplicitNamespacePackageFinder,\n295 )\n296 \n297 \n298 def _is_setuptools_namespace(location: pathlib.Path) -> bool:\n299     try:\n300         with open(location / \"__init__.py\", \"rb\") as stream:\n301             data = stream.read(4096)\n302     except OSError:\n303         return False\n304     extend_path = b\"pkgutil\" in data and b\"extend_path\" in data\n305     declare_namespace = (\n306         b\"pkg_resources\" in data and b\"declare_namespace(__name__)\" in data\n307     )\n308     return extend_path or declare_namespace\n309 \n310 \n311 def _get_zipimporters() -> Iterator[tuple[str, zipimport.zipimporter]]:\n312     for filepath, importer in sys.path_importer_cache.items():\n313         # pylint: disable-next=no-member\n314         if isinstance(importer, zipimport.zipimporter):\n315             yield filepath, importer\n316 \n317 \n318 def _search_zip(\n319     modpath: Sequence[str],\n320 ) -> tuple[Literal[ModuleType.PY_ZIPMODULE], str, str]:\n321     for filepath, importer in _get_zipimporters():\n322         if PY310_PLUS:\n323             found: Any = importer.find_spec(modpath[0])\n324         else:\n325             found = importer.find_module(modpath[0])\n326         if found:\n327             if PY310_PLUS:\n328                 if not importer.find_spec(os.path.sep.join(modpath)):\n329                     raise ImportError(\n330                         \"No module named %s in %s/%s\"\n331                         % (\".\".join(modpath[1:]), filepath, modpath)\n332                     )\n333             elif not importer.find_module(os.path.sep.join(modpath)):\n334                 raise ImportError(\n335                     \"No module named %s in %s/%s\"\n336                     % (\".\".join(modpath[1:]), filepath, modpath)\n337                 )\n338             return (\n339                 ModuleType.PY_ZIPMODULE,\n340                 os.path.abspath(filepath) + os.path.sep + os.path.sep.join(modpath),\n341                 filepath,\n342             )\n343     raise ImportError(f\"No module named {'.'.join(modpath)}\")\n344 \n345 \n346 def _find_spec_with_path(\n347     search_path: Sequence[str],\n348     modname: str,\n349     module_parts: list[str],\n350     processed: list[str],\n351     submodule_path: Sequence[str] | None,\n352 ) -> tuple[Finder, ModuleSpec]:\n353     for finder in _SPEC_FINDERS:\n354         finder_instance = finder(search_path)\n355         spec = finder_instance.find_module(\n356             modname, module_parts, processed, submodule_path\n357         )\n358         if spec is None:\n359             continue\n360         return finder_instance, spec\n361 \n362     raise ImportError(f\"No module named {'.'.join(module_parts)}\")\n363 \n364 \n365 def find_spec(modpath: list[str], path: Sequence[str] | None = None) -> ModuleSpec:\n366     \"\"\"Find a spec for the given module.\n367 \n368     :type modpath: list or tuple\n369     :param modpath:\n370       split module's name (i.e name of a module or package split\n371       on '.'), with leading empty strings for explicit relative import\n372 \n373     :type path: list or None\n374     :param path:\n375       optional list of path where the module or package should be\n376       searched (use sys.path if nothing or None is given)\n377 \n378     :rtype: ModuleSpec\n379     :return: A module spec, which describes how the module was\n380              found and where.\n381     \"\"\"\n382     _path = path or sys.path\n383 \n384     # Need a copy for not mutating the argument.\n385     modpath = modpath[:]\n386 \n387     submodule_path = None\n388     module_parts = modpath[:]\n389     processed: list[str] = []\n390 \n391     while modpath:\n392         modname = modpath.pop(0)\n393         finder, spec = _find_spec_with_path(\n394             _path, modname, module_parts, processed, submodule_path or path\n395         )\n396         processed.append(modname)\n397         if modpath:\n398             submodule_path = finder.contribute_to_path(spec, processed)\n399 \n400         if spec.type == ModuleType.PKG_DIRECTORY:\n401             spec = spec._replace(submodule_search_locations=submodule_path)\n402 \n403     return spec\n404 \n[end of astroid/interpreter/_import/spec.py]\n[start of astroid/interpreter/_import/util.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import pathlib\n8 import sys\n9 from functools import lru_cache\n10 from importlib._bootstrap_external import _NamespacePath\n11 from importlib.util import _find_spec_from_path  # type: ignore[attr-defined]\n12 \n13 from astroid.const import IS_PYPY\n14 \n15 \n16 @lru_cache(maxsize=4096)\n17 def is_namespace(modname: str) -> bool:\n18     from astroid.modutils import (  # pylint: disable=import-outside-toplevel\n19         EXT_LIB_DIRS,\n20         STD_LIB_DIRS,\n21     )\n22 \n23     STD_AND_EXT_LIB_DIRS = STD_LIB_DIRS.union(EXT_LIB_DIRS)\n24 \n25     if modname in sys.builtin_module_names:\n26         return False\n27 \n28     found_spec = None\n29 \n30     # find_spec() attempts to import parent packages when given dotted paths.\n31     # That's unacceptable here, so we fallback to _find_spec_from_path(), which does\n32     # not, but requires instead that each single parent ('astroid', 'nodes', etc.)\n33     # be specced from left to right.\n34     processed_components = []\n35     last_submodule_search_locations: _NamespacePath | None = None\n36     for component in modname.split(\".\"):\n37         processed_components.append(component)\n38         working_modname = \".\".join(processed_components)\n39         try:\n40             # Both the modname and the path are built iteratively, with the\n41             # path (e.g. ['a', 'a/b', 'a/b/c']) lagging the modname by one\n42             found_spec = _find_spec_from_path(\n43                 working_modname, path=last_submodule_search_locations\n44             )\n45         except AttributeError:\n46             return False\n47         except ValueError:\n48             if modname == \"__main__\":\n49                 return False\n50             try:\n51                 # .pth files will be on sys.modules\n52                 # __spec__ is set inconsistently on PyPy so we can't really on the heuristic here\n53                 # See: https://foss.heptapod.net/pypy/pypy/-/issues/3736\n54                 # Check first fragment of modname, e.g. \"astroid\", not \"astroid.interpreter\"\n55                 # because of cffi's behavior\n56                 # See: https://github.com/PyCQA/astroid/issues/1776\n57                 mod = sys.modules[processed_components[0]]\n58                 return (\n59                     mod.__spec__ is None\n60                     and getattr(mod, \"__file__\", None) is None\n61                     and hasattr(mod, \"__path__\")\n62                     and not IS_PYPY\n63                 )\n64             except KeyError:\n65                 return False\n66             except AttributeError:\n67                 # Workaround for \"py\" module\n68                 # https://github.com/pytest-dev/apipkg/issues/13\n69                 return False\n70         except KeyError:\n71             # Intermediate steps might raise KeyErrors\n72             # https://github.com/python/cpython/issues/93334\n73             # TODO: update if fixed in importlib\n74             # For tree a > b > c.py\n75             # >>> from importlib.machinery import PathFinder\n76             # >>> PathFinder.find_spec('a.b', ['a'])\n77             # KeyError: 'a'\n78 \n79             # Repair last_submodule_search_locations\n80             if last_submodule_search_locations:\n81                 # TODO: py38: remove except\n82                 try:\n83                     # pylint: disable=unsubscriptable-object\n84                     last_item = last_submodule_search_locations[-1]\n85                 except TypeError:\n86                     last_item = last_submodule_search_locations._recalculate()[-1]\n87                 # e.g. for failure example above, add 'a/b' and keep going\n88                 # so that find_spec('a.b.c', path=['a', 'a/b']) succeeds\n89                 assumed_location = pathlib.Path(last_item) / component\n90                 last_submodule_search_locations.append(str(assumed_location))\n91             continue\n92 \n93         # Update last_submodule_search_locations for next iteration\n94         if found_spec and found_spec.submodule_search_locations:\n95             # But immediately return False if we can detect we are in stdlib\n96             # or external lib (e.g site-packages)\n97             if any(\n98                 any(location.startswith(lib_dir) for lib_dir in STD_AND_EXT_LIB_DIRS)\n99                 for location in found_spec.submodule_search_locations\n100             ):\n101                 return False\n102             last_submodule_search_locations = found_spec.submodule_search_locations\n103 \n104     return (\n105         found_spec is not None\n106         and found_spec.submodule_search_locations is not None\n107         and found_spec.origin is None\n108     )\n109 \n[end of astroid/interpreter/_import/util.py]\n[start of astroid/manager.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"astroid manager: avoid multiple astroid build of a same module when\n6 possible by providing a class responsible to get astroid representation\n7 from various source and using a cache of built modules)\n8 \"\"\"\n9 \n10 from __future__ import annotations\n11 \n12 import collections\n13 import os\n14 import types\n15 import zipimport\n16 from collections.abc import Callable, Iterator, Sequence\n17 from importlib.util import find_spec, module_from_spec\n18 from typing import Any, ClassVar\n19 \n20 from astroid import nodes\n21 from astroid._cache import CACHE_MANAGER\n22 from astroid.const import BRAIN_MODULES_DIRECTORY\n23 from astroid.context import InferenceContext\n24 from astroid.exceptions import AstroidBuildingError, AstroidImportError\n25 from astroid.interpreter._import import spec, util\n26 from astroid.modutils import (\n27     NoSourceFile,\n28     _cache_normalize_path_,\n29     file_info_from_modpath,\n30     get_source_file,\n31     is_module_name_part_of_extension_package_whitelist,\n32     is_python_source,\n33     is_standard_module,\n34     load_module_from_name,\n35     modpath_from_file,\n36 )\n37 from astroid.transforms import TransformVisitor\n38 from astroid.typing import AstroidManagerBrain, InferenceResult\n39 \n40 ZIP_IMPORT_EXTS = (\".zip\", \".egg\", \".whl\", \".pyz\", \".pyzw\")\n41 \n42 \n43 def safe_repr(obj: Any) -> str:\n44     try:\n45         return repr(obj)\n46     except Exception:  # pylint: disable=broad-except\n47         return \"???\"\n48 \n49 \n50 class AstroidManager:\n51     \"\"\"Responsible to build astroid from files or modules.\n52 \n53     Use the Borg (singleton) pattern.\n54     \"\"\"\n55 \n56     name = \"astroid loader\"\n57     brain: AstroidManagerBrain = {\n58         \"astroid_cache\": {},\n59         \"_mod_file_cache\": {},\n60         \"_failed_import_hooks\": [],\n61         \"always_load_extensions\": False,\n62         \"optimize_ast\": False,\n63         \"extension_package_whitelist\": set(),\n64         \"_transform\": TransformVisitor(),\n65     }\n66     max_inferable_values: ClassVar[int] = 100\n67 \n68     def __init__(self) -> None:\n69         # NOTE: cache entries are added by the [re]builder\n70         self.astroid_cache = AstroidManager.brain[\"astroid_cache\"]\n71         self._mod_file_cache = AstroidManager.brain[\"_mod_file_cache\"]\n72         self._failed_import_hooks = AstroidManager.brain[\"_failed_import_hooks\"]\n73         self.always_load_extensions = AstroidManager.brain[\"always_load_extensions\"]\n74         self.optimize_ast = AstroidManager.brain[\"optimize_ast\"]\n75         self.extension_package_whitelist = AstroidManager.brain[\n76             \"extension_package_whitelist\"\n77         ]\n78         self._transform = AstroidManager.brain[\"_transform\"]\n79 \n80     @property\n81     def register_transform(self):\n82         # This and unregister_transform below are exported for convenience\n83         return self._transform.register_transform\n84 \n85     @property\n86     def unregister_transform(self):\n87         return self._transform.unregister_transform\n88 \n89     @property\n90     def builtins_module(self) -> nodes.Module:\n91         return self.astroid_cache[\"builtins\"]\n92 \n93     def visit_transforms(self, node: nodes.NodeNG) -> InferenceResult:\n94         \"\"\"Visit the transforms and apply them to the given *node*.\"\"\"\n95         return self._transform.visit(node)\n96 \n97     def ast_from_file(\n98         self,\n99         filepath: str,\n100         modname: str | None = None,\n101         fallback: bool = True,\n102         source: bool = False,\n103     ) -> nodes.Module:\n104         \"\"\"Given a module name, return the astroid object.\"\"\"\n105         try:\n106             filepath = get_source_file(filepath, include_no_ext=True)\n107             source = True\n108         except NoSourceFile:\n109             pass\n110         if modname is None:\n111             try:\n112                 modname = \".\".join(modpath_from_file(filepath))\n113             except ImportError:\n114                 modname = filepath\n115         if (\n116             modname in self.astroid_cache\n117             and self.astroid_cache[modname].file == filepath\n118         ):\n119             return self.astroid_cache[modname]\n120         if source:\n121             # pylint: disable=import-outside-toplevel; circular import\n122             from astroid.builder import AstroidBuilder\n123 \n124             return AstroidBuilder(self).file_build(filepath, modname)\n125         if fallback and modname:\n126             return self.ast_from_module_name(modname)\n127         raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n128 \n129     def ast_from_string(\n130         self, data: str, modname: str = \"\", filepath: str | None = None\n131     ) -> nodes.Module:\n132         \"\"\"Given some source code as a string, return its corresponding astroid\n133         object.\n134         \"\"\"\n135         # pylint: disable=import-outside-toplevel; circular import\n136         from astroid.builder import AstroidBuilder\n137 \n138         return AstroidBuilder(self).string_build(data, modname, filepath)\n139 \n140     def _build_stub_module(self, modname: str) -> nodes.Module:\n141         # pylint: disable=import-outside-toplevel; circular import\n142         from astroid.builder import AstroidBuilder\n143 \n144         return AstroidBuilder(self).string_build(\"\", modname)\n145 \n146     def _build_namespace_module(\n147         self, modname: str, path: Sequence[str]\n148     ) -> nodes.Module:\n149         # pylint: disable=import-outside-toplevel; circular import\n150         from astroid.builder import build_namespace_package_module\n151 \n152         return build_namespace_package_module(modname, path)\n153 \n154     def _can_load_extension(self, modname: str) -> bool:\n155         if self.always_load_extensions:\n156             return True\n157         if is_standard_module(modname):\n158             return True\n159         return is_module_name_part_of_extension_package_whitelist(\n160             modname, self.extension_package_whitelist\n161         )\n162 \n163     def ast_from_module_name(  # noqa: C901\n164         self,\n165         modname: str | None,\n166         context_file: str | None = None,\n167         use_cache: bool = True,\n168     ) -> nodes.Module:\n169         \"\"\"Given a module name, return the astroid object.\"\"\"\n170         if modname is None:\n171             raise AstroidBuildingError(\"No module name given.\")\n172         # Sometimes we don't want to use the cache. For example, when we're\n173         # importing a module with the same name as the file that is importing\n174         # we want to fallback on the import system to make sure we get the correct\n175         # module.\n176         if modname in self.astroid_cache and use_cache:\n177             return self.astroid_cache[modname]\n178         if modname == \"__main__\":\n179             return self._build_stub_module(modname)\n180         if context_file:\n181             old_cwd = os.getcwd()\n182             os.chdir(os.path.dirname(context_file))\n183         try:\n184             found_spec = self.file_from_module_name(modname, context_file)\n185             if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n186                 module = self.zip_import_data(found_spec.location)\n187                 if module is not None:\n188                     return module\n189 \n190             elif found_spec.type in (\n191                 spec.ModuleType.C_BUILTIN,\n192                 spec.ModuleType.C_EXTENSION,\n193             ):\n194                 if (\n195                     found_spec.type == spec.ModuleType.C_EXTENSION\n196                     and not self._can_load_extension(modname)\n197                 ):\n198                     return self._build_stub_module(modname)\n199                 try:\n200                     named_module = load_module_from_name(modname)\n201                 except Exception as e:\n202                     raise AstroidImportError(\n203                         \"Loading {modname} failed with:\\n{error}\",\n204                         modname=modname,\n205                         path=found_spec.location,\n206                     ) from e\n207                 return self.ast_from_module(named_module, modname)\n208 \n209             elif found_spec.type == spec.ModuleType.PY_COMPILED:\n210                 raise AstroidImportError(\n211                     \"Unable to load compiled module {modname}.\",\n212                     modname=modname,\n213                     path=found_spec.location,\n214                 )\n215 \n216             elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n217                 return self._build_namespace_module(\n218                     modname, found_spec.submodule_search_locations or []\n219                 )\n220             elif found_spec.type == spec.ModuleType.PY_FROZEN:\n221                 if found_spec.location is None:\n222                     return self._build_stub_module(modname)\n223                 # For stdlib frozen modules we can determine the location and\n224                 # can therefore create a module from the source file\n225                 return self.ast_from_file(found_spec.location, modname, fallback=False)\n226 \n227             if found_spec.location is None:\n228                 raise AstroidImportError(\n229                     \"Can't find a file for module {modname}.\", modname=modname\n230                 )\n231 \n232             return self.ast_from_file(found_spec.location, modname, fallback=False)\n233         except AstroidBuildingError as e:\n234             for hook in self._failed_import_hooks:\n235                 try:\n236                     return hook(modname)\n237                 except AstroidBuildingError:\n238                     pass\n239             raise e\n240         finally:\n241             if context_file:\n242                 os.chdir(old_cwd)\n243 \n244     def zip_import_data(self, filepath: str) -> nodes.Module | None:\n245         if zipimport is None:\n246             return None\n247 \n248         # pylint: disable=import-outside-toplevel; circular import\n249         from astroid.builder import AstroidBuilder\n250 \n251         builder = AstroidBuilder(self)\n252         for ext in ZIP_IMPORT_EXTS:\n253             try:\n254                 eggpath, resource = filepath.rsplit(ext + os.path.sep, 1)\n255             except ValueError:\n256                 continue\n257             try:\n258                 # pylint: disable-next=no-member\n259                 importer = zipimport.zipimporter(eggpath + ext)\n260                 zmodname = resource.replace(os.path.sep, \".\")\n261                 if importer.is_package(resource):\n262                     zmodname = zmodname + \".__init__\"\n263                 module = builder.string_build(\n264                     importer.get_source(resource), zmodname, filepath\n265                 )\n266                 return module\n267             except Exception:  # pylint: disable=broad-except\n268                 continue\n269         return None\n270 \n271     def file_from_module_name(\n272         self, modname: str, contextfile: str | None\n273     ) -> spec.ModuleSpec:\n274         try:\n275             value = self._mod_file_cache[(modname, contextfile)]\n276         except KeyError:\n277             try:\n278                 value = file_info_from_modpath(\n279                     modname.split(\".\"), context_file=contextfile\n280                 )\n281             except ImportError as e:\n282                 # pylint: disable-next=redefined-variable-type\n283                 value = AstroidImportError(\n284                     \"Failed to import module {modname} with error:\\n{error}.\",\n285                     modname=modname,\n286                     # we remove the traceback here to save on memory usage (since these exceptions are cached)\n287                     error=e.with_traceback(None),\n288                 )\n289             self._mod_file_cache[(modname, contextfile)] = value\n290         if isinstance(value, AstroidBuildingError):\n291             # we remove the traceback here to save on memory usage (since these exceptions are cached)\n292             raise value.with_traceback(None)  # pylint: disable=no-member\n293         return value\n294 \n295     def ast_from_module(\n296         self, module: types.ModuleType, modname: str | None = None\n297     ) -> nodes.Module:\n298         \"\"\"Given an imported module, return the astroid object.\"\"\"\n299         modname = modname or module.__name__\n300         if modname in self.astroid_cache:\n301             return self.astroid_cache[modname]\n302         try:\n303             # some builtin modules don't have __file__ attribute\n304             filepath = module.__file__\n305             if is_python_source(filepath):\n306                 # Type is checked in is_python_source\n307                 return self.ast_from_file(filepath, modname)  # type: ignore[arg-type]\n308         except AttributeError:\n309             pass\n310 \n311         # pylint: disable=import-outside-toplevel; circular import\n312         from astroid.builder import AstroidBuilder\n313 \n314         return AstroidBuilder(self).module_build(module, modname)\n315 \n316     def ast_from_class(self, klass: type, modname: str | None = None) -> nodes.ClassDef:\n317         \"\"\"Get astroid for the given class.\"\"\"\n318         if modname is None:\n319             try:\n320                 modname = klass.__module__\n321             except AttributeError as exc:\n322                 raise AstroidBuildingError(\n323                     \"Unable to get module for class {class_name}.\",\n324                     cls=klass,\n325                     class_repr=safe_repr(klass),\n326                     modname=modname,\n327                 ) from exc\n328         modastroid = self.ast_from_module_name(modname)\n329         ret = modastroid.getattr(klass.__name__)[0]\n330         assert isinstance(ret, nodes.ClassDef)\n331         return ret\n332 \n333     def infer_ast_from_something(\n334         self, obj: object, context: InferenceContext | None = None\n335     ) -> Iterator[InferenceResult]:\n336         \"\"\"Infer astroid for the given class.\"\"\"\n337         if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n338             klass = obj.__class__\n339         elif isinstance(obj, type):\n340             klass = obj\n341         else:\n342             raise AstroidBuildingError(  # pragma: no cover\n343                 \"Unable to get type for {class_repr}.\",\n344                 cls=None,\n345                 class_repr=safe_repr(obj),\n346             )\n347         try:\n348             modname = klass.__module__\n349         except AttributeError as exc:\n350             raise AstroidBuildingError(\n351                 \"Unable to get module for {class_repr}.\",\n352                 cls=klass,\n353                 class_repr=safe_repr(klass),\n354             ) from exc\n355         except Exception as exc:\n356             raise AstroidImportError(\n357                 \"Unexpected error while retrieving module for {class_repr}:\\n\"\n358                 \"{error}\",\n359                 cls=klass,\n360                 class_repr=safe_repr(klass),\n361             ) from exc\n362         try:\n363             name = klass.__name__\n364         except AttributeError as exc:\n365             raise AstroidBuildingError(\n366                 \"Unable to get name for {class_repr}:\\n\",\n367                 cls=klass,\n368                 class_repr=safe_repr(klass),\n369             ) from exc\n370         except Exception as exc:\n371             raise AstroidImportError(\n372                 \"Unexpected error while retrieving name for {class_repr}:\\n{error}\",\n373                 cls=klass,\n374                 class_repr=safe_repr(klass),\n375             ) from exc\n376         # take care, on living object __module__ is regularly wrong :(\n377         modastroid = self.ast_from_module_name(modname)\n378         if klass is obj:\n379             for inferred in modastroid.igetattr(name, context):\n380                 yield inferred\n381         else:\n382             for inferred in modastroid.igetattr(name, context):\n383                 yield inferred.instantiate_class()\n384 \n385     def register_failed_import_hook(self, hook: Callable[[str], nodes.Module]) -> None:\n386         \"\"\"Registers a hook to resolve imports that cannot be found otherwise.\n387 \n388         `hook` must be a function that accepts a single argument `modname` which\n389         contains the name of the module or package that could not be imported.\n390         If `hook` can resolve the import, must return a node of type `astroid.Module`,\n391         otherwise, it must raise `AstroidBuildingError`.\n392         \"\"\"\n393         self._failed_import_hooks.append(hook)\n394 \n395     def cache_module(self, module: nodes.Module) -> None:\n396         \"\"\"Cache a module if no module with the same name is known yet.\"\"\"\n397         self.astroid_cache.setdefault(module.name, module)\n398 \n399     def bootstrap(self) -> None:\n400         \"\"\"Bootstrap the required AST modules needed for the manager to work.\n401 \n402         The bootstrap usually involves building the AST for the builtins\n403         module, which is required by the rest of astroid to work correctly.\n404         \"\"\"\n405         from astroid import raw_building  # pylint: disable=import-outside-toplevel\n406 \n407         raw_building._astroid_bootstrapping()\n408 \n409     def clear_cache(self) -> None:\n410         \"\"\"Clear the underlying cache, bootstrap the builtins module and\n411         re-register transforms.\n412         \"\"\"\n413         # import here because of cyclic imports\n414         # pylint: disable=import-outside-toplevel\n415         from astroid.inference_tip import clear_inference_tip_cache\n416         from astroid.interpreter.objectmodel import ObjectModel\n417         from astroid.nodes.node_classes import LookupMixIn\n418         from astroid.nodes.scoped_nodes import ClassDef\n419 \n420         clear_inference_tip_cache()\n421 \n422         self.astroid_cache.clear()\n423         # NB: not a new TransformVisitor()\n424         AstroidManager.brain[\"_transform\"].transforms = collections.defaultdict(list)\n425 \n426         CACHE_MANAGER.clear_all_caches()\n427 \n428         for lru_cache in (\n429             LookupMixIn.lookup,\n430             _cache_normalize_path_,\n431             util.is_namespace,\n432             ObjectModel.attributes,\n433             ClassDef._metaclass_lookup_attribute,\n434         ):\n435             lru_cache.cache_clear()  # type: ignore[attr-defined]\n436 \n437         self.bootstrap()\n438 \n439         # Reload brain plugins. During initialisation this is done in astroid.__init__.py\n440         for module in BRAIN_MODULES_DIRECTORY.iterdir():\n441             if module.suffix == \".py\":\n442                 module_spec = find_spec(f\"astroid.brain.{module.stem}\")\n443                 assert module_spec\n444                 module_object = module_from_spec(module_spec)\n445                 assert module_spec.loader\n446                 module_spec.loader.exec_module(module_object)\n447 \n[end of astroid/manager.py]\n[start of astroid/modutils.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Python modules manipulation utility functions.\n6 \n7 :type PY_SOURCE_EXTS: tuple(str)\n8 :var PY_SOURCE_EXTS: list of possible python source file extension\n9 \n10 :type STD_LIB_DIRS: set of str\n11 :var STD_LIB_DIRS: directories where standard modules are located\n12 \n13 :type BUILTIN_MODULES: dict\n14 :var BUILTIN_MODULES: dictionary with builtin module names has key\n15 \"\"\"\n16 \n17 from __future__ import annotations\n18 \n19 import importlib\n20 import importlib.machinery\n21 import importlib.util\n22 import io\n23 import itertools\n24 import logging\n25 import os\n26 import sys\n27 import sysconfig\n28 import types\n29 from collections.abc import Callable, Iterable, Sequence\n30 from contextlib import redirect_stderr, redirect_stdout\n31 from functools import lru_cache\n32 from pathlib import Path\n33 \n34 from astroid.const import IS_JYTHON, IS_PYPY\n35 from astroid.interpreter._import import spec, util\n36 \n37 logger = logging.getLogger(__name__)\n38 \n39 \n40 if sys.platform.startswith(\"win\"):\n41     PY_SOURCE_EXTS = (\"py\", \"pyw\")\n42     PY_COMPILED_EXTS = (\"dll\", \"pyd\")\n43 else:\n44     PY_SOURCE_EXTS = (\"py\",)\n45     PY_COMPILED_EXTS = (\"so\",)\n46 \n47 \n48 # TODO: Adding `platstdlib` is a fix for a workaround in virtualenv. At some point we should\n49 # revisit whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1323.\n50 STD_LIB_DIRS = {sysconfig.get_path(\"stdlib\"), sysconfig.get_path(\"platstdlib\")}\n51 \n52 if os.name == \"nt\":\n53     STD_LIB_DIRS.add(os.path.join(sys.prefix, \"dlls\"))\n54     try:\n55         # real_prefix is defined when running inside virtual environments,\n56         # created with the **virtualenv** library.\n57         # Deprecated in virtualenv==16.7.9\n58         # See: https://github.com/pypa/virtualenv/issues/1622\n59         STD_LIB_DIRS.add(os.path.join(sys.real_prefix, \"dlls\"))  # type: ignore[attr-defined]\n60     except AttributeError:\n61         # sys.base_exec_prefix is always defined, but in a virtual environment\n62         # created with the stdlib **venv** module, it points to the original\n63         # installation, if the virtual env is activated.\n64         try:\n65             STD_LIB_DIRS.add(os.path.join(sys.base_exec_prefix, \"dlls\"))\n66         except AttributeError:\n67             pass\n68 \n69 if IS_PYPY and sys.version_info < (3, 8):\n70     # PyPy stores the stdlib in two places: sys.prefix/lib_pypy and sys.prefix/lib-python/3\n71     # sysconfig.get_path on PyPy returns the first, but without an underscore so we patch this manually.\n72     # Beginning with 3.8 the stdlib is only stored in: sys.prefix/pypy{py_version_short}\n73     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib_pypy\"))\n74     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib-python/3\"))\n75 \n76     # TODO: This is a fix for a workaround in virtualenv. At some point we should revisit\n77     # whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1324.\n78     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib_pypy\"))\n79     STD_LIB_DIRS.add(\n80         str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib-python/3\")\n81     )\n82 \n83 if os.name == \"posix\":\n84     # Need the real prefix if we're in a virtualenv, otherwise\n85     # the usual one will do.\n86     # Deprecated in virtualenv==16.7.9\n87     # See: https://github.com/pypa/virtualenv/issues/1622\n88     try:\n89         prefix: str = sys.real_prefix  # type: ignore[attr-defined]\n90     except AttributeError:\n91         prefix = sys.prefix\n92 \n93     def _posix_path(path: str) -> str:\n94         base_python = \"python%d.%d\" % sys.version_info[:2]\n95         return os.path.join(prefix, path, base_python)\n96 \n97     STD_LIB_DIRS.add(_posix_path(\"lib\"))\n98     if sys.maxsize > 2**32:\n99         # This tries to fix a problem with /usr/lib64 builds,\n100         # where systems are running both 32-bit and 64-bit code\n101         # on the same machine, which reflects into the places where\n102         # standard library could be found. More details can be found\n103         # here http://bugs.python.org/issue1294959.\n104         # An easy reproducing case would be\n105         # https://github.com/PyCQA/pylint/issues/712#issuecomment-163178753\n106         STD_LIB_DIRS.add(_posix_path(\"lib64\"))\n107 \n108 EXT_LIB_DIRS = {sysconfig.get_path(\"purelib\"), sysconfig.get_path(\"platlib\")}\n109 BUILTIN_MODULES = dict.fromkeys(sys.builtin_module_names, True)\n110 \n111 \n112 class NoSourceFile(Exception):\n113     \"\"\"Exception raised when we are not able to get a python\n114     source file for a precompiled file.\n115     \"\"\"\n116 \n117 \n118 def _normalize_path(path: str) -> str:\n119     \"\"\"Resolve symlinks in path and convert to absolute path.\n120 \n121     Note that environment variables and ~ in the path need to be expanded in\n122     advance.\n123 \n124     This can be cached by using _cache_normalize_path.\n125     \"\"\"\n126     return os.path.normcase(os.path.realpath(path))\n127 \n128 \n129 def _path_from_filename(filename: str, is_jython: bool = IS_JYTHON) -> str:\n130     if not is_jython:\n131         return filename\n132     head, has_pyclass, _ = filename.partition(\"$py.class\")\n133     if has_pyclass:\n134         return head + \".py\"\n135     return filename\n136 \n137 \n138 def _handle_blacklist(\n139     blacklist: Sequence[str], dirnames: list[str], filenames: list[str]\n140 ) -> None:\n141     \"\"\"Remove files/directories in the black list.\n142 \n143     dirnames/filenames are usually from os.walk\n144     \"\"\"\n145     for norecurs in blacklist:\n146         if norecurs in dirnames:\n147             dirnames.remove(norecurs)\n148         elif norecurs in filenames:\n149             filenames.remove(norecurs)\n150 \n151 \n152 @lru_cache()\n153 def _cache_normalize_path_(path: str) -> str:\n154     return _normalize_path(path)\n155 \n156 \n157 def _cache_normalize_path(path: str) -> str:\n158     \"\"\"Normalize path with caching.\"\"\"\n159     # _module_file calls abspath on every path in sys.path every time it's\n160     # called; on a larger codebase this easily adds up to half a second just\n161     # assembling path components. This cache alleviates that.\n162     if not path:  # don't cache result for ''\n163         return _normalize_path(path)\n164     return _cache_normalize_path_(path)\n165 \n166 \n167 def load_module_from_name(dotted_name: str) -> types.ModuleType:\n168     \"\"\"Load a Python module from its name.\n169 \n170     :type dotted_name: str\n171     :param dotted_name: python name of a module or package\n172 \n173     :raise ImportError: if the module or package is not found\n174 \n175     :rtype: module\n176     :return: the loaded module\n177     \"\"\"\n178     try:\n179         return sys.modules[dotted_name]\n180     except KeyError:\n181         pass\n182 \n183     # Capture and log anything emitted during import to avoid\n184     # contaminating JSON reports in pylint\n185     with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(\n186         io.StringIO()\n187     ) as stdout:\n188         module = importlib.import_module(dotted_name)\n189 \n190     stderr_value = stderr.getvalue()\n191     if stderr_value:\n192         logger.error(\n193             \"Captured stderr while importing %s:\\n%s\", dotted_name, stderr_value\n194         )\n195     stdout_value = stdout.getvalue()\n196     if stdout_value:\n197         logger.info(\n198             \"Captured stdout while importing %s:\\n%s\", dotted_name, stdout_value\n199         )\n200 \n201     return module\n202 \n203 \n204 def load_module_from_modpath(parts: Sequence[str]) -> types.ModuleType:\n205     \"\"\"Load a python module from its split name.\n206 \n207     :param parts:\n208       python name of a module or package split on '.'\n209 \n210     :raise ImportError: if the module or package is not found\n211 \n212     :return: the loaded module\n213     \"\"\"\n214     return load_module_from_name(\".\".join(parts))\n215 \n216 \n217 def load_module_from_file(filepath: str) -> types.ModuleType:\n218     \"\"\"Load a Python module from it's path.\n219 \n220     :type filepath: str\n221     :param filepath: path to the python module or package\n222 \n223     :raise ImportError: if the module or package is not found\n224 \n225     :rtype: module\n226     :return: the loaded module\n227     \"\"\"\n228     modpath = modpath_from_file(filepath)\n229     return load_module_from_modpath(modpath)\n230 \n231 \n232 def check_modpath_has_init(path: str, mod_path: list[str]) -> bool:\n233     \"\"\"Check there are some __init__.py all along the way.\"\"\"\n234     modpath: list[str] = []\n235     for part in mod_path:\n236         modpath.append(part)\n237         path = os.path.join(path, part)\n238         if not _has_init(path):\n239             old_namespace = util.is_namespace(\".\".join(modpath))\n240             if not old_namespace:\n241                 return False\n242     return True\n243 \n244 \n245 def _get_relative_base_path(filename: str, path_to_check: str) -> list[str] | None:\n246     \"\"\"Extracts the relative mod path of the file to import from.\n247 \n248     Check if a file is within the passed in path and if so, returns the\n249     relative mod path from the one passed in.\n250 \n251     If the filename is no in path_to_check, returns None\n252 \n253     Note this function will look for both abs and realpath of the file,\n254     this allows to find the relative base path even if the file is a\n255     symlink of a file in the passed in path\n256 \n257     Examples:\n258         _get_relative_base_path(\"/a/b/c/d.py\", \"/a/b\") ->  [\"c\",\"d\"]\n259         _get_relative_base_path(\"/a/b/c/d.py\", \"/dev\") ->  None\n260     \"\"\"\n261     importable_path = None\n262     path_to_check = os.path.normcase(path_to_check)\n263     abs_filename = os.path.abspath(filename)\n264     if os.path.normcase(abs_filename).startswith(path_to_check):\n265         importable_path = abs_filename\n266 \n267     real_filename = os.path.realpath(filename)\n268     if os.path.normcase(real_filename).startswith(path_to_check):\n269         importable_path = real_filename\n270 \n271     # if \"var\" in path_to_check:\n272     #     breakpoint()\n273 \n274     if importable_path:\n275         base_path = os.path.splitext(importable_path)[0]\n276         relative_base_path = base_path[len(path_to_check) :]\n277         return [pkg for pkg in relative_base_path.split(os.sep) if pkg]\n278 \n279     return None\n280 \n281 \n282 def modpath_from_file_with_callback(\n283     filename: str,\n284     path: Sequence[str] | None = None,\n285     is_package_cb: Callable[[str, list[str]], bool] | None = None,\n286 ) -> list[str]:\n287     filename = os.path.expanduser(_path_from_filename(filename))\n288     paths_to_check = sys.path.copy()\n289     if path:\n290         paths_to_check += path\n291     for pathname in itertools.chain(\n292         paths_to_check, map(_cache_normalize_path, paths_to_check)\n293     ):\n294         if not pathname:\n295             continue\n296         modpath = _get_relative_base_path(filename, pathname)\n297         if not modpath:\n298             continue\n299         assert is_package_cb is not None\n300         if is_package_cb(pathname, modpath[:-1]):\n301             return modpath\n302 \n303     raise ImportError(\n304         \"Unable to find module for {} in {}\".format(filename, \", \\n\".join(sys.path))\n305     )\n306 \n307 \n308 def modpath_from_file(filename: str, path: Sequence[str] | None = None) -> list[str]:\n309     \"\"\"Get the corresponding split module's name from a filename.\n310 \n311     This function will return the name of a module or package split on `.`.\n312 \n313     :type filename: str\n314     :param filename: file's path for which we want the module's name\n315 \n316     :type Optional[List[str]] path:\n317       Optional list of path where the module or package should be\n318       searched (use sys.path if nothing or None is given)\n319 \n320     :raise ImportError:\n321       if the corresponding module's name has not been found\n322 \n323     :rtype: list(str)\n324     :return: the corresponding split module's name\n325     \"\"\"\n326     return modpath_from_file_with_callback(filename, path, check_modpath_has_init)\n327 \n328 \n329 def file_from_modpath(\n330     modpath: list[str],\n331     path: Sequence[str] | None = None,\n332     context_file: str | None = None,\n333 ) -> str | None:\n334     return file_info_from_modpath(modpath, path, context_file).location\n335 \n336 \n337 def file_info_from_modpath(\n338     modpath: list[str],\n339     path: Sequence[str] | None = None,\n340     context_file: str | None = None,\n341 ) -> spec.ModuleSpec:\n342     \"\"\"given a mod path (i.e. split module / package name), return the\n343     corresponding file, giving priority to source file over precompiled\n344     file if it exists\n345 \n346     :param modpath:\n347       split module's name (i.e name of a module or package split\n348       on '.')\n349       (this means explicit relative imports that start with dots have\n350       empty strings in this list!)\n351 \n352     :param path:\n353       optional list of path where the module or package should be\n354       searched (use sys.path if nothing or None is given)\n355 \n356     :param context_file:\n357       context file to consider, necessary if the identifier has been\n358       introduced using a relative import unresolvable in the actual\n359       context (i.e. modutils)\n360 \n361     :raise ImportError: if there is no such module in the directory\n362 \n363     :return:\n364       the path to the module's file or None if it's an integrated\n365       builtin module such as 'sys'\n366     \"\"\"\n367     if context_file is not None:\n368         context: str | None = os.path.dirname(context_file)\n369     else:\n370         context = context_file\n371     if modpath[0] == \"xml\":\n372         # handle _xmlplus\n373         try:\n374             return _spec_from_modpath([\"_xmlplus\"] + modpath[1:], path, context)\n375         except ImportError:\n376             return _spec_from_modpath(modpath, path, context)\n377     elif modpath == [\"os\", \"path\"]:\n378         # FIXME: currently ignoring search_path...\n379         return spec.ModuleSpec(\n380             name=\"os.path\",\n381             location=os.path.__file__,\n382             type=spec.ModuleType.PY_SOURCE,\n383         )\n384     return _spec_from_modpath(modpath, path, context)\n385 \n386 \n387 def get_module_part(dotted_name: str, context_file: str | None = None) -> str:\n388     \"\"\"Given a dotted name return the module part of the name :\n389 \n390     >>> get_module_part('astroid.as_string.dump')\n391     'astroid.as_string'\n392 \n393     :param dotted_name: full name of the identifier we are interested in\n394 \n395     :param context_file:\n396       context file to consider, necessary if the identifier has been\n397       introduced using a relative import unresolvable in the actual\n398       context (i.e. modutils)\n399 \n400     :raise ImportError: if there is no such module in the directory\n401 \n402     :return:\n403       the module part of the name or None if we have not been able at\n404       all to import the given name\n405 \n406     XXX: deprecated, since it doesn't handle package precedence over module\n407     (see #10066)\n408     \"\"\"\n409     # os.path trick\n410     if dotted_name.startswith(\"os.path\"):\n411         return \"os.path\"\n412     parts = dotted_name.split(\".\")\n413     if context_file is not None:\n414         # first check for builtin module which won't be considered latter\n415         # in that case (path != None)\n416         if parts[0] in BUILTIN_MODULES:\n417             if len(parts) > 2:\n418                 raise ImportError(dotted_name)\n419             return parts[0]\n420         # don't use += or insert, we want a new list to be created !\n421     path: list[str] | None = None\n422     starti = 0\n423     if parts[0] == \"\":\n424         assert (\n425             context_file is not None\n426         ), \"explicit relative import, but no context_file?\"\n427         path = []  # prevent resolving the import non-relatively\n428         starti = 1\n429     while parts[starti] == \"\":  # for all further dots: change context\n430         starti += 1\n431         assert (\n432             context_file is not None\n433         ), \"explicit relative import, but no context_file?\"\n434         context_file = os.path.dirname(context_file)\n435     for i in range(starti, len(parts)):\n436         try:\n437             file_from_modpath(\n438                 parts[starti : i + 1], path=path, context_file=context_file\n439             )\n440         except ImportError:\n441             if i < max(1, len(parts) - 2):\n442                 raise\n443             return \".\".join(parts[:i])\n444     return dotted_name\n445 \n446 \n447 def get_module_files(\n448     src_directory: str, blacklist: Sequence[str], list_all: bool = False\n449 ) -> list[str]:\n450     \"\"\"Given a package directory return a list of all available python\n451     module's files in the package and its subpackages.\n452 \n453     :param src_directory:\n454       path of the directory corresponding to the package\n455 \n456     :param blacklist: iterable\n457       list of files or directories to ignore.\n458 \n459     :param list_all:\n460         get files from all paths, including ones without __init__.py\n461 \n462     :return:\n463       the list of all available python module's files in the package and\n464       its subpackages\n465     \"\"\"\n466     files: list[str] = []\n467     for directory, dirnames, filenames in os.walk(src_directory):\n468         if directory in blacklist:\n469             continue\n470         _handle_blacklist(blacklist, dirnames, filenames)\n471         # check for __init__.py\n472         if not list_all and \"__init__.py\" not in filenames:\n473             dirnames[:] = ()\n474             continue\n475         for filename in filenames:\n476             if _is_python_file(filename):\n477                 src = os.path.join(directory, filename)\n478                 files.append(src)\n479     return files\n480 \n481 \n482 def get_source_file(filename: str, include_no_ext: bool = False) -> str:\n483     \"\"\"Given a python module's file name return the matching source file\n484     name (the filename will be returned identically if it's already an.\n485 \n486     absolute path to a python source file...)\n487 \n488     :param filename: python module's file name\n489 \n490     :raise NoSourceFile: if no source file exists on the file system\n491 \n492     :return: the absolute path of the source file if it exists\n493     \"\"\"\n494     filename = os.path.abspath(_path_from_filename(filename))\n495     base, orig_ext = os.path.splitext(filename)\n496     for ext in PY_SOURCE_EXTS:\n497         source_path = f\"{base}.{ext}\"\n498         if os.path.exists(source_path):\n499             return source_path\n500     if include_no_ext and not orig_ext and os.path.exists(base):\n501         return base\n502     raise NoSourceFile(filename)\n503 \n504 \n505 def is_python_source(filename: str | None) -> bool:\n506     \"\"\"Return: True if the filename is a python source file.\"\"\"\n507     if not filename:\n508         return False\n509     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n510 \n511 \n512 def is_standard_module(modname: str, std_path: Iterable[str] | None = None) -> bool:\n513     \"\"\"Try to guess if a module is a standard python module (by default,\n514     see `std_path` parameter's description).\n515 \n516     :param modname: name of the module we are interested in\n517 \n518     :param std_path: list of path considered has standard\n519 \n520     :return:\n521       true if the module:\n522       - is located on the path listed in one of the directory in `std_path`\n523       - is a built-in module\n524     \"\"\"\n525     modname = modname.split(\".\")[0]\n526     try:\n527         filename = file_from_modpath([modname])\n528     except ImportError:\n529         # import failed, i'm probably not so wrong by supposing it's\n530         # not standard...\n531         return False\n532     # modules which are not living in a file are considered standard\n533     # (sys and __builtin__ for instance)\n534     if filename is None:\n535         # we assume there are no namespaces in stdlib\n536         return not util.is_namespace(modname)\n537     filename = _normalize_path(filename)\n538     for path in EXT_LIB_DIRS:\n539         if filename.startswith(_cache_normalize_path(path)):\n540             return False\n541     if std_path is None:\n542         std_path = STD_LIB_DIRS\n543 \n544     return any(filename.startswith(_cache_normalize_path(path)) for path in std_path)\n545 \n546 \n547 def is_relative(modname: str, from_file: str) -> bool:\n548     \"\"\"Return true if the given module name is relative to the given\n549     file name.\n550 \n551     :param modname: name of the module we are interested in\n552 \n553     :param from_file:\n554       path of the module from which modname has been imported\n555 \n556     :return:\n557       true if the module has been imported relatively to `from_file`\n558     \"\"\"\n559     if not os.path.isdir(from_file):\n560         from_file = os.path.dirname(from_file)\n561     if from_file in sys.path:\n562         return False\n563     return bool(\n564         importlib.machinery.PathFinder.find_spec(\n565             modname.split(\".\", maxsplit=1)[0], [from_file]\n566         )\n567     )\n568 \n569 \n570 # internal only functions #####################################################\n571 \n572 \n573 def _spec_from_modpath(\n574     modpath: list[str],\n575     path: Sequence[str] | None = None,\n576     context: str | None = None,\n577 ) -> spec.ModuleSpec:\n578     \"\"\"Given a mod path (i.e. split module / package name), return the\n579     corresponding spec.\n580 \n581     this function is used internally, see `file_from_modpath`'s\n582     documentation for more information\n583     \"\"\"\n584     assert modpath\n585     location = None\n586     if context is not None:\n587         try:\n588             found_spec = spec.find_spec(modpath, [context])\n589             location = found_spec.location\n590         except ImportError:\n591             found_spec = spec.find_spec(modpath, path)\n592             location = found_spec.location\n593     else:\n594         found_spec = spec.find_spec(modpath, path)\n595     if found_spec.type == spec.ModuleType.PY_COMPILED:\n596         try:\n597             assert found_spec.location is not None\n598             location = get_source_file(found_spec.location)\n599             return found_spec._replace(\n600                 location=location, type=spec.ModuleType.PY_SOURCE\n601             )\n602         except NoSourceFile:\n603             return found_spec._replace(location=location)\n604     elif found_spec.type == spec.ModuleType.C_BUILTIN:\n605         # integrated builtin module\n606         return found_spec._replace(location=None)\n607     elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n608         assert found_spec.location is not None\n609         location = _has_init(found_spec.location)\n610         return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n611     return found_spec\n612 \n613 \n614 def _is_python_file(filename: str) -> bool:\n615     \"\"\"Return true if the given filename should be considered as a python file.\n616 \n617     .pyc and .pyo are ignored\n618     \"\"\"\n619     return filename.endswith((\".py\", \".so\", \".pyd\", \".pyw\"))\n620 \n621 \n622 def _has_init(directory: str) -> str | None:\n623     \"\"\"If the given directory has a valid __init__ file, return its path,\n624     else return None.\n625     \"\"\"\n626     mod_or_pack = os.path.join(directory, \"__init__\")\n627     for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n628         if os.path.exists(mod_or_pack + \".\" + ext):\n629             return mod_or_pack + \".\" + ext\n630     return None\n631 \n632 \n633 def is_namespace(specobj: spec.ModuleSpec) -> bool:\n634     return specobj.type == spec.ModuleType.PY_NAMESPACE\n635 \n636 \n637 def is_directory(specobj: spec.ModuleSpec) -> bool:\n638     return specobj.type == spec.ModuleType.PKG_DIRECTORY\n639 \n640 \n641 def is_module_name_part_of_extension_package_whitelist(\n642     module_name: str, package_whitelist: set[str]\n643 ) -> bool:\n644     \"\"\"\n645     Returns True if one part of the module name is in the package whitelist.\n646 \n647     >>> is_module_name_part_of_extension_package_whitelist('numpy.core.umath', {'numpy'})\n648     True\n649     \"\"\"\n650     parts = module_name.split(\".\")\n651     return any(\n652         \".\".join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)\n653     )\n654 \n[end of astroid/modutils.py]\n[start of astroid/raw_building.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"this module contains a set of functions to create astroid trees from scratch\n6 (build_* functions) or from living object (object_build_* functions)\n7 \"\"\"\n8 \n9 from __future__ import annotations\n10 \n11 import builtins\n12 import inspect\n13 import os\n14 import sys\n15 import types\n16 import warnings\n17 from collections.abc import Iterable\n18 from typing import Any, Union\n19 \n20 from astroid import bases, nodes\n21 from astroid.const import _EMPTY_OBJECT_MARKER, IS_PYPY\n22 from astroid.manager import AstroidManager\n23 from astroid.nodes import node_classes\n24 \n25 _FunctionTypes = Union[\n26     types.FunctionType,\n27     types.MethodType,\n28     types.BuiltinFunctionType,\n29     types.WrapperDescriptorType,\n30     types.MethodDescriptorType,\n31     types.ClassMethodDescriptorType,\n32 ]\n33 \n34 # the keys of CONST_CLS eg python builtin types\n35 _CONSTANTS = tuple(node_classes.CONST_CLS)\n36 _BUILTINS = vars(builtins)\n37 TYPE_NONE = type(None)\n38 TYPE_NOTIMPLEMENTED = type(NotImplemented)\n39 TYPE_ELLIPSIS = type(...)\n40 \n41 \n42 def _attach_local_node(parent, node, name: str) -> None:\n43     node.name = name  # needed by add_local_node\n44     parent.add_local_node(node)\n45 \n46 \n47 def _add_dunder_class(func, member) -> None:\n48     \"\"\"Add a __class__ member to the given func node, if we can determine it.\"\"\"\n49     python_cls = member.__class__\n50     cls_name = getattr(python_cls, \"__name__\", None)\n51     if not cls_name:\n52         return\n53     cls_bases = [ancestor.__name__ for ancestor in python_cls.__bases__]\n54     ast_klass = build_class(cls_name, cls_bases, python_cls.__doc__)\n55     func.instance_attrs[\"__class__\"] = [ast_klass]\n56 \n57 \n58 def attach_dummy_node(node, name: str, runtime_object=_EMPTY_OBJECT_MARKER) -> None:\n59     \"\"\"create a dummy node and register it in the locals of the given\n60     node with the specified name\n61     \"\"\"\n62     enode = nodes.EmptyNode()\n63     enode.object = runtime_object\n64     _attach_local_node(node, enode, name)\n65 \n66 \n67 def attach_const_node(node, name: str, value) -> None:\n68     \"\"\"create a Const node and register it in the locals of the given\n69     node with the specified name\n70     \"\"\"\n71     if name not in node.special_attributes:\n72         _attach_local_node(node, nodes.const_factory(value), name)\n73 \n74 \n75 def attach_import_node(node, modname: str, membername: str) -> None:\n76     \"\"\"create a ImportFrom node and register it in the locals of the given\n77     node with the specified name\n78     \"\"\"\n79     from_node = nodes.ImportFrom(modname, [(membername, None)])\n80     _attach_local_node(node, from_node, membername)\n81 \n82 \n83 def build_module(name: str, doc: str | None = None) -> nodes.Module:\n84     \"\"\"create and initialize an astroid Module node\"\"\"\n85     node = nodes.Module(name, pure_python=False, package=False)\n86     node.postinit(\n87         body=[],\n88         doc_node=nodes.Const(value=doc) if doc else None,\n89     )\n90     return node\n91 \n92 \n93 def build_class(\n94     name: str, basenames: Iterable[str] = (), doc: str | None = None\n95 ) -> nodes.ClassDef:\n96     \"\"\"Create and initialize an astroid ClassDef node.\"\"\"\n97     node = nodes.ClassDef(name)\n98     node.postinit(\n99         bases=[nodes.Name(name=base, parent=node) for base in basenames],\n100         body=[],\n101         decorators=None,\n102         doc_node=nodes.Const(value=doc) if doc else None,\n103     )\n104     return node\n105 \n106 \n107 def build_function(\n108     name: str,\n109     args: list[str] | None = None,\n110     posonlyargs: list[str] | None = None,\n111     defaults: list[Any] | None = None,\n112     doc: str | None = None,\n113     kwonlyargs: list[str] | None = None,\n114 ) -> nodes.FunctionDef:\n115     \"\"\"create and initialize an astroid FunctionDef node\"\"\"\n116     # first argument is now a list of decorators\n117     func = nodes.FunctionDef(name)\n118     argsnode = nodes.Arguments(parent=func)\n119 \n120     # If args is None we don't have any information about the signature\n121     # (in contrast to when there are no arguments and args == []). We pass\n122     # this to the builder to indicate this.\n123     if args is not None:\n124         arguments = [nodes.AssignName(name=arg, parent=argsnode) for arg in args]\n125     else:\n126         arguments = None\n127 \n128     default_nodes: list[nodes.NodeNG] | None = []\n129     if defaults is not None:\n130         for default in defaults:\n131             default_node = nodes.const_factory(default)\n132             default_node.parent = argsnode\n133             default_nodes.append(default_node)\n134     else:\n135         default_nodes = None\n136 \n137     argsnode.postinit(\n138         args=arguments,\n139         defaults=default_nodes,\n140         kwonlyargs=[\n141             nodes.AssignName(name=arg, parent=argsnode) for arg in kwonlyargs or ()\n142         ],\n143         kw_defaults=[],\n144         annotations=[],\n145         posonlyargs=[\n146             nodes.AssignName(name=arg, parent=argsnode) for arg in posonlyargs or ()\n147         ],\n148     )\n149     func.postinit(\n150         args=argsnode,\n151         body=[],\n152         doc_node=nodes.Const(value=doc) if doc else None,\n153     )\n154     if args:\n155         register_arguments(func)\n156     return func\n157 \n158 \n159 def build_from_import(fromname: str, names: list[str]) -> nodes.ImportFrom:\n160     \"\"\"create and initialize an astroid ImportFrom import statement\"\"\"\n161     return nodes.ImportFrom(fromname, [(name, None) for name in names])\n162 \n163 \n164 def register_arguments(func: nodes.FunctionDef, args: list | None = None) -> None:\n165     \"\"\"add given arguments to local\n166 \n167     args is a list that may contains nested lists\n168     (i.e. def func(a, (b, c, d)): ...)\n169     \"\"\"\n170     # If no args are passed in, get the args from the function.\n171     if args is None:\n172         if func.args.vararg:\n173             func.set_local(func.args.vararg, func.args)\n174         if func.args.kwarg:\n175             func.set_local(func.args.kwarg, func.args)\n176         args = func.args.args\n177         # If the function has no args, there is nothing left to do.\n178         if args is None:\n179             return\n180     for arg in args:\n181         if isinstance(arg, nodes.AssignName):\n182             func.set_local(arg.name, arg)\n183         else:\n184             register_arguments(func, arg.elts)\n185 \n186 \n187 def object_build_class(\n188     node: nodes.Module | nodes.ClassDef, member: type, localname: str\n189 ) -> nodes.ClassDef:\n190     \"\"\"create astroid for a living class object\"\"\"\n191     basenames = [base.__name__ for base in member.__bases__]\n192     return _base_class_object_build(node, member, basenames, localname=localname)\n193 \n194 \n195 def _get_args_info_from_callable(\n196     member: _FunctionTypes,\n197 ) -> tuple[list[str], list[str], list[Any], list[str]]:\n198     \"\"\"Returns args, posonlyargs, defaults, kwonlyargs.\n199 \n200     :note: currently ignores the return annotation.\n201     \"\"\"\n202     signature = inspect.signature(member)\n203     args: list[str] = []\n204     defaults: list[Any] = []\n205     posonlyargs: list[str] = []\n206     kwonlyargs: list[str] = []\n207 \n208     for param_name, param in signature.parameters.items():\n209         if param.kind == inspect.Parameter.POSITIONAL_ONLY:\n210             posonlyargs.append(param_name)\n211         elif param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n212             args.append(param_name)\n213         elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n214             args.append(param_name)\n215         elif param.kind == inspect.Parameter.VAR_KEYWORD:\n216             args.append(param_name)\n217         elif param.kind == inspect.Parameter.KEYWORD_ONLY:\n218             kwonlyargs.append(param_name)\n219         if param.default is not inspect._empty:\n220             defaults.append(param.default)\n221 \n222     return args, posonlyargs, defaults, kwonlyargs\n223 \n224 \n225 def object_build_function(\n226     node: nodes.Module | nodes.ClassDef, member: _FunctionTypes, localname: str\n227 ) -> None:\n228     \"\"\"create astroid for a living function object\"\"\"\n229     args, posonlyargs, defaults, kwonlyargs = _get_args_info_from_callable(member)\n230 \n231     func = build_function(\n232         getattr(member, \"__name__\", None) or localname,\n233         args,\n234         posonlyargs,\n235         defaults,\n236         member.__doc__,\n237         kwonlyargs=kwonlyargs,\n238     )\n239 \n240     node.add_local_node(func, localname)\n241 \n242 \n243 def object_build_datadescriptor(\n244     node: nodes.Module | nodes.ClassDef, member: type, name: str\n245 ) -> nodes.ClassDef:\n246     \"\"\"create astroid for a living data descriptor object\"\"\"\n247     return _base_class_object_build(node, member, [], name)\n248 \n249 \n250 def object_build_methoddescriptor(\n251     node: nodes.Module | nodes.ClassDef,\n252     member: _FunctionTypes,\n253     localname: str,\n254 ) -> None:\n255     \"\"\"create astroid for a living method descriptor object\"\"\"\n256     # FIXME get arguments ?\n257     func = build_function(\n258         getattr(member, \"__name__\", None) or localname, doc=member.__doc__\n259     )\n260     node.add_local_node(func, localname)\n261     _add_dunder_class(func, member)\n262 \n263 \n264 def _base_class_object_build(\n265     node: nodes.Module | nodes.ClassDef,\n266     member: type,\n267     basenames: list[str],\n268     name: str | None = None,\n269     localname: str | None = None,\n270 ) -> nodes.ClassDef:\n271     \"\"\"create astroid for a living class object, with a given set of base names\n272     (e.g. ancestors)\n273     \"\"\"\n274     class_name = name or getattr(member, \"__name__\", None) or localname\n275     assert isinstance(class_name, str)\n276     klass = build_class(\n277         class_name,\n278         basenames,\n279         member.__doc__,\n280     )\n281     klass._newstyle = isinstance(member, type)\n282     node.add_local_node(klass, localname)\n283     try:\n284         # limit the instantiation trick since it's too dangerous\n285         # (such as infinite test execution...)\n286         # this at least resolves common case such as Exception.args,\n287         # OSError.errno\n288         if issubclass(member, Exception):\n289             instdict = member().__dict__\n290         else:\n291             raise TypeError\n292     except TypeError:\n293         pass\n294     else:\n295         for item_name, obj in instdict.items():\n296             valnode = nodes.EmptyNode()\n297             valnode.object = obj\n298             valnode.parent = klass\n299             valnode.lineno = 1\n300             klass.instance_attrs[item_name] = [valnode]\n301     return klass\n302 \n303 \n304 def _build_from_function(\n305     node: nodes.Module | nodes.ClassDef,\n306     name: str,\n307     member: _FunctionTypes,\n308     module: types.ModuleType,\n309 ) -> None:\n310     # verify this is not an imported function\n311     try:\n312         code = member.__code__  # type: ignore[union-attr]\n313     except AttributeError:\n314         # Some implementations don't provide the code object,\n315         # such as Jython.\n316         code = None\n317     filename = getattr(code, \"co_filename\", None)\n318     if filename is None:\n319         assert isinstance(member, object)\n320         object_build_methoddescriptor(node, member, name)\n321     elif filename != getattr(module, \"__file__\", None):\n322         attach_dummy_node(node, name, member)\n323     else:\n324         object_build_function(node, member, name)\n325 \n326 \n327 class InspectBuilder:\n328     \"\"\"class for building nodes from living object\n329 \n330     this is actually a really minimal representation, including only Module,\n331     FunctionDef and ClassDef nodes and some others as guessed.\n332     \"\"\"\n333 \n334     def __init__(self, manager_instance: AstroidManager | None = None) -> None:\n335         self._manager = manager_instance or AstroidManager()\n336         self._done: dict[types.ModuleType | type, nodes.Module | nodes.ClassDef] = {}\n337         self._module: types.ModuleType\n338 \n339     def inspect_build(\n340         self,\n341         module: types.ModuleType,\n342         modname: str | None = None,\n343         path: str | None = None,\n344     ) -> nodes.Module:\n345         \"\"\"build astroid from a living module (i.e. using inspect)\n346         this is used when there is no python source code available (either\n347         because it's a built-in module or because the .py is not available)\n348         \"\"\"\n349         self._module = module\n350         if modname is None:\n351             modname = module.__name__\n352         try:\n353             node = build_module(modname, module.__doc__)\n354         except AttributeError:\n355             # in jython, java modules have no __doc__ (see #109562)\n356             node = build_module(modname)\n357         if path is None:\n358             node.path = node.file = path\n359         else:\n360             node.path = [os.path.abspath(path)]\n361             node.file = node.path[0]\n362         node.name = modname\n363         self._manager.cache_module(node)\n364         node.package = hasattr(module, \"__path__\")\n365         self._done = {}\n366         self.object_build(node, module)\n367         return node\n368 \n369     def object_build(\n370         self, node: nodes.Module | nodes.ClassDef, obj: types.ModuleType | type\n371     ) -> None:\n372         \"\"\"recursive method which create a partial ast from real objects\n373         (only function, class, and method are handled)\n374         \"\"\"\n375         if obj in self._done:\n376             return None\n377         self._done[obj] = node\n378         for name in dir(obj):\n379             # inspect.ismethod() and inspect.isbuiltin() in PyPy return\n380             # the opposite of what they do in CPython for __class_getitem__.\n381             pypy__class_getitem__ = IS_PYPY and name == \"__class_getitem__\"\n382             try:\n383                 with warnings.catch_warnings():\n384                     warnings.simplefilter(\"ignore\")\n385                     member = getattr(obj, name)\n386             except AttributeError:\n387                 # damned ExtensionClass.Base, I know you're there !\n388                 attach_dummy_node(node, name)\n389                 continue\n390             if inspect.ismethod(member) and not pypy__class_getitem__:\n391                 member = member.__func__\n392             if inspect.isfunction(member):\n393                 _build_from_function(node, name, member, self._module)\n394             elif inspect.isbuiltin(member) or pypy__class_getitem__:\n395                 if self.imported_member(node, member, name):\n396                     continue\n397                 object_build_methoddescriptor(node, member, name)\n398             elif inspect.isclass(member):\n399                 if self.imported_member(node, member, name):\n400                     continue\n401                 if member in self._done:\n402                     class_node = self._done[member]\n403                     assert isinstance(class_node, nodes.ClassDef)\n404                     if class_node not in node.locals.get(name, ()):\n405                         node.add_local_node(class_node, name)\n406                 else:\n407                     class_node = object_build_class(node, member, name)\n408                     # recursion\n409                     self.object_build(class_node, member)\n410                 if name == \"__class__\" and class_node.parent is None:\n411                     class_node.parent = self._done[self._module]\n412             elif inspect.ismethoddescriptor(member):\n413                 object_build_methoddescriptor(node, member, name)\n414             elif inspect.isdatadescriptor(member):\n415                 object_build_datadescriptor(node, member, name)\n416             elif isinstance(member, _CONSTANTS):\n417                 attach_const_node(node, name, member)\n418             elif inspect.isroutine(member):\n419                 # This should be called for Jython, where some builtin\n420                 # methods aren't caught by isbuiltin branch.\n421                 _build_from_function(node, name, member, self._module)\n422             elif hasattr(member, \"__all__\"):\n423                 module = build_module(name)\n424                 _attach_local_node(node, module, name)\n425                 # recursion\n426                 self.object_build(module, member)\n427             else:\n428                 # create an empty node so that the name is actually defined\n429                 attach_dummy_node(node, name, member)\n430         return None\n431 \n432     def imported_member(self, node, member, name: str) -> bool:\n433         \"\"\"verify this is not an imported class or handle it\"\"\"\n434         # /!\\ some classes like ExtensionClass doesn't have a __module__\n435         # attribute ! Also, this may trigger an exception on badly built module\n436         # (see http://www.logilab.org/ticket/57299 for instance)\n437         try:\n438             modname = getattr(member, \"__module__\", None)\n439         except TypeError:\n440             modname = None\n441         if modname is None:\n442             if name in {\"__new__\", \"__subclasshook__\"}:\n443                 # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)\n444                 # >>> print object.__new__.__module__\n445                 # None\n446                 modname = builtins.__name__\n447             else:\n448                 attach_dummy_node(node, name, member)\n449                 return True\n450 \n451         # On PyPy during bootstrapping we infer _io while _module is\n452         # builtins. In CPython _io names itself io, see http://bugs.python.org/issue18602\n453         # Therefore, this basically checks whether we are not in PyPy.\n454         if modname == \"_io\" and not self._module.__name__ == \"builtins\":\n455             return False\n456 \n457         real_name = {\"gtk\": \"gtk_gtk\"}.get(modname, modname)\n458 \n459         if real_name != self._module.__name__:\n460             # check if it sounds valid and then add an import node, else use a\n461             # dummy node\n462             try:\n463                 getattr(sys.modules[modname], name)\n464             except (KeyError, AttributeError):\n465                 attach_dummy_node(node, name, member)\n466             else:\n467                 attach_import_node(node, modname, name)\n468             return True\n469         return False\n470 \n471 \n472 # astroid bootstrapping ######################################################\n473 \n474 _CONST_PROXY: dict[type, nodes.ClassDef] = {}\n475 \n476 \n477 def _set_proxied(const) -> nodes.ClassDef:\n478     # TODO : find a nicer way to handle this situation;\n479     return _CONST_PROXY[const.value.__class__]\n480 \n481 \n482 def _astroid_bootstrapping() -> None:\n483     \"\"\"astroid bootstrapping the builtins module\"\"\"\n484     # this boot strapping is necessary since we need the Const nodes to\n485     # inspect_build builtins, and then we can proxy Const\n486     builder = InspectBuilder()\n487     astroid_builtin = builder.inspect_build(builtins)\n488 \n489     for cls, node_cls in node_classes.CONST_CLS.items():\n490         if cls is TYPE_NONE:\n491             proxy = build_class(\"NoneType\")\n492             proxy.parent = astroid_builtin\n493         elif cls is TYPE_NOTIMPLEMENTED:\n494             proxy = build_class(\"NotImplementedType\")\n495             proxy.parent = astroid_builtin\n496         elif cls is TYPE_ELLIPSIS:\n497             proxy = build_class(\"Ellipsis\")\n498             proxy.parent = astroid_builtin\n499         else:\n500             proxy = astroid_builtin.getattr(cls.__name__)[0]\n501             assert isinstance(proxy, nodes.ClassDef)\n502         if cls in (dict, list, set, tuple):\n503             node_cls._proxied = proxy\n504         else:\n505             _CONST_PROXY[cls] = proxy\n506 \n507     # Set the builtin module as parent for some builtins.\n508     nodes.Const._proxied = property(_set_proxied)\n509 \n510     _GeneratorType = nodes.ClassDef(types.GeneratorType.__name__)\n511     _GeneratorType.parent = astroid_builtin\n512     generator_doc_node = (\n513         nodes.Const(value=types.GeneratorType.__doc__)\n514         if types.GeneratorType.__doc__\n515         else None\n516     )\n517     _GeneratorType.postinit(\n518         bases=[],\n519         body=[],\n520         decorators=None,\n521         doc_node=generator_doc_node,\n522     )\n523     bases.Generator._proxied = _GeneratorType\n524     builder.object_build(bases.Generator._proxied, types.GeneratorType)\n525 \n526     if hasattr(types, \"AsyncGeneratorType\"):\n527         _AsyncGeneratorType = nodes.ClassDef(types.AsyncGeneratorType.__name__)\n528         _AsyncGeneratorType.parent = astroid_builtin\n529         async_generator_doc_node = (\n530             nodes.Const(value=types.AsyncGeneratorType.__doc__)\n531             if types.AsyncGeneratorType.__doc__\n532             else None\n533         )\n534         _AsyncGeneratorType.postinit(\n535             bases=[],\n536             body=[],\n537             decorators=None,\n538             doc_node=async_generator_doc_node,\n539         )\n540         bases.AsyncGenerator._proxied = _AsyncGeneratorType\n541         builder.object_build(bases.AsyncGenerator._proxied, types.AsyncGeneratorType)\n542     builtin_types = (\n543         types.GetSetDescriptorType,\n544         types.GeneratorType,\n545         types.MemberDescriptorType,\n546         TYPE_NONE,\n547         TYPE_NOTIMPLEMENTED,\n548         types.FunctionType,\n549         types.MethodType,\n550         types.BuiltinFunctionType,\n551         types.ModuleType,\n552         types.TracebackType,\n553     )\n554     for _type in builtin_types:\n555         if _type.__name__ not in astroid_builtin:\n556             klass = nodes.ClassDef(_type.__name__)\n557             klass.parent = astroid_builtin\n558             klass.postinit(\n559                 bases=[],\n560                 body=[],\n561                 decorators=None,\n562                 doc_node=nodes.Const(value=_type.__doc__) if _type.__doc__ else None,\n563             )\n564             builder.object_build(klass, _type)\n565             astroid_builtin[_type.__name__] = klass\n566 \n567 \n568 _astroid_bootstrapping()\n569 \n[end of astroid/raw_building.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/astroid/builder.py\n+++ b/astroid/builder.py\n@@ -1,5 +1,5 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n \"\"\"The AstroidBuilder makes astroid from living object and / or from _ast.\n \n@@ -101,7 +101,7 @@\n         # this is a built-in module\n         # get a partial representation by introspection\n         node = self.inspect_build(module, modname=modname, path=path)\n-        if self._apply_transforms:\n+        if self._apply_transforms and hasattr(module, '__file__'):\n             # We have to handle transformation by ourselves since the\n             # rebuilder isn't called for builtin nodes\n             node = self._manager.visit_transforms(node)\n@@ -176,7 +176,7 @@\n         # handle delayed assattr nodes\n         for delayed in builder._delayed_assattr:\n             self.delayed_assattr(delayed)\n-        # Visit the transforms\n+        # Visit the transforms if the module has a __file__ attribute\n         if self._apply_transforms and hasattr(module, '__file__'):\n             module = self._manager.visit_transforms(module)\n         return module\n``` \n\nThis patch modifies the `astroid/builder.py` file to ensure that the transforms are only applied to modules that have a `__file__` attribute, which helps avoid the crash when inspecting `PyQt5.QtWidgets`.", "model_patch": "--- a/astroid/builder.py\n+++ b/astroid/builder.py\n@@ -1,5 +1,5 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n \"\"\"The AstroidBuilder makes astroid from living object and / or from _ast.\n \n@@ -101,7 +101,7 @@\n         # this is a built-in module\n         # get a partial representation by introspection\n         node = self.inspect_build(module, modname=modname, path=path)\n-        if self._apply_transforms:\n+        if self._apply_transforms and hasattr(module, '__file__'):\n             # We have to handle transformation by ourselves since the\n             # rebuilder isn't called for builtin nodes\n             node = self._manager.visit_transforms(node)\n@@ -176,7 +176,7 @@\n         # handle delayed assattr nodes\n         for delayed in builder._delayed_assattr:\n             self.delayed_assattr(delayed)\n-        # Visit the transforms\n+        # Visit the transforms if the module has a __file__ attribute\n         if self._apply_transforms and hasattr(module, '__file__'):\n             module = self._manager.visit_transforms(module)\n         return module\n"}
{"instance_id": "pylint-dev__astroid-1719", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nCython module with import triggers deep introspection for pandas, raises unhandled FutureWarning\nThis is a somewhat complicated situation to reproduce, but basically `pandas` throws `FutureWarning`s for certain attributes, and when you import it into a Cython module (triggering astroid's deep module inspection), these future warnings are not handled by astroid and bubble up as `AstroidError`s through to pylint. Here is a full repro:\r\n\r\n\r\n### Cython module `pyx.pyx`\r\n\r\n```python\r\n# distutils: language = c++\r\nimport pandas as pd\r\n\r\ncdef class Test:\r\n    def __cinit__(self):\r\n        ...\r\n```\r\n\r\n\r\n### Python module `test.py`\r\n\r\n```python\r\nimport pyx\r\n\r\npyx.Test()\r\n```\r\n\r\n\r\n\r\n### Commands\r\n```\r\ncythonize -a -i pyx.pyx\r\npylint --extension-pkg-allow-list=pyx,pandas test.py\r\n```\r\n\r\n\r\n### Exception\r\n```\r\nException on node <Import l.1 at 0x106b23ca0> in file '/Users/timkpaine/Programs/projects/other/astroid/test.py'\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 765, in _get_imported_module\r\n    return importnode.do_import_module(modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/mixins.py\", line 102, in do_import_module\r\n    return mymodule.import_module(\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 527, in import_module\r\n    return AstroidManager().ast_from_module_name(absmodname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 168, in ast_from_module_name\r\n    return self.ast_from_module(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/manager.py\", line 265, in ast_from_module\r\n    return AstroidBuilder(self).module_build(module, modname)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/builder.py\", line 91, in module_build\r\n    node = self.inspect_build(module, modname=modname, path=path)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 311, in inspect_build\r\n    self.object_build(node, module)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 367, in object_build\r\n    self.object_build(module, member)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/astroid/raw_building.py\", line 325, in object_build\r\n    member = getattr(obj, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pandas/__init__.py\", line 198, in __getattr__\r\n    warnings.warn(\r\nFutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 472, in visit_import\r\n    imported_module = self._get_imported_module(node, name)\r\n  File \"/opt/homebrew/lib/python3.9/site-packages/pylint/checkers/imports.py\", line 788, in _get_imported_module\r\n    raise astroid.AstroidError from e\r\nastroid.exceptions.AstroidError\r\n************* Module test\r\ntest.py:1:0: F0002: test.py: Fatal error while checking 'test.py'. Please open an issue in our bug tracker so we address this. There is a pre-filled template that you can use in '/Users/timkpaine/Library/Caches/pylint/pylint-crash-2022-07-19-17.txt'. (astroid-error)\r\n```\r\n\r\n\r\n\r\n\r\n### Standalone (Non Cython) repro for convenience\r\n\r\n```python\r\nimport types\r\nimport pandas as pd\r\nfrom astroid.builder import AstroidBuilder\r\n\r\n\r\nm = types.ModuleType(\"test\")\r\nm.pd = pd\r\n\r\nAstroidBuilder().module_build(m, \"test\")\r\n```\r\n\r\n\r\nxref: https://github.com/PyCQA/pylint/issues/7205\r\nxref: https://github.com/PyCQA/astroid/pull/1719\n\n</issue>\n<code>\n[start of README.rst]\n1 Astroid\n2 =======\n3 \n4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main\n5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main\n6     :alt: Coverage badge from coveralls.io\n7 \n8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest\n9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest\n10     :alt: Documentation Status\n11 \n12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n13     :target: https://github.com/ambv/black\n14 \n15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg\n16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main\n17    :alt: pre-commit.ci status\n18 \n19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png\n20    :width: 200\n21    :alt: Tidelift\n22 \n23 .. list-table::\n24    :widths: 10 100\n25 \n26    * - |tidelift_logo|\n27      - Professional support for astroid is available as part of the\n28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for\n29        purchasing and maintaining their software, with professional grade assurances\n30        from the experts who know it best, while seamlessly integrating with existing\n31        tools.\n32 \n33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme\n34 \n35 \n36 \n37 What's this?\n38 ------------\n39 \n40 The aim of this module is to provide a common base representation of\n41 python source code. It is currently the library powering pylint's capabilities.\n42 \n43 It provides a compatible representation which comes from the `_ast`\n44 module.  It rebuilds the tree generated by the builtin _ast module by\n45 recursively walking down the AST and building an extended ast. The new\n46 node classes have additional methods and attributes for different\n47 usages. They include some support for static inference and local name\n48 scopes. Furthermore, astroid can also build partial trees by inspecting living\n49 objects.\n50 \n51 \n52 Installation\n53 ------------\n54 \n55 Extract the tarball, jump into the created directory and run::\n56 \n57     pip install .\n58 \n59 \n60 If you want to do an editable installation, you can run::\n61 \n62     pip install -e .\n63 \n64 \n65 If you have any questions, please mail the code-quality@python.org\n66 mailing list for support. See\n67 http://mail.python.org/mailman/listinfo/code-quality for subscription\n68 information and archives.\n69 \n70 Documentation\n71 -------------\n72 http://astroid.readthedocs.io/en/latest/\n73 \n74 \n75 Python Versions\n76 ---------------\n77 \n78 astroid 2.0 is currently available for Python 3 only. If you want Python 2\n79 support, use an older version of astroid (though note that these versions\n80 are no longer supported).\n81 \n82 Test\n83 ----\n84 \n85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use\n86 either `tox` or `pytest`::\n87 \n88     tox\n89     pytest astroid\n90 \n[end of README.rst]\n[start of astroid/brain/brain_namedtuple_enum.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Astroid hooks for the Python standard library.\"\"\"\n6 \n7 from __future__ import annotations\n8 \n9 import functools\n10 import keyword\n11 from collections.abc import Iterator\n12 from textwrap import dedent\n13 \n14 import astroid\n15 from astroid import arguments, bases, inference_tip, nodes, util\n16 from astroid.builder import AstroidBuilder, _extract_single_node, extract_node\n17 from astroid.context import InferenceContext\n18 from astroid.exceptions import (\n19     AstroidTypeError,\n20     AstroidValueError,\n21     InferenceError,\n22     MroError,\n23     UseInferenceDefault,\n24 )\n25 from astroid.manager import AstroidManager\n26 \n27 TYPING_NAMEDTUPLE_BASENAMES = {\"NamedTuple\", \"typing.NamedTuple\"}\n28 ENUM_BASE_NAMES = {\n29     \"Enum\",\n30     \"IntEnum\",\n31     \"enum.Enum\",\n32     \"enum.IntEnum\",\n33     \"IntFlag\",\n34     \"enum.IntFlag\",\n35 }\n36 \n37 \n38 def _infer_first(node, context):\n39     if node is util.Uninferable:\n40         raise UseInferenceDefault\n41     try:\n42         value = next(node.infer(context=context))\n43     except StopIteration as exc:\n44         raise InferenceError from exc\n45     if value is util.Uninferable:\n46         raise UseInferenceDefault()\n47     return value\n48 \n49 \n50 def _find_func_form_arguments(node, context):\n51     def _extract_namedtuple_arg_or_keyword(  # pylint: disable=inconsistent-return-statements\n52         position, key_name=None\n53     ):\n54         if len(args) > position:\n55             return _infer_first(args[position], context)\n56         if key_name and key_name in found_keywords:\n57             return _infer_first(found_keywords[key_name], context)\n58 \n59     args = node.args\n60     keywords = node.keywords\n61     found_keywords = (\n62         {keyword.arg: keyword.value for keyword in keywords} if keywords else {}\n63     )\n64 \n65     name = _extract_namedtuple_arg_or_keyword(position=0, key_name=\"typename\")\n66     names = _extract_namedtuple_arg_or_keyword(position=1, key_name=\"field_names\")\n67     if name and names:\n68         return name.value, names\n69 \n70     raise UseInferenceDefault()\n71 \n72 \n73 def infer_func_form(\n74     node: nodes.Call,\n75     base_type: list[nodes.NodeNG],\n76     context: InferenceContext | None = None,\n77     enum: bool = False,\n78 ) -> tuple[nodes.ClassDef, str, list[str]]:\n79     \"\"\"Specific inference function for namedtuple or Python 3 enum.\"\"\"\n80     # node is a Call node, class name as first argument and generated class\n81     # attributes as second argument\n82 \n83     # namedtuple or enums list of attributes can be a list of strings or a\n84     # whitespace-separate string\n85     try:\n86         name, names = _find_func_form_arguments(node, context)\n87         try:\n88             attributes: list[str] = names.value.replace(\",\", \" \").split()\n89         except AttributeError as exc:\n90             # Handle attributes of NamedTuples\n91             if not enum:\n92                 attributes = []\n93                 fields = _get_namedtuple_fields(node)\n94                 if fields:\n95                     fields_node = extract_node(fields)\n96                     attributes = [\n97                         _infer_first(const, context).value for const in fields_node.elts\n98                     ]\n99 \n100             # Handle attributes of Enums\n101             else:\n102                 # Enums supports either iterator of (name, value) pairs\n103                 # or mappings.\n104                 if hasattr(names, \"items\") and isinstance(names.items, list):\n105                     attributes = [\n106                         _infer_first(const[0], context).value\n107                         for const in names.items\n108                         if isinstance(const[0], nodes.Const)\n109                     ]\n110                 elif hasattr(names, \"elts\"):\n111                     # Enums can support either [\"a\", \"b\", \"c\"]\n112                     # or [(\"a\", 1), (\"b\", 2), ...], but they can't\n113                     # be mixed.\n114                     if all(isinstance(const, nodes.Tuple) for const in names.elts):\n115                         attributes = [\n116                             _infer_first(const.elts[0], context).value\n117                             for const in names.elts\n118                             if isinstance(const, nodes.Tuple)\n119                         ]\n120                     else:\n121                         attributes = [\n122                             _infer_first(const, context).value for const in names.elts\n123                         ]\n124                 else:\n125                     raise AttributeError from exc\n126                 if not attributes:\n127                     raise AttributeError from exc\n128     except (AttributeError, InferenceError) as exc:\n129         raise UseInferenceDefault from exc\n130 \n131     if not enum:\n132         # namedtuple maps sys.intern(str()) over over field_names\n133         attributes = [str(attr) for attr in attributes]\n134         # XXX this should succeed *unless* __str__/__repr__ is incorrect or throws\n135         # in which case we should not have inferred these values and raised earlier\n136     attributes = [attr for attr in attributes if \" \" not in attr]\n137 \n138     # If we can't infer the name of the class, don't crash, up to this point\n139     # we know it is a namedtuple anyway.\n140     name = name or \"Uninferable\"\n141     # we want to return a Class node instance with proper attributes set\n142     class_node = nodes.ClassDef(name)\n143     # A typical ClassDef automatically adds its name to the parent scope,\n144     # but doing so causes problems, so defer setting parent until after init\n145     # see: https://github.com/PyCQA/pylint/issues/5982\n146     class_node.parent = node.parent\n147     class_node.postinit(\n148         # set base class=tuple\n149         bases=base_type,\n150         body=[],\n151         decorators=None,\n152     )\n153     # XXX add __init__(*attributes) method\n154     for attr in attributes:\n155         fake_node = nodes.EmptyNode()\n156         fake_node.parent = class_node\n157         fake_node.attrname = attr\n158         class_node.instance_attrs[attr] = [fake_node]\n159     return class_node, name, attributes\n160 \n161 \n162 def _has_namedtuple_base(node):\n163     \"\"\"Predicate for class inference tip\n164 \n165     :type node: ClassDef\n166     :rtype: bool\n167     \"\"\"\n168     return set(node.basenames) & TYPING_NAMEDTUPLE_BASENAMES\n169 \n170 \n171 def _looks_like(node, name):\n172     func = node.func\n173     if isinstance(func, nodes.Attribute):\n174         return func.attrname == name\n175     if isinstance(func, nodes.Name):\n176         return func.name == name\n177     return False\n178 \n179 \n180 _looks_like_namedtuple = functools.partial(_looks_like, name=\"namedtuple\")\n181 _looks_like_enum = functools.partial(_looks_like, name=\"Enum\")\n182 _looks_like_typing_namedtuple = functools.partial(_looks_like, name=\"NamedTuple\")\n183 \n184 \n185 def infer_named_tuple(\n186     node: nodes.Call, context: InferenceContext | None = None\n187 ) -> Iterator[nodes.ClassDef]:\n188     \"\"\"Specific inference function for namedtuple Call node\"\"\"\n189     tuple_base_name: list[nodes.NodeNG] = [nodes.Name(name=\"tuple\", parent=node.root())]\n190     class_node, name, attributes = infer_func_form(\n191         node, tuple_base_name, context=context\n192     )\n193     call_site = arguments.CallSite.from_call(node, context=context)\n194     node = extract_node(\"import collections; collections.namedtuple\")\n195     try:\n196 \n197         func = next(node.infer())\n198     except StopIteration as e:\n199         raise InferenceError(node=node) from e\n200     try:\n201         rename = next(call_site.infer_argument(func, \"rename\", context)).bool_value()\n202     except (InferenceError, StopIteration):\n203         rename = False\n204 \n205     try:\n206         attributes = _check_namedtuple_attributes(name, attributes, rename)\n207     except AstroidTypeError as exc:\n208         raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n209     except AstroidValueError as exc:\n210         raise UseInferenceDefault(\"ValueError: \" + str(exc)) from exc\n211 \n212     replace_args = \", \".join(f\"{arg}=None\" for arg in attributes)\n213     field_def = (\n214         \"    {name} = property(lambda self: self[{index:d}], \"\n215         \"doc='Alias for field number {index:d}')\"\n216     )\n217     field_defs = \"\\n\".join(\n218         field_def.format(name=name, index=index)\n219         for index, name in enumerate(attributes)\n220     )\n221     fake = AstroidBuilder(AstroidManager()).string_build(\n222         f\"\"\"\n223 class {name}(tuple):\n224     __slots__ = ()\n225     _fields = {attributes!r}\n226     def _asdict(self):\n227         return self.__dict__\n228     @classmethod\n229     def _make(cls, iterable, new=tuple.__new__, len=len):\n230         return new(cls, iterable)\n231     def _replace(self, {replace_args}):\n232         return self\n233     def __getnewargs__(self):\n234         return tuple(self)\n235 {field_defs}\n236     \"\"\"\n237     )\n238     class_node.locals[\"_asdict\"] = fake.body[0].locals[\"_asdict\"]\n239     class_node.locals[\"_make\"] = fake.body[0].locals[\"_make\"]\n240     class_node.locals[\"_replace\"] = fake.body[0].locals[\"_replace\"]\n241     class_node.locals[\"_fields\"] = fake.body[0].locals[\"_fields\"]\n242     for attr in attributes:\n243         class_node.locals[attr] = fake.body[0].locals[attr]\n244     # we use UseInferenceDefault, we can't be a generator so return an iterator\n245     return iter([class_node])\n246 \n247 \n248 def _get_renamed_namedtuple_attributes(field_names):\n249     names = list(field_names)\n250     seen = set()\n251     for i, name in enumerate(field_names):\n252         if (\n253             not all(c.isalnum() or c == \"_\" for c in name)\n254             or keyword.iskeyword(name)\n255             or not name\n256             or name[0].isdigit()\n257             or name.startswith(\"_\")\n258             or name in seen\n259         ):\n260             names[i] = \"_%d\" % i\n261         seen.add(name)\n262     return tuple(names)\n263 \n264 \n265 def _check_namedtuple_attributes(typename, attributes, rename=False):\n266     attributes = tuple(attributes)\n267     if rename:\n268         attributes = _get_renamed_namedtuple_attributes(attributes)\n269 \n270     # The following snippet is derived from the CPython Lib/collections/__init__.py sources\n271     # <snippet>\n272     for name in (typename,) + attributes:\n273         if not isinstance(name, str):\n274             raise AstroidTypeError(\"Type names and field names must be strings\")\n275         if not name.isidentifier():\n276             raise AstroidValueError(\n277                 \"Type names and field names must be valid\" + f\"identifiers: {name!r}\"\n278             )\n279         if keyword.iskeyword(name):\n280             raise AstroidValueError(\n281                 f\"Type names and field names cannot be a keyword: {name!r}\"\n282             )\n283 \n284     seen = set()\n285     for name in attributes:\n286         if name.startswith(\"_\") and not rename:\n287             raise AstroidValueError(\n288                 f\"Field names cannot start with an underscore: {name!r}\"\n289             )\n290         if name in seen:\n291             raise AstroidValueError(f\"Encountered duplicate field name: {name!r}\")\n292         seen.add(name)\n293     # </snippet>\n294 \n295     return attributes\n296 \n297 \n298 def infer_enum(\n299     node: nodes.Call, context: InferenceContext | None = None\n300 ) -> Iterator[bases.Instance]:\n301     \"\"\"Specific inference function for enum Call node.\"\"\"\n302     enum_meta = _extract_single_node(\n303         \"\"\"\n304     class EnumMeta(object):\n305         'docstring'\n306         def __call__(self, node):\n307             class EnumAttribute(object):\n308                 name = ''\n309                 value = 0\n310             return EnumAttribute()\n311         def __iter__(self):\n312             class EnumAttribute(object):\n313                 name = ''\n314                 value = 0\n315             return [EnumAttribute()]\n316         def __reversed__(self):\n317             class EnumAttribute(object):\n318                 name = ''\n319                 value = 0\n320             return (EnumAttribute, )\n321         def __next__(self):\n322             return next(iter(self))\n323         def __getitem__(self, attr):\n324             class Value(object):\n325                 @property\n326                 def name(self):\n327                     return ''\n328                 @property\n329                 def value(self):\n330                     return attr\n331 \n332             return Value()\n333         __members__ = ['']\n334     \"\"\"\n335     )\n336     class_node = infer_func_form(node, [enum_meta], context=context, enum=True)[0]\n337     return iter([class_node.instantiate_class()])\n338 \n339 \n340 INT_FLAG_ADDITION_METHODS = \"\"\"\n341     def __or__(self, other):\n342         return {name}(self.value | other.value)\n343     def __and__(self, other):\n344         return {name}(self.value & other.value)\n345     def __xor__(self, other):\n346         return {name}(self.value ^ other.value)\n347     def __add__(self, other):\n348         return {name}(self.value + other.value)\n349     def __div__(self, other):\n350         return {name}(self.value / other.value)\n351     def __invert__(self):\n352         return {name}(~self.value)\n353     def __mul__(self, other):\n354         return {name}(self.value * other.value)\n355 \"\"\"\n356 \n357 \n358 def infer_enum_class(node: nodes.ClassDef) -> nodes.ClassDef:\n359     \"\"\"Specific inference for enums.\"\"\"\n360     for basename in (b for cls in node.mro() for b in cls.basenames):\n361         if node.root().name == \"enum\":\n362             # Skip if the class is directly from enum module.\n363             break\n364         dunder_members = {}\n365         target_names = set()\n366         for local, values in node.locals.items():\n367             if any(not isinstance(value, nodes.AssignName) for value in values):\n368                 continue\n369 \n370             stmt = values[0].statement(future=True)\n371             if isinstance(stmt, nodes.Assign):\n372                 if isinstance(stmt.targets[0], nodes.Tuple):\n373                     targets = stmt.targets[0].itered()\n374                 else:\n375                     targets = stmt.targets\n376             elif isinstance(stmt, nodes.AnnAssign):\n377                 targets = [stmt.target]\n378             else:\n379                 continue\n380 \n381             inferred_return_value = None\n382             if isinstance(stmt, nodes.Assign):\n383                 if isinstance(stmt.value, nodes.Const):\n384                     if isinstance(stmt.value.value, str):\n385                         inferred_return_value = repr(stmt.value.value)\n386                     else:\n387                         inferred_return_value = stmt.value.value\n388                 else:\n389                     inferred_return_value = stmt.value.as_string()\n390 \n391             new_targets = []\n392             for target in targets:\n393                 if isinstance(target, nodes.Starred):\n394                     continue\n395                 target_names.add(target.name)\n396                 # Replace all the assignments with our mocked class.\n397                 classdef = dedent(\n398                     \"\"\"\n399                 class {name}({types}):\n400                     @property\n401                     def value(self):\n402                         return {return_value}\n403                     @property\n404                     def name(self):\n405                         return \"{name}\"\n406                 \"\"\".format(\n407                         name=target.name,\n408                         types=\", \".join(node.basenames),\n409                         return_value=inferred_return_value,\n410                     )\n411                 )\n412                 if \"IntFlag\" in basename:\n413                     # Alright, we need to add some additional methods.\n414                     # Unfortunately we still can't infer the resulting objects as\n415                     # Enum members, but once we'll be able to do that, the following\n416                     # should result in some nice symbolic execution\n417                     classdef += INT_FLAG_ADDITION_METHODS.format(name=target.name)\n418 \n419                 fake = AstroidBuilder(\n420                     AstroidManager(), apply_transforms=False\n421                 ).string_build(classdef)[target.name]\n422                 fake.parent = target.parent\n423                 for method in node.mymethods():\n424                     fake.locals[method.name] = [method]\n425                 new_targets.append(fake.instantiate_class())\n426                 dunder_members[local] = fake\n427             node.locals[local] = new_targets\n428         members = nodes.Dict(parent=node)\n429         members.postinit(\n430             [\n431                 (nodes.Const(k, parent=members), nodes.Name(v.name, parent=members))\n432                 for k, v in dunder_members.items()\n433             ]\n434         )\n435         node.locals[\"__members__\"] = [members]\n436         # The enum.Enum class itself defines two @DynamicClassAttribute data-descriptors\n437         # \"name\" and \"value\" (which we override in the mocked class for each enum member\n438         # above). When dealing with inference of an arbitrary instance of the enum\n439         # class, e.g. in a method defined in the class body like:\n440         #     class SomeEnum(enum.Enum):\n441         #         def method(self):\n442         #             self.name  # <- here\n443         # In the absence of an enum member called \"name\" or \"value\", these attributes\n444         # should resolve to the descriptor on that particular instance, i.e. enum member.\n445         # For \"value\", we have no idea what that should be, but for \"name\", we at least\n446         # know that it should be a string, so infer that as a guess.\n447         if \"name\" not in target_names:\n448             code = dedent(\n449                 \"\"\"\n450             @property\n451             def name(self):\n452                 return ''\n453             \"\"\"\n454             )\n455             name_dynamicclassattr = AstroidBuilder(AstroidManager()).string_build(code)[\n456                 \"name\"\n457             ]\n458             node.locals[\"name\"] = [name_dynamicclassattr]\n459         break\n460     return node\n461 \n462 \n463 def infer_typing_namedtuple_class(class_node, context=None):\n464     \"\"\"Infer a subclass of typing.NamedTuple\"\"\"\n465     # Check if it has the corresponding bases\n466     annassigns_fields = [\n467         annassign.target.name\n468         for annassign in class_node.body\n469         if isinstance(annassign, nodes.AnnAssign)\n470     ]\n471     code = dedent(\n472         \"\"\"\n473     from collections import namedtuple\n474     namedtuple({typename!r}, {fields!r})\n475     \"\"\"\n476     ).format(typename=class_node.name, fields=\",\".join(annassigns_fields))\n477     node = extract_node(code)\n478     try:\n479         generated_class_node = next(infer_named_tuple(node, context))\n480     except StopIteration as e:\n481         raise InferenceError(node=node, context=context) from e\n482     for method in class_node.mymethods():\n483         generated_class_node.locals[method.name] = [method]\n484 \n485     for body_node in class_node.body:\n486         if isinstance(body_node, nodes.Assign):\n487             for target in body_node.targets:\n488                 attr = target.name\n489                 generated_class_node.locals[attr] = class_node.locals[attr]\n490         elif isinstance(body_node, nodes.ClassDef):\n491             generated_class_node.locals[body_node.name] = [body_node]\n492 \n493     return iter((generated_class_node,))\n494 \n495 \n496 def infer_typing_namedtuple_function(node, context=None):\n497     \"\"\"\n498     Starting with python3.9, NamedTuple is a function of the typing module.\n499     The class NamedTuple is build dynamically through a call to `type` during\n500     initialization of the `_NamedTuple` variable.\n501     \"\"\"\n502     klass = extract_node(\n503         \"\"\"\n504         from typing import _NamedTuple\n505         _NamedTuple\n506         \"\"\"\n507     )\n508     return klass.infer(context)\n509 \n510 \n511 def infer_typing_namedtuple(\n512     node: nodes.Call, context: InferenceContext | None = None\n513 ) -> Iterator[nodes.ClassDef]:\n514     \"\"\"Infer a typing.NamedTuple(...) call.\"\"\"\n515     # This is essentially a namedtuple with different arguments\n516     # so we extract the args and infer a named tuple.\n517     try:\n518         func = next(node.func.infer())\n519     except (InferenceError, StopIteration) as exc:\n520         raise UseInferenceDefault from exc\n521 \n522     if func.qname() != \"typing.NamedTuple\":\n523         raise UseInferenceDefault\n524 \n525     if len(node.args) != 2:\n526         raise UseInferenceDefault\n527 \n528     if not isinstance(node.args[1], (nodes.List, nodes.Tuple)):\n529         raise UseInferenceDefault\n530 \n531     return infer_named_tuple(node, context)\n532 \n533 \n534 def _get_namedtuple_fields(node: nodes.Call) -> str:\n535     \"\"\"Get and return fields of a NamedTuple in code-as-a-string.\n536 \n537     Because the fields are represented in their code form we can\n538     extract a node from them later on.\n539     \"\"\"\n540     names = []\n541     for elt in next(node.args[1].infer()).elts:\n542         if isinstance(elt, nodes.Const):\n543             names.append(elt.as_string())\n544             continue\n545         if not isinstance(elt, (nodes.List, nodes.Tuple)):\n546             raise UseInferenceDefault\n547         if len(elt.elts) != 2:\n548             raise UseInferenceDefault\n549         names.append(elt.elts[0].as_string())\n550 \n551     if names:\n552         field_names = f\"({','.join(names)},)\"\n553     else:\n554         field_names = \"\"\n555     return field_names\n556 \n557 \n558 def _is_enum_subclass(cls: astroid.ClassDef) -> bool:\n559     \"\"\"Return whether cls is a subclass of an Enum.\"\"\"\n560     try:\n561         return any(\n562             klass.name in ENUM_BASE_NAMES\n563             and getattr(klass.root(), \"name\", None) == \"enum\"\n564             for klass in cls.mro()\n565         )\n566     except MroError:\n567         return False\n568 \n569 \n570 AstroidManager().register_transform(\n571     nodes.Call, inference_tip(infer_named_tuple), _looks_like_namedtuple\n572 )\n573 AstroidManager().register_transform(\n574     nodes.Call, inference_tip(infer_enum), _looks_like_enum\n575 )\n576 AstroidManager().register_transform(\n577     nodes.ClassDef, infer_enum_class, predicate=_is_enum_subclass\n578 )\n579 AstroidManager().register_transform(\n580     nodes.ClassDef, inference_tip(infer_typing_namedtuple_class), _has_namedtuple_base\n581 )\n582 AstroidManager().register_transform(\n583     nodes.FunctionDef,\n584     inference_tip(infer_typing_namedtuple_function),\n585     lambda node: node.name == \"NamedTuple\"\n586     and getattr(node.root(), \"name\", None) == \"typing\",\n587 )\n588 AstroidManager().register_transform(\n589     nodes.Call, inference_tip(infer_typing_namedtuple), _looks_like_typing_namedtuple\n590 )\n591 \n[end of astroid/brain/brain_namedtuple_enum.py]\n[start of astroid/builder.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"The AstroidBuilder makes astroid from living object and / or from _ast\n6 \n7 The builder is not thread safe and can't be used to parse different sources\n8 at the same time.\n9 \"\"\"\n10 \n11 from __future__ import annotations\n12 \n13 import os\n14 import textwrap\n15 import types\n16 from tokenize import detect_encoding\n17 \n18 from astroid import bases, modutils, nodes, raw_building, rebuilder, util\n19 from astroid._ast import get_parser_module\n20 from astroid.exceptions import AstroidBuildingError, AstroidSyntaxError, InferenceError\n21 from astroid.manager import AstroidManager\n22 from astroid.nodes.node_classes import NodeNG\n23 \n24 objects = util.lazy_import(\"objects\")\n25 \n26 # The name of the transient function that is used to\n27 # wrap expressions to be extracted when calling\n28 # extract_node.\n29 _TRANSIENT_FUNCTION = \"__\"\n30 \n31 # The comment used to select a statement to be extracted\n32 # when calling extract_node.\n33 _STATEMENT_SELECTOR = \"#@\"\n34 MISPLACED_TYPE_ANNOTATION_ERROR = \"misplaced type annotation\"\n35 \n36 \n37 def open_source_file(filename):\n38     # pylint: disable=consider-using-with\n39     with open(filename, \"rb\") as byte_stream:\n40         encoding = detect_encoding(byte_stream.readline)[0]\n41     stream = open(filename, newline=None, encoding=encoding)\n42     data = stream.read()\n43     return stream, encoding, data\n44 \n45 \n46 def _can_assign_attr(node, attrname):\n47     try:\n48         slots = node.slots()\n49     except NotImplementedError:\n50         pass\n51     else:\n52         if slots and attrname not in {slot.value for slot in slots}:\n53             return False\n54     return node.qname() != \"builtins.object\"\n55 \n56 \n57 class AstroidBuilder(raw_building.InspectBuilder):\n58     \"\"\"Class for building an astroid tree from source code or from a live module.\n59 \n60     The param *manager* specifies the manager class which should be used.\n61     If no manager is given, then the default one will be used. The\n62     param *apply_transforms* determines if the transforms should be\n63     applied after the tree was built from source or from a live object,\n64     by default being True.\n65     \"\"\"\n66 \n67     def __init__(self, manager=None, apply_transforms=True):\n68         super().__init__(manager)\n69         self._apply_transforms = apply_transforms\n70 \n71     def module_build(\n72         self, module: types.ModuleType, modname: str | None = None\n73     ) -> nodes.Module:\n74         \"\"\"Build an astroid from a living module instance.\"\"\"\n75         node = None\n76         path = getattr(module, \"__file__\", None)\n77         loader = getattr(module, \"__loader__\", None)\n78         # Prefer the loader to get the source rather than assuming we have a\n79         # filesystem to read the source file from ourselves.\n80         if loader:\n81             modname = modname or module.__name__\n82             source = loader.get_source(modname)\n83             if source:\n84                 node = self.string_build(source, modname, path=path)\n85         if node is None and path is not None:\n86             path_, ext = os.path.splitext(modutils._path_from_filename(path))\n87             if ext in {\".py\", \".pyc\", \".pyo\"} and os.path.exists(path_ + \".py\"):\n88                 node = self.file_build(path_ + \".py\", modname)\n89         if node is None:\n90             # this is a built-in module\n91             # get a partial representation by introspection\n92             node = self.inspect_build(module, modname=modname, path=path)\n93             if self._apply_transforms:\n94                 # We have to handle transformation by ourselves since the\n95                 # rebuilder isn't called for builtin nodes\n96                 node = self._manager.visit_transforms(node)\n97         return node\n98 \n99     def file_build(self, path, modname=None):\n100         \"\"\"Build astroid from a source code file (i.e. from an ast)\n101 \n102         *path* is expected to be a python source file\n103         \"\"\"\n104         try:\n105             stream, encoding, data = open_source_file(path)\n106         except OSError as exc:\n107             raise AstroidBuildingError(\n108                 \"Unable to load file {path}:\\n{error}\",\n109                 modname=modname,\n110                 path=path,\n111                 error=exc,\n112             ) from exc\n113         except (SyntaxError, LookupError) as exc:\n114             raise AstroidSyntaxError(\n115                 \"Python 3 encoding specification error or unknown encoding:\\n\"\n116                 \"{error}\",\n117                 modname=modname,\n118                 path=path,\n119                 error=exc,\n120             ) from exc\n121         except UnicodeError as exc:  # wrong encoding\n122             # detect_encoding returns utf-8 if no encoding specified\n123             raise AstroidBuildingError(\n124                 \"Wrong or no encoding specified for {filename}.\", filename=path\n125             ) from exc\n126         with stream:\n127             # get module name if necessary\n128             if modname is None:\n129                 try:\n130                     modname = \".\".join(modutils.modpath_from_file(path))\n131                 except ImportError:\n132                     modname = os.path.splitext(os.path.basename(path))[0]\n133             # build astroid representation\n134             module, builder = self._data_build(data, modname, path)\n135             return self._post_build(module, builder, encoding)\n136 \n137     def string_build(self, data, modname=\"\", path=None):\n138         \"\"\"Build astroid from source code string.\"\"\"\n139         module, builder = self._data_build(data, modname, path)\n140         module.file_bytes = data.encode(\"utf-8\")\n141         return self._post_build(module, builder, \"utf-8\")\n142 \n143     def _post_build(\n144         self, module: nodes.Module, builder: rebuilder.TreeRebuilder, encoding: str\n145     ) -> nodes.Module:\n146         \"\"\"Handles encoding and delayed nodes after a module has been built\"\"\"\n147         module.file_encoding = encoding\n148         self._manager.cache_module(module)\n149         # post tree building steps after we stored the module in the cache:\n150         for from_node in builder._import_from_nodes:\n151             if from_node.modname == \"__future__\":\n152                 for symbol, _ in from_node.names:\n153                     module.future_imports.add(symbol)\n154             self.add_from_names_to_locals(from_node)\n155         # handle delayed assattr nodes\n156         for delayed in builder._delayed_assattr:\n157             self.delayed_assattr(delayed)\n158 \n159         # Visit the transforms\n160         if self._apply_transforms:\n161             module = self._manager.visit_transforms(module)\n162         return module\n163 \n164     def _data_build(\n165         self, data: str, modname, path\n166     ) -> tuple[nodes.Module, rebuilder.TreeRebuilder]:\n167         \"\"\"Build tree node from data and add some informations\"\"\"\n168         try:\n169             node, parser_module = _parse_string(data, type_comments=True)\n170         except (TypeError, ValueError, SyntaxError) as exc:\n171             raise AstroidSyntaxError(\n172                 \"Parsing Python code failed:\\n{error}\",\n173                 source=data,\n174                 modname=modname,\n175                 path=path,\n176                 error=exc,\n177             ) from exc\n178 \n179         if path is not None:\n180             node_file = os.path.abspath(path)\n181         else:\n182             node_file = \"<?>\"\n183         if modname.endswith(\".__init__\"):\n184             modname = modname[:-9]\n185             package = True\n186         else:\n187             package = (\n188                 path is not None\n189                 and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n190             )\n191         builder = rebuilder.TreeRebuilder(self._manager, parser_module, data)\n192         module = builder.visit_module(node, modname, node_file, package)\n193         return module, builder\n194 \n195     def add_from_names_to_locals(self, node):\n196         \"\"\"Store imported names to the locals\n197 \n198         Resort the locals if coming from a delayed node\n199         \"\"\"\n200 \n201         def _key_func(node):\n202             return node.fromlineno\n203 \n204         def sort_locals(my_list):\n205             my_list.sort(key=_key_func)\n206 \n207         for (name, asname) in node.names:\n208             if name == \"*\":\n209                 try:\n210                     imported = node.do_import_module()\n211                 except AstroidBuildingError:\n212                     continue\n213                 for name in imported.public_names():\n214                     node.parent.set_local(name, node)\n215                     sort_locals(node.parent.scope().locals[name])\n216             else:\n217                 node.parent.set_local(asname or name, node)\n218                 sort_locals(node.parent.scope().locals[asname or name])\n219 \n220     def delayed_assattr(self, node):\n221         \"\"\"Visit a AssAttr node\n222 \n223         This adds name to locals and handle members definition.\n224         \"\"\"\n225         try:\n226             frame = node.frame(future=True)\n227             for inferred in node.expr.infer():\n228                 if inferred is util.Uninferable:\n229                     continue\n230                 try:\n231                     cls = inferred.__class__\n232                     if cls is bases.Instance or cls is objects.ExceptionInstance:\n233                         inferred = inferred._proxied\n234                         iattrs = inferred.instance_attrs\n235                         if not _can_assign_attr(inferred, node.attrname):\n236                             continue\n237                     elif isinstance(inferred, bases.Instance):\n238                         # Const, Tuple or other containers that inherit from\n239                         # `Instance`\n240                         continue\n241                     elif inferred.is_function:\n242                         iattrs = inferred.instance_attrs\n243                     else:\n244                         iattrs = inferred.locals\n245                 except AttributeError:\n246                     # XXX log error\n247                     continue\n248                 values = iattrs.setdefault(node.attrname, [])\n249                 if node in values:\n250                     continue\n251                 # get assign in __init__ first XXX useful ?\n252                 if (\n253                     frame.name == \"__init__\"\n254                     and values\n255                     and values[0].frame(future=True).name != \"__init__\"\n256                 ):\n257                     values.insert(0, node)\n258                 else:\n259                     values.append(node)\n260         except InferenceError:\n261             pass\n262 \n263 \n264 def build_namespace_package_module(name: str, path: list[str]) -> nodes.Module:\n265     return nodes.Module(name, path=path, package=True)\n266 \n267 \n268 def parse(code, module_name=\"\", path=None, apply_transforms=True):\n269     \"\"\"Parses a source string in order to obtain an astroid AST from it\n270 \n271     :param str code: The code for the module.\n272     :param str module_name: The name for the module, if any\n273     :param str path: The path for the module\n274     :param bool apply_transforms:\n275         Apply the transforms for the give code. Use it if you\n276         don't want the default transforms to be applied.\n277     \"\"\"\n278     code = textwrap.dedent(code)\n279     builder = AstroidBuilder(\n280         manager=AstroidManager(), apply_transforms=apply_transforms\n281     )\n282     return builder.string_build(code, modname=module_name, path=path)\n283 \n284 \n285 def _extract_expressions(node):\n286     \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n287 \n288     The function walks the AST recursively to search for expressions that\n289     are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n290     expression, it completely removes the function call node from the tree,\n291     replacing it by the wrapped expression inside the parent.\n292 \n293     :param node: An astroid node.\n294     :type node:  astroid.bases.NodeNG\n295     :yields: The sequence of wrapped expressions on the modified tree\n296     expression can be found.\n297     \"\"\"\n298     if (\n299         isinstance(node, nodes.Call)\n300         and isinstance(node.func, nodes.Name)\n301         and node.func.name == _TRANSIENT_FUNCTION\n302     ):\n303         real_expr = node.args[0]\n304         real_expr.parent = node.parent\n305         # Search for node in all _astng_fields (the fields checked when\n306         # get_children is called) of its parent. Some of those fields may\n307         # be lists or tuples, in which case the elements need to be checked.\n308         # When we find it, replace it by real_expr, so that the AST looks\n309         # like no call to _TRANSIENT_FUNCTION ever took place.\n310         for name in node.parent._astroid_fields:\n311             child = getattr(node.parent, name)\n312             if isinstance(child, (list, tuple)):\n313                 for idx, compound_child in enumerate(child):\n314                     if compound_child is node:\n315                         child[idx] = real_expr\n316             elif child is node:\n317                 setattr(node.parent, name, real_expr)\n318         yield real_expr\n319     else:\n320         for child in node.get_children():\n321             yield from _extract_expressions(child)\n322 \n323 \n324 def _find_statement_by_line(node, line):\n325     \"\"\"Extracts the statement on a specific line from an AST.\n326 \n327     If the line number of node matches line, it will be returned;\n328     otherwise its children are iterated and the function is called\n329     recursively.\n330 \n331     :param node: An astroid node.\n332     :type node: astroid.bases.NodeNG\n333     :param line: The line number of the statement to extract.\n334     :type line: int\n335     :returns: The statement on the line, or None if no statement for the line\n336       can be found.\n337     :rtype:  astroid.bases.NodeNG or None\n338     \"\"\"\n339     if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n340         # This is an inaccuracy in the AST: the nodes that can be\n341         # decorated do not carry explicit information on which line\n342         # the actual definition (class/def), but .fromline seems to\n343         # be close enough.\n344         node_line = node.fromlineno\n345     else:\n346         node_line = node.lineno\n347 \n348     if node_line == line:\n349         return node\n350 \n351     for child in node.get_children():\n352         result = _find_statement_by_line(child, line)\n353         if result:\n354             return result\n355 \n356     return None\n357 \n358 \n359 def extract_node(code: str, module_name: str = \"\") -> NodeNG | list[NodeNG]:\n360     \"\"\"Parses some Python code as a module and extracts a designated AST node.\n361 \n362     Statements:\n363      To extract one or more statement nodes, append #@ to the end of the line\n364 \n365      Examples:\n366        >>> def x():\n367        >>>   def y():\n368        >>>     return 1 #@\n369 \n370        The return statement will be extracted.\n371 \n372        >>> class X(object):\n373        >>>   def meth(self): #@\n374        >>>     pass\n375 \n376       The function object 'meth' will be extracted.\n377 \n378     Expressions:\n379      To extract arbitrary expressions, surround them with the fake\n380      function call __(...). After parsing, the surrounded expression\n381      will be returned and the whole AST (accessible via the returned\n382      node's parent attribute) will look like the function call was\n383      never there in the first place.\n384 \n385      Examples:\n386        >>> a = __(1)\n387 \n388        The const node will be extracted.\n389 \n390        >>> def x(d=__(foo.bar)): pass\n391 \n392        The node containing the default argument will be extracted.\n393 \n394        >>> def foo(a, b):\n395        >>>   return 0 < __(len(a)) < b\n396 \n397        The node containing the function call 'len' will be extracted.\n398 \n399     If no statements or expressions are selected, the last toplevel\n400     statement will be returned.\n401 \n402     If the selected statement is a discard statement, (i.e. an expression\n403     turned into a statement), the wrapped expression is returned instead.\n404 \n405     For convenience, singleton lists are unpacked.\n406 \n407     :param str code: A piece of Python code that is parsed as\n408     a module. Will be passed through textwrap.dedent first.\n409     :param str module_name: The name of the module.\n410     :returns: The designated node from the parse tree, or a list of nodes.\n411     \"\"\"\n412 \n413     def _extract(node):\n414         if isinstance(node, nodes.Expr):\n415             return node.value\n416 \n417         return node\n418 \n419     requested_lines = []\n420     for idx, line in enumerate(code.splitlines()):\n421         if line.strip().endswith(_STATEMENT_SELECTOR):\n422             requested_lines.append(idx + 1)\n423 \n424     tree = parse(code, module_name=module_name)\n425     if not tree.body:\n426         raise ValueError(\"Empty tree, cannot extract from it\")\n427 \n428     extracted = []\n429     if requested_lines:\n430         extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n431 \n432     # Modifies the tree.\n433     extracted.extend(_extract_expressions(tree))\n434 \n435     if not extracted:\n436         extracted.append(tree.body[-1])\n437 \n438     extracted = [_extract(node) for node in extracted]\n439     if len(extracted) == 1:\n440         return extracted[0]\n441     return extracted\n442 \n443 \n444 def _extract_single_node(code: str, module_name: str = \"\") -> NodeNG:\n445     \"\"\"Call extract_node while making sure that only one value is returned.\"\"\"\n446     ret = extract_node(code, module_name)\n447     if isinstance(ret, list):\n448         return ret[0]\n449     return ret\n450 \n451 \n452 def _parse_string(data, type_comments=True):\n453     parser_module = get_parser_module(type_comments=type_comments)\n454     try:\n455         parsed = parser_module.parse(data + \"\\n\", type_comments=type_comments)\n456     except SyntaxError as exc:\n457         # If the type annotations are misplaced for some reason, we do not want\n458         # to fail the entire parsing of the file, so we need to retry the parsing without\n459         # type comment support.\n460         if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n461             raise\n462 \n463         parser_module = get_parser_module(type_comments=False)\n464         parsed = parser_module.parse(data + \"\\n\", type_comments=False)\n465     return parsed, parser_module\n466 \n[end of astroid/builder.py]\n[start of astroid/interpreter/_import/spec.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import abc\n8 import enum\n9 import importlib\n10 import importlib.machinery\n11 import importlib.util\n12 import os\n13 import pathlib\n14 import sys\n15 import zipimport\n16 from collections.abc import Iterator, Sequence\n17 from pathlib import Path\n18 from typing import Any, NamedTuple\n19 \n20 from astroid.const import PY310_PLUS\n21 from astroid.modutils import EXT_LIB_DIRS\n22 \n23 from . import util\n24 \n25 if sys.version_info >= (3, 8):\n26     from typing import Literal\n27 else:\n28     from typing_extensions import Literal\n29 \n30 \n31 class ModuleType(enum.Enum):\n32     \"\"\"Python module types used for ModuleSpec.\"\"\"\n33 \n34     C_BUILTIN = enum.auto()\n35     C_EXTENSION = enum.auto()\n36     PKG_DIRECTORY = enum.auto()\n37     PY_CODERESOURCE = enum.auto()\n38     PY_COMPILED = enum.auto()\n39     PY_FROZEN = enum.auto()\n40     PY_RESOURCE = enum.auto()\n41     PY_SOURCE = enum.auto()\n42     PY_ZIPMODULE = enum.auto()\n43     PY_NAMESPACE = enum.auto()\n44 \n45 \n46 class ModuleSpec(NamedTuple):\n47     \"\"\"Defines a class similar to PEP 420's ModuleSpec\n48 \n49     A module spec defines a name of a module, its type, location\n50     and where submodules can be found, if the module is a package.\n51     \"\"\"\n52 \n53     name: str\n54     type: ModuleType | None\n55     location: str | None = None\n56     origin: str | None = None\n57     submodule_search_locations: Sequence[str] | None = None\n58 \n59 \n60 class Finder:\n61     \"\"\"A finder is a class which knows how to find a particular module.\"\"\"\n62 \n63     def __init__(self, path: Sequence[str] | None = None) -> None:\n64         self._path = path or sys.path\n65 \n66     @abc.abstractmethod\n67     def find_module(\n68         self,\n69         modname: str,\n70         module_parts: Sequence[str],\n71         processed: list[str],\n72         submodule_path: Sequence[str] | None,\n73     ) -> ModuleSpec | None:\n74         \"\"\"Find the given module\n75 \n76         Each finder is responsible for each protocol of finding, as long as\n77         they all return a ModuleSpec.\n78 \n79         :param modname: The module which needs to be searched.\n80         :param module_parts: It should be a list of strings,\n81                                   where each part contributes to the module's\n82                                   namespace.\n83         :param processed: What parts from the module parts were processed\n84                                so far.\n85         :param submodule_path: A list of paths where the module\n86                                     can be looked into.\n87         :returns: A ModuleSpec, describing how and where the module was found,\n88                   None, otherwise.\n89         \"\"\"\n90 \n91     def contribute_to_path(\n92         self, spec: ModuleSpec, processed: list[str]\n93     ) -> Sequence[str] | None:\n94         \"\"\"Get a list of extra paths where this finder can search.\"\"\"\n95 \n96 \n97 class ImportlibFinder(Finder):\n98     \"\"\"A finder based on the importlib module.\"\"\"\n99 \n100     _SUFFIXES: Sequence[tuple[str, ModuleType]] = (\n101         [(s, ModuleType.C_EXTENSION) for s in importlib.machinery.EXTENSION_SUFFIXES]\n102         + [(s, ModuleType.PY_SOURCE) for s in importlib.machinery.SOURCE_SUFFIXES]\n103         + [(s, ModuleType.PY_COMPILED) for s in importlib.machinery.BYTECODE_SUFFIXES]\n104     )\n105 \n106     def find_module(\n107         self,\n108         modname: str,\n109         module_parts: Sequence[str],\n110         processed: list[str],\n111         submodule_path: Sequence[str] | None,\n112     ) -> ModuleSpec | None:\n113         if submodule_path is not None:\n114             submodule_path = list(submodule_path)\n115         elif modname in sys.builtin_module_names:\n116             return ModuleSpec(\n117                 name=modname,\n118                 location=None,\n119                 type=ModuleType.C_BUILTIN,\n120             )\n121         else:\n122             try:\n123                 spec = importlib.util.find_spec(modname)\n124                 if spec and spec.loader is importlib.machinery.FrozenImporter:  # type: ignore[comparison-overlap]\n125                     # No need for BuiltinImporter; builtins handled above\n126                     return ModuleSpec(\n127                         name=modname,\n128                         location=getattr(spec.loader_state, \"filename\", None),\n129                         type=ModuleType.PY_FROZEN,\n130                     )\n131             except ValueError:\n132                 pass\n133             submodule_path = sys.path\n134 \n135         for entry in submodule_path:\n136             package_directory = os.path.join(entry, modname)\n137             for suffix in (\".py\", importlib.machinery.BYTECODE_SUFFIXES[0]):\n138                 package_file_name = \"__init__\" + suffix\n139                 file_path = os.path.join(package_directory, package_file_name)\n140                 if os.path.isfile(file_path):\n141                     return ModuleSpec(\n142                         name=modname,\n143                         location=package_directory,\n144                         type=ModuleType.PKG_DIRECTORY,\n145                     )\n146             for suffix, type_ in ImportlibFinder._SUFFIXES:\n147                 file_name = modname + suffix\n148                 file_path = os.path.join(entry, file_name)\n149                 if os.path.isfile(file_path):\n150                     return ModuleSpec(name=modname, location=file_path, type=type_)\n151         return None\n152 \n153     def contribute_to_path(\n154         self, spec: ModuleSpec, processed: list[str]\n155     ) -> Sequence[str] | None:\n156         if spec.location is None:\n157             # Builtin.\n158             return None\n159 \n160         if _is_setuptools_namespace(Path(spec.location)):\n161             # extend_path is called, search sys.path for module/packages\n162             # of this name see pkgutil.extend_path documentation\n163             path = [\n164                 os.path.join(p, *processed)\n165                 for p in sys.path\n166                 if os.path.isdir(os.path.join(p, *processed))\n167             ]\n168         elif spec.name == \"distutils\" and not any(\n169             spec.location.lower().startswith(ext_lib_dir.lower())\n170             for ext_lib_dir in EXT_LIB_DIRS\n171         ):\n172             # virtualenv below 20.0 patches distutils in an unexpected way\n173             # so we just find the location of distutils that will be\n174             # imported to avoid spurious import-error messages\n175             # https://github.com/PyCQA/pylint/issues/5645\n176             # A regression test to create this scenario exists in release-tests.yml\n177             # and can be triggered manually from GitHub Actions\n178             distutils_spec = importlib.util.find_spec(\"distutils\")\n179             if distutils_spec and distutils_spec.origin:\n180                 origin_path = Path(\n181                     distutils_spec.origin\n182                 )  # e.g. .../distutils/__init__.py\n183                 path = [str(origin_path.parent)]  # e.g. .../distutils\n184             else:\n185                 path = [spec.location]\n186         else:\n187             path = [spec.location]\n188         return path\n189 \n190 \n191 class ExplicitNamespacePackageFinder(ImportlibFinder):\n192     \"\"\"A finder for the explicit namespace packages.\"\"\"\n193 \n194     def find_module(\n195         self,\n196         modname: str,\n197         module_parts: Sequence[str],\n198         processed: list[str],\n199         submodule_path: Sequence[str] | None,\n200     ) -> ModuleSpec | None:\n201         if processed:\n202             modname = \".\".join(processed + [modname])\n203         if util.is_namespace(modname) and modname in sys.modules:\n204             submodule_path = sys.modules[modname].__path__\n205             return ModuleSpec(\n206                 name=modname,\n207                 location=\"\",\n208                 origin=\"namespace\",\n209                 type=ModuleType.PY_NAMESPACE,\n210                 submodule_search_locations=submodule_path,\n211             )\n212         return None\n213 \n214     def contribute_to_path(\n215         self, spec: ModuleSpec, processed: list[str]\n216     ) -> Sequence[str] | None:\n217         return spec.submodule_search_locations\n218 \n219 \n220 class ZipFinder(Finder):\n221     \"\"\"Finder that knows how to find a module inside zip files.\"\"\"\n222 \n223     def __init__(self, path: Sequence[str]) -> None:\n224         super().__init__(path)\n225         for entry_path in path:\n226             if entry_path not in sys.path_importer_cache:\n227                 # pylint: disable=no-member\n228                 try:\n229                     sys.path_importer_cache[entry_path] = zipimport.zipimporter(  # type: ignore[assignment]\n230                         entry_path\n231                     )\n232                 except zipimport.ZipImportError:\n233                     continue\n234 \n235     def find_module(\n236         self,\n237         modname: str,\n238         module_parts: Sequence[str],\n239         processed: list[str],\n240         submodule_path: Sequence[str] | None,\n241     ) -> ModuleSpec | None:\n242         try:\n243             file_type, filename, path = _search_zip(module_parts)\n244         except ImportError:\n245             return None\n246 \n247         return ModuleSpec(\n248             name=modname,\n249             location=filename,\n250             origin=\"egg\",\n251             type=file_type,\n252             submodule_search_locations=path,\n253         )\n254 \n255 \n256 class PathSpecFinder(Finder):\n257     \"\"\"Finder based on importlib.machinery.PathFinder.\"\"\"\n258 \n259     def find_module(\n260         self,\n261         modname: str,\n262         module_parts: Sequence[str],\n263         processed: list[str],\n264         submodule_path: Sequence[str] | None,\n265     ) -> ModuleSpec | None:\n266         spec = importlib.machinery.PathFinder.find_spec(modname, path=submodule_path)\n267         if spec is not None:\n268             is_namespace_pkg = spec.origin is None\n269             location = spec.origin if not is_namespace_pkg else None\n270             module_type = ModuleType.PY_NAMESPACE if is_namespace_pkg else None\n271             return ModuleSpec(\n272                 name=spec.name,\n273                 location=location,\n274                 origin=spec.origin,\n275                 type=module_type,\n276                 submodule_search_locations=list(spec.submodule_search_locations or []),\n277             )\n278         return spec\n279 \n280     def contribute_to_path(\n281         self, spec: ModuleSpec, processed: list[str]\n282     ) -> Sequence[str] | None:\n283         if spec.type == ModuleType.PY_NAMESPACE:\n284             return spec.submodule_search_locations\n285         return None\n286 \n287 \n288 _SPEC_FINDERS = (\n289     ImportlibFinder,\n290     ZipFinder,\n291     PathSpecFinder,\n292     ExplicitNamespacePackageFinder,\n293 )\n294 \n295 \n296 def _is_setuptools_namespace(location: pathlib.Path) -> bool:\n297     try:\n298         with open(location / \"__init__.py\", \"rb\") as stream:\n299             data = stream.read(4096)\n300     except OSError:\n301         return False\n302     else:\n303         extend_path = b\"pkgutil\" in data and b\"extend_path\" in data\n304         declare_namespace = (\n305             b\"pkg_resources\" in data and b\"declare_namespace(__name__)\" in data\n306         )\n307         return extend_path or declare_namespace\n308 \n309 \n310 def _get_zipimporters() -> Iterator[tuple[str, zipimport.zipimporter]]:\n311     for filepath, importer in sys.path_importer_cache.items():\n312         # pylint: disable-next=no-member\n313         if isinstance(importer, zipimport.zipimporter):\n314             yield filepath, importer\n315 \n316 \n317 def _search_zip(\n318     modpath: Sequence[str],\n319 ) -> tuple[Literal[ModuleType.PY_ZIPMODULE], str, str]:\n320     for filepath, importer in _get_zipimporters():\n321         if PY310_PLUS:\n322             found: Any = importer.find_spec(modpath[0])\n323         else:\n324             found = importer.find_module(modpath[0])\n325         if found:\n326             if PY310_PLUS:\n327                 if not importer.find_spec(os.path.sep.join(modpath)):\n328                     raise ImportError(\n329                         \"No module named %s in %s/%s\"\n330                         % (\".\".join(modpath[1:]), filepath, modpath)\n331                     )\n332             elif not importer.find_module(os.path.sep.join(modpath)):\n333                 raise ImportError(\n334                     \"No module named %s in %s/%s\"\n335                     % (\".\".join(modpath[1:]), filepath, modpath)\n336                 )\n337             return (\n338                 ModuleType.PY_ZIPMODULE,\n339                 os.path.abspath(filepath) + os.path.sep + os.path.sep.join(modpath),\n340                 filepath,\n341             )\n342     raise ImportError(f\"No module named {'.'.join(modpath)}\")\n343 \n344 \n345 def _find_spec_with_path(\n346     search_path: Sequence[str],\n347     modname: str,\n348     module_parts: list[str],\n349     processed: list[str],\n350     submodule_path: Sequence[str] | None,\n351 ) -> tuple[Finder, ModuleSpec]:\n352     for finder in _SPEC_FINDERS:\n353         finder_instance = finder(search_path)\n354         spec = finder_instance.find_module(\n355             modname, module_parts, processed, submodule_path\n356         )\n357         if spec is None:\n358             continue\n359         return finder_instance, spec\n360 \n361     raise ImportError(f\"No module named {'.'.join(module_parts)}\")\n362 \n363 \n364 def find_spec(modpath: list[str], path: Sequence[str] | None = None) -> ModuleSpec:\n365     \"\"\"Find a spec for the given module.\n366 \n367     :type modpath: list or tuple\n368     :param modpath:\n369       split module's name (i.e name of a module or package split\n370       on '.'), with leading empty strings for explicit relative import\n371 \n372     :type path: list or None\n373     :param path:\n374       optional list of path where the module or package should be\n375       searched (use sys.path if nothing or None is given)\n376 \n377     :rtype: ModuleSpec\n378     :return: A module spec, which describes how the module was\n379              found and where.\n380     \"\"\"\n381     _path = path or sys.path\n382 \n383     # Need a copy for not mutating the argument.\n384     modpath = modpath[:]\n385 \n386     submodule_path = None\n387     module_parts = modpath[:]\n388     processed: list[str] = []\n389 \n390     while modpath:\n391         modname = modpath.pop(0)\n392         finder, spec = _find_spec_with_path(\n393             _path, modname, module_parts, processed, submodule_path or path\n394         )\n395         processed.append(modname)\n396         if modpath:\n397             submodule_path = finder.contribute_to_path(spec, processed)\n398 \n399         if spec.type == ModuleType.PKG_DIRECTORY:\n400             spec = spec._replace(submodule_search_locations=submodule_path)\n401 \n402     return spec\n403 \n[end of astroid/interpreter/_import/spec.py]\n[start of astroid/manager.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"astroid manager: avoid multiple astroid build of a same module when\n6 possible by providing a class responsible to get astroid representation\n7 from various source and using a cache of built modules)\n8 \"\"\"\n9 \n10 from __future__ import annotations\n11 \n12 import collections\n13 import os\n14 import types\n15 import zipimport\n16 from importlib.util import find_spec, module_from_spec\n17 from typing import TYPE_CHECKING, ClassVar\n18 \n19 from astroid.const import BRAIN_MODULES_DIRECTORY\n20 from astroid.exceptions import AstroidBuildingError, AstroidImportError\n21 from astroid.interpreter._import import spec, util\n22 from astroid.modutils import (\n23     NoSourceFile,\n24     _cache_normalize_path_,\n25     file_info_from_modpath,\n26     get_source_file,\n27     is_module_name_part_of_extension_package_whitelist,\n28     is_python_source,\n29     is_standard_module,\n30     load_module_from_name,\n31     modpath_from_file,\n32 )\n33 from astroid.transforms import TransformVisitor\n34 from astroid.typing import AstroidManagerBrain\n35 \n36 if TYPE_CHECKING:\n37     from astroid import nodes\n38 \n39 ZIP_IMPORT_EXTS = (\".zip\", \".egg\", \".whl\", \".pyz\", \".pyzw\")\n40 \n41 \n42 def safe_repr(obj):\n43     try:\n44         return repr(obj)\n45     except Exception:  # pylint: disable=broad-except\n46         return \"???\"\n47 \n48 \n49 class AstroidManager:\n50     \"\"\"Responsible to build astroid from files or modules.\n51 \n52     Use the Borg (singleton) pattern.\n53     \"\"\"\n54 \n55     name = \"astroid loader\"\n56     brain: AstroidManagerBrain = {\n57         \"astroid_cache\": {},\n58         \"_mod_file_cache\": {},\n59         \"_failed_import_hooks\": [],\n60         \"always_load_extensions\": False,\n61         \"optimize_ast\": False,\n62         \"extension_package_whitelist\": set(),\n63         \"_transform\": TransformVisitor(),\n64     }\n65     max_inferable_values: ClassVar[int] = 100\n66 \n67     def __init__(self) -> None:\n68         # NOTE: cache entries are added by the [re]builder\n69         self.astroid_cache = AstroidManager.brain[\"astroid_cache\"]\n70         self._mod_file_cache = AstroidManager.brain[\"_mod_file_cache\"]\n71         self._failed_import_hooks = AstroidManager.brain[\"_failed_import_hooks\"]\n72         self.always_load_extensions = AstroidManager.brain[\"always_load_extensions\"]\n73         self.optimize_ast = AstroidManager.brain[\"optimize_ast\"]\n74         self.extension_package_whitelist = AstroidManager.brain[\n75             \"extension_package_whitelist\"\n76         ]\n77         self._transform = AstroidManager.brain[\"_transform\"]\n78 \n79     @property\n80     def register_transform(self):\n81         # This and unregister_transform below are exported for convenience\n82         return self._transform.register_transform\n83 \n84     @property\n85     def unregister_transform(self):\n86         return self._transform.unregister_transform\n87 \n88     @property\n89     def builtins_module(self) -> nodes.Module:\n90         return self.astroid_cache[\"builtins\"]\n91 \n92     def visit_transforms(self, node):\n93         \"\"\"Visit the transforms and apply them to the given *node*.\"\"\"\n94         return self._transform.visit(node)\n95 \n96     def ast_from_file(self, filepath, modname=None, fallback=True, source=False):\n97         \"\"\"given a module name, return the astroid object\"\"\"\n98         try:\n99             filepath = get_source_file(filepath, include_no_ext=True)\n100             source = True\n101         except NoSourceFile:\n102             pass\n103         if modname is None:\n104             try:\n105                 modname = \".\".join(modpath_from_file(filepath))\n106             except ImportError:\n107                 modname = filepath\n108         if (\n109             modname in self.astroid_cache\n110             and self.astroid_cache[modname].file == filepath\n111         ):\n112             return self.astroid_cache[modname]\n113         if source:\n114             # pylint: disable=import-outside-toplevel; circular import\n115             from astroid.builder import AstroidBuilder\n116 \n117             return AstroidBuilder(self).file_build(filepath, modname)\n118         if fallback and modname:\n119             return self.ast_from_module_name(modname)\n120         raise AstroidBuildingError(\"Unable to build an AST for {path}.\", path=filepath)\n121 \n122     def ast_from_string(self, data, modname=\"\", filepath=None):\n123         \"\"\"Given some source code as a string, return its corresponding astroid object\"\"\"\n124         # pylint: disable=import-outside-toplevel; circular import\n125         from astroid.builder import AstroidBuilder\n126 \n127         return AstroidBuilder(self).string_build(data, modname, filepath)\n128 \n129     def _build_stub_module(self, modname):\n130         # pylint: disable=import-outside-toplevel; circular import\n131         from astroid.builder import AstroidBuilder\n132 \n133         return AstroidBuilder(self).string_build(\"\", modname)\n134 \n135     def _build_namespace_module(self, modname: str, path: list[str]) -> nodes.Module:\n136         # pylint: disable=import-outside-toplevel; circular import\n137         from astroid.builder import build_namespace_package_module\n138 \n139         return build_namespace_package_module(modname, path)\n140 \n141     def _can_load_extension(self, modname: str) -> bool:\n142         if self.always_load_extensions:\n143             return True\n144         if is_standard_module(modname):\n145             return True\n146         return is_module_name_part_of_extension_package_whitelist(\n147             modname, self.extension_package_whitelist\n148         )\n149 \n150     def ast_from_module_name(self, modname, context_file=None):\n151         \"\"\"given a module name, return the astroid object\"\"\"\n152         if modname in self.astroid_cache:\n153             return self.astroid_cache[modname]\n154         if modname == \"__main__\":\n155             return self._build_stub_module(modname)\n156         if context_file:\n157             old_cwd = os.getcwd()\n158             os.chdir(os.path.dirname(context_file))\n159         try:\n160             found_spec = self.file_from_module_name(modname, context_file)\n161             if found_spec.type == spec.ModuleType.PY_ZIPMODULE:\n162                 module = self.zip_import_data(found_spec.location)\n163                 if module is not None:\n164                     return module\n165 \n166             elif found_spec.type in (\n167                 spec.ModuleType.C_BUILTIN,\n168                 spec.ModuleType.C_EXTENSION,\n169             ):\n170                 if (\n171                     found_spec.type == spec.ModuleType.C_EXTENSION\n172                     and not self._can_load_extension(modname)\n173                 ):\n174                     return self._build_stub_module(modname)\n175                 try:\n176                     module = load_module_from_name(modname)\n177                 except Exception as e:\n178                     raise AstroidImportError(\n179                         \"Loading {modname} failed with:\\n{error}\",\n180                         modname=modname,\n181                         path=found_spec.location,\n182                     ) from e\n183                 return self.ast_from_module(module, modname)\n184 \n185             elif found_spec.type == spec.ModuleType.PY_COMPILED:\n186                 raise AstroidImportError(\n187                     \"Unable to load compiled module {modname}.\",\n188                     modname=modname,\n189                     path=found_spec.location,\n190                 )\n191 \n192             elif found_spec.type == spec.ModuleType.PY_NAMESPACE:\n193                 return self._build_namespace_module(\n194                     modname, found_spec.submodule_search_locations\n195                 )\n196             elif found_spec.type == spec.ModuleType.PY_FROZEN:\n197                 if found_spec.location is None:\n198                     return self._build_stub_module(modname)\n199                 # For stdlib frozen modules we can determine the location and\n200                 # can therefore create a module from the source file\n201                 return self.ast_from_file(found_spec.location, modname, fallback=False)\n202 \n203             if found_spec.location is None:\n204                 raise AstroidImportError(\n205                     \"Can't find a file for module {modname}.\", modname=modname\n206                 )\n207 \n208             return self.ast_from_file(found_spec.location, modname, fallback=False)\n209         except AstroidBuildingError as e:\n210             for hook in self._failed_import_hooks:\n211                 try:\n212                     return hook(modname)\n213                 except AstroidBuildingError:\n214                     pass\n215             raise e\n216         finally:\n217             if context_file:\n218                 os.chdir(old_cwd)\n219 \n220     def zip_import_data(self, filepath):\n221         if zipimport is None:\n222             return None\n223 \n224         # pylint: disable=import-outside-toplevel; circular import\n225         from astroid.builder import AstroidBuilder\n226 \n227         builder = AstroidBuilder(self)\n228         for ext in ZIP_IMPORT_EXTS:\n229             try:\n230                 eggpath, resource = filepath.rsplit(ext + os.path.sep, 1)\n231             except ValueError:\n232                 continue\n233             try:\n234                 # pylint: disable-next=no-member\n235                 importer = zipimport.zipimporter(eggpath + ext)\n236                 zmodname = resource.replace(os.path.sep, \".\")\n237                 if importer.is_package(resource):\n238                     zmodname = zmodname + \".__init__\"\n239                 module = builder.string_build(\n240                     importer.get_source(resource), zmodname, filepath\n241                 )\n242                 return module\n243             except Exception:  # pylint: disable=broad-except\n244                 continue\n245         return None\n246 \n247     def file_from_module_name(self, modname, contextfile):\n248         try:\n249             value = self._mod_file_cache[(modname, contextfile)]\n250         except KeyError:\n251             try:\n252                 value = file_info_from_modpath(\n253                     modname.split(\".\"), context_file=contextfile\n254                 )\n255             except ImportError as e:\n256                 # pylint: disable-next=redefined-variable-type\n257                 value = AstroidImportError(\n258                     \"Failed to import module {modname} with error:\\n{error}.\",\n259                     modname=modname,\n260                     # we remove the traceback here to save on memory usage (since these exceptions are cached)\n261                     error=e.with_traceback(None),\n262                 )\n263             self._mod_file_cache[(modname, contextfile)] = value\n264         if isinstance(value, AstroidBuildingError):\n265             # we remove the traceback here to save on memory usage (since these exceptions are cached)\n266             raise value.with_traceback(None)  # pylint: disable=no-member\n267         return value\n268 \n269     def ast_from_module(self, module: types.ModuleType, modname: str | None = None):\n270         \"\"\"given an imported module, return the astroid object\"\"\"\n271         modname = modname or module.__name__\n272         if modname in self.astroid_cache:\n273             return self.astroid_cache[modname]\n274         try:\n275             # some builtin modules don't have __file__ attribute\n276             filepath = module.__file__\n277             if is_python_source(filepath):\n278                 return self.ast_from_file(filepath, modname)\n279         except AttributeError:\n280             pass\n281 \n282         # pylint: disable=import-outside-toplevel; circular import\n283         from astroid.builder import AstroidBuilder\n284 \n285         return AstroidBuilder(self).module_build(module, modname)\n286 \n287     def ast_from_class(self, klass, modname=None):\n288         \"\"\"get astroid for the given class\"\"\"\n289         if modname is None:\n290             try:\n291                 modname = klass.__module__\n292             except AttributeError as exc:\n293                 raise AstroidBuildingError(\n294                     \"Unable to get module for class {class_name}.\",\n295                     cls=klass,\n296                     class_repr=safe_repr(klass),\n297                     modname=modname,\n298                 ) from exc\n299         modastroid = self.ast_from_module_name(modname)\n300         return modastroid.getattr(klass.__name__)[0]  # XXX\n301 \n302     def infer_ast_from_something(self, obj, context=None):\n303         \"\"\"infer astroid for the given class\"\"\"\n304         if hasattr(obj, \"__class__\") and not isinstance(obj, type):\n305             klass = obj.__class__\n306         else:\n307             klass = obj\n308         try:\n309             modname = klass.__module__\n310         except AttributeError as exc:\n311             raise AstroidBuildingError(\n312                 \"Unable to get module for {class_repr}.\",\n313                 cls=klass,\n314                 class_repr=safe_repr(klass),\n315             ) from exc\n316         except Exception as exc:\n317             raise AstroidImportError(\n318                 \"Unexpected error while retrieving module for {class_repr}:\\n\"\n319                 \"{error}\",\n320                 cls=klass,\n321                 class_repr=safe_repr(klass),\n322             ) from exc\n323         try:\n324             name = klass.__name__\n325         except AttributeError as exc:\n326             raise AstroidBuildingError(\n327                 \"Unable to get name for {class_repr}:\\n\",\n328                 cls=klass,\n329                 class_repr=safe_repr(klass),\n330             ) from exc\n331         except Exception as exc:\n332             raise AstroidImportError(\n333                 \"Unexpected error while retrieving name for {class_repr}:\\n{error}\",\n334                 cls=klass,\n335                 class_repr=safe_repr(klass),\n336             ) from exc\n337         # take care, on living object __module__ is regularly wrong :(\n338         modastroid = self.ast_from_module_name(modname)\n339         if klass is obj:\n340             for inferred in modastroid.igetattr(name, context):\n341                 yield inferred\n342         else:\n343             for inferred in modastroid.igetattr(name, context):\n344                 yield inferred.instantiate_class()\n345 \n346     def register_failed_import_hook(self, hook):\n347         \"\"\"Registers a hook to resolve imports that cannot be found otherwise.\n348 \n349         `hook` must be a function that accepts a single argument `modname` which\n350         contains the name of the module or package that could not be imported.\n351         If `hook` can resolve the import, must return a node of type `astroid.Module`,\n352         otherwise, it must raise `AstroidBuildingError`.\n353         \"\"\"\n354         self._failed_import_hooks.append(hook)\n355 \n356     def cache_module(self, module):\n357         \"\"\"Cache a module if no module with the same name is known yet.\"\"\"\n358         self.astroid_cache.setdefault(module.name, module)\n359 \n360     def bootstrap(self) -> None:\n361         \"\"\"Bootstrap the required AST modules needed for the manager to work\n362 \n363         The bootstrap usually involves building the AST for the builtins\n364         module, which is required by the rest of astroid to work correctly.\n365         \"\"\"\n366         from astroid import raw_building  # pylint: disable=import-outside-toplevel\n367 \n368         raw_building._astroid_bootstrapping()\n369 \n370     def clear_cache(self) -> None:\n371         \"\"\"Clear the underlying cache, bootstrap the builtins module and\n372         re-register transforms.\"\"\"\n373         # import here because of cyclic imports\n374         # pylint: disable=import-outside-toplevel\n375         from astroid.inference_tip import clear_inference_tip_cache\n376         from astroid.interpreter.objectmodel import ObjectModel\n377         from astroid.nodes.node_classes import LookupMixIn\n378 \n379         clear_inference_tip_cache()\n380 \n381         self.astroid_cache.clear()\n382         # NB: not a new TransformVisitor()\n383         AstroidManager.brain[\"_transform\"].transforms = collections.defaultdict(list)\n384 \n385         for lru_cache in (\n386             LookupMixIn.lookup,\n387             _cache_normalize_path_,\n388             util.is_namespace,\n389             ObjectModel.attributes,\n390         ):\n391             lru_cache.cache_clear()  # type: ignore[attr-defined]\n392 \n393         self.bootstrap()\n394 \n395         # Reload brain plugins. During initialisation this is done in astroid.__init__.py\n396         for module in BRAIN_MODULES_DIRECTORY.iterdir():\n397             if module.suffix == \".py\":\n398                 module_spec = find_spec(f\"astroid.brain.{module.stem}\")\n399                 assert module_spec\n400                 module_object = module_from_spec(module_spec)\n401                 assert module_spec.loader\n402                 module_spec.loader.exec_module(module_object)\n403 \n[end of astroid/manager.py]\n[start of astroid/modutils.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Python modules manipulation utility functions.\n6 \n7 :type PY_SOURCE_EXTS: tuple(str)\n8 :var PY_SOURCE_EXTS: list of possible python source file extension\n9 \n10 :type STD_LIB_DIRS: set of str\n11 :var STD_LIB_DIRS: directories where standard modules are located\n12 \n13 :type BUILTIN_MODULES: dict\n14 :var BUILTIN_MODULES: dictionary with builtin module names has key\n15 \"\"\"\n16 \n17 from __future__ import annotations\n18 \n19 import importlib\n20 import importlib.machinery\n21 import importlib.util\n22 import io\n23 import itertools\n24 import logging\n25 import os\n26 import sys\n27 import sysconfig\n28 import types\n29 from collections.abc import Callable, Iterable, Sequence\n30 from contextlib import redirect_stderr, redirect_stdout\n31 from functools import lru_cache\n32 from pathlib import Path\n33 \n34 from astroid.const import IS_JYTHON, IS_PYPY\n35 from astroid.interpreter._import import spec, util\n36 \n37 logger = logging.getLogger(__name__)\n38 \n39 \n40 if sys.platform.startswith(\"win\"):\n41     PY_SOURCE_EXTS = (\"py\", \"pyw\")\n42     PY_COMPILED_EXTS = (\"dll\", \"pyd\")\n43 else:\n44     PY_SOURCE_EXTS = (\"py\",)\n45     PY_COMPILED_EXTS = (\"so\",)\n46 \n47 \n48 # TODO: Adding `platstdlib` is a fix for a workaround in virtualenv. At some point we should\n49 # revisit whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1323.\n50 STD_LIB_DIRS = {sysconfig.get_path(\"stdlib\"), sysconfig.get_path(\"platstdlib\")}\n51 \n52 if os.name == \"nt\":\n53     STD_LIB_DIRS.add(os.path.join(sys.prefix, \"dlls\"))\n54     try:\n55         # real_prefix is defined when running inside virtual environments,\n56         # created with the **virtualenv** library.\n57         # Deprecated in virtualenv==16.7.9\n58         # See: https://github.com/pypa/virtualenv/issues/1622\n59         STD_LIB_DIRS.add(os.path.join(sys.real_prefix, \"dlls\"))  # type: ignore[attr-defined]\n60     except AttributeError:\n61         # sys.base_exec_prefix is always defined, but in a virtual environment\n62         # created with the stdlib **venv** module, it points to the original\n63         # installation, if the virtual env is activated.\n64         try:\n65             STD_LIB_DIRS.add(os.path.join(sys.base_exec_prefix, \"dlls\"))\n66         except AttributeError:\n67             pass\n68 \n69 if IS_PYPY and sys.version_info < (3, 8):\n70     # PyPy stores the stdlib in two places: sys.prefix/lib_pypy and sys.prefix/lib-python/3\n71     # sysconfig.get_path on PyPy returns the first, but without an underscore so we patch this manually.\n72     # Beginning with 3.8 the stdlib is only stored in: sys.prefix/pypy{py_version_short}\n73     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib_pypy\"))\n74     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib-python/3\"))\n75 \n76     # TODO: This is a fix for a workaround in virtualenv. At some point we should revisit\n77     # whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1324.\n78     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib_pypy\"))\n79     STD_LIB_DIRS.add(\n80         str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib-python/3\")\n81     )\n82 \n83 if os.name == \"posix\":\n84     # Need the real prefix if we're in a virtualenv, otherwise\n85     # the usual one will do.\n86     # Deprecated in virtualenv==16.7.9\n87     # See: https://github.com/pypa/virtualenv/issues/1622\n88     try:\n89         prefix: str = sys.real_prefix  # type: ignore[attr-defined]\n90     except AttributeError:\n91         prefix = sys.prefix\n92 \n93     def _posix_path(path: str) -> str:\n94         base_python = \"python%d.%d\" % sys.version_info[:2]\n95         return os.path.join(prefix, path, base_python)\n96 \n97     STD_LIB_DIRS.add(_posix_path(\"lib\"))\n98     if sys.maxsize > 2**32:\n99         # This tries to fix a problem with /usr/lib64 builds,\n100         # where systems are running both 32-bit and 64-bit code\n101         # on the same machine, which reflects into the places where\n102         # standard library could be found. More details can be found\n103         # here http://bugs.python.org/issue1294959.\n104         # An easy reproducing case would be\n105         # https://github.com/PyCQA/pylint/issues/712#issuecomment-163178753\n106         STD_LIB_DIRS.add(_posix_path(\"lib64\"))\n107 \n108 EXT_LIB_DIRS = {sysconfig.get_path(\"purelib\"), sysconfig.get_path(\"platlib\")}\n109 BUILTIN_MODULES = dict.fromkeys(sys.builtin_module_names, True)\n110 \n111 \n112 class NoSourceFile(Exception):\n113     \"\"\"exception raised when we are not able to get a python\n114     source file for a precompiled file\n115     \"\"\"\n116 \n117 \n118 def _normalize_path(path: str) -> str:\n119     \"\"\"Resolve symlinks in path and convert to absolute path.\n120 \n121     Note that environment variables and ~ in the path need to be expanded in\n122     advance.\n123 \n124     This can be cached by using _cache_normalize_path.\n125     \"\"\"\n126     return os.path.normcase(os.path.realpath(path))\n127 \n128 \n129 def _path_from_filename(filename: str, is_jython: bool = IS_JYTHON) -> str:\n130     if not is_jython:\n131         return filename\n132     head, has_pyclass, _ = filename.partition(\"$py.class\")\n133     if has_pyclass:\n134         return head + \".py\"\n135     return filename\n136 \n137 \n138 def _handle_blacklist(\n139     blacklist: Sequence[str], dirnames: list[str], filenames: list[str]\n140 ) -> None:\n141     \"\"\"remove files/directories in the black list\n142 \n143     dirnames/filenames are usually from os.walk\n144     \"\"\"\n145     for norecurs in blacklist:\n146         if norecurs in dirnames:\n147             dirnames.remove(norecurs)\n148         elif norecurs in filenames:\n149             filenames.remove(norecurs)\n150 \n151 \n152 @lru_cache()\n153 def _cache_normalize_path_(path: str) -> str:\n154     return _normalize_path(path)\n155 \n156 \n157 def _cache_normalize_path(path: str) -> str:\n158     \"\"\"Normalize path with caching.\"\"\"\n159     # _module_file calls abspath on every path in sys.path every time it's\n160     # called; on a larger codebase this easily adds up to half a second just\n161     # assembling path components. This cache alleviates that.\n162     if not path:  # don't cache result for ''\n163         return _normalize_path(path)\n164     return _cache_normalize_path_(path)\n165 \n166 \n167 def load_module_from_name(dotted_name: str) -> types.ModuleType:\n168     \"\"\"Load a Python module from its name.\n169 \n170     :type dotted_name: str\n171     :param dotted_name: python name of a module or package\n172 \n173     :raise ImportError: if the module or package is not found\n174 \n175     :rtype: module\n176     :return: the loaded module\n177     \"\"\"\n178     try:\n179         return sys.modules[dotted_name]\n180     except KeyError:\n181         pass\n182 \n183     # Capture and log anything emitted during import to avoid\n184     # contaminating JSON reports in pylint\n185     with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(\n186         io.StringIO()\n187     ) as stdout:\n188         module = importlib.import_module(dotted_name)\n189 \n190     stderr_value = stderr.getvalue()\n191     if stderr_value:\n192         logger.error(\n193             \"Captured stderr while importing %s:\\n%s\", dotted_name, stderr_value\n194         )\n195     stdout_value = stdout.getvalue()\n196     if stdout_value:\n197         logger.info(\n198             \"Captured stdout while importing %s:\\n%s\", dotted_name, stdout_value\n199         )\n200 \n201     return module\n202 \n203 \n204 def load_module_from_modpath(parts: Sequence[str]) -> types.ModuleType:\n205     \"\"\"Load a python module from its split name.\n206 \n207     :param parts:\n208       python name of a module or package split on '.'\n209 \n210     :raise ImportError: if the module or package is not found\n211 \n212     :return: the loaded module\n213     \"\"\"\n214     return load_module_from_name(\".\".join(parts))\n215 \n216 \n217 def load_module_from_file(filepath: str) -> types.ModuleType:\n218     \"\"\"Load a Python module from it's path.\n219 \n220     :type filepath: str\n221     :param filepath: path to the python module or package\n222 \n223     :raise ImportError: if the module or package is not found\n224 \n225     :rtype: module\n226     :return: the loaded module\n227     \"\"\"\n228     modpath = modpath_from_file(filepath)\n229     return load_module_from_modpath(modpath)\n230 \n231 \n232 def check_modpath_has_init(path: str, mod_path: list[str]) -> bool:\n233     \"\"\"check there are some __init__.py all along the way\"\"\"\n234     modpath: list[str] = []\n235     for part in mod_path:\n236         modpath.append(part)\n237         path = os.path.join(path, part)\n238         if not _has_init(path):\n239             old_namespace = util.is_namespace(\".\".join(modpath))\n240             if not old_namespace:\n241                 return False\n242     return True\n243 \n244 \n245 def _get_relative_base_path(filename: str, path_to_check: str) -> list[str] | None:\n246     \"\"\"Extracts the relative mod path of the file to import from\n247 \n248     Check if a file is within the passed in path and if so, returns the\n249     relative mod path from the one passed in.\n250 \n251     If the filename is no in path_to_check, returns None\n252 \n253     Note this function will look for both abs and realpath of the file,\n254     this allows to find the relative base path even if the file is a\n255     symlink of a file in the passed in path\n256 \n257     Examples:\n258         _get_relative_base_path(\"/a/b/c/d.py\", \"/a/b\") ->  [\"c\",\"d\"]\n259         _get_relative_base_path(\"/a/b/c/d.py\", \"/dev\") ->  None\n260     \"\"\"\n261     importable_path = None\n262     path_to_check = os.path.normcase(path_to_check)\n263     abs_filename = os.path.abspath(filename)\n264     if os.path.normcase(abs_filename).startswith(path_to_check):\n265         importable_path = abs_filename\n266 \n267     real_filename = os.path.realpath(filename)\n268     if os.path.normcase(real_filename).startswith(path_to_check):\n269         importable_path = real_filename\n270 \n271     # if \"var\" in path_to_check:\n272     #     breakpoint()\n273 \n274     if importable_path:\n275         base_path = os.path.splitext(importable_path)[0]\n276         relative_base_path = base_path[len(path_to_check) :]\n277         return [pkg for pkg in relative_base_path.split(os.sep) if pkg]\n278 \n279     return None\n280 \n281 \n282 def modpath_from_file_with_callback(\n283     filename: str,\n284     path: Sequence[str] | None = None,\n285     is_package_cb: Callable[[str, list[str]], bool] | None = None,\n286 ) -> list[str]:\n287     filename = os.path.expanduser(_path_from_filename(filename))\n288     paths_to_check = sys.path.copy()\n289     if path:\n290         paths_to_check += path\n291     for pathname in itertools.chain(\n292         paths_to_check, map(_cache_normalize_path, paths_to_check)\n293     ):\n294         if not pathname:\n295             continue\n296         modpath = _get_relative_base_path(filename, pathname)\n297         if not modpath:\n298             continue\n299         assert is_package_cb is not None\n300         if is_package_cb(pathname, modpath[:-1]):\n301             return modpath\n302 \n303     raise ImportError(\n304         \"Unable to find module for {} in {}\".format(filename, \", \\n\".join(sys.path))\n305     )\n306 \n307 \n308 def modpath_from_file(filename: str, path: Sequence[str] | None = None) -> list[str]:\n309     \"\"\"Get the corresponding split module's name from a filename\n310 \n311     This function will return the name of a module or package split on `.`.\n312 \n313     :type filename: str\n314     :param filename: file's path for which we want the module's name\n315 \n316     :type Optional[List[str]] path:\n317       Optional list of path where the module or package should be\n318       searched (use sys.path if nothing or None is given)\n319 \n320     :raise ImportError:\n321       if the corresponding module's name has not been found\n322 \n323     :rtype: list(str)\n324     :return: the corresponding split module's name\n325     \"\"\"\n326     return modpath_from_file_with_callback(filename, path, check_modpath_has_init)\n327 \n328 \n329 def file_from_modpath(\n330     modpath: list[str],\n331     path: Sequence[str] | None = None,\n332     context_file: str | None = None,\n333 ) -> str | None:\n334     return file_info_from_modpath(modpath, path, context_file).location\n335 \n336 \n337 def file_info_from_modpath(\n338     modpath: list[str],\n339     path: Sequence[str] | None = None,\n340     context_file: str | None = None,\n341 ) -> spec.ModuleSpec:\n342     \"\"\"given a mod path (i.e. split module / package name), return the\n343     corresponding file, giving priority to source file over precompiled\n344     file if it exists\n345 \n346     :param modpath:\n347       split module's name (i.e name of a module or package split\n348       on '.')\n349       (this means explicit relative imports that start with dots have\n350       empty strings in this list!)\n351 \n352     :param path:\n353       optional list of path where the module or package should be\n354       searched (use sys.path if nothing or None is given)\n355 \n356     :param context_file:\n357       context file to consider, necessary if the identifier has been\n358       introduced using a relative import unresolvable in the actual\n359       context (i.e. modutils)\n360 \n361     :raise ImportError: if there is no such module in the directory\n362 \n363     :return:\n364       the path to the module's file or None if it's an integrated\n365       builtin module such as 'sys'\n366     \"\"\"\n367     if context_file is not None:\n368         context: str | None = os.path.dirname(context_file)\n369     else:\n370         context = context_file\n371     if modpath[0] == \"xml\":\n372         # handle _xmlplus\n373         try:\n374             return _spec_from_modpath([\"_xmlplus\"] + modpath[1:], path, context)\n375         except ImportError:\n376             return _spec_from_modpath(modpath, path, context)\n377     elif modpath == [\"os\", \"path\"]:\n378         # FIXME: currently ignoring search_path...\n379         return spec.ModuleSpec(\n380             name=\"os.path\",\n381             location=os.path.__file__,\n382             type=spec.ModuleType.PY_SOURCE,\n383         )\n384     return _spec_from_modpath(modpath, path, context)\n385 \n386 \n387 def get_module_part(dotted_name: str, context_file: str | None = None) -> str:\n388     \"\"\"given a dotted name return the module part of the name :\n389 \n390     >>> get_module_part('astroid.as_string.dump')\n391     'astroid.as_string'\n392 \n393     :param dotted_name: full name of the identifier we are interested in\n394 \n395     :param context_file:\n396       context file to consider, necessary if the identifier has been\n397       introduced using a relative import unresolvable in the actual\n398       context (i.e. modutils)\n399 \n400 \n401     :raise ImportError: if there is no such module in the directory\n402 \n403     :return:\n404       the module part of the name or None if we have not been able at\n405       all to import the given name\n406 \n407     XXX: deprecated, since it doesn't handle package precedence over module\n408     (see #10066)\n409     \"\"\"\n410     # os.path trick\n411     if dotted_name.startswith(\"os.path\"):\n412         return \"os.path\"\n413     parts = dotted_name.split(\".\")\n414     if context_file is not None:\n415         # first check for builtin module which won't be considered latter\n416         # in that case (path != None)\n417         if parts[0] in BUILTIN_MODULES:\n418             if len(parts) > 2:\n419                 raise ImportError(dotted_name)\n420             return parts[0]\n421         # don't use += or insert, we want a new list to be created !\n422     path: list[str] | None = None\n423     starti = 0\n424     if parts[0] == \"\":\n425         assert (\n426             context_file is not None\n427         ), \"explicit relative import, but no context_file?\"\n428         path = []  # prevent resolving the import non-relatively\n429         starti = 1\n430     while parts[starti] == \"\":  # for all further dots: change context\n431         starti += 1\n432         assert (\n433             context_file is not None\n434         ), \"explicit relative import, but no context_file?\"\n435         context_file = os.path.dirname(context_file)\n436     for i in range(starti, len(parts)):\n437         try:\n438             file_from_modpath(\n439                 parts[starti : i + 1], path=path, context_file=context_file\n440             )\n441         except ImportError:\n442             if i < max(1, len(parts) - 2):\n443                 raise\n444             return \".\".join(parts[:i])\n445     return dotted_name\n446 \n447 \n448 def get_module_files(\n449     src_directory: str, blacklist: Sequence[str], list_all: bool = False\n450 ) -> list[str]:\n451     \"\"\"given a package directory return a list of all available python\n452     module's files in the package and its subpackages\n453 \n454     :param src_directory:\n455       path of the directory corresponding to the package\n456 \n457     :param blacklist: iterable\n458       list of files or directories to ignore.\n459 \n460     :param list_all:\n461         get files from all paths, including ones without __init__.py\n462 \n463     :return:\n464       the list of all available python module's files in the package and\n465       its subpackages\n466     \"\"\"\n467     files: list[str] = []\n468     for directory, dirnames, filenames in os.walk(src_directory):\n469         if directory in blacklist:\n470             continue\n471         _handle_blacklist(blacklist, dirnames, filenames)\n472         # check for __init__.py\n473         if not list_all and \"__init__.py\" not in filenames:\n474             dirnames[:] = ()\n475             continue\n476         for filename in filenames:\n477             if _is_python_file(filename):\n478                 src = os.path.join(directory, filename)\n479                 files.append(src)\n480     return files\n481 \n482 \n483 def get_source_file(filename: str, include_no_ext: bool = False) -> str:\n484     \"\"\"given a python module's file name return the matching source file\n485     name (the filename will be returned identically if it's already an\n486     absolute path to a python source file...)\n487 \n488     :param filename: python module's file name\n489 \n490     :raise NoSourceFile: if no source file exists on the file system\n491 \n492     :return: the absolute path of the source file if it exists\n493     \"\"\"\n494     filename = os.path.abspath(_path_from_filename(filename))\n495     base, orig_ext = os.path.splitext(filename)\n496     for ext in PY_SOURCE_EXTS:\n497         source_path = f\"{base}.{ext}\"\n498         if os.path.exists(source_path):\n499             return source_path\n500     if include_no_ext and not orig_ext and os.path.exists(base):\n501         return base\n502     raise NoSourceFile(filename)\n503 \n504 \n505 def is_python_source(filename: str | None) -> bool:\n506     \"\"\"\n507     return: True if the filename is a python source file\n508     \"\"\"\n509     if not filename:\n510         return False\n511     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n512 \n513 \n514 def is_standard_module(modname: str, std_path: Iterable[str] | None = None) -> bool:\n515     \"\"\"try to guess if a module is a standard python module (by default,\n516     see `std_path` parameter's description)\n517 \n518     :param modname: name of the module we are interested in\n519 \n520     :param std_path: list of path considered has standard\n521 \n522     :return:\n523       true if the module:\n524       - is located on the path listed in one of the directory in `std_path`\n525       - is a built-in module\n526     \"\"\"\n527     modname = modname.split(\".\")[0]\n528     try:\n529         filename = file_from_modpath([modname])\n530     except ImportError:\n531         # import failed, i'm probably not so wrong by supposing it's\n532         # not standard...\n533         return False\n534     # modules which are not living in a file are considered standard\n535     # (sys and __builtin__ for instance)\n536     if filename is None:\n537         # we assume there are no namespaces in stdlib\n538         return not util.is_namespace(modname)\n539     filename = _normalize_path(filename)\n540     for path in EXT_LIB_DIRS:\n541         if filename.startswith(_cache_normalize_path(path)):\n542             return False\n543     if std_path is None:\n544         std_path = STD_LIB_DIRS\n545 \n546     return any(filename.startswith(_cache_normalize_path(path)) for path in std_path)\n547 \n548 \n549 def is_relative(modname: str, from_file: str) -> bool:\n550     \"\"\"return true if the given module name is relative to the given\n551     file name\n552 \n553     :param modname: name of the module we are interested in\n554 \n555     :param from_file:\n556       path of the module from which modname has been imported\n557 \n558     :return:\n559       true if the module has been imported relatively to `from_file`\n560     \"\"\"\n561     if not os.path.isdir(from_file):\n562         from_file = os.path.dirname(from_file)\n563     if from_file in sys.path:\n564         return False\n565     return bool(\n566         importlib.machinery.PathFinder.find_spec(\n567             modname.split(\".\", maxsplit=1)[0], [from_file]\n568         )\n569     )\n570 \n571 \n572 # internal only functions #####################################################\n573 \n574 \n575 def _spec_from_modpath(\n576     modpath: list[str],\n577     path: Sequence[str] | None = None,\n578     context: str | None = None,\n579 ) -> spec.ModuleSpec:\n580     \"\"\"given a mod path (i.e. split module / package name), return the\n581     corresponding spec\n582 \n583     this function is used internally, see `file_from_modpath`'s\n584     documentation for more information\n585     \"\"\"\n586     assert modpath\n587     location = None\n588     if context is not None:\n589         try:\n590             found_spec = spec.find_spec(modpath, [context])\n591             location = found_spec.location\n592         except ImportError:\n593             found_spec = spec.find_spec(modpath, path)\n594             location = found_spec.location\n595     else:\n596         found_spec = spec.find_spec(modpath, path)\n597     if found_spec.type == spec.ModuleType.PY_COMPILED:\n598         try:\n599             assert found_spec.location is not None\n600             location = get_source_file(found_spec.location)\n601             return found_spec._replace(\n602                 location=location, type=spec.ModuleType.PY_SOURCE\n603             )\n604         except NoSourceFile:\n605             return found_spec._replace(location=location)\n606     elif found_spec.type == spec.ModuleType.C_BUILTIN:\n607         # integrated builtin module\n608         return found_spec._replace(location=None)\n609     elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n610         assert found_spec.location is not None\n611         location = _has_init(found_spec.location)\n612         return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n613     return found_spec\n614 \n615 \n616 def _is_python_file(filename: str) -> bool:\n617     \"\"\"return true if the given filename should be considered as a python file\n618 \n619     .pyc and .pyo are ignored\n620     \"\"\"\n621     return filename.endswith((\".py\", \".so\", \".pyd\", \".pyw\"))\n622 \n623 \n624 def _has_init(directory: str) -> str | None:\n625     \"\"\"if the given directory has a valid __init__ file, return its path,\n626     else return None\n627     \"\"\"\n628     mod_or_pack = os.path.join(directory, \"__init__\")\n629     for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n630         if os.path.exists(mod_or_pack + \".\" + ext):\n631             return mod_or_pack + \".\" + ext\n632     return None\n633 \n634 \n635 def is_namespace(specobj: spec.ModuleSpec) -> bool:\n636     return specobj.type == spec.ModuleType.PY_NAMESPACE\n637 \n638 \n639 def is_directory(specobj: spec.ModuleSpec) -> bool:\n640     return specobj.type == spec.ModuleType.PKG_DIRECTORY\n641 \n642 \n643 def is_module_name_part_of_extension_package_whitelist(\n644     module_name: str, package_whitelist: set[str]\n645 ) -> bool:\n646     \"\"\"\n647     Returns True if one part of the module name is in the package whitelist\n648 \n649     >>> is_module_name_part_of_extension_package_whitelist('numpy.core.umath', {'numpy'})\n650     True\n651     \"\"\"\n652     parts = module_name.split(\".\")\n653     return any(\n654         \".\".join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)\n655     )\n656 \n[end of astroid/modutils.py]\n[start of astroid/nodes/_base_nodes.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"This module contains some base nodes that can be inherited for the different nodes.\n6 \n7 Previously these were called Mixin nodes.\n8 \"\"\"\n9 \n10 from __future__ import annotations\n11 \n12 import itertools\n13 import sys\n14 from collections.abc import Iterator\n15 from typing import TYPE_CHECKING, ClassVar\n16 \n17 from astroid import decorators\n18 from astroid.exceptions import AttributeInferenceError\n19 from astroid.nodes.node_ng import NodeNG\n20 \n21 if TYPE_CHECKING:\n22     from astroid import nodes\n23 \n24 if sys.version_info >= (3, 8):\n25     from functools import cached_property\n26 else:\n27     from astroid.decorators import cachedproperty as cached_property\n28 \n29 \n30 class Statement(NodeNG):\n31     \"\"\"Statement node adding a few attributes\n32 \n33     NOTE: This class is part of the public API of 'astroid.nodes'.\n34     \"\"\"\n35 \n36     is_statement = True\n37     \"\"\"Whether this node indicates a statement.\"\"\"\n38 \n39     def next_sibling(self):\n40         \"\"\"The next sibling statement node.\n41 \n42         :returns: The next sibling statement node.\n43         :rtype: NodeNG or None\n44         \"\"\"\n45         stmts = self.parent.child_sequence(self)\n46         index = stmts.index(self)\n47         try:\n48             return stmts[index + 1]\n49         except IndexError:\n50             return None\n51 \n52     def previous_sibling(self):\n53         \"\"\"The previous sibling statement.\n54 \n55         :returns: The previous sibling statement node.\n56         :rtype: NodeNG or None\n57         \"\"\"\n58         stmts = self.parent.child_sequence(self)\n59         index = stmts.index(self)\n60         if index >= 1:\n61             return stmts[index - 1]\n62         return None\n63 \n64 \n65 class NoChildrenNode(NodeNG):\n66     \"\"\"Base nodes for nodes with no children, e.g. Pass.\"\"\"\n67 \n68     def get_children(self) -> Iterator[NodeNG]:\n69         yield from ()\n70 \n71 \n72 class FilterStmtsBaseNode(NodeNG):\n73     \"\"\"Base node for statement filtering and assignment type\"\"\"\n74 \n75     def _get_filtered_stmts(self, _, node, _stmts, mystmt: Statement | None):\n76         \"\"\"method used in _filter_stmts to get statements and trigger break\"\"\"\n77         if self.statement(future=True) is mystmt:\n78             # original node's statement is the assignment, only keep\n79             # current node (gen exp, list comp)\n80             return [node], True\n81         return _stmts, False\n82 \n83     def assign_type(self):\n84         return self\n85 \n86 \n87 class AssignTypeNode(NodeNG):\n88     \"\"\"Base node for nodes that can 'assign' such as AnnAssign.\"\"\"\n89 \n90     def assign_type(self):\n91         return self\n92 \n93     def _get_filtered_stmts(self, lookup_node, node, _stmts, mystmt: Statement | None):\n94         \"\"\"method used in filter_stmts\"\"\"\n95         if self is mystmt:\n96             return _stmts, True\n97         if self.statement(future=True) is mystmt:\n98             # original node's statement is the assignment, only keep\n99             # current node (gen exp, list comp)\n100             return [node], True\n101         return _stmts, False\n102 \n103 \n104 class ParentAssignNode(AssignTypeNode):\n105     \"\"\"Base node for nodes whose assign_type is determined by the parent node.\"\"\"\n106 \n107     def assign_type(self):\n108         return self.parent.assign_type()\n109 \n110 \n111 class ImportNode(FilterStmtsBaseNode, NoChildrenNode, Statement):\n112     \"\"\"Base node for From and Import Nodes\"\"\"\n113 \n114     modname: str | None\n115     \"\"\"The module that is being imported from.\n116 \n117     This is ``None`` for relative imports.\n118     \"\"\"\n119 \n120     names: list[tuple[str, str | None]]\n121     \"\"\"What is being imported from the module.\n122 \n123     Each entry is a :class:`tuple` of the name being imported,\n124     and the alias that the name is assigned to (if any).\n125     \"\"\"\n126 \n127     def _infer_name(self, frame, name):\n128         return name\n129 \n130     def do_import_module(self, modname: str | None = None) -> nodes.Module:\n131         \"\"\"return the ast for a module whose name is <modname> imported by <self>\"\"\"\n132         # handle special case where we are on a package node importing a module\n133         # using the same name as the package, which may end in an infinite loop\n134         # on relative imports\n135         # XXX: no more needed ?\n136         mymodule = self.root()\n137         level = getattr(self, \"level\", None)  # Import as no level\n138         if modname is None:\n139             modname = self.modname\n140         # XXX we should investigate deeper if we really want to check\n141         # importing itself: modname and mymodule.name be relative or absolute\n142         # pylint: disable-next=no-member # pylint doesn't recognize type of mymodule\n143         if mymodule.relative_to_absolute_name(modname, level) == mymodule.name:\n144             # FIXME: we used to raise InferenceError here, but why ?\n145             return mymodule\n146 \n147         # pylint: disable-next=no-member # pylint doesn't recognize type of mymodule\n148         return mymodule.import_module(\n149             modname, level=level, relative_only=level and level >= 1\n150         )\n151 \n152     def real_name(self, asname):\n153         \"\"\"get name from 'as' name\"\"\"\n154         for name, _asname in self.names:\n155             if name == \"*\":\n156                 return asname\n157             if not _asname:\n158                 name = name.split(\".\", 1)[0]\n159                 _asname = name\n160             if asname == _asname:\n161                 return name\n162         raise AttributeInferenceError(\n163             \"Could not find original name for {attribute} in {target!r}\",\n164             target=self,\n165             attribute=asname,\n166         )\n167 \n168 \n169 class MultiLineBlockNode(NodeNG):\n170     \"\"\"Base node for multi-line blocks, e.g. For and FunctionDef.\n171     Note that this does not apply to every node with a `body` field.\n172     For instance, an If node has a multi-line body, but the body of an\n173     IfExpr is not multi-line, and hence cannot contain Return nodes,\n174     Assign nodes, etc.\n175     \"\"\"\n176 \n177     _multi_line_block_fields: ClassVar[tuple[str, ...]] = ()\n178 \n179     @cached_property\n180     def _multi_line_blocks(self):\n181         return tuple(getattr(self, field) for field in self._multi_line_block_fields)\n182 \n183     def _get_return_nodes_skip_functions(self):\n184         for block in self._multi_line_blocks:\n185             for child_node in block:\n186                 if child_node.is_function:\n187                     continue\n188                 yield from child_node._get_return_nodes_skip_functions()\n189 \n190     def _get_yield_nodes_skip_lambdas(self):\n191         for block in self._multi_line_blocks:\n192             for child_node in block:\n193                 if child_node.is_lambda:\n194                     continue\n195                 yield from child_node._get_yield_nodes_skip_lambdas()\n196 \n197     @decorators.cached\n198     def _get_assign_nodes(self):\n199         children_assign_nodes = (\n200             child_node._get_assign_nodes()\n201             for block in self._multi_line_blocks\n202             for child_node in block\n203         )\n204         return list(itertools.chain.from_iterable(children_assign_nodes))\n205 \n206 \n207 class MultiLineWithElseBlockNode(MultiLineBlockNode):\n208     \"\"\"Base node for multi-line blocks that can have else statements.\"\"\"\n209 \n210     @cached_property\n211     def blockstart_tolineno(self):\n212         return self.lineno\n213 \n214     def _elsed_block_range(self, lineno, orelse, last=None):\n215         \"\"\"handle block line numbers range for try/finally, for, if and while\n216         statements\n217         \"\"\"\n218         if lineno == self.fromlineno:\n219             return lineno, lineno\n220         if orelse:\n221             if lineno >= orelse[0].fromlineno:\n222                 return lineno, orelse[-1].tolineno\n223             return lineno, orelse[0].fromlineno - 1\n224         return lineno, last or self.tolineno\n225 \n[end of astroid/nodes/_base_nodes.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/astroid/manager.py\n+++ b/astroid/manager.py\n@@ -1,5 +1,6 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+import warnings\n from __future__ import annotations\n \n import collections\n@@ -78,6 +79,8 @@\n \n     def ast_from_module_name(self, modname, context_file=None):\n         \"\"\"given a module name, return the astroid object\"\"\"\n+        # Suppress FutureWarnings from pandas\n+        warnings.simplefilter(\"ignore\", category=FutureWarning)\n         if modname in self.astroid_cache:\n             return self.astroid_cache[modname]\n         if modname == \"__main__\":\n```", "model_patch": "--- a/astroid/manager.py\n+++ b/astroid/manager.py\n@@ -1,5 +1,6 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+import warnings\n from __future__ import annotations\n \n import collections\n@@ -78,6 +79,8 @@\n \n     def ast_from_module_name(self, modname, context_file=None):\n         \"\"\"given a module name, return the astroid object\"\"\"\n+        # Suppress FutureWarnings from pandas\n+        warnings.simplefilter(\"ignore\", category=FutureWarning)\n         if modname in self.astroid_cache:\n             return self.astroid_cache[modname]\n         if modname == \"__main__\":\n"}
{"instance_id": "pylint-dev__astroid-1903", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nUnhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\nUnhandled AttributeError during str.format template evaluation\n### Steps to reproduce\r\n\r\n1. Use `astroid` to parse code that provides arguments to a `str.format` template that attempts to access non-existent attributes\r\n\r\n```py\r\ndaniel_age = 12\r\n\"My name is {0.name}\".format(daniel_age)  # int literal has no 'name' attribute\r\n```\r\n\r\n### Current behavior\r\n\r\n1. unhandled `AttributeError` when it attempts to [evaluate the templated string](https://github.com/PyCQA/astroid/blob/8bdec591f228e7db6a0be66b6ca814227ff50001/astroid/brain/brain_builtin_inference.py#L956)\r\n\r\n### Expected behavior\r\n\r\n1. could raise an `AstroidTypeError` to indicate that the template formatting is invalid\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n`2.13.0-dev0`\r\n\r\nRelates to pycqa/pylint#7939.\n\n</issue>\n<code>\n[start of README.rst]\n1 Astroid\n2 =======\n3 \n4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main\n5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main\n6     :alt: Coverage badge from coveralls.io\n7 \n8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest\n9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest\n10     :alt: Documentation Status\n11 \n12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n13     :target: https://github.com/ambv/black\n14 \n15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg\n16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main\n17    :alt: pre-commit.ci status\n18 \n19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png\n20    :width: 200\n21    :alt: Tidelift\n22 \n23 .. list-table::\n24    :widths: 10 100\n25 \n26    * - |tidelift_logo|\n27      - Professional support for astroid is available as part of the\n28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for\n29        purchasing and maintaining their software, with professional grade assurances\n30        from the experts who know it best, while seamlessly integrating with existing\n31        tools.\n32 \n33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme\n34 \n35 \n36 \n37 What's this?\n38 ------------\n39 \n40 The aim of this module is to provide a common base representation of\n41 python source code. It is currently the library powering pylint's capabilities.\n42 \n43 It provides a compatible representation which comes from the `_ast`\n44 module.  It rebuilds the tree generated by the builtin _ast module by\n45 recursively walking down the AST and building an extended ast. The new\n46 node classes have additional methods and attributes for different\n47 usages. They include some support for static inference and local name\n48 scopes. Furthermore, astroid can also build partial trees by inspecting living\n49 objects.\n50 \n51 \n52 Installation\n53 ------------\n54 \n55 Extract the tarball, jump into the created directory and run::\n56 \n57     pip install .\n58 \n59 \n60 If you want to do an editable installation, you can run::\n61 \n62     pip install -e .\n63 \n64 \n65 If you have any questions, please mail the code-quality@python.org\n66 mailing list for support. See\n67 http://mail.python.org/mailman/listinfo/code-quality for subscription\n68 information and archives.\n69 \n70 Documentation\n71 -------------\n72 http://astroid.readthedocs.io/en/latest/\n73 \n74 \n75 Python Versions\n76 ---------------\n77 \n78 astroid 2.0 is currently available for Python 3 only. If you want Python 2\n79 support, use an older version of astroid (though note that these versions\n80 are no longer supported).\n81 \n82 Test\n83 ----\n84 \n85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use\n86 either `tox` or `pytest`::\n87 \n88     tox\n89     pytest\n90 \n[end of README.rst]\n[start of astroid/brain/brain_builtin_inference.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Astroid hooks for various builtins.\"\"\"\n6 \n7 from __future__ import annotations\n8 \n9 import itertools\n10 from collections.abc import Iterator\n11 from functools import partial\n12 \n13 from astroid import arguments, helpers, inference_tip, nodes, objects, util\n14 from astroid.builder import AstroidBuilder\n15 from astroid.context import InferenceContext\n16 from astroid.exceptions import (\n17     AstroidTypeError,\n18     AttributeInferenceError,\n19     InferenceError,\n20     MroError,\n21     UseInferenceDefault,\n22 )\n23 from astroid.manager import AstroidManager\n24 from astroid.nodes import scoped_nodes\n25 \n26 OBJECT_DUNDER_NEW = \"object.__new__\"\n27 \n28 STR_CLASS = \"\"\"\n29 class whatever(object):\n30     def join(self, iterable):\n31         return {rvalue}\n32     def replace(self, old, new, count=None):\n33         return {rvalue}\n34     def format(self, *args, **kwargs):\n35         return {rvalue}\n36     def encode(self, encoding='ascii', errors=None):\n37         return b''\n38     def decode(self, encoding='ascii', errors=None):\n39         return u''\n40     def capitalize(self):\n41         return {rvalue}\n42     def title(self):\n43         return {rvalue}\n44     def lower(self):\n45         return {rvalue}\n46     def upper(self):\n47         return {rvalue}\n48     def swapcase(self):\n49         return {rvalue}\n50     def index(self, sub, start=None, end=None):\n51         return 0\n52     def find(self, sub, start=None, end=None):\n53         return 0\n54     def count(self, sub, start=None, end=None):\n55         return 0\n56     def strip(self, chars=None):\n57         return {rvalue}\n58     def lstrip(self, chars=None):\n59         return {rvalue}\n60     def rstrip(self, chars=None):\n61         return {rvalue}\n62     def rjust(self, width, fillchar=None):\n63         return {rvalue}\n64     def center(self, width, fillchar=None):\n65         return {rvalue}\n66     def ljust(self, width, fillchar=None):\n67         return {rvalue}\n68 \"\"\"\n69 \n70 \n71 BYTES_CLASS = \"\"\"\n72 class whatever(object):\n73     def join(self, iterable):\n74         return {rvalue}\n75     def replace(self, old, new, count=None):\n76         return {rvalue}\n77     def decode(self, encoding='ascii', errors=None):\n78         return u''\n79     def capitalize(self):\n80         return {rvalue}\n81     def title(self):\n82         return {rvalue}\n83     def lower(self):\n84         return {rvalue}\n85     def upper(self):\n86         return {rvalue}\n87     def swapcase(self):\n88         return {rvalue}\n89     def index(self, sub, start=None, end=None):\n90         return 0\n91     def find(self, sub, start=None, end=None):\n92         return 0\n93     def count(self, sub, start=None, end=None):\n94         return 0\n95     def strip(self, chars=None):\n96         return {rvalue}\n97     def lstrip(self, chars=None):\n98         return {rvalue}\n99     def rstrip(self, chars=None):\n100         return {rvalue}\n101     def rjust(self, width, fillchar=None):\n102         return {rvalue}\n103     def center(self, width, fillchar=None):\n104         return {rvalue}\n105     def ljust(self, width, fillchar=None):\n106         return {rvalue}\n107 \"\"\"\n108 \n109 \n110 def _extend_string_class(class_node, code, rvalue):\n111     \"\"\"function to extend builtin str/unicode class\"\"\"\n112     code = code.format(rvalue=rvalue)\n113     fake = AstroidBuilder(AstroidManager()).string_build(code)[\"whatever\"]\n114     for method in fake.mymethods():\n115         method.parent = class_node\n116         method.lineno = None\n117         method.col_offset = None\n118         if \"__class__\" in method.locals:\n119             method.locals[\"__class__\"] = [class_node]\n120         class_node.locals[method.name] = [method]\n121         method.parent = class_node\n122 \n123 \n124 def _extend_builtins(class_transforms):\n125     builtin_ast = AstroidManager().builtins_module\n126     for class_name, transform in class_transforms.items():\n127         transform(builtin_ast[class_name])\n128 \n129 \n130 _extend_builtins(\n131     {\n132         \"bytes\": partial(_extend_string_class, code=BYTES_CLASS, rvalue=\"b''\"),\n133         \"str\": partial(_extend_string_class, code=STR_CLASS, rvalue=\"''\"),\n134     }\n135 )\n136 \n137 \n138 def _builtin_filter_predicate(node, builtin_name) -> bool:\n139     if (\n140         builtin_name == \"type\"\n141         and node.root().name == \"re\"\n142         and isinstance(node.func, nodes.Name)\n143         and node.func.name == \"type\"\n144         and isinstance(node.parent, nodes.Assign)\n145         and len(node.parent.targets) == 1\n146         and isinstance(node.parent.targets[0], nodes.AssignName)\n147         and node.parent.targets[0].name in {\"Pattern\", \"Match\"}\n148     ):\n149         # Handle re.Pattern and re.Match in brain_re\n150         # Match these patterns from stdlib/re.py\n151         # ```py\n152         # Pattern = type(...)\n153         # Match = type(...)\n154         # ```\n155         return False\n156     if isinstance(node.func, nodes.Name) and node.func.name == builtin_name:\n157         return True\n158     if isinstance(node.func, nodes.Attribute):\n159         return (\n160             node.func.attrname == \"fromkeys\"\n161             and isinstance(node.func.expr, nodes.Name)\n162             and node.func.expr.name == \"dict\"\n163         )\n164     return False\n165 \n166 \n167 def register_builtin_transform(transform, builtin_name):\n168     \"\"\"Register a new transform function for the given *builtin_name*.\n169 \n170     The transform function must accept two parameters, a node and\n171     an optional context.\n172     \"\"\"\n173 \n174     def _transform_wrapper(node, context: InferenceContext | None = None):\n175         result = transform(node, context=context)\n176         if result:\n177             if not result.parent:\n178                 # Let the transformation function determine\n179                 # the parent for its result. Otherwise,\n180                 # we set it to be the node we transformed from.\n181                 result.parent = node\n182 \n183             if result.lineno is None:\n184                 result.lineno = node.lineno\n185             # Can be a 'Module' see https://github.com/PyCQA/pylint/issues/4671\n186             # We don't have a regression test on this one: tread carefully\n187             if hasattr(result, \"col_offset\") and result.col_offset is None:\n188                 result.col_offset = node.col_offset\n189         return iter([result])\n190 \n191     AstroidManager().register_transform(\n192         nodes.Call,\n193         inference_tip(_transform_wrapper),\n194         partial(_builtin_filter_predicate, builtin_name=builtin_name),\n195     )\n196 \n197 \n198 def _container_generic_inference(node, context, node_type, transform):\n199     args = node.args\n200     if not args:\n201         return node_type()\n202     if len(node.args) > 1:\n203         raise UseInferenceDefault()\n204 \n205     (arg,) = args\n206     transformed = transform(arg)\n207     if not transformed:\n208         try:\n209             inferred = next(arg.infer(context=context))\n210         except (InferenceError, StopIteration) as exc:\n211             raise UseInferenceDefault from exc\n212         if inferred is util.Uninferable:\n213             raise UseInferenceDefault\n214         transformed = transform(inferred)\n215     if not transformed or transformed is util.Uninferable:\n216         raise UseInferenceDefault\n217     return transformed\n218 \n219 \n220 def _container_generic_transform(  # pylint: disable=inconsistent-return-statements\n221     arg, context, klass, iterables, build_elts\n222 ):\n223     if isinstance(arg, klass):\n224         return arg\n225     if isinstance(arg, iterables):\n226         if all(isinstance(elt, nodes.Const) for elt in arg.elts):\n227             elts = [elt.value for elt in arg.elts]\n228         else:\n229             # TODO: Does not handle deduplication for sets.\n230             elts = []\n231             for element in arg.elts:\n232                 if not element:\n233                     continue\n234                 inferred = helpers.safe_infer(element, context=context)\n235                 if inferred:\n236                     evaluated_object = nodes.EvaluatedObject(\n237                         original=element, value=inferred\n238                     )\n239                     elts.append(evaluated_object)\n240     elif isinstance(arg, nodes.Dict):\n241         # Dicts need to have consts as strings already.\n242         if not all(isinstance(elt[0], nodes.Const) for elt in arg.items):\n243             raise UseInferenceDefault()\n244         elts = [item[0].value for item in arg.items]\n245     elif isinstance(arg, nodes.Const) and isinstance(arg.value, (str, bytes)):\n246         elts = arg.value\n247     else:\n248         return\n249     return klass.from_elements(elts=build_elts(elts))\n250 \n251 \n252 def _infer_builtin_container(\n253     node, context, klass=None, iterables=None, build_elts=None\n254 ):\n255     transform_func = partial(\n256         _container_generic_transform,\n257         context=context,\n258         klass=klass,\n259         iterables=iterables,\n260         build_elts=build_elts,\n261     )\n262 \n263     return _container_generic_inference(node, context, klass, transform_func)\n264 \n265 \n266 # pylint: disable=invalid-name\n267 infer_tuple = partial(\n268     _infer_builtin_container,\n269     klass=nodes.Tuple,\n270     iterables=(\n271         nodes.List,\n272         nodes.Set,\n273         objects.FrozenSet,\n274         objects.DictItems,\n275         objects.DictKeys,\n276         objects.DictValues,\n277     ),\n278     build_elts=tuple,\n279 )\n280 \n281 infer_list = partial(\n282     _infer_builtin_container,\n283     klass=nodes.List,\n284     iterables=(\n285         nodes.Tuple,\n286         nodes.Set,\n287         objects.FrozenSet,\n288         objects.DictItems,\n289         objects.DictKeys,\n290         objects.DictValues,\n291     ),\n292     build_elts=list,\n293 )\n294 \n295 infer_set = partial(\n296     _infer_builtin_container,\n297     klass=nodes.Set,\n298     iterables=(nodes.List, nodes.Tuple, objects.FrozenSet, objects.DictKeys),\n299     build_elts=set,\n300 )\n301 \n302 infer_frozenset = partial(\n303     _infer_builtin_container,\n304     klass=objects.FrozenSet,\n305     iterables=(nodes.List, nodes.Tuple, nodes.Set, objects.FrozenSet, objects.DictKeys),\n306     build_elts=frozenset,\n307 )\n308 \n309 \n310 def _get_elts(arg, context):\n311     def is_iterable(n):\n312         return isinstance(n, (nodes.List, nodes.Tuple, nodes.Set))\n313 \n314     try:\n315         inferred = next(arg.infer(context))\n316     except (InferenceError, StopIteration) as exc:\n317         raise UseInferenceDefault from exc\n318     if isinstance(inferred, nodes.Dict):\n319         items = inferred.items\n320     elif is_iterable(inferred):\n321         items = []\n322         for elt in inferred.elts:\n323             # If an item is not a pair of two items,\n324             # then fallback to the default inference.\n325             # Also, take in consideration only hashable items,\n326             # tuples and consts. We are choosing Names as well.\n327             if not is_iterable(elt):\n328                 raise UseInferenceDefault()\n329             if len(elt.elts) != 2:\n330                 raise UseInferenceDefault()\n331             if not isinstance(elt.elts[0], (nodes.Tuple, nodes.Const, nodes.Name)):\n332                 raise UseInferenceDefault()\n333             items.append(tuple(elt.elts))\n334     else:\n335         raise UseInferenceDefault()\n336     return items\n337 \n338 \n339 def infer_dict(node, context: InferenceContext | None = None):\n340     \"\"\"Try to infer a dict call to a Dict node.\n341 \n342     The function treats the following cases:\n343 \n344         * dict()\n345         * dict(mapping)\n346         * dict(iterable)\n347         * dict(iterable, **kwargs)\n348         * dict(mapping, **kwargs)\n349         * dict(**kwargs)\n350 \n351     If a case can't be inferred, we'll fallback to default inference.\n352     \"\"\"\n353     call = arguments.CallSite.from_call(node, context=context)\n354     if call.has_invalid_arguments() or call.has_invalid_keywords():\n355         raise UseInferenceDefault\n356 \n357     args = call.positional_arguments\n358     kwargs = list(call.keyword_arguments.items())\n359 \n360     if not args and not kwargs:\n361         # dict()\n362         return nodes.Dict()\n363     if kwargs and not args:\n364         # dict(a=1, b=2, c=4)\n365         items = [(nodes.Const(key), value) for key, value in kwargs]\n366     elif len(args) == 1 and kwargs:\n367         # dict(some_iterable, b=2, c=4)\n368         elts = _get_elts(args[0], context)\n369         keys = [(nodes.Const(key), value) for key, value in kwargs]\n370         items = elts + keys\n371     elif len(args) == 1:\n372         items = _get_elts(args[0], context)\n373     else:\n374         raise UseInferenceDefault()\n375     value = nodes.Dict(\n376         col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n377     )\n378     value.postinit(items)\n379     return value\n380 \n381 \n382 def infer_super(node, context: InferenceContext | None = None):\n383     \"\"\"Understand super calls.\n384 \n385     There are some restrictions for what can be understood:\n386 \n387         * unbounded super (one argument form) is not understood.\n388 \n389         * if the super call is not inside a function (classmethod or method),\n390           then the default inference will be used.\n391 \n392         * if the super arguments can't be inferred, the default inference\n393           will be used.\n394     \"\"\"\n395     if len(node.args) == 1:\n396         # Ignore unbounded super.\n397         raise UseInferenceDefault\n398 \n399     scope = node.scope()\n400     if not isinstance(scope, nodes.FunctionDef):\n401         # Ignore non-method uses of super.\n402         raise UseInferenceDefault\n403     if scope.type not in (\"classmethod\", \"method\"):\n404         # Not interested in staticmethods.\n405         raise UseInferenceDefault\n406 \n407     cls = scoped_nodes.get_wrapping_class(scope)\n408     if not node.args:\n409         mro_pointer = cls\n410         # In we are in a classmethod, the interpreter will fill\n411         # automatically the class as the second argument, not an instance.\n412         if scope.type == \"classmethod\":\n413             mro_type = cls\n414         else:\n415             mro_type = cls.instantiate_class()\n416     else:\n417         try:\n418             mro_pointer = next(node.args[0].infer(context=context))\n419         except (InferenceError, StopIteration) as exc:\n420             raise UseInferenceDefault from exc\n421         try:\n422             mro_type = next(node.args[1].infer(context=context))\n423         except (InferenceError, StopIteration) as exc:\n424             raise UseInferenceDefault from exc\n425 \n426     if mro_pointer is util.Uninferable or mro_type is util.Uninferable:\n427         # No way we could understand this.\n428         raise UseInferenceDefault\n429 \n430     super_obj = objects.Super(\n431         mro_pointer=mro_pointer, mro_type=mro_type, self_class=cls, scope=scope\n432     )\n433     super_obj.parent = node\n434     return super_obj\n435 \n436 \n437 def _infer_getattr_args(node, context):\n438     if len(node.args) not in (2, 3):\n439         # Not a valid getattr call.\n440         raise UseInferenceDefault\n441 \n442     try:\n443         obj = next(node.args[0].infer(context=context))\n444         attr = next(node.args[1].infer(context=context))\n445     except (InferenceError, StopIteration) as exc:\n446         raise UseInferenceDefault from exc\n447 \n448     if obj is util.Uninferable or attr is util.Uninferable:\n449         # If one of the arguments is something we can't infer,\n450         # then also make the result of the getattr call something\n451         # which is unknown.\n452         return util.Uninferable, util.Uninferable\n453 \n454     is_string = isinstance(attr, nodes.Const) and isinstance(attr.value, str)\n455     if not is_string:\n456         raise UseInferenceDefault\n457 \n458     return obj, attr.value\n459 \n460 \n461 def infer_getattr(node, context: InferenceContext | None = None):\n462     \"\"\"Understand getattr calls\n463 \n464     If one of the arguments is an Uninferable object, then the\n465     result will be an Uninferable object. Otherwise, the normal attribute\n466     lookup will be done.\n467     \"\"\"\n468     obj, attr = _infer_getattr_args(node, context)\n469     if (\n470         obj is util.Uninferable\n471         or attr is util.Uninferable\n472         or not hasattr(obj, \"igetattr\")\n473     ):\n474         return util.Uninferable\n475 \n476     try:\n477         return next(obj.igetattr(attr, context=context))\n478     except (StopIteration, InferenceError, AttributeInferenceError):\n479         if len(node.args) == 3:\n480             # Try to infer the default and return it instead.\n481             try:\n482                 return next(node.args[2].infer(context=context))\n483             except (StopIteration, InferenceError) as exc:\n484                 raise UseInferenceDefault from exc\n485 \n486     raise UseInferenceDefault\n487 \n488 \n489 def infer_hasattr(node, context: InferenceContext | None = None):\n490     \"\"\"Understand hasattr calls\n491 \n492     This always guarantees three possible outcomes for calling\n493     hasattr: Const(False) when we are sure that the object\n494     doesn't have the intended attribute, Const(True) when\n495     we know that the object has the attribute and Uninferable\n496     when we are unsure of the outcome of the function call.\n497     \"\"\"\n498     try:\n499         obj, attr = _infer_getattr_args(node, context)\n500         if (\n501             obj is util.Uninferable\n502             or attr is util.Uninferable\n503             or not hasattr(obj, \"getattr\")\n504         ):\n505             return util.Uninferable\n506         obj.getattr(attr, context=context)\n507     except UseInferenceDefault:\n508         # Can't infer something from this function call.\n509         return util.Uninferable\n510     except AttributeInferenceError:\n511         # Doesn't have it.\n512         return nodes.Const(False)\n513     return nodes.Const(True)\n514 \n515 \n516 def infer_callable(node, context: InferenceContext | None = None):\n517     \"\"\"Understand callable calls\n518 \n519     This follows Python's semantics, where an object\n520     is callable if it provides an attribute __call__,\n521     even though that attribute is something which can't be\n522     called.\n523     \"\"\"\n524     if len(node.args) != 1:\n525         # Invalid callable call.\n526         raise UseInferenceDefault\n527 \n528     argument = node.args[0]\n529     try:\n530         inferred = next(argument.infer(context=context))\n531     except (InferenceError, StopIteration):\n532         return util.Uninferable\n533     if inferred is util.Uninferable:\n534         return util.Uninferable\n535     return nodes.Const(inferred.callable())\n536 \n537 \n538 def infer_property(\n539     node: nodes.Call, context: InferenceContext | None = None\n540 ) -> objects.Property:\n541     \"\"\"Understand `property` class\n542 \n543     This only infers the output of `property`\n544     call, not the arguments themselves.\n545     \"\"\"\n546     if len(node.args) < 1:\n547         # Invalid property call.\n548         raise UseInferenceDefault\n549 \n550     getter = node.args[0]\n551     try:\n552         inferred = next(getter.infer(context=context))\n553     except (InferenceError, StopIteration) as exc:\n554         raise UseInferenceDefault from exc\n555 \n556     if not isinstance(inferred, (nodes.FunctionDef, nodes.Lambda)):\n557         raise UseInferenceDefault\n558 \n559     prop_func = objects.Property(\n560         function=inferred,\n561         name=inferred.name,\n562         lineno=node.lineno,\n563         parent=node,\n564         col_offset=node.col_offset,\n565     )\n566     prop_func.postinit(\n567         body=[],\n568         args=inferred.args,\n569         doc_node=getattr(inferred, \"doc_node\", None),\n570     )\n571     return prop_func\n572 \n573 \n574 def infer_bool(node, context: InferenceContext | None = None):\n575     \"\"\"Understand bool calls.\"\"\"\n576     if len(node.args) > 1:\n577         # Invalid bool call.\n578         raise UseInferenceDefault\n579 \n580     if not node.args:\n581         return nodes.Const(False)\n582 \n583     argument = node.args[0]\n584     try:\n585         inferred = next(argument.infer(context=context))\n586     except (InferenceError, StopIteration):\n587         return util.Uninferable\n588     if inferred is util.Uninferable:\n589         return util.Uninferable\n590 \n591     bool_value = inferred.bool_value(context=context)\n592     if bool_value is util.Uninferable:\n593         return util.Uninferable\n594     return nodes.Const(bool_value)\n595 \n596 \n597 def infer_type(node, context: InferenceContext | None = None):\n598     \"\"\"Understand the one-argument form of *type*.\"\"\"\n599     if len(node.args) != 1:\n600         raise UseInferenceDefault\n601 \n602     return helpers.object_type(node.args[0], context)\n603 \n604 \n605 def infer_slice(node, context: InferenceContext | None = None):\n606     \"\"\"Understand `slice` calls.\"\"\"\n607     args = node.args\n608     if not 0 < len(args) <= 3:\n609         raise UseInferenceDefault\n610 \n611     infer_func = partial(helpers.safe_infer, context=context)\n612     args = [infer_func(arg) for arg in args]\n613     for arg in args:\n614         if not arg or arg is util.Uninferable:\n615             raise UseInferenceDefault\n616         if not isinstance(arg, nodes.Const):\n617             raise UseInferenceDefault\n618         if not isinstance(arg.value, (type(None), int)):\n619             raise UseInferenceDefault\n620 \n621     if len(args) < 3:\n622         # Make sure we have 3 arguments.\n623         args.extend([None] * (3 - len(args)))\n624 \n625     slice_node = nodes.Slice(\n626         lineno=node.lineno, col_offset=node.col_offset, parent=node.parent\n627     )\n628     slice_node.postinit(*args)\n629     return slice_node\n630 \n631 \n632 def _infer_object__new__decorator(node, context: InferenceContext | None = None):\n633     # Instantiate class immediately\n634     # since that's what @object.__new__ does\n635     return iter((node.instantiate_class(),))\n636 \n637 \n638 def _infer_object__new__decorator_check(node) -> bool:\n639     \"\"\"Predicate before inference_tip\n640 \n641     Check if the given ClassDef has an @object.__new__ decorator\n642     \"\"\"\n643     if not node.decorators:\n644         return False\n645 \n646     for decorator in node.decorators.nodes:\n647         if isinstance(decorator, nodes.Attribute):\n648             if decorator.as_string() == OBJECT_DUNDER_NEW:\n649                 return True\n650     return False\n651 \n652 \n653 def infer_issubclass(callnode, context: InferenceContext | None = None):\n654     \"\"\"Infer issubclass() calls\n655 \n656     :param nodes.Call callnode: an `issubclass` call\n657     :param InferenceContext context: the context for the inference\n658     :rtype nodes.Const: Boolean Const value of the `issubclass` call\n659     :raises UseInferenceDefault: If the node cannot be inferred\n660     \"\"\"\n661     call = arguments.CallSite.from_call(callnode, context=context)\n662     if call.keyword_arguments:\n663         # issubclass doesn't support keyword arguments\n664         raise UseInferenceDefault(\"TypeError: issubclass() takes no keyword arguments\")\n665     if len(call.positional_arguments) != 2:\n666         raise UseInferenceDefault(\n667             f\"Expected two arguments, got {len(call.positional_arguments)}\"\n668         )\n669     # The left hand argument is the obj to be checked\n670     obj_node, class_or_tuple_node = call.positional_arguments\n671 \n672     try:\n673         obj_type = next(obj_node.infer(context=context))\n674     except (InferenceError, StopIteration) as exc:\n675         raise UseInferenceDefault from exc\n676     if not isinstance(obj_type, nodes.ClassDef):\n677         raise UseInferenceDefault(\"TypeError: arg 1 must be class\")\n678 \n679     # The right hand argument is the class(es) that the given\n680     # object is to be checked against.\n681     try:\n682         class_container = _class_or_tuple_to_container(\n683             class_or_tuple_node, context=context\n684         )\n685     except InferenceError as exc:\n686         raise UseInferenceDefault from exc\n687     try:\n688         issubclass_bool = helpers.object_issubclass(obj_type, class_container, context)\n689     except AstroidTypeError as exc:\n690         raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n691     except MroError as exc:\n692         raise UseInferenceDefault from exc\n693     return nodes.Const(issubclass_bool)\n694 \n695 \n696 def infer_isinstance(callnode, context: InferenceContext | None = None):\n697     \"\"\"Infer isinstance calls\n698 \n699     :param nodes.Call callnode: an isinstance call\n700     :rtype nodes.Const: Boolean Const value of isinstance call\n701 \n702     :raises UseInferenceDefault: If the node cannot be inferred\n703     \"\"\"\n704     call = arguments.CallSite.from_call(callnode, context=context)\n705     if call.keyword_arguments:\n706         # isinstance doesn't support keyword arguments\n707         raise UseInferenceDefault(\"TypeError: isinstance() takes no keyword arguments\")\n708     if len(call.positional_arguments) != 2:\n709         raise UseInferenceDefault(\n710             f\"Expected two arguments, got {len(call.positional_arguments)}\"\n711         )\n712     # The left hand argument is the obj to be checked\n713     obj_node, class_or_tuple_node = call.positional_arguments\n714     # The right hand argument is the class(es) that the given\n715     # obj is to be check is an instance of\n716     try:\n717         class_container = _class_or_tuple_to_container(\n718             class_or_tuple_node, context=context\n719         )\n720     except InferenceError as exc:\n721         raise UseInferenceDefault from exc\n722     try:\n723         isinstance_bool = helpers.object_isinstance(obj_node, class_container, context)\n724     except AstroidTypeError as exc:\n725         raise UseInferenceDefault(\"TypeError: \" + str(exc)) from exc\n726     except MroError as exc:\n727         raise UseInferenceDefault from exc\n728     if isinstance_bool is util.Uninferable:\n729         raise UseInferenceDefault\n730     return nodes.Const(isinstance_bool)\n731 \n732 \n733 def _class_or_tuple_to_container(node, context: InferenceContext | None = None):\n734     # Move inferences results into container\n735     # to simplify later logic\n736     # raises InferenceError if any of the inferences fall through\n737     try:\n738         node_infer = next(node.infer(context=context))\n739     except StopIteration as e:\n740         raise InferenceError(node=node, context=context) from e\n741     # arg2 MUST be a type or a TUPLE of types\n742     # for isinstance\n743     if isinstance(node_infer, nodes.Tuple):\n744         try:\n745             class_container = [\n746                 next(node.infer(context=context)) for node in node_infer.elts\n747             ]\n748         except StopIteration as e:\n749             raise InferenceError(node=node, context=context) from e\n750         class_container = [\n751             klass_node for klass_node in class_container if klass_node is not None\n752         ]\n753     else:\n754         class_container = [node_infer]\n755     return class_container\n756 \n757 \n758 def infer_len(node, context: InferenceContext | None = None):\n759     \"\"\"Infer length calls\n760 \n761     :param nodes.Call node: len call to infer\n762     :param context.InferenceContext: node context\n763     :rtype nodes.Const: a Const node with the inferred length, if possible\n764     \"\"\"\n765     call = arguments.CallSite.from_call(node, context=context)\n766     if call.keyword_arguments:\n767         raise UseInferenceDefault(\"TypeError: len() must take no keyword arguments\")\n768     if len(call.positional_arguments) != 1:\n769         raise UseInferenceDefault(\n770             \"TypeError: len() must take exactly one argument \"\n771             \"({len}) given\".format(len=len(call.positional_arguments))\n772         )\n773     [argument_node] = call.positional_arguments\n774 \n775     try:\n776         return nodes.Const(helpers.object_len(argument_node, context=context))\n777     except (AstroidTypeError, InferenceError) as exc:\n778         raise UseInferenceDefault(str(exc)) from exc\n779 \n780 \n781 def infer_str(node, context: InferenceContext | None = None):\n782     \"\"\"Infer str() calls\n783 \n784     :param nodes.Call node: str() call to infer\n785     :param context.InferenceContext: node context\n786     :rtype nodes.Const: a Const containing an empty string\n787     \"\"\"\n788     call = arguments.CallSite.from_call(node, context=context)\n789     if call.keyword_arguments:\n790         raise UseInferenceDefault(\"TypeError: str() must take no keyword arguments\")\n791     try:\n792         return nodes.Const(\"\")\n793     except (AstroidTypeError, InferenceError) as exc:\n794         raise UseInferenceDefault(str(exc)) from exc\n795 \n796 \n797 def infer_int(node, context: InferenceContext | None = None):\n798     \"\"\"Infer int() calls\n799 \n800     :param nodes.Call node: int() call to infer\n801     :param context.InferenceContext: node context\n802     :rtype nodes.Const: a Const containing the integer value of the int() call\n803     \"\"\"\n804     call = arguments.CallSite.from_call(node, context=context)\n805     if call.keyword_arguments:\n806         raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n807 \n808     if call.positional_arguments:\n809         try:\n810             first_value = next(call.positional_arguments[0].infer(context=context))\n811         except (InferenceError, StopIteration) as exc:\n812             raise UseInferenceDefault(str(exc)) from exc\n813 \n814         if first_value is util.Uninferable:\n815             raise UseInferenceDefault\n816 \n817         if isinstance(first_value, nodes.Const) and isinstance(\n818             first_value.value, (int, str)\n819         ):\n820             try:\n821                 actual_value = int(first_value.value)\n822             except ValueError:\n823                 return nodes.Const(0)\n824             return nodes.Const(actual_value)\n825 \n826     return nodes.Const(0)\n827 \n828 \n829 def infer_dict_fromkeys(node, context: InferenceContext | None = None):\n830     \"\"\"Infer dict.fromkeys\n831 \n832     :param nodes.Call node: dict.fromkeys() call to infer\n833     :param context.InferenceContext context: node context\n834     :rtype nodes.Dict:\n835         a Dictionary containing the values that astroid was able to infer.\n836         In case the inference failed for any reason, an empty dictionary\n837         will be inferred instead.\n838     \"\"\"\n839 \n840     def _build_dict_with_elements(elements):\n841         new_node = nodes.Dict(\n842             col_offset=node.col_offset, lineno=node.lineno, parent=node.parent\n843         )\n844         new_node.postinit(elements)\n845         return new_node\n846 \n847     call = arguments.CallSite.from_call(node, context=context)\n848     if call.keyword_arguments:\n849         raise UseInferenceDefault(\"TypeError: int() must take no keyword arguments\")\n850     if len(call.positional_arguments) not in {1, 2}:\n851         raise UseInferenceDefault(\n852             \"TypeError: Needs between 1 and 2 positional arguments\"\n853         )\n854 \n855     default = nodes.Const(None)\n856     values = call.positional_arguments[0]\n857     try:\n858         inferred_values = next(values.infer(context=context))\n859     except (InferenceError, StopIteration):\n860         return _build_dict_with_elements([])\n861     if inferred_values is util.Uninferable:\n862         return _build_dict_with_elements([])\n863 \n864     # Limit to a couple of potential values, as this can become pretty complicated\n865     accepted_iterable_elements = (nodes.Const,)\n866     if isinstance(inferred_values, (nodes.List, nodes.Set, nodes.Tuple)):\n867         elements = inferred_values.elts\n868         for element in elements:\n869             if not isinstance(element, accepted_iterable_elements):\n870                 # Fallback to an empty dict\n871                 return _build_dict_with_elements([])\n872 \n873         elements_with_value = [(element, default) for element in elements]\n874         return _build_dict_with_elements(elements_with_value)\n875     if isinstance(inferred_values, nodes.Const) and isinstance(\n876         inferred_values.value, (str, bytes)\n877     ):\n878         elements = [\n879             (nodes.Const(element), default) for element in inferred_values.value\n880         ]\n881         return _build_dict_with_elements(elements)\n882     if isinstance(inferred_values, nodes.Dict):\n883         keys = inferred_values.itered()\n884         for key in keys:\n885             if not isinstance(key, accepted_iterable_elements):\n886                 # Fallback to an empty dict\n887                 return _build_dict_with_elements([])\n888 \n889         elements_with_value = [(element, default) for element in keys]\n890         return _build_dict_with_elements(elements_with_value)\n891 \n892     # Fallback to an empty dictionary\n893     return _build_dict_with_elements([])\n894 \n895 \n896 def _infer_copy_method(\n897     node: nodes.Call, context: InferenceContext | None = None\n898 ) -> Iterator[nodes.NodeNG]:\n899     assert isinstance(node.func, nodes.Attribute)\n900     inferred_orig, inferred_copy = itertools.tee(node.func.expr.infer(context=context))\n901     if all(\n902         isinstance(\n903             inferred_node, (nodes.Dict, nodes.List, nodes.Set, objects.FrozenSet)\n904         )\n905         for inferred_node in inferred_orig\n906     ):\n907         return inferred_copy\n908 \n909     raise UseInferenceDefault()\n910 \n911 \n912 def _is_str_format_call(node: nodes.Call) -> bool:\n913     \"\"\"Catch calls to str.format().\"\"\"\n914     if not isinstance(node.func, nodes.Attribute) or not node.func.attrname == \"format\":\n915         return False\n916 \n917     if isinstance(node.func.expr, nodes.Name):\n918         value = helpers.safe_infer(node.func.expr)\n919     else:\n920         value = node.func.expr\n921 \n922     return isinstance(value, nodes.Const) and isinstance(value.value, str)\n923 \n924 \n925 def _infer_str_format_call(\n926     node: nodes.Call, context: InferenceContext | None = None\n927 ) -> Iterator[nodes.Const | type[util.Uninferable]]:\n928     \"\"\"Return a Const node based on the template and passed arguments.\"\"\"\n929     call = arguments.CallSite.from_call(node, context=context)\n930     if isinstance(node.func.expr, nodes.Name):\n931         value: nodes.Const = helpers.safe_infer(node.func.expr)\n932     else:\n933         value = node.func.expr\n934 \n935     format_template = value.value\n936 \n937     # Get the positional arguments passed\n938     inferred_positional = [\n939         helpers.safe_infer(i, context) for i in call.positional_arguments\n940     ]\n941     if not all(isinstance(i, nodes.Const) for i in inferred_positional):\n942         return iter([util.Uninferable])\n943     pos_values: list[str] = [i.value for i in inferred_positional]\n944 \n945     # Get the keyword arguments passed\n946     inferred_keyword = {\n947         k: helpers.safe_infer(v, context) for k, v in call.keyword_arguments.items()\n948     }\n949     if not all(isinstance(i, nodes.Const) for i in inferred_keyword.values()):\n950         return iter([util.Uninferable])\n951     keyword_values: dict[str, str] = {k: v.value for k, v in inferred_keyword.items()}\n952 \n953     try:\n954         formatted_string = format_template.format(*pos_values, **keyword_values)\n955     except (IndexError, KeyError, TypeError, ValueError):\n956         # IndexError: there are too few arguments to interpolate\n957         # TypeError: Unsupported format string\n958         # ValueError: Unknown format code\n959         return iter([util.Uninferable])\n960 \n961     return iter([nodes.const_factory(formatted_string)])\n962 \n963 \n964 # Builtins inference\n965 register_builtin_transform(infer_bool, \"bool\")\n966 register_builtin_transform(infer_super, \"super\")\n967 register_builtin_transform(infer_callable, \"callable\")\n968 register_builtin_transform(infer_property, \"property\")\n969 register_builtin_transform(infer_getattr, \"getattr\")\n970 register_builtin_transform(infer_hasattr, \"hasattr\")\n971 register_builtin_transform(infer_tuple, \"tuple\")\n972 register_builtin_transform(infer_set, \"set\")\n973 register_builtin_transform(infer_list, \"list\")\n974 register_builtin_transform(infer_dict, \"dict\")\n975 register_builtin_transform(infer_frozenset, \"frozenset\")\n976 register_builtin_transform(infer_type, \"type\")\n977 register_builtin_transform(infer_slice, \"slice\")\n978 register_builtin_transform(infer_isinstance, \"isinstance\")\n979 register_builtin_transform(infer_issubclass, \"issubclass\")\n980 register_builtin_transform(infer_len, \"len\")\n981 register_builtin_transform(infer_str, \"str\")\n982 register_builtin_transform(infer_int, \"int\")\n983 register_builtin_transform(infer_dict_fromkeys, \"dict.fromkeys\")\n984 \n985 \n986 # Infer object.__new__ calls\n987 AstroidManager().register_transform(\n988     nodes.ClassDef,\n989     inference_tip(_infer_object__new__decorator),\n990     _infer_object__new__decorator_check,\n991 )\n992 \n993 AstroidManager().register_transform(\n994     nodes.Call,\n995     inference_tip(_infer_copy_method),\n996     lambda node: isinstance(node.func, nodes.Attribute)\n997     and node.func.attrname == \"copy\",\n998 )\n999 \n1000 AstroidManager().register_transform(\n1001     nodes.Call, inference_tip(_infer_str_format_call), _is_str_format_call\n1002 )\n1003 \n[end of astroid/brain/brain_builtin_inference.py]\n[start of astroid/exceptions.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"this module contains exceptions used in the astroid library\n6 \"\"\"\n7 \n8 from __future__ import annotations\n9 \n10 from collections.abc import Sequence\n11 from typing import TYPE_CHECKING, Any\n12 \n13 from astroid import util\n14 \n15 if TYPE_CHECKING:\n16     from astroid import arguments, bases, nodes, objects\n17     from astroid.context import InferenceContext\n18 \n19 __all__ = (\n20     \"AstroidBuildingError\",\n21     \"AstroidBuildingException\",\n22     \"AstroidError\",\n23     \"AstroidImportError\",\n24     \"AstroidIndexError\",\n25     \"AstroidSyntaxError\",\n26     \"AstroidTypeError\",\n27     \"AstroidValueError\",\n28     \"AttributeInferenceError\",\n29     \"BinaryOperationError\",\n30     \"DuplicateBasesError\",\n31     \"InconsistentMroError\",\n32     \"InferenceError\",\n33     \"InferenceOverwriteError\",\n34     \"MroError\",\n35     \"NameInferenceError\",\n36     \"NoDefault\",\n37     \"NotFoundError\",\n38     \"OperationError\",\n39     \"ParentMissingError\",\n40     \"ResolveError\",\n41     \"StatementMissing\",\n42     \"SuperArgumentTypeError\",\n43     \"SuperError\",\n44     \"TooManyLevelsError\",\n45     \"UnaryOperationError\",\n46     \"UnresolvableName\",\n47     \"UseInferenceDefault\",\n48 )\n49 \n50 \n51 class AstroidError(Exception):\n52     \"\"\"base exception class for all astroid related exceptions\n53 \n54     AstroidError and its subclasses are structured, intended to hold\n55     objects representing state when the exception is thrown.  Field\n56     values are passed to the constructor as keyword-only arguments.\n57     Each subclass has its own set of standard fields, but use your\n58     best judgment to decide whether a specific exception instance\n59     needs more or fewer fields for debugging.  Field values may be\n60     used to lazily generate the error message: self.message.format()\n61     will be called with the field names and values supplied as keyword\n62     arguments.\n63     \"\"\"\n64 \n65     def __init__(self, message: str = \"\", **kws: Any) -> None:\n66         super().__init__(message)\n67         self.message = message\n68         for key, value in kws.items():\n69             setattr(self, key, value)\n70 \n71     def __str__(self) -> str:\n72         return self.message.format(**vars(self))\n73 \n74 \n75 class AstroidBuildingError(AstroidError):\n76     \"\"\"exception class when we are unable to build an astroid representation\n77 \n78     Standard attributes:\n79         modname: Name of the module that AST construction failed for.\n80         error: Exception raised during construction.\n81     \"\"\"\n82 \n83     def __init__(\n84         self,\n85         message: str = \"Failed to import module {modname}.\",\n86         modname: str | None = None,\n87         error: Exception | None = None,\n88         source: str | None = None,\n89         path: str | None = None,\n90         cls: type | None = None,\n91         class_repr: str | None = None,\n92         **kws: Any,\n93     ) -> None:\n94         self.modname = modname\n95         self.error = error\n96         self.source = source\n97         self.path = path\n98         self.cls = cls\n99         self.class_repr = class_repr\n100         super().__init__(message, **kws)\n101 \n102 \n103 class AstroidImportError(AstroidBuildingError):\n104     \"\"\"Exception class used when a module can't be imported by astroid.\"\"\"\n105 \n106 \n107 class TooManyLevelsError(AstroidImportError):\n108     \"\"\"Exception class which is raised when a relative import was beyond the top-level.\n109 \n110     Standard attributes:\n111         level: The level which was attempted.\n112         name: the name of the module on which the relative import was attempted.\n113     \"\"\"\n114 \n115     def __init__(\n116         self,\n117         message: str = \"Relative import with too many levels \"\n118         \"({level}) for module {name!r}\",\n119         level: int | None = None,\n120         name: str | None = None,\n121         **kws: Any,\n122     ) -> None:\n123         self.level = level\n124         self.name = name\n125         super().__init__(message, **kws)\n126 \n127 \n128 class AstroidSyntaxError(AstroidBuildingError):\n129     \"\"\"Exception class used when a module can't be parsed.\"\"\"\n130 \n131     def __init__(\n132         self,\n133         message: str,\n134         modname: str | None,\n135         error: Exception,\n136         path: str | None,\n137         source: str | None = None,\n138     ) -> None:\n139         super().__init__(message, modname, error, source, path)\n140 \n141 \n142 class NoDefault(AstroidError):\n143     \"\"\"raised by function's `default_value` method when an argument has\n144     no default value\n145 \n146     Standard attributes:\n147         func: Function node.\n148         name: Name of argument without a default.\n149     \"\"\"\n150 \n151     def __init__(\n152         self,\n153         message: str = \"{func!r} has no default for {name!r}.\",\n154         func: nodes.FunctionDef | None = None,\n155         name: str | None = None,\n156         **kws: Any,\n157     ) -> None:\n158         self.func = func\n159         self.name = name\n160         super().__init__(message, **kws)\n161 \n162 \n163 class ResolveError(AstroidError):\n164     \"\"\"Base class of astroid resolution/inference error.\n165 \n166     ResolveError is not intended to be raised.\n167 \n168     Standard attributes:\n169         context: InferenceContext object.\n170     \"\"\"\n171 \n172     def __init__(\n173         self, message: str = \"\", context: InferenceContext | None = None, **kws: Any\n174     ) -> None:\n175         self.context = context\n176         super().__init__(message, **kws)\n177 \n178 \n179 class MroError(ResolveError):\n180     \"\"\"Error raised when there is a problem with method resolution of a class.\n181 \n182     Standard attributes:\n183         mros: A sequence of sequences containing ClassDef nodes.\n184         cls: ClassDef node whose MRO resolution failed.\n185         context: InferenceContext object.\n186     \"\"\"\n187 \n188     def __init__(\n189         self,\n190         message: str,\n191         mros: list[nodes.ClassDef],\n192         cls: nodes.ClassDef,\n193         context: InferenceContext | None = None,\n194         **kws: Any,\n195     ) -> None:\n196         self.mros = mros\n197         self.cls = cls\n198         self.context = context\n199         super().__init__(message, **kws)\n200 \n201     def __str__(self) -> str:\n202         mro_names = \", \".join(f\"({', '.join(b.name for b in m)})\" for m in self.mros)\n203         return self.message.format(mros=mro_names, cls=self.cls)\n204 \n205 \n206 class DuplicateBasesError(MroError):\n207     \"\"\"Error raised when there are duplicate bases in the same class bases.\"\"\"\n208 \n209 \n210 class InconsistentMroError(MroError):\n211     \"\"\"Error raised when a class's MRO is inconsistent.\"\"\"\n212 \n213 \n214 class SuperError(ResolveError):\n215     \"\"\"Error raised when there is a problem with a *super* call.\n216 \n217     Standard attributes:\n218         *super_*: The Super instance that raised the exception.\n219         context: InferenceContext object.\n220     \"\"\"\n221 \n222     def __init__(self, message: str, super_: objects.Super, **kws: Any) -> None:\n223         self.super_ = super_\n224         super().__init__(message, **kws)\n225 \n226     def __str__(self) -> str:\n227         return self.message.format(**vars(self.super_))\n228 \n229 \n230 class InferenceError(ResolveError):  # pylint: disable=too-many-instance-attributes\n231     \"\"\"raised when we are unable to infer a node\n232 \n233     Standard attributes:\n234         node: The node inference was called on.\n235         context: InferenceContext object.\n236     \"\"\"\n237 \n238     def __init__(  # pylint: disable=too-many-arguments\n239         self,\n240         message: str = \"Inference failed for {node!r}.\",\n241         node: nodes.NodeNG | bases.Instance | None = None,\n242         context: InferenceContext | None = None,\n243         target: nodes.NodeNG | bases.Instance | None = None,\n244         targets: nodes.Tuple | None = None,\n245         attribute: str | None = None,\n246         unknown: nodes.NodeNG | bases.Instance | None = None,\n247         assign_path: list[int] | None = None,\n248         caller: nodes.Call | None = None,\n249         stmts: Sequence[nodes.NodeNG | bases.Instance] | None = None,\n250         frame: nodes.LocalsDictNodeNG | None = None,\n251         call_site: arguments.CallSite | None = None,\n252         func: nodes.FunctionDef | None = None,\n253         arg: str | None = None,\n254         positional_arguments: list | None = None,\n255         unpacked_args: list | None = None,\n256         keyword_arguments: dict | None = None,\n257         unpacked_kwargs: dict | None = None,\n258         **kws: Any,\n259     ) -> None:\n260         self.node = node\n261         self.context = context\n262         self.target = target\n263         self.targets = targets\n264         self.attribute = attribute\n265         self.unknown = unknown\n266         self.assign_path = assign_path\n267         self.caller = caller\n268         self.stmts = stmts\n269         self.frame = frame\n270         self.call_site = call_site\n271         self.func = func\n272         self.arg = arg\n273         self.positional_arguments = positional_arguments\n274         self.unpacked_args = unpacked_args\n275         self.keyword_arguments = keyword_arguments\n276         self.unpacked_kwargs = unpacked_kwargs\n277         super().__init__(message, **kws)\n278 \n279 \n280 # Why does this inherit from InferenceError rather than ResolveError?\n281 # Changing it causes some inference tests to fail.\n282 class NameInferenceError(InferenceError):\n283     \"\"\"Raised when a name lookup fails, corresponds to NameError.\n284 \n285     Standard attributes:\n286         name: The name for which lookup failed, as a string.\n287         scope: The node representing the scope in which the lookup occurred.\n288         context: InferenceContext object.\n289     \"\"\"\n290 \n291     def __init__(\n292         self,\n293         message: str = \"{name!r} not found in {scope!r}.\",\n294         name: str | None = None,\n295         scope: nodes.LocalsDictNodeNG | None = None,\n296         context: InferenceContext | None = None,\n297         **kws: Any,\n298     ) -> None:\n299         self.name = name\n300         self.scope = scope\n301         self.context = context\n302         super().__init__(message, **kws)\n303 \n304 \n305 class AttributeInferenceError(ResolveError):\n306     \"\"\"Raised when an attribute lookup fails, corresponds to AttributeError.\n307 \n308     Standard attributes:\n309         target: The node for which lookup failed.\n310         attribute: The attribute for which lookup failed, as a string.\n311         context: InferenceContext object.\n312     \"\"\"\n313 \n314     def __init__(\n315         self,\n316         message: str = \"{attribute!r} not found on {target!r}.\",\n317         attribute: str = \"\",\n318         target: nodes.NodeNG | bases.Instance | None = None,\n319         context: InferenceContext | None = None,\n320         mros: list[nodes.ClassDef] | None = None,\n321         super_: nodes.ClassDef | None = None,\n322         cls: nodes.ClassDef | None = None,\n323         **kws: Any,\n324     ) -> None:\n325         self.attribute = attribute\n326         self.target = target\n327         self.context = context\n328         self.mros = mros\n329         self.super_ = super_\n330         self.cls = cls\n331         super().__init__(message, **kws)\n332 \n333 \n334 class UseInferenceDefault(Exception):\n335     \"\"\"exception to be raised in custom inference function to indicate that it\n336     should go back to the default behaviour\n337     \"\"\"\n338 \n339 \n340 class _NonDeducibleTypeHierarchy(Exception):\n341     \"\"\"Raised when is_subtype / is_supertype can't deduce the relation between two types.\"\"\"\n342 \n343 \n344 class AstroidIndexError(AstroidError):\n345     \"\"\"Raised when an Indexable / Mapping does not have an index / key.\"\"\"\n346 \n347     def __init__(\n348         self,\n349         message: str = \"\",\n350         node: nodes.NodeNG | bases.Instance | None = None,\n351         index: nodes.Subscript | None = None,\n352         context: InferenceContext | None = None,\n353         **kws: Any,\n354     ) -> None:\n355         self.node = node\n356         self.index = index\n357         self.context = context\n358         super().__init__(message, **kws)\n359 \n360 \n361 class AstroidTypeError(AstroidError):\n362     \"\"\"Raised when a TypeError would be expected in Python code.\"\"\"\n363 \n364     def __init__(\n365         self,\n366         message: str = \"\",\n367         node: nodes.NodeNG | bases.Instance | None = None,\n368         index: nodes.Subscript | None = None,\n369         context: InferenceContext | None = None,\n370         **kws: Any,\n371     ) -> None:\n372         self.node = node\n373         self.index = index\n374         self.context = context\n375         super().__init__(message, **kws)\n376 \n377 \n378 class AstroidValueError(AstroidError):\n379     \"\"\"Raised when a ValueError would be expected in Python code.\"\"\"\n380 \n381 \n382 class InferenceOverwriteError(AstroidError):\n383     \"\"\"Raised when an inference tip is overwritten\n384 \n385     Currently only used for debugging.\n386     \"\"\"\n387 \n388 \n389 class ParentMissingError(AstroidError):\n390     \"\"\"Raised when a node which is expected to have a parent attribute is missing one\n391 \n392     Standard attributes:\n393         target: The node for which the parent lookup failed.\n394     \"\"\"\n395 \n396     def __init__(self, target: nodes.NodeNG) -> None:\n397         self.target = target\n398         super().__init__(message=f\"Parent not found on {target!r}.\")\n399 \n400 \n401 class StatementMissing(ParentMissingError):\n402     \"\"\"Raised when a call to node.statement() does not return a node. This is because\n403     a node in the chain does not have a parent attribute and therefore does not\n404     return a node for statement().\n405 \n406     Standard attributes:\n407         target: The node for which the parent lookup failed.\n408     \"\"\"\n409 \n410     def __init__(self, target: nodes.NodeNG) -> None:\n411         super(ParentMissingError, self).__init__(\n412             message=f\"Statement not found on {target!r}\"\n413         )\n414 \n415 \n416 # Backwards-compatibility aliases\n417 OperationError = util.BadOperationMessage\n418 UnaryOperationError = util.BadUnaryOperationMessage\n419 BinaryOperationError = util.BadBinaryOperationMessage\n420 \n421 SuperArgumentTypeError = SuperError\n422 UnresolvableName = NameInferenceError\n423 NotFoundError = AttributeInferenceError\n424 AstroidBuildingException = AstroidBuildingError\n425 \n[end of astroid/exceptions.py]\n[start of astroid/helpers.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"\n6 Various helper utilities.\n7 \"\"\"\n8 \n9 from __future__ import annotations\n10 \n11 from collections.abc import Generator\n12 \n13 from astroid import bases, manager, nodes, raw_building, util\n14 from astroid.context import CallContext, InferenceContext\n15 from astroid.exceptions import (\n16     AstroidTypeError,\n17     AttributeInferenceError,\n18     InferenceError,\n19     MroError,\n20     _NonDeducibleTypeHierarchy,\n21 )\n22 from astroid.nodes import scoped_nodes\n23 from astroid.typing import InferenceResult, SuccessfulInferenceResult\n24 \n25 \n26 def _build_proxy_class(cls_name: str, builtins: nodes.Module) -> nodes.ClassDef:\n27     proxy = raw_building.build_class(cls_name)\n28     proxy.parent = builtins\n29     return proxy\n30 \n31 \n32 def _function_type(\n33     function: nodes.Lambda | bases.UnboundMethod, builtins: nodes.Module\n34 ) -> nodes.ClassDef:\n35     if isinstance(function, scoped_nodes.Lambda):\n36         if function.root().name == \"builtins\":\n37             cls_name = \"builtin_function_or_method\"\n38         else:\n39             cls_name = \"function\"\n40     elif isinstance(function, bases.BoundMethod):\n41         cls_name = \"method\"\n42     else:\n43         cls_name = \"function\"\n44     return _build_proxy_class(cls_name, builtins)\n45 \n46 \n47 def _object_type(\n48     node: SuccessfulInferenceResult, context: InferenceContext | None = None\n49 ) -> Generator[InferenceResult | None, None, None]:\n50     astroid_manager = manager.AstroidManager()\n51     builtins = astroid_manager.builtins_module\n52     context = context or InferenceContext()\n53 \n54     for inferred in node.infer(context=context):\n55         if isinstance(inferred, scoped_nodes.ClassDef):\n56             if inferred.newstyle:\n57                 metaclass = inferred.metaclass(context=context)\n58                 if metaclass:\n59                     yield metaclass\n60                     continue\n61             yield builtins.getattr(\"type\")[0]\n62         elif isinstance(inferred, (scoped_nodes.Lambda, bases.UnboundMethod)):\n63             yield _function_type(inferred, builtins)\n64         elif isinstance(inferred, scoped_nodes.Module):\n65             yield _build_proxy_class(\"module\", builtins)\n66         elif isinstance(inferred, nodes.Unknown):\n67             raise InferenceError\n68         elif inferred is util.Uninferable:\n69             yield inferred\n70         elif isinstance(inferred, (bases.Proxy, nodes.Slice)):\n71             yield inferred._proxied\n72         else:  # pragma: no cover\n73             raise AssertionError(f\"We don't handle {type(inferred)} currently\")\n74 \n75 \n76 def object_type(\n77     node: SuccessfulInferenceResult, context: InferenceContext | None = None\n78 ) -> InferenceResult | None:\n79     \"\"\"Obtain the type of the given node\n80 \n81     This is used to implement the ``type`` builtin, which means that it's\n82     used for inferring type calls, as well as used in a couple of other places\n83     in the inference.\n84     The node will be inferred first, so this function can support all\n85     sorts of objects, as long as they support inference.\n86     \"\"\"\n87 \n88     try:\n89         types = set(_object_type(node, context))\n90     except InferenceError:\n91         return util.Uninferable\n92     if len(types) > 1 or not types:\n93         return util.Uninferable\n94     return list(types)[0]\n95 \n96 \n97 def _object_type_is_subclass(\n98     obj_type, class_or_seq, context: InferenceContext | None = None\n99 ):\n100     if not isinstance(class_or_seq, (tuple, list)):\n101         class_seq = (class_or_seq,)\n102     else:\n103         class_seq = class_or_seq\n104 \n105     if obj_type is util.Uninferable:\n106         return util.Uninferable\n107 \n108     # Instances are not types\n109     class_seq = [\n110         item if not isinstance(item, bases.Instance) else util.Uninferable\n111         for item in class_seq\n112     ]\n113     # strict compatibility with issubclass\n114     # issubclass(type, (object, 1)) evaluates to true\n115     # issubclass(object, (1, type)) raises TypeError\n116     for klass in class_seq:\n117         if klass is util.Uninferable:\n118             raise AstroidTypeError(\"arg 2 must be a type or tuple of types\")\n119 \n120         for obj_subclass in obj_type.mro():\n121             if obj_subclass == klass:\n122                 return True\n123     return False\n124 \n125 \n126 def object_isinstance(node, class_or_seq, context: InferenceContext | None = None):\n127     \"\"\"Check if a node 'isinstance' any node in class_or_seq\n128 \n129     :param node: A given node\n130     :param class_or_seq: Union[nodes.NodeNG, Sequence[nodes.NodeNG]]\n131     :rtype: bool\n132 \n133     :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n134     \"\"\"\n135     obj_type = object_type(node, context)\n136     if obj_type is util.Uninferable:\n137         return util.Uninferable\n138     return _object_type_is_subclass(obj_type, class_or_seq, context=context)\n139 \n140 \n141 def object_issubclass(node, class_or_seq, context: InferenceContext | None = None):\n142     \"\"\"Check if a type is a subclass of any node in class_or_seq\n143 \n144     :param node: A given node\n145     :param class_or_seq: Union[Nodes.NodeNG, Sequence[nodes.NodeNG]]\n146     :rtype: bool\n147 \n148     :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n149     :raises AstroidError: if the type of the given node cannot be inferred\n150         or its type's mro doesn't work\n151     \"\"\"\n152     if not isinstance(node, nodes.ClassDef):\n153         raise TypeError(f\"{node} needs to be a ClassDef node\")\n154     return _object_type_is_subclass(node, class_or_seq, context=context)\n155 \n156 \n157 def safe_infer(\n158     node: nodes.NodeNG | bases.Proxy, context: InferenceContext | None = None\n159 ) -> InferenceResult | None:\n160     \"\"\"Return the inferred value for the given node.\n161 \n162     Return None if inference failed or if there is some ambiguity (more than\n163     one node has been inferred).\n164     \"\"\"\n165     try:\n166         inferit = node.infer(context=context)\n167         value = next(inferit)\n168     except (InferenceError, StopIteration):\n169         return None\n170     try:\n171         next(inferit)\n172         return None  # None if there is ambiguity on the inferred node\n173     except InferenceError:\n174         return None  # there is some kind of ambiguity\n175     except StopIteration:\n176         return value\n177 \n178 \n179 def has_known_bases(klass, context: InferenceContext | None = None) -> bool:\n180     \"\"\"Return whether all base classes of a class could be inferred.\"\"\"\n181     try:\n182         return klass._all_bases_known\n183     except AttributeError:\n184         pass\n185     for base in klass.bases:\n186         result = safe_infer(base, context=context)\n187         # TODO: check for A->B->A->B pattern in class structure too?\n188         if (\n189             not isinstance(result, scoped_nodes.ClassDef)\n190             or result is klass\n191             or not has_known_bases(result, context=context)\n192         ):\n193             klass._all_bases_known = False\n194             return False\n195     klass._all_bases_known = True\n196     return True\n197 \n198 \n199 def _type_check(type1, type2) -> bool:\n200     if not all(map(has_known_bases, (type1, type2))):\n201         raise _NonDeducibleTypeHierarchy\n202 \n203     if not all([type1.newstyle, type2.newstyle]):\n204         return False\n205     try:\n206         return type1 in type2.mro()[:-1]\n207     except MroError as e:\n208         # The MRO is invalid.\n209         raise _NonDeducibleTypeHierarchy from e\n210 \n211 \n212 def is_subtype(type1, type2) -> bool:\n213     \"\"\"Check if *type1* is a subtype of *type2*.\"\"\"\n214     return _type_check(type1=type2, type2=type1)\n215 \n216 \n217 def is_supertype(type1, type2) -> bool:\n218     \"\"\"Check if *type2* is a supertype of *type1*.\"\"\"\n219     return _type_check(type1, type2)\n220 \n221 \n222 def class_instance_as_index(node: SuccessfulInferenceResult) -> nodes.Const | None:\n223     \"\"\"Get the value as an index for the given instance.\n224 \n225     If an instance provides an __index__ method, then it can\n226     be used in some scenarios where an integer is expected,\n227     for instance when multiplying or subscripting a list.\n228     \"\"\"\n229     context = InferenceContext()\n230     try:\n231         for inferred in node.igetattr(\"__index__\", context=context):\n232             if not isinstance(inferred, bases.BoundMethod):\n233                 continue\n234 \n235             context.boundnode = node\n236             context.callcontext = CallContext(args=[], callee=inferred)\n237             for result in inferred.infer_call_result(node, context=context):\n238                 if isinstance(result, nodes.Const) and isinstance(result.value, int):\n239                     return result\n240     except InferenceError:\n241         pass\n242     return None\n243 \n244 \n245 def object_len(node, context: InferenceContext | None = None):\n246     \"\"\"Infer length of given node object\n247 \n248     :param Union[nodes.ClassDef, nodes.Instance] node:\n249     :param node: Node to infer length of\n250 \n251     :raises AstroidTypeError: If an invalid node is returned\n252         from __len__ method or no __len__ method exists\n253     :raises InferenceError: If the given node cannot be inferred\n254         or if multiple nodes are inferred or if the code executed in python\n255         would result in a infinite recursive check for length\n256     :rtype int: Integer length of node\n257     \"\"\"\n258     # pylint: disable=import-outside-toplevel; circular import\n259     from astroid.objects import FrozenSet\n260 \n261     inferred_node = safe_infer(node, context=context)\n262 \n263     # prevent self referential length calls from causing a recursion error\n264     # see https://github.com/PyCQA/astroid/issues/777\n265     node_frame = node.frame(future=True)\n266     if (\n267         isinstance(node_frame, scoped_nodes.FunctionDef)\n268         and node_frame.name == \"__len__\"\n269         and hasattr(inferred_node, \"_proxied\")\n270         and inferred_node._proxied == node_frame.parent\n271     ):\n272         message = (\n273             \"Self referential __len__ function will \"\n274             \"cause a RecursionError on line {} of {}\".format(\n275                 node.lineno, node.root().file\n276             )\n277         )\n278         raise InferenceError(message)\n279 \n280     if inferred_node is None or inferred_node is util.Uninferable:\n281         raise InferenceError(node=node)\n282     if isinstance(inferred_node, nodes.Const) and isinstance(\n283         inferred_node.value, (bytes, str)\n284     ):\n285         return len(inferred_node.value)\n286     if isinstance(inferred_node, (nodes.List, nodes.Set, nodes.Tuple, FrozenSet)):\n287         return len(inferred_node.elts)\n288     if isinstance(inferred_node, nodes.Dict):\n289         return len(inferred_node.items)\n290 \n291     node_type = object_type(inferred_node, context=context)\n292     if not node_type:\n293         raise InferenceError(node=node)\n294 \n295     try:\n296         len_call = next(node_type.igetattr(\"__len__\", context=context))\n297     except StopIteration as e:\n298         raise AstroidTypeError(str(e)) from e\n299     except AttributeInferenceError as e:\n300         raise AstroidTypeError(\n301             f\"object of type '{node_type.pytype()}' has no len()\"\n302         ) from e\n303 \n304     inferred = len_call.infer_call_result(node, context)\n305     if inferred is util.Uninferable:\n306         raise InferenceError(node=node, context=context)\n307     result_of_len = next(inferred, None)\n308     if (\n309         isinstance(result_of_len, nodes.Const)\n310         and result_of_len.pytype() == \"builtins.int\"\n311     ):\n312         return result_of_len.value\n313     if (\n314         result_of_len is None\n315         or isinstance(result_of_len, bases.Instance)\n316         and result_of_len.is_subtype_of(\"builtins.int\")\n317     ):\n318         # Fake a result as we don't know the arguments of the instance call.\n319         return 0\n320     raise AstroidTypeError(\n321         f\"'{result_of_len}' object cannot be interpreted as an integer\"\n322     )\n323 \n[end of astroid/helpers.py]\n[start of astroid/nodes/node_ng.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import pprint\n8 import sys\n9 import warnings\n10 from collections.abc import Generator, Iterator\n11 from functools import singledispatch as _singledispatch\n12 from typing import (\n13     TYPE_CHECKING,\n14     Any,\n15     ClassVar,\n16     Tuple,\n17     Type,\n18     TypeVar,\n19     Union,\n20     cast,\n21     overload,\n22 )\n23 \n24 from astroid import decorators, util\n25 from astroid.context import InferenceContext\n26 from astroid.exceptions import (\n27     AstroidError,\n28     InferenceError,\n29     ParentMissingError,\n30     StatementMissing,\n31     UseInferenceDefault,\n32 )\n33 from astroid.manager import AstroidManager\n34 from astroid.nodes.as_string import AsStringVisitor\n35 from astroid.nodes.const import OP_PRECEDENCE\n36 from astroid.nodes.utils import Position\n37 from astroid.typing import InferenceErrorInfo, InferenceResult, InferFn\n38 \n39 if TYPE_CHECKING:\n40     from astroid import nodes\n41 \n42 if sys.version_info >= (3, 8):\n43     from typing import Literal\n44 else:\n45     from typing_extensions import Literal\n46 \n47 if sys.version_info >= (3, 8):\n48     from functools import cached_property\n49 else:\n50     from astroid.decorators import cachedproperty as cached_property\n51 \n52 # Types for 'NodeNG.nodes_of_class()'\n53 _NodesT = TypeVar(\"_NodesT\", bound=\"NodeNG\")\n54 _NodesT2 = TypeVar(\"_NodesT2\", bound=\"NodeNG\")\n55 _NodesT3 = TypeVar(\"_NodesT3\", bound=\"NodeNG\")\n56 SkipKlassT = Union[None, Type[\"NodeNG\"], Tuple[Type[\"NodeNG\"], ...]]\n57 \n58 \n59 class NodeNG:\n60     \"\"\"A node of the new Abstract Syntax Tree (AST).\n61 \n62     This is the base class for all Astroid node classes.\n63     \"\"\"\n64 \n65     is_statement: ClassVar[bool] = False\n66     \"\"\"Whether this node indicates a statement.\"\"\"\n67     optional_assign: ClassVar[\n68         bool\n69     ] = False  # True for For (and for Comprehension if py <3.0)\n70     \"\"\"Whether this node optionally assigns a variable.\n71 \n72     This is for loop assignments because loop won't necessarily perform an\n73     assignment if the loop has no iterations.\n74     This is also the case from comprehensions in Python 2.\n75     \"\"\"\n76     is_function: ClassVar[bool] = False  # True for FunctionDef nodes\n77     \"\"\"Whether this node indicates a function.\"\"\"\n78     is_lambda: ClassVar[bool] = False\n79 \n80     # Attributes below are set by the builder module or by raw factories\n81     _astroid_fields: ClassVar[tuple[str, ...]] = ()\n82     \"\"\"Node attributes that contain child nodes.\n83 \n84     This is redefined in most concrete classes.\n85     \"\"\"\n86     _other_fields: ClassVar[tuple[str, ...]] = ()\n87     \"\"\"Node attributes that do not contain child nodes.\"\"\"\n88     _other_other_fields: ClassVar[tuple[str, ...]] = ()\n89     \"\"\"Attributes that contain AST-dependent fields.\"\"\"\n90     # instance specific inference function infer(node, context)\n91     _explicit_inference: InferFn | None = None\n92 \n93     def __init__(\n94         self,\n95         lineno: int | None = None,\n96         col_offset: int | None = None,\n97         parent: NodeNG | None = None,\n98         *,\n99         end_lineno: int | None = None,\n100         end_col_offset: int | None = None,\n101     ) -> None:\n102         \"\"\"\n103         :param lineno: The line that this node appears on in the source code.\n104 \n105         :param col_offset: The column that this node appears on in the\n106             source code.\n107 \n108         :param parent: The parent node in the syntax tree.\n109 \n110         :param end_lineno: The last line this node appears on in the source code.\n111 \n112         :param end_col_offset: The end column this node appears on in the\n113             source code. Note: This is after the last symbol.\n114         \"\"\"\n115         self.lineno: int | None = lineno\n116         \"\"\"The line that this node appears on in the source code.\"\"\"\n117 \n118         self.col_offset: int | None = col_offset\n119         \"\"\"The column that this node appears on in the source code.\"\"\"\n120 \n121         self.parent: NodeNG | None = parent\n122         \"\"\"The parent node in the syntax tree.\"\"\"\n123 \n124         self.end_lineno: int | None = end_lineno\n125         \"\"\"The last line this node appears on in the source code.\"\"\"\n126 \n127         self.end_col_offset: int | None = end_col_offset\n128         \"\"\"The end column this node appears on in the source code.\n129         Note: This is after the last symbol.\n130         \"\"\"\n131 \n132         self.position: Position | None = None\n133         \"\"\"Position of keyword(s) and name. Used as fallback for block nodes\n134         which might not provide good enough positional information.\n135         E.g. ClassDef, FunctionDef.\n136         \"\"\"\n137 \n138     def infer(\n139         self, context: InferenceContext | None = None, **kwargs: Any\n140     ) -> Generator[InferenceResult, None, None]:\n141         \"\"\"Get a generator of the inferred values.\n142 \n143         This is the main entry point to the inference system.\n144 \n145         .. seealso:: :ref:`inference`\n146 \n147         If the instance has some explicit inference function set, it will be\n148         called instead of the default interface.\n149 \n150         :returns: The inferred values.\n151         :rtype: iterable\n152         \"\"\"\n153         if context is not None:\n154             context = context.extra_context.get(self, context)\n155         if self._explicit_inference is not None:\n156             # explicit_inference is not bound, give it self explicitly\n157             try:\n158                 # pylint: disable=not-callable\n159                 results = list(self._explicit_inference(self, context, **kwargs))\n160                 if context is not None:\n161                     context.nodes_inferred += len(results)\n162                 yield from results\n163                 return\n164             except UseInferenceDefault:\n165                 pass\n166 \n167         if not context:\n168             # nodes_inferred?\n169             yield from self._infer(context=context, **kwargs)\n170             return\n171 \n172         key = (self, context.lookupname, context.callcontext, context.boundnode)\n173         if key in context.inferred:\n174             yield from context.inferred[key]\n175             return\n176 \n177         results = []\n178 \n179         # Limit inference amount to help with performance issues with\n180         # exponentially exploding possible results.\n181         limit = AstroidManager.max_inferable_values\n182         for i, result in enumerate(self._infer(context=context, **kwargs)):\n183             if i >= limit or (context.nodes_inferred > context.max_inferred):\n184                 results.append(util.Uninferable)\n185                 yield util.Uninferable\n186                 break\n187             results.append(result)\n188             yield result\n189             context.nodes_inferred += 1\n190 \n191         # Cache generated results for subsequent inferences of the\n192         # same node using the same context\n193         context.inferred[key] = tuple(results)\n194         return\n195 \n196     def _repr_name(self) -> str:\n197         \"\"\"Get a name for nice representation.\n198 \n199         This is either :attr:`name`, :attr:`attrname`, or the empty string.\n200 \n201         :returns: The nice name.\n202         :rtype: str\n203         \"\"\"\n204         if all(name not in self._astroid_fields for name in (\"name\", \"attrname\")):\n205             return getattr(self, \"name\", \"\") or getattr(self, \"attrname\", \"\")\n206         return \"\"\n207 \n208     def __str__(self) -> str:\n209         rname = self._repr_name()\n210         cname = type(self).__name__\n211         if rname:\n212             string = \"%(cname)s.%(rname)s(%(fields)s)\"\n213             alignment = len(cname) + len(rname) + 2\n214         else:\n215             string = \"%(cname)s(%(fields)s)\"\n216             alignment = len(cname) + 1\n217         result = []\n218         for field in self._other_fields + self._astroid_fields:\n219             value = getattr(self, field)\n220             width = 80 - len(field) - alignment\n221             lines = pprint.pformat(value, indent=2, width=width).splitlines(True)\n222 \n223             inner = [lines[0]]\n224             for line in lines[1:]:\n225                 inner.append(\" \" * alignment + line)\n226             result.append(f\"{field}={''.join(inner)}\")\n227 \n228         return string % {\n229             \"cname\": cname,\n230             \"rname\": rname,\n231             \"fields\": (\",\\n\" + \" \" * alignment).join(result),\n232         }\n233 \n234     def __repr__(self) -> str:\n235         rname = self._repr_name()\n236         if rname:\n237             string = \"<%(cname)s.%(rname)s l.%(lineno)s at 0x%(id)x>\"\n238         else:\n239             string = \"<%(cname)s l.%(lineno)s at 0x%(id)x>\"\n240         return string % {\n241             \"cname\": type(self).__name__,\n242             \"rname\": rname,\n243             \"lineno\": self.fromlineno,\n244             \"id\": id(self),\n245         }\n246 \n247     def accept(self, visitor):\n248         \"\"\"Visit this node using the given visitor.\"\"\"\n249         func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\n250         return func(self)\n251 \n252     def get_children(self) -> Iterator[NodeNG]:\n253         \"\"\"Get the child nodes below this node.\"\"\"\n254         for field in self._astroid_fields:\n255             attr = getattr(self, field)\n256             if attr is None:\n257                 continue\n258             if isinstance(attr, (list, tuple)):\n259                 yield from attr\n260             else:\n261                 yield attr\n262         yield from ()\n263 \n264     def last_child(self) -> NodeNG | None:\n265         \"\"\"An optimized version of list(get_children())[-1]\"\"\"\n266         for field in self._astroid_fields[::-1]:\n267             attr = getattr(self, field)\n268             if not attr:  # None or empty list / tuple\n269                 continue\n270             if isinstance(attr, (list, tuple)):\n271                 return attr[-1]\n272             return attr\n273         return None\n274 \n275     def node_ancestors(self) -> Iterator[NodeNG]:\n276         \"\"\"Yield parent, grandparent, etc until there are no more.\"\"\"\n277         parent = self.parent\n278         while parent is not None:\n279             yield parent\n280             parent = parent.parent\n281 \n282     def parent_of(self, node) -> bool:\n283         \"\"\"Check if this node is the parent of the given node.\n284 \n285         :param node: The node to check if it is the child.\n286         :type node: NodeNG\n287 \n288         :returns: Whether this node is the parent of the given node.\n289         \"\"\"\n290         return any(self is parent for parent in node.node_ancestors())\n291 \n292     @overload\n293     def statement(self, *, future: None = ...) -> nodes.Statement | nodes.Module:\n294         ...\n295 \n296     @overload\n297     def statement(self, *, future: Literal[True]) -> nodes.Statement:\n298         ...\n299 \n300     def statement(\n301         self, *, future: Literal[None, True] = None\n302     ) -> nodes.Statement | nodes.Module:\n303         \"\"\"The first parent node, including self, marked as statement node.\n304 \n305         TODO: Deprecate the future parameter and only raise StatementMissing and return\n306         nodes.Statement\n307 \n308         :raises AttributeError: If self has no parent attribute\n309         :raises StatementMissing: If self has no parent attribute and future is True\n310         \"\"\"\n311         if self.is_statement:\n312             return cast(\"nodes.Statement\", self)\n313         if not self.parent:\n314             if future:\n315                 raise StatementMissing(target=self)\n316             warnings.warn(\n317                 \"In astroid 3.0.0 NodeNG.statement() will return either a nodes.Statement \"\n318                 \"or raise a StatementMissing exception. AttributeError will no longer be raised. \"\n319                 \"This behaviour can already be triggered \"\n320                 \"by passing 'future=True' to a statement() call.\",\n321                 DeprecationWarning,\n322             )\n323             raise AttributeError(f\"{self} object has no attribute 'parent'\")\n324         return self.parent.statement(future=future)\n325 \n326     def frame(\n327         self, *, future: Literal[None, True] = None\n328     ) -> nodes.FunctionDef | nodes.Module | nodes.ClassDef | nodes.Lambda:\n329         \"\"\"The first parent frame node.\n330 \n331         A frame node is a :class:`Module`, :class:`FunctionDef`,\n332         :class:`ClassDef` or :class:`Lambda`.\n333 \n334         :returns: The first parent frame node.\n335         \"\"\"\n336         if self.parent is None:\n337             if future:\n338                 raise ParentMissingError(target=self)\n339             warnings.warn(\n340                 \"In astroid 3.0.0 NodeNG.frame() will return either a Frame node, \"\n341                 \"or raise ParentMissingError. AttributeError will no longer be raised. \"\n342                 \"This behaviour can already be triggered \"\n343                 \"by passing 'future=True' to a frame() call.\",\n344                 DeprecationWarning,\n345             )\n346             raise AttributeError(f\"{self} object has no attribute 'parent'\")\n347 \n348         return self.parent.frame(future=future)\n349 \n350     def scope(self) -> nodes.LocalsDictNodeNG:\n351         \"\"\"The first parent node defining a new scope.\n352         These can be Module, FunctionDef, ClassDef, Lambda, or GeneratorExp nodes.\n353 \n354         :returns: The first parent scope node.\n355         \"\"\"\n356         if not self.parent:\n357             raise ParentMissingError(target=self)\n358         return self.parent.scope()\n359 \n360     def root(self) -> nodes.Module:\n361         \"\"\"Return the root node of the syntax tree.\n362 \n363         :returns: The root node.\n364         \"\"\"\n365         if self.parent:\n366             return self.parent.root()\n367         return self  # type: ignore[return-value] # Only 'Module' does not have a parent node.\n368 \n369     def child_sequence(self, child):\n370         \"\"\"Search for the sequence that contains this child.\n371 \n372         :param child: The child node to search sequences for.\n373         :type child: NodeNG\n374 \n375         :returns: The sequence containing the given child node.\n376         :rtype: iterable(NodeNG)\n377 \n378         :raises AstroidError: If no sequence could be found that contains\n379             the given child.\n380         \"\"\"\n381         for field in self._astroid_fields:\n382             node_or_sequence = getattr(self, field)\n383             if node_or_sequence is child:\n384                 return [node_or_sequence]\n385             # /!\\ compiler.ast Nodes have an __iter__ walking over child nodes\n386             if (\n387                 isinstance(node_or_sequence, (tuple, list))\n388                 and child in node_or_sequence\n389             ):\n390                 return node_or_sequence\n391 \n392         msg = \"Could not find %s in %s's children\"\n393         raise AstroidError(msg % (repr(child), repr(self)))\n394 \n395     def locate_child(self, child):\n396         \"\"\"Find the field of this node that contains the given child.\n397 \n398         :param child: The child node to search fields for.\n399         :type child: NodeNG\n400 \n401         :returns: A tuple of the name of the field that contains the child,\n402             and the sequence or node that contains the child node.\n403         :rtype: tuple(str, iterable(NodeNG) or NodeNG)\n404 \n405         :raises AstroidError: If no field could be found that contains\n406             the given child.\n407         \"\"\"\n408         for field in self._astroid_fields:\n409             node_or_sequence = getattr(self, field)\n410             # /!\\ compiler.ast Nodes have an __iter__ walking over child nodes\n411             if child is node_or_sequence:\n412                 return field, child\n413             if (\n414                 isinstance(node_or_sequence, (tuple, list))\n415                 and child in node_or_sequence\n416             ):\n417                 return field, node_or_sequence\n418         msg = \"Could not find %s in %s's children\"\n419         raise AstroidError(msg % (repr(child), repr(self)))\n420 \n421     # FIXME : should we merge child_sequence and locate_child ? locate_child\n422     # is only used in are_exclusive, child_sequence one time in pylint.\n423 \n424     def next_sibling(self):\n425         \"\"\"The next sibling statement node.\n426 \n427         :returns: The next sibling statement node.\n428         :rtype: NodeNG or None\n429         \"\"\"\n430         return self.parent.next_sibling()\n431 \n432     def previous_sibling(self):\n433         \"\"\"The previous sibling statement.\n434 \n435         :returns: The previous sibling statement node.\n436         :rtype: NodeNG or None\n437         \"\"\"\n438         return self.parent.previous_sibling()\n439 \n440     # these are lazy because they're relatively expensive to compute for every\n441     # single node, and they rarely get looked at\n442 \n443     @cached_property\n444     def fromlineno(self) -> int | None:\n445         \"\"\"The first line that this node appears on in the source code.\"\"\"\n446         if self.lineno is None:\n447             return self._fixed_source_line()\n448         return self.lineno\n449 \n450     @cached_property\n451     def tolineno(self) -> int | None:\n452         \"\"\"The last line that this node appears on in the source code.\"\"\"\n453         if self.end_lineno is not None:\n454             return self.end_lineno\n455         if not self._astroid_fields:\n456             # can't have children\n457             last_child = None\n458         else:\n459             last_child = self.last_child()\n460         if last_child is None:\n461             return self.fromlineno\n462         return last_child.tolineno\n463 \n464     def _fixed_source_line(self) -> int | None:\n465         \"\"\"Attempt to find the line that this node appears on.\n466 \n467         We need this method since not all nodes have :attr:`lineno` set.\n468         \"\"\"\n469         line = self.lineno\n470         _node = self\n471         try:\n472             while line is None:\n473                 _node = next(_node.get_children())\n474                 line = _node.lineno\n475         except StopIteration:\n476             parent = self.parent\n477             while parent and line is None:\n478                 line = parent.lineno\n479                 parent = parent.parent\n480         return line\n481 \n482     def block_range(self, lineno):\n483         \"\"\"Get a range from the given line number to where this node ends.\n484 \n485         :param lineno: The line number to start the range at.\n486         :type lineno: int\n487 \n488         :returns: The range of line numbers that this node belongs to,\n489             starting at the given line number.\n490         :rtype: tuple(int, int or None)\n491         \"\"\"\n492         return lineno, self.tolineno\n493 \n494     def set_local(self, name: str, stmt: NodeNG) -> None:\n495         \"\"\"Define that the given name is declared in the given statement node.\n496 \n497         This definition is stored on the parent scope node.\n498 \n499         .. seealso:: :meth:`scope`\n500 \n501         :param name: The name that is being defined.\n502 \n503         :param stmt: The statement that defines the given name.\n504         \"\"\"\n505         assert self.parent\n506         self.parent.set_local(name, stmt)\n507 \n508     @overload\n509     def nodes_of_class(\n510         self,\n511         klass: type[_NodesT],\n512         skip_klass: SkipKlassT = ...,\n513     ) -> Iterator[_NodesT]:\n514         ...\n515 \n516     @overload\n517     def nodes_of_class(\n518         self,\n519         klass: tuple[type[_NodesT], type[_NodesT2]],\n520         skip_klass: SkipKlassT = ...,\n521     ) -> Iterator[_NodesT] | Iterator[_NodesT2]:\n522         ...\n523 \n524     @overload\n525     def nodes_of_class(\n526         self,\n527         klass: tuple[type[_NodesT], type[_NodesT2], type[_NodesT3]],\n528         skip_klass: SkipKlassT = ...,\n529     ) -> Iterator[_NodesT] | Iterator[_NodesT2] | Iterator[_NodesT3]:\n530         ...\n531 \n532     @overload\n533     def nodes_of_class(\n534         self,\n535         klass: tuple[type[_NodesT], ...],\n536         skip_klass: SkipKlassT = ...,\n537     ) -> Iterator[_NodesT]:\n538         ...\n539 \n540     def nodes_of_class(  # type: ignore[misc] # mypy doesn't correctly recognize the overloads\n541         self,\n542         klass: (\n543             type[_NodesT]\n544             | tuple[type[_NodesT], type[_NodesT2]]\n545             | tuple[type[_NodesT], type[_NodesT2], type[_NodesT3]]\n546             | tuple[type[_NodesT], ...]\n547         ),\n548         skip_klass: SkipKlassT = None,\n549     ) -> Iterator[_NodesT] | Iterator[_NodesT2] | Iterator[_NodesT3]:\n550         \"\"\"Get the nodes (including this one or below) of the given types.\n551 \n552         :param klass: The types of node to search for.\n553 \n554         :param skip_klass: The types of node to ignore. This is useful to ignore\n555             subclasses of :attr:`klass`.\n556 \n557         :returns: The node of the given types.\n558         \"\"\"\n559         if isinstance(self, klass):\n560             yield self\n561 \n562         if skip_klass is None:\n563             for child_node in self.get_children():\n564                 yield from child_node.nodes_of_class(klass, skip_klass)\n565 \n566             return\n567 \n568         for child_node in self.get_children():\n569             if isinstance(child_node, skip_klass):\n570                 continue\n571             yield from child_node.nodes_of_class(klass, skip_klass)\n572 \n573     @decorators.cached\n574     def _get_assign_nodes(self):\n575         return []\n576 \n577     def _get_name_nodes(self):\n578         for child_node in self.get_children():\n579             yield from child_node._get_name_nodes()\n580 \n581     def _get_return_nodes_skip_functions(self):\n582         yield from ()\n583 \n584     def _get_yield_nodes_skip_lambdas(self):\n585         yield from ()\n586 \n587     def _infer_name(self, frame, name):\n588         # overridden for ImportFrom, Import, Global, TryExcept and Arguments\n589         pass\n590 \n591     def _infer(\n592         self, context: InferenceContext | None = None, **kwargs: Any\n593     ) -> Generator[InferenceResult, None, InferenceErrorInfo | None]:\n594         \"\"\"we don't know how to resolve a statement by default\"\"\"\n595         # this method is overridden by most concrete classes\n596         raise InferenceError(\n597             \"No inference function for {node!r}.\", node=self, context=context\n598         )\n599 \n600     def inferred(self):\n601         \"\"\"Get a list of the inferred values.\n602 \n603         .. seealso:: :ref:`inference`\n604 \n605         :returns: The inferred values.\n606         :rtype: list\n607         \"\"\"\n608         return list(self.infer())\n609 \n610     def instantiate_class(self):\n611         \"\"\"Instantiate an instance of the defined class.\n612 \n613         .. note::\n614 \n615             On anything other than a :class:`ClassDef` this will return self.\n616 \n617         :returns: An instance of the defined class.\n618         :rtype: object\n619         \"\"\"\n620         return self\n621 \n622     def has_base(self, node) -> bool:\n623         \"\"\"Check if this node inherits from the given type.\n624 \n625         :param node: The node defining the base to look for.\n626             Usually this is a :class:`Name` node.\n627         :type node: NodeNG\n628         \"\"\"\n629         return False\n630 \n631     def callable(self) -> bool:\n632         \"\"\"Whether this node defines something that is callable.\n633 \n634         :returns: Whether this defines something that is callable.\n635         \"\"\"\n636         return False\n637 \n638     def eq(self, value) -> bool:\n639         return False\n640 \n641     def as_string(self) -> str:\n642         \"\"\"Get the source code that this node represents.\"\"\"\n643         return AsStringVisitor()(self)\n644 \n645     def repr_tree(\n646         self,\n647         ids=False,\n648         include_linenos=False,\n649         ast_state=False,\n650         indent=\"   \",\n651         max_depth=0,\n652         max_width=80,\n653     ) -> str:\n654         \"\"\"Get a string representation of the AST from this node.\n655 \n656         :param ids: If true, includes the ids with the node type names.\n657         :type ids: bool\n658 \n659         :param include_linenos: If true, includes the line numbers and\n660             column offsets.\n661         :type include_linenos: bool\n662 \n663         :param ast_state: If true, includes information derived from\n664             the whole AST like local and global variables.\n665         :type ast_state: bool\n666 \n667         :param indent: A string to use to indent the output string.\n668         :type indent: str\n669 \n670         :param max_depth: If set to a positive integer, won't return\n671             nodes deeper than max_depth in the string.\n672         :type max_depth: int\n673 \n674         :param max_width: Attempt to format the output string to stay\n675             within this number of characters, but can exceed it under some\n676             circumstances. Only positive integer values are valid, the default is 80.\n677         :type max_width: int\n678 \n679         :returns: The string representation of the AST.\n680         :rtype: str\n681         \"\"\"\n682 \n683         @_singledispatch\n684         def _repr_tree(node, result, done, cur_indent=\"\", depth=1):\n685             \"\"\"Outputs a representation of a non-tuple/list, non-node that's\n686             contained within an AST, including strings.\n687             \"\"\"\n688             lines = pprint.pformat(\n689                 node, width=max(max_width - len(cur_indent), 1)\n690             ).splitlines(True)\n691             result.append(lines[0])\n692             result.extend([cur_indent + line for line in lines[1:]])\n693             return len(lines) != 1\n694 \n695         # pylint: disable=unused-variable,useless-suppression; doesn't understand singledispatch\n696         @_repr_tree.register(tuple)\n697         @_repr_tree.register(list)\n698         def _repr_seq(node, result, done, cur_indent=\"\", depth=1):\n699             \"\"\"Outputs a representation of a sequence that's contained within an AST.\"\"\"\n700             cur_indent += indent\n701             result.append(\"[\")\n702             if not node:\n703                 broken = False\n704             elif len(node) == 1:\n705                 broken = _repr_tree(node[0], result, done, cur_indent, depth)\n706             elif len(node) == 2:\n707                 broken = _repr_tree(node[0], result, done, cur_indent, depth)\n708                 if not broken:\n709                     result.append(\", \")\n710                 else:\n711                     result.append(\",\\n\")\n712                     result.append(cur_indent)\n713                 broken = _repr_tree(node[1], result, done, cur_indent, depth) or broken\n714             else:\n715                 result.append(\"\\n\")\n716                 result.append(cur_indent)\n717                 for child in node[:-1]:\n718                     _repr_tree(child, result, done, cur_indent, depth)\n719                     result.append(\",\\n\")\n720                     result.append(cur_indent)\n721                 _repr_tree(node[-1], result, done, cur_indent, depth)\n722                 broken = True\n723             result.append(\"]\")\n724             return broken\n725 \n726         # pylint: disable=unused-variable,useless-suppression; doesn't understand singledispatch\n727         @_repr_tree.register(NodeNG)\n728         def _repr_node(node, result, done, cur_indent=\"\", depth=1):\n729             \"\"\"Outputs a strings representation of an astroid node.\"\"\"\n730             if node in done:\n731                 result.append(\n732                     indent + f\"<Recursion on {type(node).__name__} with id={id(node)}\"\n733                 )\n734                 return False\n735             done.add(node)\n736 \n737             if max_depth and depth > max_depth:\n738                 result.append(\"...\")\n739                 return False\n740             depth += 1\n741             cur_indent += indent\n742             if ids:\n743                 result.append(f\"{type(node).__name__}<0x{id(node):x}>(\\n\")\n744             else:\n745                 result.append(f\"{type(node).__name__}(\")\n746             fields = []\n747             if include_linenos:\n748                 fields.extend((\"lineno\", \"col_offset\"))\n749             fields.extend(node._other_fields)\n750             fields.extend(node._astroid_fields)\n751             if ast_state:\n752                 fields.extend(node._other_other_fields)\n753             if not fields:\n754                 broken = False\n755             elif len(fields) == 1:\n756                 result.append(f\"{fields[0]}=\")\n757                 broken = _repr_tree(\n758                     getattr(node, fields[0]), result, done, cur_indent, depth\n759                 )\n760             else:\n761                 result.append(\"\\n\")\n762                 result.append(cur_indent)\n763                 for field in fields[:-1]:\n764                     # TODO: Remove this after removal of the 'doc' attribute\n765                     if field == \"doc\":\n766                         continue\n767                     result.append(f\"{field}=\")\n768                     _repr_tree(getattr(node, field), result, done, cur_indent, depth)\n769                     result.append(\",\\n\")\n770                     result.append(cur_indent)\n771                 result.append(f\"{fields[-1]}=\")\n772                 _repr_tree(getattr(node, fields[-1]), result, done, cur_indent, depth)\n773                 broken = True\n774             result.append(\")\")\n775             return broken\n776 \n777         result: list[str] = []\n778         _repr_tree(self, result, set())\n779         return \"\".join(result)\n780 \n781     def bool_value(self, context: InferenceContext | None = None):\n782         \"\"\"Determine the boolean value of this node.\n783 \n784         The boolean value of a node can have three\n785         possible values:\n786 \n787             * False: For instance, empty data structures,\n788               False, empty strings, instances which return\n789               explicitly False from the __nonzero__ / __bool__\n790               method.\n791             * True: Most of constructs are True by default:\n792               classes, functions, modules etc\n793             * Uninferable: The inference engine is uncertain of the\n794               node's value.\n795 \n796         :returns: The boolean value of this node.\n797         :rtype: bool or Uninferable\n798         \"\"\"\n799         return util.Uninferable\n800 \n801     def op_precedence(self):\n802         # Look up by class name or default to highest precedence\n803         return OP_PRECEDENCE.get(self.__class__.__name__, len(OP_PRECEDENCE))\n804 \n805     def op_left_associative(self) -> Literal[True]:\n806         # Everything is left associative except `**` and IfExp\n807         return True\n808 \n[end of astroid/nodes/node_ng.py]\n[start of doc/conf.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 #\n6 # Astroid documentation build configuration file, created by\n7 # sphinx-quickstart on Wed Jun 26 15:00:40 2013.\n8 #\n9 # This file is execfile()d with the current directory set to its containing dir.\n10 #\n11 # Note that not all possible configuration values are present in this\n12 # autogenerated file.\n13 #\n14 # All configuration values have a default; values that are commented out\n15 # serve to show the default.\n16 \n17 import os\n18 import sys\n19 from datetime import datetime\n20 \n21 # If extensions (or modules to document with autodoc) are in another directory,\n22 # add these directories to sys.path here. If the directory is relative to the\n23 # documentation root, use os.path.abspath to make it absolute, like shown here.\n24 sys.path.insert(0, os.path.abspath(\"../../\"))\n25 \n26 # -- General configuration -----------------------------------------------------\n27 \n28 # If your documentation needs a minimal Sphinx version, state it here.\n29 # needs_sphinx = '1.0'\n30 \n31 # Add any Sphinx extension module names here, as strings. They can be extensions\n32 # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n33 extensions = [\n34     \"sphinx.ext.autodoc\",\n35     \"sphinx.ext.autosummary\",\n36     \"sphinx.ext.doctest\",\n37     \"sphinx.ext.intersphinx\",\n38     \"sphinx.ext.todo\",\n39     \"sphinx.ext.viewcode\",\n40     \"sphinx.ext.napoleon\",\n41 ]\n42 \n43 # Add any paths that contain templates here, relative to this directory.\n44 templates_path = [\"_templates\"]\n45 \n46 # The suffix of source filenames.\n47 source_suffix = \".rst\"\n48 \n49 # The encoding of source files.\n50 # source_encoding = 'utf-8-sig'\n51 \n52 # The master toctree document.\n53 master_doc = \"index\"\n54 \n55 # General information about the project.\n56 project = \"Astroid\"\n57 current_year = datetime.utcnow().year\n58 copyright = f\"2003-{current_year}, Logilab, PyCQA and contributors\"\n59 \n60 # The version info for the project you're documenting, acts as replacement for\n61 # |version| and |release|, also used in various other places throughout the\n62 # built documents.\n63 #\n64 # The short X.Y version.\n65 from astroid.__pkginfo__ import __version__\n66 \n67 # The full version, including alpha/beta/rc tags.\n68 release = __version__\n69 \n70 # The language for content autogenerated by Sphinx. Refer to documentation\n71 # for a list of supported languages.\n72 # language = None\n73 \n74 # There are two options for replacing |today|: either, you set today to some\n75 # non-false value, then it is used:\n76 # today = ''\n77 # Else, today_fmt is used as the format for a strftime call.\n78 # today_fmt = '%B %d, %Y'\n79 \n80 # List of patterns, relative to source directory, that match files and\n81 # directories to ignore when looking for source files.\n82 exclude_patterns = [\"_build\"]\n83 \n84 # The reST default role (used for this markup: `text`) to use for all documents.\n85 # default_role = None\n86 \n87 # If true, '()' will be appended to :func: etc. cross-reference text.\n88 add_function_parentheses = True\n89 \n90 # If true, the current module name will be prepended to all description\n91 # unit titles (such as .. function::).\n92 # add_module_names = True\n93 \n94 # If true, sectionauthor and moduleauthor directives will be shown in the\n95 # output. They are ignored by default.\n96 # show_authors = False\n97 \n98 # The name of the Pygments (syntax highlighting) style to use.\n99 pygments_style = \"sphinx\"\n100 \n101 # A list of ignored prefixes for module index sorting.\n102 # modindex_common_prefix = []\n103 \n104 \n105 # -- Customization --\n106 \n107 primary_domain = \"py\"\n108 todo_include_todos = True\n109 \n110 # -- Options for HTML output ---------------------------------------------------\n111 \n112 # The theme to use for HTML and HTML Help pages.  See the documentation for\n113 # a list of builtin themes.\n114 html_theme = \"nature\"\n115 \n116 # Theme options are theme-specific and customize the look and feel of a theme\n117 # further.  For a list of options available for each theme, see the\n118 # documentation.\n119 # html_theme_options = {}\n120 \n121 # Add any paths that contain custom themes here, relative to this directory.\n122 # html_theme_path = []\n123 \n124 # The name for this set of Sphinx documents.  If None, it defaults to\n125 # \"<project> v<release> documentation\".\n126 # html_title = None\n127 \n128 # A shorter title for the navigation bar.  Default is the same as html_title.\n129 # html_short_title = None\n130 \n131 # The name of an image file (relative to this directory) to place at the top\n132 # of the sidebar.\n133 # html_logo = None\n134 \n135 # The name of an image file (within the static path) to use as favicon of the\n136 # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n137 # pixels large.\n138 # html_favicon = None\n139 \n140 # Add any paths that contain custom static files (such as style sheets) here,\n141 # relative to this directory. They are copied after the builtin static files,\n142 # so a file named \"default.css\" will overwrite the builtin \"default.css\".\n143 html_static_path = [\"media\"]\n144 \n145 # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n146 # using the given strftime format.\n147 # html_last_updated_fmt = '%b %d, %Y'\n148 \n149 # If true, SmartyPants will be used to convert quotes and dashes to\n150 # typographically correct entities.\n151 # html_use_smartypants = True\n152 \n153 # Custom sidebar templates, maps document names to template names.\n154 # html_sidebars = {}\n155 \n156 # Additional templates that should be rendered to pages, maps page names to\n157 # template names.\n158 # html_additional_pages = {}\n159 \n160 # If false, no module index is generated.\n161 # html_domain_indices = True\n162 \n163 # If false, no index is generated.\n164 # html_use_index = True\n165 \n166 # If true, the index is split into individual pages for each letter.\n167 # html_split_index = False\n168 \n169 # If true, links to the reST sources are added to the pages.\n170 # html_show_sourcelink = True\n171 \n172 # If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n173 # html_show_sphinx = True\n174 \n175 # If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n176 # html_show_copyright = True\n177 \n178 # If true, an OpenSearch description file will be output, and all pages will\n179 # contain a <link> tag referring to it.  The value of this option must be the\n180 # base URL from which the finished HTML is served.\n181 # html_use_opensearch = ''\n182 \n183 # This is the file name suffix for HTML files (e.g. \".xhtml\").\n184 # html_file_suffix = None\n185 \n186 # Output file base name for HTML help builder.\n187 htmlhelp_basename = \"Pylintdoc\"\n188 \n189 \n190 # -- Options for LaTeX output --------------------------------------------------\n191 \n192 # The paper size ('letter' or 'a4').\n193 # latex_paper_size = 'letter'\n194 \n195 # The font size ('10pt', '11pt' or '12pt').\n196 # latex_font_size = '10pt'\n197 \n198 # Grouping the document tree into LaTeX files. List of tuples\n199 # (source start file, target name, title, author, documentclass [howto/manual]).\n200 latex_documents = [\n201     (\n202         \"index\",\n203         \"Astroid.tex\",\n204         \"Astroid Documentation\",\n205         \"Logilab, PyCQA and contributors\",\n206         \"manual\",\n207     ),\n208 ]\n209 \n210 # The name of an image file (relative to this directory) to place at the top of\n211 # the title page.\n212 # latex_logo = None\n213 \n214 # For \"manual\" documents, if this is true, then toplevel headings are parts,\n215 # not chapters.\n216 # latex_use_parts = False\n217 \n218 # If true, show page references after internal links.\n219 # latex_show_pagerefs = False\n220 \n221 # If true, show URL addresses after external links.\n222 # latex_show_urls = False\n223 \n224 # Additional stuff for the LaTeX preamble.\n225 # latex_preamble = ''\n226 \n227 # Documents to append as an appendix to all manuals.\n228 # latex_appendices = []\n229 \n230 # If false, no module index is generated.\n231 # latex_domain_indices = True\n232 \n233 \n234 # -- Options for manual page output --------------------------------------------\n235 \n236 # One entry per manual page. List of tuples\n237 # (source start file, name, description, authors, manual section).\n238 man_pages = [\n239     (\n240         \"index\",\n241         \"astroid\",\n242         \"Astroid Documentation\",\n243         [\"Logilab, PyCQA and contributors\"],\n244         1,\n245     )\n246 ]\n247 \n248 autodoc_default_options = {\n249     \"members\": True,\n250     \"undoc-members\": True,\n251     \"show-inheritance\": True,\n252 }\n253 autoclass_content = \"both\"\n254 autodoc_member_order = \"groupwise\"\n255 autodoc_typehints = \"description\"\n256 intersphinx_mapping = {\n257     \"green_tree_snakes\": (\n258         \"http://greentreesnakes.readthedocs.io/en/latest/\",\n259         \"ast_objects.inv\",\n260     ),\n261 }\n262 \n[end of doc/conf.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/astroid/brain/brain_builtin_inference.py\n+++ b/astroid/brain/brain_builtin_inference.py\n@@ -955,6 +955,14 @@\n         return iter([util.Uninferable])\n \n     try:\n+        # Check for invalid attribute access in format strings\n+        for pos_value in pos_values:\n+            if not hasattr(pos_value, 'name'):\n+                raise AstroidTypeError(\n+                    f\"Invalid attribute access in format string: {pos_value} has no 'name' attribute.\"\n+                )\n+\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n     except (IndexError, KeyError, TypeError, ValueError):\n         # IndexError: there are too few arguments to interpolate\n```", "model_patch": "--- a/astroid/brain/brain_builtin_inference.py\n+++ b/astroid/brain/brain_builtin_inference.py\n@@ -955,6 +955,14 @@\n         return iter([util.Uninferable])\n \n     try:\n+        # Check for invalid attribute access in format strings\n+        for pos_value in pos_values:\n+            if not hasattr(pos_value, 'name'):\n+                raise AstroidTypeError(\n+                    f\"Invalid attribute access in format string: {pos_value} has no 'name' attribute.\"\n+                )\n+\n         formatted_string = format_template.format(*pos_values, **keyword_values)\n     except (IndexError, KeyError, TypeError, ValueError):\n         # IndexError: there are too few arguments to interpolate\n"}
{"instance_id": "pylint-dev__astroid-2309", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nRegression in Astroid version 2.15.7 in handling subscriptable type parameters\nAstroid version 2.15.7 fails to correctly handle a subscriptable type parameter  most likely due to the change in this [PR](https://github.com/pylint-dev/astroid/pull/2239). \r\n\r\n### Steps to reproduce\r\n\r\n```python\r\nfrom collections.abc import Mapping\r\nfrom typing import Generic, TypeVar, TypedDict\r\nfrom dataclasses import dataclass\r\n\r\nclass Identity(TypedDict):\r\n    \"\"\"It's the identity.\"\"\"\r\n\r\n    name: str\r\n\r\nT = TypeVar(\"T\", bound=Mapping)\r\n\r\n@dataclass\r\nclass Animal(Generic[T]):\r\n    \"\"\"It's an animal.\"\"\"\r\n\r\n    identity: T\r\n\r\nclass Dog(Animal[Identity]):\r\n    \"\"\"It's a Dog.\"\"\"\r\n\r\ndog = Dog(identity=Identity(name=\"Dog\"))\r\nprint(dog.identity[\"name\"])\r\n```\r\n\r\n### Current behavior\r\nPylint (running Astroid 2.15.7) gives the following error for the example above:\r\n```\r\nE1136: Value 'dog.identity' is unsubscriptable (unsubscriptable-object)\r\n```\r\n### Expected behavior\r\nAstroid should correctly handle a subscriptable type parameter.\r\n\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.15.7\n\n</issue>\n<code>\n[start of README.rst]\n1 Astroid\n2 =======\n3 \n4 .. image:: https://codecov.io/gh/PyCQA/astroid/branch/main/graph/badge.svg?token=Buxy4WptLb\n5     :target: https://codecov.io/gh/PyCQA/astroid\n6     :alt: Coverage badge from codecov\n7 \n8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest\n9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest\n10     :alt: Documentation Status\n11 \n12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n13     :target: https://github.com/ambv/black\n14 \n15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg\n16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main\n17    :alt: pre-commit.ci status\n18 \n19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png\n20    :width: 200\n21    :alt: Tidelift\n22 \n23 .. list-table::\n24    :widths: 10 100\n25 \n26    * - |tidelift_logo|\n27      - Professional support for astroid is available as part of the\n28        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for\n29        purchasing and maintaining their software, with professional grade assurances\n30        from the experts who know it best, while seamlessly integrating with existing\n31        tools.\n32 \n33 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme\n34 \n35 \n36 \n37 What's this?\n38 ------------\n39 \n40 The aim of this module is to provide a common base representation of\n41 python source code. It is currently the library powering pylint's capabilities.\n42 \n43 It provides a compatible representation which comes from the `_ast`\n44 module.  It rebuilds the tree generated by the builtin _ast module by\n45 recursively walking down the AST and building an extended ast. The new\n46 node classes have additional methods and attributes for different\n47 usages. They include some support for static inference and local name\n48 scopes. Furthermore, astroid can also build partial trees by inspecting living\n49 objects.\n50 \n51 \n52 Installation\n53 ------------\n54 \n55 Extract the tarball, jump into the created directory and run::\n56 \n57     pip install .\n58 \n59 \n60 If you want to do an editable installation, you can run::\n61 \n62     pip install -e .\n63 \n64 \n65 If you have any questions, please mail the code-quality@python.org\n66 mailing list for support. See\n67 http://mail.python.org/mailman/listinfo/code-quality for subscription\n68 information and archives.\n69 \n70 Documentation\n71 -------------\n72 http://astroid.readthedocs.io/en/latest/\n73 \n74 \n75 Python Versions\n76 ---------------\n77 \n78 astroid 2.0 is currently available for Python 3 only. If you want Python 2\n79 support, use an older version of astroid (though note that these versions\n80 are no longer supported).\n81 \n82 Test\n83 ----\n84 \n85 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use\n86 either `tox` or `pytest`::\n87 \n88     tox\n89     pytest\n90 \n[end of README.rst]\n[start of astroid/__init__.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Python Abstract Syntax Tree New Generation.\n6 \n7 The aim of this module is to provide a common base representation of\n8 python source code for projects such as pychecker, pyreverse,\n9 pylint... Well, actually the development of this library is essentially\n10 governed by pylint's needs.\n11 \n12 It mimics the class defined in the python's _ast module with some\n13 additional methods and attributes. New nodes instances are not fully\n14 compatible with python's _ast.\n15 \n16 Instance attributes are added by a\n17 builder object, which can either generate extended ast (let's call\n18 them astroid ;) by visiting an existent ast tree or by inspecting living\n19 object. Methods are added by monkey patching ast classes.\n20 \n21 Main modules are:\n22 \n23 * nodes and scoped_nodes for more information about methods and\n24   attributes added to different node classes\n25 \n26 * the manager contains a high level object to get astroid trees from\n27   source files and living objects. It maintains a cache of previously\n28   constructed tree for quick access\n29 \n30 * builder contains the class responsible to build astroid trees\n31 \"\"\"\n32 \n33 import functools\n34 import tokenize\n35 from importlib import import_module\n36 \n37 # isort: off\n38 # We have an isort: off on '__version__' because the packaging need to access\n39 # the version before the dependencies are installed (in particular 'wrapt'\n40 # that is imported in astroid.inference)\n41 from astroid.__pkginfo__ import __version__, version\n42 from astroid.nodes import node_classes, scoped_nodes\n43 \n44 # isort: on\n45 \n46 from astroid import inference, raw_building\n47 from astroid.astroid_manager import MANAGER\n48 from astroid.bases import BaseInstance, BoundMethod, Instance, UnboundMethod\n49 from astroid.brain.helpers import register_module_extender\n50 from astroid.builder import extract_node, parse\n51 from astroid.const import BRAIN_MODULES_DIRECTORY, PY310_PLUS, Context, Del, Load, Store\n52 from astroid.exceptions import (\n53     AstroidBuildingError,\n54     AstroidBuildingException,\n55     AstroidError,\n56     AstroidImportError,\n57     AstroidIndexError,\n58     AstroidSyntaxError,\n59     AstroidTypeError,\n60     AstroidValueError,\n61     AttributeInferenceError,\n62     BinaryOperationError,\n63     DuplicateBasesError,\n64     InconsistentMroError,\n65     InferenceError,\n66     InferenceOverwriteError,\n67     MroError,\n68     NameInferenceError,\n69     NoDefault,\n70     NotFoundError,\n71     OperationError,\n72     ParentMissingError,\n73     ResolveError,\n74     StatementMissing,\n75     SuperArgumentTypeError,\n76     SuperError,\n77     TooManyLevelsError,\n78     UnaryOperationError,\n79     UnresolvableName,\n80     UseInferenceDefault,\n81 )\n82 from astroid.inference_tip import _inference_tip_cached, inference_tip\n83 from astroid.objects import ExceptionInstance\n84 \n85 # isort: off\n86 # It's impossible to import from astroid.nodes with a wildcard, because\n87 # there is a cyclic import that prevent creating an __all__ in astroid/nodes\n88 # and we need astroid/scoped_nodes and astroid/node_classes to work. So\n89 # importing with a wildcard would clash with astroid/nodes/scoped_nodes\n90 # and astroid/nodes/node_classes.\n91 from astroid.nodes import (  # pylint: disable=redefined-builtin (Ellipsis)\n92     CONST_CLS,\n93     AnnAssign,\n94     Arguments,\n95     Assert,\n96     Assign,\n97     AssignAttr,\n98     AssignName,\n99     AsyncFor,\n100     AsyncFunctionDef,\n101     AsyncWith,\n102     Attribute,\n103     AugAssign,\n104     Await,\n105     BinOp,\n106     BoolOp,\n107     Break,\n108     Call,\n109     ClassDef,\n110     Compare,\n111     Comprehension,\n112     ComprehensionScope,\n113     Const,\n114     Continue,\n115     Decorators,\n116     DelAttr,\n117     Delete,\n118     DelName,\n119     Dict,\n120     DictComp,\n121     DictUnpack,\n122     Ellipsis,\n123     EmptyNode,\n124     EvaluatedObject,\n125     ExceptHandler,\n126     Expr,\n127     ExtSlice,\n128     For,\n129     FormattedValue,\n130     FunctionDef,\n131     GeneratorExp,\n132     Global,\n133     If,\n134     IfExp,\n135     Import,\n136     ImportFrom,\n137     Index,\n138     JoinedStr,\n139     Keyword,\n140     Lambda,\n141     List,\n142     ListComp,\n143     Match,\n144     MatchAs,\n145     MatchCase,\n146     MatchClass,\n147     MatchMapping,\n148     MatchOr,\n149     MatchSequence,\n150     MatchSingleton,\n151     MatchStar,\n152     MatchValue,\n153     Module,\n154     Name,\n155     NamedExpr,\n156     NodeNG,\n157     Nonlocal,\n158     Pass,\n159     Raise,\n160     Return,\n161     Set,\n162     SetComp,\n163     Slice,\n164     Starred,\n165     Subscript,\n166     TryExcept,\n167     TryFinally,\n168     TryStar,\n169     Tuple,\n170     UnaryOp,\n171     Unknown,\n172     While,\n173     With,\n174     Yield,\n175     YieldFrom,\n176     are_exclusive,\n177     builtin_lookup,\n178     unpack_infer,\n179     function_to_method,\n180 )\n181 \n182 # isort: on\n183 \n184 from astroid.util import Uninferable\n185 \n186 # Performance hack for tokenize. See https://bugs.python.org/issue43014\n187 # Adapted from https://github.com/PyCQA/pycodestyle/pull/993\n188 if (\n189     not PY310_PLUS\n190     and callable(getattr(tokenize, \"_compile\", None))\n191     and getattr(tokenize._compile, \"__wrapped__\", None) is None  # type: ignore[attr-defined]\n192 ):\n193     tokenize._compile = functools.lru_cache()(tokenize._compile)  # type: ignore[attr-defined]\n194 \n195 # load brain plugins\n196 for module in BRAIN_MODULES_DIRECTORY.iterdir():\n197     if module.suffix == \".py\":\n198         import_module(f\"astroid.brain.{module.stem}\")\n199 \n[end of astroid/__init__.py]\n[start of astroid/brain/brain_dataclasses.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"\n6 Astroid hook for the dataclasses library.\n7 \n8 Support built-in dataclasses, pydantic.dataclasses, and marshmallow_dataclass-annotated\n9 dataclasses. References:\n10 - https://docs.python.org/3/library/dataclasses.html\n11 - https://pydantic-docs.helpmanual.io/usage/dataclasses/\n12 - https://lovasoa.github.io/marshmallow_dataclass/\n13 \"\"\"\n14 \n15 from __future__ import annotations\n16 \n17 import sys\n18 from collections.abc import Iterator\n19 from typing import Tuple, Union\n20 \n21 from astroid import bases, context, helpers, nodes\n22 from astroid.builder import parse\n23 from astroid.const import PY39_PLUS, PY310_PLUS\n24 from astroid.exceptions import AstroidSyntaxError, InferenceError, UseInferenceDefault\n25 from astroid.inference_tip import inference_tip\n26 from astroid.manager import AstroidManager\n27 from astroid.typing import InferenceResult\n28 from astroid.util import Uninferable, UninferableBase\n29 \n30 if sys.version_info >= (3, 8):\n31     from typing import Literal\n32 else:\n33     from typing_extensions import Literal\n34 \n35 _FieldDefaultReturn = Union[\n36     None,\n37     Tuple[Literal[\"default\"], nodes.NodeNG],\n38     Tuple[Literal[\"default_factory\"], nodes.Call],\n39 ]\n40 \n41 DATACLASSES_DECORATORS = frozenset((\"dataclass\",))\n42 FIELD_NAME = \"field\"\n43 DATACLASS_MODULES = frozenset(\n44     (\"dataclasses\", \"marshmallow_dataclass\", \"pydantic.dataclasses\")\n45 )\n46 DEFAULT_FACTORY = \"_HAS_DEFAULT_FACTORY\"  # based on typing.py\n47 \n48 \n49 def is_decorated_with_dataclass(\n50     node: nodes.ClassDef, decorator_names: frozenset[str] = DATACLASSES_DECORATORS\n51 ) -> bool:\n52     \"\"\"Return True if a decorated node has a `dataclass` decorator applied.\"\"\"\n53     if not isinstance(node, nodes.ClassDef) or not node.decorators:\n54         return False\n55 \n56     return any(\n57         _looks_like_dataclass_decorator(decorator_attribute, decorator_names)\n58         for decorator_attribute in node.decorators.nodes\n59     )\n60 \n61 \n62 def dataclass_transform(node: nodes.ClassDef) -> None:\n63     \"\"\"Rewrite a dataclass to be easily understood by pylint.\"\"\"\n64     node.is_dataclass = True\n65 \n66     for assign_node in _get_dataclass_attributes(node):\n67         name = assign_node.target.name\n68 \n69         rhs_node = nodes.Unknown(\n70             lineno=assign_node.lineno,\n71             col_offset=assign_node.col_offset,\n72             parent=assign_node,\n73         )\n74         rhs_node = AstroidManager().visit_transforms(rhs_node)\n75         node.instance_attrs[name] = [rhs_node]\n76 \n77     if not _check_generate_dataclass_init(node):\n78         return\n79 \n80     kw_only_decorated = False\n81     if PY310_PLUS and node.decorators.nodes:\n82         for decorator in node.decorators.nodes:\n83             if not isinstance(decorator, nodes.Call):\n84                 kw_only_decorated = False\n85                 break\n86             for keyword in decorator.keywords:\n87                 if keyword.arg == \"kw_only\":\n88                     kw_only_decorated = keyword.value.bool_value()\n89 \n90     init_str = _generate_dataclass_init(\n91         node,\n92         list(_get_dataclass_attributes(node, init=True)),\n93         kw_only_decorated,\n94     )\n95 \n96     try:\n97         init_node = parse(init_str)[\"__init__\"]\n98     except AstroidSyntaxError:\n99         pass\n100     else:\n101         init_node.parent = node\n102         init_node.lineno, init_node.col_offset = None, None\n103         node.locals[\"__init__\"] = [init_node]\n104 \n105         root = node.root()\n106         if DEFAULT_FACTORY not in root.locals:\n107             new_assign = parse(f\"{DEFAULT_FACTORY} = object()\").body[0]\n108             new_assign.parent = root\n109             root.locals[DEFAULT_FACTORY] = [new_assign.targets[0]]\n110 \n111 \n112 def _get_dataclass_attributes(\n113     node: nodes.ClassDef, init: bool = False\n114 ) -> Iterator[nodes.AnnAssign]:\n115     \"\"\"Yield the AnnAssign nodes of dataclass attributes for the node.\n116 \n117     If init is True, also include InitVars.\n118     \"\"\"\n119     for assign_node in node.body:\n120         if not isinstance(assign_node, nodes.AnnAssign) or not isinstance(\n121             assign_node.target, nodes.AssignName\n122         ):\n123             continue\n124 \n125         # Annotation is never None\n126         if _is_class_var(assign_node.annotation):  # type: ignore[arg-type]\n127             continue\n128 \n129         if _is_keyword_only_sentinel(assign_node.annotation):\n130             continue\n131 \n132         # Annotation is never None\n133         if not init and _is_init_var(assign_node.annotation):  # type: ignore[arg-type]\n134             continue\n135 \n136         yield assign_node\n137 \n138 \n139 def _check_generate_dataclass_init(node: nodes.ClassDef) -> bool:\n140     \"\"\"Return True if we should generate an __init__ method for node.\n141 \n142     This is True when:\n143         - node doesn't define its own __init__ method\n144         - the dataclass decorator was called *without* the keyword argument init=False\n145     \"\"\"\n146     if \"__init__\" in node.locals:\n147         return False\n148 \n149     found = None\n150 \n151     for decorator_attribute in node.decorators.nodes:\n152         if not isinstance(decorator_attribute, nodes.Call):\n153             continue\n154 \n155         if _looks_like_dataclass_decorator(decorator_attribute):\n156             found = decorator_attribute\n157 \n158     if found is None:\n159         return True\n160 \n161     # Check for keyword arguments of the form init=False\n162     return not any(\n163         keyword.arg == \"init\"\n164         and not keyword.value.bool_value()  # type: ignore[union-attr] # value is never None\n165         for keyword in found.keywords\n166     )\n167 \n168 \n169 def _find_arguments_from_base_classes(\n170     node: nodes.ClassDef,\n171 ) -> tuple[\n172     dict[str, tuple[str | None, str | None]], dict[str, tuple[str | None, str | None]]\n173 ]:\n174     \"\"\"Iterate through all bases and get their typing and defaults.\"\"\"\n175     pos_only_store: dict[str, tuple[str | None, str | None]] = {}\n176     kw_only_store: dict[str, tuple[str | None, str | None]] = {}\n177     # See TODO down below\n178     # all_have_defaults = True\n179 \n180     for base in reversed(node.mro()):\n181         if not base.is_dataclass:\n182             continue\n183         try:\n184             base_init: nodes.FunctionDef = base.locals[\"__init__\"][0]\n185         except KeyError:\n186             continue\n187 \n188         pos_only, kw_only = base_init.args._get_arguments_data()\n189         for posarg, data in pos_only.items():\n190             # if data[1] is None:\n191             #     if all_have_defaults and pos_only_store:\n192             #         # TODO: This should return an Uninferable as this would raise\n193             #         # a TypeError at runtime. However, transforms can't return\n194             #         # Uninferables currently.\n195             #         pass\n196             #     all_have_defaults = False\n197             pos_only_store[posarg] = data\n198 \n199         for kwarg, data in kw_only.items():\n200             kw_only_store[kwarg] = data\n201     return pos_only_store, kw_only_store\n202 \n203 \n204 def _parse_arguments_into_strings(\n205     pos_only_store: dict[str, tuple[str | None, str | None]],\n206     kw_only_store: dict[str, tuple[str | None, str | None]],\n207 ) -> tuple[str, str]:\n208     \"\"\"Parse positional and keyword arguments into strings for an __init__ method.\"\"\"\n209     pos_only, kw_only = \"\", \"\"\n210     for pos_arg, data in pos_only_store.items():\n211         pos_only += pos_arg\n212         if data[0]:\n213             pos_only += \": \" + data[0]\n214         if data[1]:\n215             pos_only += \" = \" + data[1]\n216         pos_only += \", \"\n217     for kw_arg, data in kw_only_store.items():\n218         kw_only += kw_arg\n219         if data[0]:\n220             kw_only += \": \" + data[0]\n221         if data[1]:\n222             kw_only += \" = \" + data[1]\n223         kw_only += \", \"\n224 \n225     return pos_only, kw_only\n226 \n227 \n228 def _get_previous_field_default(node: nodes.ClassDef, name: str) -> nodes.NodeNG | None:\n229     \"\"\"Get the default value of a previously defined field.\"\"\"\n230     for base in reversed(node.mro()):\n231         if not base.is_dataclass:\n232             continue\n233         if name in base.locals:\n234             for assign in base.locals[name]:\n235                 if (\n236                     isinstance(assign.parent, nodes.AnnAssign)\n237                     and assign.parent.value\n238                     and isinstance(assign.parent.value, nodes.Call)\n239                     and _looks_like_dataclass_field_call(assign.parent.value)\n240                 ):\n241                     default = _get_field_default(assign.parent.value)\n242                     if default:\n243                         return default[1]\n244     return None\n245 \n246 \n247 def _generate_dataclass_init(  # pylint: disable=too-many-locals\n248     node: nodes.ClassDef, assigns: list[nodes.AnnAssign], kw_only_decorated: bool\n249 ) -> str:\n250     \"\"\"Return an init method for a dataclass given the targets.\"\"\"\n251     params: list[str] = []\n252     kw_only_params: list[str] = []\n253     assignments: list[str] = []\n254 \n255     prev_pos_only_store, prev_kw_only_store = _find_arguments_from_base_classes(node)\n256 \n257     for assign in assigns:\n258         name, annotation, value = assign.target.name, assign.annotation, assign.value\n259 \n260         # Check whether this assign is overriden by a property assignment\n261         property_node: nodes.FunctionDef | None = None\n262         for additional_assign in node.locals[name]:\n263             if not isinstance(additional_assign, nodes.FunctionDef):\n264                 continue\n265             if not additional_assign.decorators:\n266                 continue\n267             if \"builtins.property\" in additional_assign.decoratornames():\n268                 property_node = additional_assign\n269                 break\n270 \n271         is_field = isinstance(value, nodes.Call) and _looks_like_dataclass_field_call(\n272             value, check_scope=False\n273         )\n274 \n275         if is_field:\n276             # Skip any fields that have `init=False`\n277             if any(\n278                 keyword.arg == \"init\" and not keyword.value.bool_value()\n279                 for keyword in value.keywords  # type: ignore[union-attr] # value is never None\n280             ):\n281                 # Also remove the name from the previous arguments to be inserted later\n282                 prev_pos_only_store.pop(name, None)\n283                 prev_kw_only_store.pop(name, None)\n284                 continue\n285 \n286         if _is_init_var(annotation):  # type: ignore[arg-type] # annotation is never None\n287             init_var = True\n288             if isinstance(annotation, nodes.Subscript):\n289                 annotation = annotation.slice\n290             else:\n291                 # Cannot determine type annotation for parameter from InitVar\n292                 annotation = None\n293             assignment_str = \"\"\n294         else:\n295             init_var = False\n296             assignment_str = f\"self.{name} = {name}\"\n297 \n298         ann_str, default_str = None, None\n299         if annotation is not None:\n300             ann_str = annotation.as_string()\n301 \n302         if value:\n303             if is_field:\n304                 result = _get_field_default(value)  # type: ignore[arg-type]\n305                 if result:\n306                     default_type, default_node = result\n307                     if default_type == \"default\":\n308                         default_str = default_node.as_string()\n309                     elif default_type == \"default_factory\":\n310                         default_str = DEFAULT_FACTORY\n311                         assignment_str = (\n312                             f\"self.{name} = {default_node.as_string()} \"\n313                             f\"if {name} is {DEFAULT_FACTORY} else {name}\"\n314                         )\n315             else:\n316                 default_str = value.as_string()\n317         elif property_node:\n318             # We set the result of the property call as default\n319             # This hides the fact that this would normally be a 'property object'\n320             # But we can't represent those as string\n321             try:\n322                 # Call str to make sure also Uninferable gets stringified\n323                 default_str = str(next(property_node.infer_call_result()).as_string())\n324             except (InferenceError, StopIteration):\n325                 pass\n326         else:\n327             # Even with `init=False` the default value still can be propogated to\n328             # later assignments. Creating weird signatures like:\n329             # (self, a: str = 1) -> None\n330             previous_default = _get_previous_field_default(node, name)\n331             if previous_default:\n332                 default_str = previous_default.as_string()\n333 \n334         # Construct the param string to add to the init if necessary\n335         param_str = name\n336         if ann_str is not None:\n337             param_str += f\": {ann_str}\"\n338         if default_str is not None:\n339             param_str += f\" = {default_str}\"\n340 \n341         # If the field is a kw_only field, we need to add it to the kw_only_params\n342         # This overwrites whether or not the class is kw_only decorated\n343         if is_field:\n344             kw_only = [k for k in value.keywords if k.arg == \"kw_only\"]  # type: ignore[union-attr]\n345             if kw_only:\n346                 if kw_only[0].value.bool_value():\n347                     kw_only_params.append(param_str)\n348                 else:\n349                     params.append(param_str)\n350                 continue\n351         # If kw_only decorated, we need to add all parameters to the kw_only_params\n352         if kw_only_decorated:\n353             if name in prev_kw_only_store:\n354                 prev_kw_only_store[name] = (ann_str, default_str)\n355             else:\n356                 kw_only_params.append(param_str)\n357         else:\n358             # If the name was previously seen, overwrite that data\n359             # pylint: disable-next=else-if-used\n360             if name in prev_pos_only_store:\n361                 prev_pos_only_store[name] = (ann_str, default_str)\n362             elif name in prev_kw_only_store:\n363                 params = [name] + params\n364                 prev_kw_only_store.pop(name)\n365             else:\n366                 params.append(param_str)\n367 \n368         if not init_var:\n369             assignments.append(assignment_str)\n370 \n371     prev_pos_only, prev_kw_only = _parse_arguments_into_strings(\n372         prev_pos_only_store, prev_kw_only_store\n373     )\n374 \n375     # Construct the new init method paramter string\n376     # First we do the positional only parameters, making sure to add the\n377     # the self parameter and the comma to allow adding keyword only parameters\n378     params_string = \"\" if \"self\" in prev_pos_only else \"self, \"\n379     params_string += prev_pos_only + \", \".join(params)\n380     if not params_string.endswith(\", \"):\n381         params_string += \", \"\n382 \n383     # Then we add the keyword only parameters\n384     if prev_kw_only or kw_only_params:\n385         params_string += \"*, \"\n386     params_string += f\"{prev_kw_only}{', '.join(kw_only_params)}\"\n387 \n388     assignments_string = \"\\n    \".join(assignments) if assignments else \"pass\"\n389     return f\"def __init__({params_string}) -> None:\\n    {assignments_string}\"\n390 \n391 \n392 def infer_dataclass_attribute(\n393     node: nodes.Unknown, ctx: context.InferenceContext | None = None\n394 ) -> Iterator[InferenceResult]:\n395     \"\"\"Inference tip for an Unknown node that was dynamically generated to\n396     represent a dataclass attribute.\n397 \n398     In the case that a default value is provided, that is inferred first.\n399     Then, an Instance of the annotated class is yielded.\n400     \"\"\"\n401     assign = node.parent\n402     if not isinstance(assign, nodes.AnnAssign):\n403         yield Uninferable\n404         return\n405 \n406     annotation, value = assign.annotation, assign.value\n407     if value is not None:\n408         yield from value.infer(context=ctx)\n409     if annotation is not None:\n410         yield from _infer_instance_from_annotation(annotation, ctx=ctx)\n411     else:\n412         yield Uninferable\n413 \n414 \n415 def infer_dataclass_field_call(\n416     node: nodes.Call, ctx: context.InferenceContext | None = None\n417 ) -> Iterator[InferenceResult]:\n418     \"\"\"Inference tip for dataclass field calls.\"\"\"\n419     if not isinstance(node.parent, (nodes.AnnAssign, nodes.Assign)):\n420         raise UseInferenceDefault\n421     result = _get_field_default(node)\n422     if not result:\n423         yield Uninferable\n424     else:\n425         default_type, default = result\n426         if default_type == \"default\":\n427             yield from default.infer(context=ctx)\n428         else:\n429             new_call = parse(default.as_string()).body[0].value\n430             new_call.parent = node.parent\n431             yield from new_call.infer(context=ctx)\n432 \n433 \n434 def _looks_like_dataclass_decorator(\n435     node: nodes.NodeNG, decorator_names: frozenset[str] = DATACLASSES_DECORATORS\n436 ) -> bool:\n437     \"\"\"Return True if node looks like a dataclass decorator.\n438 \n439     Uses inference to lookup the value of the node, and if that fails,\n440     matches against specific names.\n441     \"\"\"\n442     if isinstance(node, nodes.Call):  # decorator with arguments\n443         node = node.func\n444     try:\n445         inferred = next(node.infer())\n446     except (InferenceError, StopIteration):\n447         inferred = Uninferable\n448 \n449     if isinstance(inferred, UninferableBase):\n450         if isinstance(node, nodes.Name):\n451             return node.name in decorator_names\n452         if isinstance(node, nodes.Attribute):\n453             return node.attrname in decorator_names\n454 \n455         return False\n456 \n457     return (\n458         isinstance(inferred, nodes.FunctionDef)\n459         and inferred.name in decorator_names\n460         and inferred.root().name in DATACLASS_MODULES\n461     )\n462 \n463 \n464 def _looks_like_dataclass_attribute(node: nodes.Unknown) -> bool:\n465     \"\"\"Return True if node was dynamically generated as the child of an AnnAssign\n466     statement.\n467     \"\"\"\n468     parent = node.parent\n469     if not parent:\n470         return False\n471 \n472     scope = parent.scope()\n473     return (\n474         isinstance(parent, nodes.AnnAssign)\n475         and isinstance(scope, nodes.ClassDef)\n476         and is_decorated_with_dataclass(scope)\n477     )\n478 \n479 \n480 def _looks_like_dataclass_field_call(\n481     node: nodes.Call, check_scope: bool = True\n482 ) -> bool:\n483     \"\"\"Return True if node is calling dataclasses field or Field\n484     from an AnnAssign statement directly in the body of a ClassDef.\n485 \n486     If check_scope is False, skips checking the statement and body.\n487     \"\"\"\n488     if check_scope:\n489         stmt = node.statement(future=True)\n490         scope = stmt.scope()\n491         if not (\n492             isinstance(stmt, nodes.AnnAssign)\n493             and stmt.value is not None\n494             and isinstance(scope, nodes.ClassDef)\n495             and is_decorated_with_dataclass(scope)\n496         ):\n497             return False\n498 \n499     try:\n500         inferred = next(node.func.infer())\n501     except (InferenceError, StopIteration):\n502         return False\n503 \n504     if not isinstance(inferred, nodes.FunctionDef):\n505         return False\n506 \n507     return inferred.name == FIELD_NAME and inferred.root().name in DATACLASS_MODULES\n508 \n509 \n510 def _get_field_default(field_call: nodes.Call) -> _FieldDefaultReturn:\n511     \"\"\"Return a the default value of a field call, and the corresponding keyword\n512     argument name.\n513 \n514     field(default=...) results in the ... node\n515     field(default_factory=...) results in a Call node with func ... and no arguments\n516 \n517     If neither or both arguments are present, return (\"\", None) instead,\n518     indicating that there is not a valid default value.\n519     \"\"\"\n520     default, default_factory = None, None\n521     for keyword in field_call.keywords:\n522         if keyword.arg == \"default\":\n523             default = keyword.value\n524         elif keyword.arg == \"default_factory\":\n525             default_factory = keyword.value\n526 \n527     if default is not None and default_factory is None:\n528         return \"default\", default\n529 \n530     if default is None and default_factory is not None:\n531         new_call = nodes.Call(\n532             lineno=field_call.lineno,\n533             col_offset=field_call.col_offset,\n534             parent=field_call.parent,\n535         )\n536         new_call.postinit(func=default_factory)\n537         return \"default_factory\", new_call\n538 \n539     return None\n540 \n541 \n542 def _is_class_var(node: nodes.NodeNG) -> bool:\n543     \"\"\"Return True if node is a ClassVar, with or without subscripting.\"\"\"\n544     if PY39_PLUS:\n545         try:\n546             inferred = next(node.infer())\n547         except (InferenceError, StopIteration):\n548             return False\n549 \n550         return getattr(inferred, \"name\", \"\") == \"ClassVar\"\n551 \n552     # Before Python 3.9, inference returns typing._SpecialForm instead of ClassVar.\n553     # Our backup is to inspect the node's structure.\n554     return isinstance(node, nodes.Subscript) and (\n555         isinstance(node.value, nodes.Name)\n556         and node.value.name == \"ClassVar\"\n557         or isinstance(node.value, nodes.Attribute)\n558         and node.value.attrname == \"ClassVar\"\n559     )\n560 \n561 \n562 def _is_keyword_only_sentinel(node: nodes.NodeNG) -> bool:\n563     \"\"\"Return True if node is the KW_ONLY sentinel.\"\"\"\n564     if not PY310_PLUS:\n565         return False\n566     inferred = helpers.safe_infer(node)\n567     return (\n568         isinstance(inferred, bases.Instance)\n569         and inferred.qname() == \"dataclasses._KW_ONLY_TYPE\"\n570     )\n571 \n572 \n573 def _is_init_var(node: nodes.NodeNG) -> bool:\n574     \"\"\"Return True if node is an InitVar, with or without subscripting.\"\"\"\n575     try:\n576         inferred = next(node.infer())\n577     except (InferenceError, StopIteration):\n578         return False\n579 \n580     return getattr(inferred, \"name\", \"\") == \"InitVar\"\n581 \n582 \n583 # Allowed typing classes for which we support inferring instances\n584 _INFERABLE_TYPING_TYPES = frozenset(\n585     (\n586         \"Dict\",\n587         \"FrozenSet\",\n588         \"List\",\n589         \"Set\",\n590         \"Tuple\",\n591     )\n592 )\n593 \n594 \n595 def _infer_instance_from_annotation(\n596     node: nodes.NodeNG, ctx: context.InferenceContext | None = None\n597 ) -> Iterator[UninferableBase | bases.Instance]:\n598     \"\"\"Infer an instance corresponding to the type annotation represented by node.\n599 \n600     Currently has limited support for the typing module.\n601     \"\"\"\n602     klass = None\n603     try:\n604         klass = next(node.infer(context=ctx))\n605     except (InferenceError, StopIteration):\n606         yield Uninferable\n607     if not isinstance(klass, nodes.ClassDef):\n608         yield Uninferable\n609     elif klass.root().name in {\n610         \"typing\",\n611         \"_collections_abc\",\n612         \"\",\n613     }:  # \"\" because of synthetic nodes in brain_typing.py\n614         if klass.name in _INFERABLE_TYPING_TYPES:\n615             yield klass.instantiate_class()\n616         else:\n617             yield Uninferable\n618     else:\n619         yield klass.instantiate_class()\n620 \n621 \n622 AstroidManager().register_transform(\n623     nodes.ClassDef, dataclass_transform, is_decorated_with_dataclass\n624 )\n625 \n626 AstroidManager().register_transform(\n627     nodes.Call,\n628     inference_tip(infer_dataclass_field_call, raise_on_overwrite=True),\n629     _looks_like_dataclass_field_call,\n630 )\n631 \n632 AstroidManager().register_transform(\n633     nodes.Unknown,\n634     inference_tip(infer_dataclass_attribute, raise_on_overwrite=True),\n635     _looks_like_dataclass_attribute,\n636 )\n637 \n[end of astroid/brain/brain_dataclasses.py]\n[start of astroid/brain/brain_typing.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Astroid hooks for typing.py support.\"\"\"\n6 \n7 from __future__ import annotations\n8 \n9 import sys\n10 import typing\n11 from collections.abc import Iterator\n12 from functools import partial\n13 \n14 from astroid import context, extract_node, inference_tip\n15 from astroid.builder import _extract_single_node\n16 from astroid.const import PY38_PLUS, PY39_PLUS\n17 from astroid.exceptions import (\n18     AttributeInferenceError,\n19     InferenceError,\n20     UseInferenceDefault,\n21 )\n22 from astroid.manager import AstroidManager\n23 from astroid.nodes.node_classes import (\n24     Assign,\n25     AssignName,\n26     Attribute,\n27     Call,\n28     Const,\n29     JoinedStr,\n30     Name,\n31     NodeNG,\n32     Subscript,\n33     Tuple,\n34 )\n35 from astroid.nodes.scoped_nodes import ClassDef, FunctionDef\n36 \n37 if sys.version_info >= (3, 8):\n38     from typing import Final\n39 else:\n40     from typing_extensions import Final\n41 \n42 TYPING_TYPEVARS = {\"TypeVar\", \"NewType\"}\n43 TYPING_TYPEVARS_QUALIFIED: Final = {\n44     \"typing.TypeVar\",\n45     \"typing.NewType\",\n46     \"typing_extensions.TypeVar\",\n47 }\n48 TYPING_TYPEDDICT_QUALIFIED: Final = {\"typing.TypedDict\", \"typing_extensions.TypedDict\"}\n49 TYPING_TYPE_TEMPLATE = \"\"\"\n50 class Meta(type):\n51     def __getitem__(self, item):\n52         return self\n53 \n54     @property\n55     def __args__(self):\n56         return ()\n57 \n58 class {0}(metaclass=Meta):\n59     pass\n60 \"\"\"\n61 TYPING_MEMBERS = set(getattr(typing, \"__all__\", []))\n62 \n63 TYPING_ALIAS = frozenset(\n64     (\n65         \"typing.Hashable\",\n66         \"typing.Awaitable\",\n67         \"typing.Coroutine\",\n68         \"typing.AsyncIterable\",\n69         \"typing.AsyncIterator\",\n70         \"typing.Iterable\",\n71         \"typing.Iterator\",\n72         \"typing.Reversible\",\n73         \"typing.Sized\",\n74         \"typing.Container\",\n75         \"typing.Collection\",\n76         \"typing.Callable\",\n77         \"typing.AbstractSet\",\n78         \"typing.MutableSet\",\n79         \"typing.Mapping\",\n80         \"typing.MutableMapping\",\n81         \"typing.Sequence\",\n82         \"typing.MutableSequence\",\n83         \"typing.ByteString\",\n84         \"typing.Tuple\",\n85         \"typing.List\",\n86         \"typing.Deque\",\n87         \"typing.Set\",\n88         \"typing.FrozenSet\",\n89         \"typing.MappingView\",\n90         \"typing.KeysView\",\n91         \"typing.ItemsView\",\n92         \"typing.ValuesView\",\n93         \"typing.ContextManager\",\n94         \"typing.AsyncContextManager\",\n95         \"typing.Dict\",\n96         \"typing.DefaultDict\",\n97         \"typing.OrderedDict\",\n98         \"typing.Counter\",\n99         \"typing.ChainMap\",\n100         \"typing.Generator\",\n101         \"typing.AsyncGenerator\",\n102         \"typing.Type\",\n103         \"typing.Pattern\",\n104         \"typing.Match\",\n105     )\n106 )\n107 \n108 CLASS_GETITEM_TEMPLATE = \"\"\"\n109 @classmethod\n110 def __class_getitem__(cls, item):\n111     return cls\n112 \"\"\"\n113 \n114 \n115 def looks_like_typing_typevar_or_newtype(node) -> bool:\n116     func = node.func\n117     if isinstance(func, Attribute):\n118         return func.attrname in TYPING_TYPEVARS\n119     if isinstance(func, Name):\n120         return func.name in TYPING_TYPEVARS\n121     return False\n122 \n123 \n124 def infer_typing_typevar_or_newtype(\n125     node: Call, context_itton: context.InferenceContext | None = None\n126 ) -> Iterator[ClassDef]:\n127     \"\"\"Infer a typing.TypeVar(...) or typing.NewType(...) call.\"\"\"\n128     try:\n129         func = next(node.func.infer(context=context_itton))\n130     except (InferenceError, StopIteration) as exc:\n131         raise UseInferenceDefault from exc\n132 \n133     if func.qname() not in TYPING_TYPEVARS_QUALIFIED:\n134         raise UseInferenceDefault\n135     if not node.args:\n136         raise UseInferenceDefault\n137     # Cannot infer from a dynamic class name (f-string)\n138     if isinstance(node.args[0], JoinedStr):\n139         raise UseInferenceDefault\n140 \n141     typename = node.args[0].as_string().strip(\"'\")\n142     node = ClassDef(\n143         name=typename,\n144         lineno=node.lineno,\n145         col_offset=node.col_offset,\n146         parent=node.parent,\n147         end_lineno=node.end_lineno,\n148         end_col_offset=node.end_col_offset,\n149     )\n150     return node.infer(context=context_itton)\n151 \n152 \n153 def _looks_like_typing_subscript(node) -> bool:\n154     \"\"\"Try to figure out if a Subscript node *might* be a typing-related subscript.\"\"\"\n155     if isinstance(node, Name):\n156         return node.name in TYPING_MEMBERS\n157     if isinstance(node, Attribute):\n158         return node.attrname in TYPING_MEMBERS\n159     if isinstance(node, Subscript):\n160         return _looks_like_typing_subscript(node.value)\n161     return False\n162 \n163 \n164 def infer_typing_attr(\n165     node: Subscript, ctx: context.InferenceContext | None = None\n166 ) -> Iterator[ClassDef]:\n167     \"\"\"Infer a typing.X[...] subscript.\"\"\"\n168     try:\n169         value = next(node.value.infer())  # type: ignore[union-attr] # value shouldn't be None for Subscript.\n170     except (InferenceError, StopIteration) as exc:\n171         raise UseInferenceDefault from exc\n172 \n173     if not value.qname().startswith(\"typing.\") or value.qname() in TYPING_ALIAS:\n174         # If typing subscript belongs to an alias handle it separately.\n175         raise UseInferenceDefault\n176 \n177     if isinstance(value, ClassDef) and value.qname() in {\n178         \"typing.Generic\",\n179         \"typing.Annotated\",\n180         \"typing_extensions.Annotated\",\n181     }:\n182         # typing.Generic and typing.Annotated (PY39) are subscriptable\n183         # through __class_getitem__. Since astroid can't easily\n184         # infer the native methods, replace them for an easy inference tip\n185         func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n186         value.locals[\"__class_getitem__\"] = [func_to_add]\n187         if (\n188             isinstance(node.parent, ClassDef)\n189             and node in node.parent.bases\n190             and getattr(node.parent, \"__cache\", None)\n191         ):\n192             # node.parent.slots is evaluated and cached before the inference tip\n193             # is first applied. Remove the last result to allow a recalculation of slots\n194             cache = node.parent.__cache  # type: ignore[attr-defined] # Unrecognized getattr\n195             if cache.get(node.parent.slots) is not None:\n196                 del cache[node.parent.slots]\n197         return iter([value])\n198 \n199     node = extract_node(TYPING_TYPE_TEMPLATE.format(value.qname().split(\".\")[-1]))\n200     return node.infer(context=ctx)\n201 \n202 \n203 def _looks_like_typedDict(  # pylint: disable=invalid-name\n204     node: FunctionDef | ClassDef,\n205 ) -> bool:\n206     \"\"\"Check if node is TypedDict FunctionDef.\"\"\"\n207     return node.qname() in TYPING_TYPEDDICT_QUALIFIED\n208 \n209 \n210 def infer_old_typedDict(  # pylint: disable=invalid-name\n211     node: ClassDef, ctx: context.InferenceContext | None = None\n212 ) -> Iterator[ClassDef]:\n213     func_to_add = _extract_single_node(\"dict\")\n214     node.locals[\"__call__\"] = [func_to_add]\n215     return iter([node])\n216 \n217 \n218 def infer_typedDict(  # pylint: disable=invalid-name\n219     node: FunctionDef, ctx: context.InferenceContext | None = None\n220 ) -> Iterator[ClassDef]:\n221     \"\"\"Replace TypedDict FunctionDef with ClassDef.\"\"\"\n222     class_def = ClassDef(\n223         name=\"TypedDict\",\n224         lineno=node.lineno,\n225         col_offset=node.col_offset,\n226         parent=node.parent,\n227     )\n228     class_def.postinit(bases=[extract_node(\"dict\")], body=[], decorators=None)\n229     func_to_add = _extract_single_node(\"dict\")\n230     class_def.locals[\"__call__\"] = [func_to_add]\n231     return iter([class_def])\n232 \n233 \n234 def _looks_like_typing_alias(node: Call) -> bool:\n235     \"\"\"\n236     Returns True if the node corresponds to a call to _alias function.\n237 \n238     For example :\n239 \n240     MutableSet = _alias(collections.abc.MutableSet, T)\n241 \n242     :param node: call node\n243     \"\"\"\n244     return (\n245         isinstance(node.func, Name)\n246         and node.func.name == \"_alias\"\n247         and (\n248             # _alias function works also for builtins object such as list and dict\n249             isinstance(node.args[0], (Attribute, Name))\n250         )\n251     )\n252 \n253 \n254 def _forbid_class_getitem_access(node: ClassDef) -> None:\n255     \"\"\"Disable the access to __class_getitem__ method for the node in parameters.\"\"\"\n256 \n257     def full_raiser(origin_func, attr, *args, **kwargs):\n258         \"\"\"\n259         Raises an AttributeInferenceError in case of access to __class_getitem__ method.\n260         Otherwise, just call origin_func.\n261         \"\"\"\n262         if attr == \"__class_getitem__\":\n263             raise AttributeInferenceError(\"__class_getitem__ access is not allowed\")\n264         return origin_func(attr, *args, **kwargs)\n265 \n266     try:\n267         node.getattr(\"__class_getitem__\")\n268         # If we are here, then we are sure to modify an object that does have\n269         # __class_getitem__ method (which origin is the protocol defined in\n270         # collections module) whereas the typing module considers it should not.\n271         # We do not want __class_getitem__ to be found in the classdef\n272         partial_raiser = partial(full_raiser, node.getattr)\n273         node.getattr = partial_raiser\n274     except AttributeInferenceError:\n275         pass\n276 \n277 \n278 def infer_typing_alias(\n279     node: Call, ctx: context.InferenceContext | None = None\n280 ) -> Iterator[ClassDef]:\n281     \"\"\"\n282     Infers the call to _alias function\n283     Insert ClassDef, with same name as aliased class,\n284     in mro to simulate _GenericAlias.\n285 \n286     :param node: call node\n287     :param context: inference context\n288     \"\"\"\n289     if (\n290         not isinstance(node.parent, Assign)\n291         or not len(node.parent.targets) == 1\n292         or not isinstance(node.parent.targets[0], AssignName)\n293     ):\n294         raise UseInferenceDefault\n295     try:\n296         res = next(node.args[0].infer(context=ctx))\n297     except StopIteration as e:\n298         raise InferenceError(node=node.args[0], context=ctx) from e\n299 \n300     assign_name = node.parent.targets[0]\n301 \n302     class_def = ClassDef(\n303         name=assign_name.name,\n304         lineno=assign_name.lineno,\n305         col_offset=assign_name.col_offset,\n306         parent=node.parent,\n307     )\n308     if isinstance(res, ClassDef):\n309         # Only add `res` as base if it's a `ClassDef`\n310         # This isn't the case for `typing.Pattern` and `typing.Match`\n311         class_def.postinit(bases=[res], body=[], decorators=None)\n312 \n313     maybe_type_var = node.args[1]\n314     if (\n315         not PY39_PLUS\n316         and not (isinstance(maybe_type_var, Tuple) and not maybe_type_var.elts)\n317         or PY39_PLUS\n318         and isinstance(maybe_type_var, Const)\n319         and maybe_type_var.value > 0\n320     ):\n321         # If typing alias is subscriptable, add `__class_getitem__` to ClassDef\n322         func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n323         class_def.locals[\"__class_getitem__\"] = [func_to_add]\n324     else:\n325         # If not, make sure that `__class_getitem__` access is forbidden.\n326         # This is an issue in cases where the aliased class implements it,\n327         # but the typing alias isn't subscriptable. E.g., `typing.ByteString` for PY39+\n328         _forbid_class_getitem_access(class_def)\n329     return iter([class_def])\n330 \n331 \n332 def _looks_like_special_alias(node: Call) -> bool:\n333     \"\"\"Return True if call is for Tuple or Callable alias.\n334 \n335     In PY37 and PY38 the call is to '_VariadicGenericAlias' with 'tuple' as\n336     first argument. In PY39+ it is replaced by a call to '_TupleType'.\n337 \n338     PY37: Tuple = _VariadicGenericAlias(tuple, (), inst=False, special=True)\n339     PY39: Tuple = _TupleType(tuple, -1, inst=False, name='Tuple')\n340 \n341     PY37: Callable = _VariadicGenericAlias(collections.abc.Callable, (), special=True)\n342     PY39: Callable = _CallableType(collections.abc.Callable, 2)\n343     \"\"\"\n344     return isinstance(node.func, Name) and (\n345         not PY39_PLUS\n346         and node.func.name == \"_VariadicGenericAlias\"\n347         and (\n348             isinstance(node.args[0], Name)\n349             and node.args[0].name == \"tuple\"\n350             or isinstance(node.args[0], Attribute)\n351             and node.args[0].as_string() == \"collections.abc.Callable\"\n352         )\n353         or PY39_PLUS\n354         and (\n355             node.func.name == \"_TupleType\"\n356             and isinstance(node.args[0], Name)\n357             and node.args[0].name == \"tuple\"\n358             or node.func.name == \"_CallableType\"\n359             and isinstance(node.args[0], Attribute)\n360             and node.args[0].as_string() == \"collections.abc.Callable\"\n361         )\n362     )\n363 \n364 \n365 def infer_special_alias(\n366     node: Call, ctx: context.InferenceContext | None = None\n367 ) -> Iterator[ClassDef]:\n368     \"\"\"Infer call to tuple alias as new subscriptable class typing.Tuple.\"\"\"\n369     if not (\n370         isinstance(node.parent, Assign)\n371         and len(node.parent.targets) == 1\n372         and isinstance(node.parent.targets[0], AssignName)\n373     ):\n374         raise UseInferenceDefault\n375     try:\n376         res = next(node.args[0].infer(context=ctx))\n377     except StopIteration as e:\n378         raise InferenceError(node=node.args[0], context=ctx) from e\n379 \n380     assign_name = node.parent.targets[0]\n381     class_def = ClassDef(\n382         name=assign_name.name,\n383         parent=node.parent,\n384     )\n385     class_def.postinit(bases=[res], body=[], decorators=None)\n386     func_to_add = _extract_single_node(CLASS_GETITEM_TEMPLATE)\n387     class_def.locals[\"__class_getitem__\"] = [func_to_add]\n388     return iter([class_def])\n389 \n390 \n391 def _looks_like_typing_cast(node: Call) -> bool:\n392     return isinstance(node, Call) and (\n393         isinstance(node.func, Name)\n394         and node.func.name == \"cast\"\n395         or isinstance(node.func, Attribute)\n396         and node.func.attrname == \"cast\"\n397     )\n398 \n399 \n400 def infer_typing_cast(\n401     node: Call, ctx: context.InferenceContext | None = None\n402 ) -> Iterator[NodeNG]:\n403     \"\"\"Infer call to cast() returning same type as casted-from var.\"\"\"\n404     if not isinstance(node.func, (Name, Attribute)):\n405         raise UseInferenceDefault\n406 \n407     try:\n408         func = next(node.func.infer(context=ctx))\n409     except (InferenceError, StopIteration) as exc:\n410         raise UseInferenceDefault from exc\n411     if (\n412         not isinstance(func, FunctionDef)\n413         or func.qname() != \"typing.cast\"\n414         or len(node.args) != 2\n415     ):\n416         raise UseInferenceDefault\n417 \n418     return node.args[1].infer(context=ctx)\n419 \n420 \n421 AstroidManager().register_transform(\n422     Call,\n423     inference_tip(infer_typing_typevar_or_newtype),\n424     looks_like_typing_typevar_or_newtype,\n425 )\n426 AstroidManager().register_transform(\n427     Subscript, inference_tip(infer_typing_attr), _looks_like_typing_subscript\n428 )\n429 AstroidManager().register_transform(\n430     Call, inference_tip(infer_typing_cast), _looks_like_typing_cast\n431 )\n432 \n433 if PY39_PLUS:\n434     AstroidManager().register_transform(\n435         FunctionDef, inference_tip(infer_typedDict), _looks_like_typedDict\n436     )\n437 elif PY38_PLUS:\n438     AstroidManager().register_transform(\n439         ClassDef, inference_tip(infer_old_typedDict), _looks_like_typedDict\n440     )\n441 \n442 AstroidManager().register_transform(\n443     Call, inference_tip(infer_typing_alias), _looks_like_typing_alias\n444 )\n445 AstroidManager().register_transform(\n446     Call, inference_tip(infer_special_alias), _looks_like_special_alias\n447 )\n448 \n[end of astroid/brain/brain_typing.py]\n[start of astroid/interpreter/objectmodel.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"\n6 Data object model, as per https://docs.python.org/3/reference/datamodel.html.\n7 \n8 This module describes, at least partially, a data object model for some\n9 of astroid's nodes. The model contains special attributes that nodes such\n10 as functions, classes, modules etc have, such as __doc__, __class__,\n11 __module__ etc, being used when doing attribute lookups over nodes.\n12 \n13 For instance, inferring `obj.__class__` will first trigger an inference\n14 of the `obj` variable. If it was successfully inferred, then an attribute\n15 `__class__ will be looked for in the inferred object. This is the part\n16 where the data model occurs. The model is attached to those nodes\n17 and the lookup mechanism will try to see if attributes such as\n18 `__class__` are defined by the model or not. If they are defined,\n19 the model will be requested to return the corresponding value of that\n20 attribute. Thus the model can be viewed as a special part of the lookup\n21 mechanism.\n22 \"\"\"\n23 \n24 from __future__ import annotations\n25 \n26 import itertools\n27 import os\n28 import pprint\n29 import sys\n30 import types\n31 from functools import lru_cache\n32 from typing import TYPE_CHECKING, Any\n33 \n34 import astroid\n35 from astroid import bases, nodes, util\n36 from astroid.context import InferenceContext, copy_context\n37 from astroid.exceptions import AttributeInferenceError, InferenceError, NoDefault\n38 from astroid.manager import AstroidManager\n39 from astroid.nodes import node_classes\n40 \n41 objects = util.lazy_import(\"objects\")\n42 builder = util.lazy_import(\"builder\")\n43 \n44 if sys.version_info >= (3, 8):\n45     from typing import Literal\n46 else:\n47     from typing_extensions import Literal\n48 \n49 if TYPE_CHECKING:\n50     from astroid import builder\n51     from astroid.objects import Property\n52 \n53 IMPL_PREFIX = \"attr_\"\n54 LEN_OF_IMPL_PREFIX = len(IMPL_PREFIX)\n55 \n56 \n57 def _dunder_dict(instance, attributes):\n58     obj = node_classes.Dict(parent=instance)\n59 \n60     # Convert the keys to node strings\n61     keys = [\n62         node_classes.Const(value=value, parent=obj) for value in list(attributes.keys())\n63     ]\n64 \n65     # The original attribute has a list of elements for each key,\n66     # but that is not useful for retrieving the special attribute's value.\n67     # In this case, we're picking the last value from each list.\n68     values = [elem[-1] for elem in attributes.values()]\n69 \n70     obj.postinit(list(zip(keys, values)))\n71     return obj\n72 \n73 \n74 def _get_bound_node(model: ObjectModel) -> Any:\n75     # TODO: Use isinstance instead of try ... except after _instance has typing\n76     try:\n77         return model._instance._proxied\n78     except AttributeError:\n79         return model._instance\n80 \n81 \n82 class ObjectModel:\n83     def __init__(self):\n84         self._instance = None\n85 \n86     def __repr__(self):\n87         result = []\n88         cname = type(self).__name__\n89         string = \"%(cname)s(%(fields)s)\"\n90         alignment = len(cname) + 1\n91         for field in sorted(self.attributes()):\n92             width = 80 - len(field) - alignment\n93             lines = pprint.pformat(field, indent=2, width=width).splitlines(True)\n94 \n95             inner = [lines[0]]\n96             for line in lines[1:]:\n97                 inner.append(\" \" * alignment + line)\n98             result.append(field)\n99 \n100         return string % {\n101             \"cname\": cname,\n102             \"fields\": (\",\\n\" + \" \" * alignment).join(result),\n103         }\n104 \n105     def __call__(self, instance):\n106         self._instance = instance\n107         return self\n108 \n109     def __get__(self, instance, cls=None):\n110         # ObjectModel needs to be a descriptor so that just doing\n111         # `special_attributes = SomeObjectModel` should be enough in the body of a node.\n112         # But at the same time, node.special_attributes should return an object\n113         # which can be used for manipulating the special attributes. That's the reason\n114         # we pass the instance through which it got accessed to ObjectModel.__call__,\n115         # returning itself afterwards, so we can still have access to the\n116         # underlying data model and to the instance for which it got accessed.\n117         return self(instance)\n118 \n119     def __contains__(self, name) -> bool:\n120         return name in self.attributes()\n121 \n122     @lru_cache()  # noqa\n123     def attributes(self) -> list[str]:\n124         \"\"\"Get the attributes which are exported by this object model.\"\"\"\n125         return [o[LEN_OF_IMPL_PREFIX:] for o in dir(self) if o.startswith(IMPL_PREFIX)]\n126 \n127     def lookup(self, name):\n128         \"\"\"Look up the given *name* in the current model.\n129 \n130         It should return an AST or an interpreter object,\n131         but if the name is not found, then an AttributeInferenceError will be raised.\n132         \"\"\"\n133         if name in self.attributes():\n134             return getattr(self, IMPL_PREFIX + name)\n135         raise AttributeInferenceError(target=self._instance, attribute=name)\n136 \n137     @property\n138     def attr___new__(self) -> bases.BoundMethod:\n139         \"\"\"Calling cls.__new__(type) on an object returns an instance of 'type'.\"\"\"\n140         node: nodes.FunctionDef = builder.extract_node(\n141             \"\"\"def __new__(self, cls): return cls()\"\"\"\n142         )\n143         # We set the parent as being the ClassDef of 'object' as that\n144         # triggers correct inference as a call to __new__ in bases.py\n145         node.parent = AstroidManager().builtins_module[\"object\"]\n146 \n147         return bases.BoundMethod(proxy=node, bound=_get_bound_node(self))\n148 \n149     @property\n150     def attr___init__(self) -> bases.BoundMethod:\n151         \"\"\"Calling cls.__init__() normally returns None.\"\"\"\n152         # The *args and **kwargs are necessary not to trigger warnings about missing\n153         # or extra parameters for '__init__' methods we don't infer correctly.\n154         # This BoundMethod is the fallback value for those.\n155         node: nodes.FunctionDef = builder.extract_node(\n156             \"\"\"def __init__(self, *args, **kwargs): return None\"\"\"\n157         )\n158         # We set the parent as being the ClassDef of 'object' as that\n159         # is where this method originally comes from\n160         node.parent = AstroidManager().builtins_module[\"object\"]\n161 \n162         return bases.BoundMethod(proxy=node, bound=_get_bound_node(self))\n163 \n164 \n165 class ModuleModel(ObjectModel):\n166     def _builtins(self):\n167         builtins_ast_module = AstroidManager().builtins_module\n168         return builtins_ast_module.special_attributes.lookup(\"__dict__\")\n169 \n170     @property\n171     def attr_builtins(self):\n172         return self._builtins()\n173 \n174     @property\n175     def attr___path__(self):\n176         if not self._instance.package:\n177             raise AttributeInferenceError(target=self._instance, attribute=\"__path__\")\n178 \n179         path_objs = [\n180             node_classes.Const(\n181                 value=path\n182                 if not path.endswith(\"__init__.py\")\n183                 else os.path.dirname(path),\n184                 parent=self._instance,\n185             )\n186             for path in self._instance.path\n187         ]\n188 \n189         container = node_classes.List(parent=self._instance)\n190         container.postinit(path_objs)\n191 \n192         return container\n193 \n194     @property\n195     def attr___name__(self):\n196         return node_classes.Const(value=self._instance.name, parent=self._instance)\n197 \n198     @property\n199     def attr___doc__(self):\n200         return node_classes.Const(\n201             value=getattr(self._instance.doc_node, \"value\", None),\n202             parent=self._instance,\n203         )\n204 \n205     @property\n206     def attr___file__(self):\n207         return node_classes.Const(value=self._instance.file, parent=self._instance)\n208 \n209     @property\n210     def attr___dict__(self):\n211         return _dunder_dict(self._instance, self._instance.globals)\n212 \n213     @property\n214     def attr___package__(self):\n215         if not self._instance.package:\n216             value = \"\"\n217         else:\n218             value = self._instance.name\n219 \n220         return node_classes.Const(value=value, parent=self._instance)\n221 \n222     # These are related to the Python 3 implementation of the\n223     # import system,\n224     # https://docs.python.org/3/reference/import.html#import-related-module-attributes\n225 \n226     @property\n227     def attr___spec__(self):\n228         # No handling for now.\n229         return node_classes.Unknown()\n230 \n231     @property\n232     def attr___loader__(self):\n233         # No handling for now.\n234         return node_classes.Unknown()\n235 \n236     @property\n237     def attr___cached__(self):\n238         # No handling for now.\n239         return node_classes.Unknown()\n240 \n241 \n242 class FunctionModel(ObjectModel):\n243     @property\n244     def attr___name__(self):\n245         return node_classes.Const(value=self._instance.name, parent=self._instance)\n246 \n247     @property\n248     def attr___doc__(self):\n249         return node_classes.Const(\n250             value=getattr(self._instance.doc_node, \"value\", None),\n251             parent=self._instance,\n252         )\n253 \n254     @property\n255     def attr___qualname__(self):\n256         return node_classes.Const(value=self._instance.qname(), parent=self._instance)\n257 \n258     @property\n259     def attr___defaults__(self):\n260         func = self._instance\n261         if not func.args.defaults:\n262             return node_classes.Const(value=None, parent=func)\n263 \n264         defaults_obj = node_classes.Tuple(parent=func)\n265         defaults_obj.postinit(func.args.defaults)\n266         return defaults_obj\n267 \n268     @property\n269     def attr___annotations__(self):\n270         obj = node_classes.Dict(parent=self._instance)\n271 \n272         if not self._instance.returns:\n273             returns = None\n274         else:\n275             returns = self._instance.returns\n276 \n277         args = self._instance.args\n278         pair_annotations = itertools.chain(\n279             zip(args.args or [], args.annotations),\n280             zip(args.kwonlyargs, args.kwonlyargs_annotations),\n281             zip(args.posonlyargs or [], args.posonlyargs_annotations),\n282         )\n283 \n284         annotations = {\n285             arg.name: annotation for (arg, annotation) in pair_annotations if annotation\n286         }\n287         if args.varargannotation:\n288             annotations[args.vararg] = args.varargannotation\n289         if args.kwargannotation:\n290             annotations[args.kwarg] = args.kwargannotation\n291         if returns:\n292             annotations[\"return\"] = returns\n293 \n294         items = [\n295             (node_classes.Const(key, parent=obj), value)\n296             for (key, value) in annotations.items()\n297         ]\n298 \n299         obj.postinit(items)\n300         return obj\n301 \n302     @property\n303     def attr___dict__(self):\n304         return node_classes.Dict(parent=self._instance)\n305 \n306     attr___globals__ = attr___dict__\n307 \n308     @property\n309     def attr___kwdefaults__(self):\n310         def _default_args(args, parent):\n311             for arg in args.kwonlyargs:\n312                 try:\n313                     default = args.default_value(arg.name)\n314                 except NoDefault:\n315                     continue\n316 \n317                 name = node_classes.Const(arg.name, parent=parent)\n318                 yield name, default\n319 \n320         args = self._instance.args\n321         obj = node_classes.Dict(parent=self._instance)\n322         defaults = dict(_default_args(args, obj))\n323 \n324         obj.postinit(list(defaults.items()))\n325         return obj\n326 \n327     @property\n328     def attr___module__(self):\n329         return node_classes.Const(self._instance.root().qname())\n330 \n331     @property\n332     def attr___get__(self):\n333         func = self._instance\n334 \n335         class DescriptorBoundMethod(bases.BoundMethod):\n336             \"\"\"Bound method which knows how to understand calling descriptor\n337             binding.\n338             \"\"\"\n339 \n340             def implicit_parameters(self) -> Literal[0]:\n341                 # Different than BoundMethod since the signature\n342                 # is different.\n343                 return 0\n344 \n345             def infer_call_result(\n346                 self, caller, context: InferenceContext | None = None\n347             ):\n348                 if len(caller.args) > 2 or len(caller.args) < 1:\n349                     raise InferenceError(\n350                         \"Invalid arguments for descriptor binding\",\n351                         target=self,\n352                         context=context,\n353                     )\n354 \n355                 context = copy_context(context)\n356                 try:\n357                     cls = next(caller.args[0].infer(context=context))\n358                 except StopIteration as e:\n359                     raise InferenceError(context=context, node=caller.args[0]) from e\n360 \n361                 if isinstance(cls, util.UninferableBase):\n362                     raise InferenceError(\n363                         \"Invalid class inferred\", target=self, context=context\n364                     )\n365 \n366                 # For some reason func is a Node that the below\n367                 # code is not expecting\n368                 if isinstance(func, bases.BoundMethod):\n369                     yield func\n370                     return\n371 \n372                 # Rebuild the original value, but with the parent set as the\n373                 # class where it will be bound.\n374                 new_func = func.__class__(\n375                     name=func.name,\n376                     lineno=func.lineno,\n377                     col_offset=func.col_offset,\n378                     parent=func.parent,\n379                 )\n380                 # pylint: disable=no-member\n381                 new_func.postinit(\n382                     func.args,\n383                     func.body,\n384                     func.decorators,\n385                     func.returns,\n386                     doc_node=func.doc_node,\n387                 )\n388 \n389                 # Build a proper bound method that points to our newly built function.\n390                 proxy = bases.UnboundMethod(new_func)\n391                 yield bases.BoundMethod(proxy=proxy, bound=cls)\n392 \n393             @property\n394             def args(self):\n395                 \"\"\"Overwrite the underlying args to match those of the underlying func.\n396 \n397                 Usually the underlying *func* is a function/method, as in:\n398 \n399                     def test(self):\n400                         pass\n401 \n402                 This has only the *self* parameter but when we access test.__get__\n403                 we get a new object which has two parameters, *self* and *type*.\n404                 \"\"\"\n405                 nonlocal func\n406                 positional_or_keyword_params = func.args.args.copy()\n407                 positional_or_keyword_params.append(astroid.AssignName(name=\"type\"))\n408 \n409                 positional_only_params = func.args.posonlyargs.copy()\n410 \n411                 arguments = astroid.Arguments(parent=func.args.parent)\n412                 arguments.postinit(\n413                     args=positional_or_keyword_params,\n414                     posonlyargs=positional_only_params,\n415                     defaults=[],\n416                     kwonlyargs=[],\n417                     kw_defaults=[],\n418                     annotations=[],\n419                 )\n420                 return arguments\n421 \n422         return DescriptorBoundMethod(proxy=self._instance, bound=self._instance)\n423 \n424     # These are here just for completion.\n425     @property\n426     def attr___ne__(self):\n427         return node_classes.Unknown()\n428 \n429     attr___subclasshook__ = attr___ne__\n430     attr___str__ = attr___ne__\n431     attr___sizeof__ = attr___ne__\n432     attr___setattr___ = attr___ne__\n433     attr___repr__ = attr___ne__\n434     attr___reduce__ = attr___ne__\n435     attr___reduce_ex__ = attr___ne__\n436     attr___lt__ = attr___ne__\n437     attr___eq__ = attr___ne__\n438     attr___gt__ = attr___ne__\n439     attr___format__ = attr___ne__\n440     attr___delattr___ = attr___ne__\n441     attr___getattribute__ = attr___ne__\n442     attr___hash__ = attr___ne__\n443     attr___dir__ = attr___ne__\n444     attr___call__ = attr___ne__\n445     attr___class__ = attr___ne__\n446     attr___closure__ = attr___ne__\n447     attr___code__ = attr___ne__\n448 \n449 \n450 class ClassModel(ObjectModel):\n451     def __init__(self):\n452         # Add a context so that inferences called from an instance don't recurse endlessly\n453         self.context = InferenceContext()\n454 \n455         super().__init__()\n456 \n457     @property\n458     def attr___module__(self):\n459         return node_classes.Const(self._instance.root().qname())\n460 \n461     @property\n462     def attr___name__(self):\n463         return node_classes.Const(self._instance.name)\n464 \n465     @property\n466     def attr___qualname__(self):\n467         return node_classes.Const(self._instance.qname())\n468 \n469     @property\n470     def attr___doc__(self):\n471         return node_classes.Const(getattr(self._instance.doc_node, \"value\", None))\n472 \n473     @property\n474     def attr___mro__(self):\n475         if not self._instance.newstyle:\n476             raise AttributeInferenceError(target=self._instance, attribute=\"__mro__\")\n477 \n478         mro = self._instance.mro()\n479         obj = node_classes.Tuple(parent=self._instance)\n480         obj.postinit(mro)\n481         return obj\n482 \n483     @property\n484     def attr_mro(self):\n485         if not self._instance.newstyle:\n486             raise AttributeInferenceError(target=self._instance, attribute=\"mro\")\n487 \n488         other_self = self\n489 \n490         # Cls.mro is a method and we need to return one in order to have a proper inference.\n491         # The method we're returning is capable of inferring the underlying MRO though.\n492         class MroBoundMethod(bases.BoundMethod):\n493             def infer_call_result(\n494                 self, caller, context: InferenceContext | None = None\n495             ):\n496                 yield other_self.attr___mro__\n497 \n498         implicit_metaclass = self._instance.implicit_metaclass()\n499         mro_method = implicit_metaclass.locals[\"mro\"][0]\n500         return MroBoundMethod(proxy=mro_method, bound=implicit_metaclass)\n501 \n502     @property\n503     def attr___bases__(self):\n504         obj = node_classes.Tuple()\n505         context = InferenceContext()\n506         elts = list(self._instance._inferred_bases(context))\n507         obj.postinit(elts=elts)\n508         return obj\n509 \n510     @property\n511     def attr___class__(self):\n512         # pylint: disable=import-outside-toplevel; circular import\n513         from astroid import helpers\n514 \n515         return helpers.object_type(self._instance)\n516 \n517     @property\n518     def attr___subclasses__(self):\n519         \"\"\"Get the subclasses of the underlying class.\n520 \n521         This looks only in the current module for retrieving the subclasses,\n522         thus it might miss a couple of them.\n523         \"\"\"\n524         if not self._instance.newstyle:\n525             raise AttributeInferenceError(\n526                 target=self._instance, attribute=\"__subclasses__\"\n527             )\n528 \n529         qname = self._instance.qname()\n530         root = self._instance.root()\n531         classes = [\n532             cls\n533             for cls in root.nodes_of_class(nodes.ClassDef)\n534             if cls != self._instance and cls.is_subtype_of(qname, context=self.context)\n535         ]\n536 \n537         obj = node_classes.List(parent=self._instance)\n538         obj.postinit(classes)\n539 \n540         class SubclassesBoundMethod(bases.BoundMethod):\n541             def infer_call_result(\n542                 self, caller, context: InferenceContext | None = None\n543             ):\n544                 yield obj\n545 \n546         implicit_metaclass = self._instance.implicit_metaclass()\n547         subclasses_method = implicit_metaclass.locals[\"__subclasses__\"][0]\n548         return SubclassesBoundMethod(proxy=subclasses_method, bound=implicit_metaclass)\n549 \n550     @property\n551     def attr___dict__(self):\n552         return node_classes.Dict(parent=self._instance)\n553 \n554     @property\n555     def attr___call__(self):\n556         \"\"\"Calling a class A() returns an instance of A.\"\"\"\n557         return self._instance.instantiate_class()\n558 \n559 \n560 class SuperModel(ObjectModel):\n561     @property\n562     def attr___thisclass__(self):\n563         return self._instance.mro_pointer\n564 \n565     @property\n566     def attr___self_class__(self):\n567         return self._instance._self_class\n568 \n569     @property\n570     def attr___self__(self):\n571         return self._instance.type\n572 \n573     @property\n574     def attr___class__(self):\n575         return self._instance._proxied\n576 \n577 \n578 class UnboundMethodModel(ObjectModel):\n579     @property\n580     def attr___class__(self):\n581         # pylint: disable=import-outside-toplevel; circular import\n582         from astroid import helpers\n583 \n584         return helpers.object_type(self._instance)\n585 \n586     @property\n587     def attr___func__(self):\n588         return self._instance._proxied\n589 \n590     @property\n591     def attr___self__(self):\n592         return node_classes.Const(value=None, parent=self._instance)\n593 \n594     attr_im_func = attr___func__\n595     attr_im_class = attr___class__\n596     attr_im_self = attr___self__\n597 \n598 \n599 class ContextManagerModel(ObjectModel):\n600     \"\"\"Model for context managers.\n601 \n602     Based on 3.3.9 of the Data Model documentation:\n603     https://docs.python.org/3/reference/datamodel.html#with-statement-context-managers\n604     \"\"\"\n605 \n606     @property\n607     def attr___enter__(self) -> bases.BoundMethod:\n608         \"\"\"Representation of the base implementation of __enter__.\n609 \n610         As per Python documentation:\n611         Enter the runtime context related to this object. The with statement\n612         will bind this method's return value to the target(s) specified in the\n613         as clause of the statement, if any.\n614         \"\"\"\n615         node: nodes.FunctionDef = builder.extract_node(\"\"\"def __enter__(self): ...\"\"\")\n616         # We set the parent as being the ClassDef of 'object' as that\n617         # is where this method originally comes from\n618         node.parent = AstroidManager().builtins_module[\"object\"]\n619 \n620         return bases.BoundMethod(proxy=node, bound=_get_bound_node(self))\n621 \n622     @property\n623     def attr___exit__(self) -> bases.BoundMethod:\n624         \"\"\"Representation of the base implementation of __exit__.\n625 \n626         As per Python documentation:\n627         Exit the runtime context related to this object. The parameters describe the\n628         exception that caused the context to be exited. If the context was exited\n629         without an exception, all three arguments will be None.\n630         \"\"\"\n631         node: nodes.FunctionDef = builder.extract_node(\n632             \"\"\"def __exit__(self, exc_type, exc_value, traceback): ...\"\"\"\n633         )\n634         # We set the parent as being the ClassDef of 'object' as that\n635         # is where this method originally comes from\n636         node.parent = AstroidManager().builtins_module[\"object\"]\n637 \n638         return bases.BoundMethod(proxy=node, bound=_get_bound_node(self))\n639 \n640 \n641 class BoundMethodModel(FunctionModel):\n642     @property\n643     def attr___func__(self):\n644         return self._instance._proxied._proxied\n645 \n646     @property\n647     def attr___self__(self):\n648         return self._instance.bound\n649 \n650 \n651 class GeneratorModel(FunctionModel, ContextManagerModel):\n652     def __new__(cls, *args, **kwargs):\n653         # Append the values from the GeneratorType unto this object.\n654         ret = super().__new__(cls, *args, **kwargs)\n655         generator = AstroidManager().builtins_module[\"generator\"]\n656         for name, values in generator.locals.items():\n657             method = values[0]\n658 \n659             def patched(cls, meth=method):\n660                 return meth\n661 \n662             setattr(type(ret), IMPL_PREFIX + name, property(patched))\n663 \n664         return ret\n665 \n666     @property\n667     def attr___name__(self):\n668         return node_classes.Const(\n669             value=self._instance.parent.name, parent=self._instance\n670         )\n671 \n672     @property\n673     def attr___doc__(self):\n674         return node_classes.Const(\n675             value=getattr(self._instance.parent.doc_node, \"value\", None),\n676             parent=self._instance,\n677         )\n678 \n679 \n680 class AsyncGeneratorModel(GeneratorModel):\n681     def __new__(cls, *args, **kwargs):\n682         # Append the values from the AGeneratorType unto this object.\n683         ret = super().__new__(cls, *args, **kwargs)\n684         astroid_builtins = AstroidManager().builtins_module\n685         generator = astroid_builtins.get(\"async_generator\")\n686         if generator is None:\n687             # Make it backward compatible.\n688             generator = astroid_builtins.get(\"generator\")\n689 \n690         for name, values in generator.locals.items():\n691             method = values[0]\n692 \n693             def patched(cls, meth=method):\n694                 return meth\n695 \n696             setattr(type(ret), IMPL_PREFIX + name, property(patched))\n697 \n698         return ret\n699 \n700 \n701 class InstanceModel(ObjectModel):\n702     @property\n703     def attr___class__(self):\n704         return self._instance._proxied\n705 \n706     @property\n707     def attr___module__(self):\n708         return node_classes.Const(self._instance.root().qname())\n709 \n710     @property\n711     def attr___doc__(self):\n712         return node_classes.Const(getattr(self._instance.doc_node, \"value\", None))\n713 \n714     @property\n715     def attr___dict__(self):\n716         return _dunder_dict(self._instance, self._instance.instance_attrs)\n717 \n718 \n719 # Exception instances\n720 \n721 \n722 class ExceptionInstanceModel(InstanceModel):\n723     @property\n724     def attr_args(self) -> nodes.Tuple:\n725         return nodes.Tuple(parent=self._instance)\n726 \n727     @property\n728     def attr___traceback__(self):\n729         builtins_ast_module = AstroidManager().builtins_module\n730         traceback_type = builtins_ast_module[types.TracebackType.__name__]\n731         return traceback_type.instantiate_class()\n732 \n733 \n734 class SyntaxErrorInstanceModel(ExceptionInstanceModel):\n735     @property\n736     def attr_text(self):\n737         return node_classes.Const(\"\")\n738 \n739 \n740 class OSErrorInstanceModel(ExceptionInstanceModel):\n741     @property\n742     def attr_filename(self):\n743         return node_classes.Const(\"\")\n744 \n745     @property\n746     def attr_errno(self):\n747         return node_classes.Const(0)\n748 \n749     @property\n750     def attr_strerror(self):\n751         return node_classes.Const(\"\")\n752 \n753     attr_filename2 = attr_filename\n754 \n755 \n756 class ImportErrorInstanceModel(ExceptionInstanceModel):\n757     @property\n758     def attr_name(self):\n759         return node_classes.Const(\"\")\n760 \n761     @property\n762     def attr_path(self):\n763         return node_classes.Const(\"\")\n764 \n765 \n766 class UnicodeDecodeErrorInstanceModel(ExceptionInstanceModel):\n767     @property\n768     def attr_object(self):\n769         return node_classes.Const(\"\")\n770 \n771 \n772 BUILTIN_EXCEPTIONS = {\n773     \"builtins.SyntaxError\": SyntaxErrorInstanceModel,\n774     \"builtins.ImportError\": ImportErrorInstanceModel,\n775     \"builtins.UnicodeDecodeError\": UnicodeDecodeErrorInstanceModel,\n776     # These are all similar to OSError in terms of attributes\n777     \"builtins.OSError\": OSErrorInstanceModel,\n778     \"builtins.BlockingIOError\": OSErrorInstanceModel,\n779     \"builtins.BrokenPipeError\": OSErrorInstanceModel,\n780     \"builtins.ChildProcessError\": OSErrorInstanceModel,\n781     \"builtins.ConnectionAbortedError\": OSErrorInstanceModel,\n782     \"builtins.ConnectionError\": OSErrorInstanceModel,\n783     \"builtins.ConnectionRefusedError\": OSErrorInstanceModel,\n784     \"builtins.ConnectionResetError\": OSErrorInstanceModel,\n785     \"builtins.FileExistsError\": OSErrorInstanceModel,\n786     \"builtins.FileNotFoundError\": OSErrorInstanceModel,\n787     \"builtins.InterruptedError\": OSErrorInstanceModel,\n788     \"builtins.IsADirectoryError\": OSErrorInstanceModel,\n789     \"builtins.NotADirectoryError\": OSErrorInstanceModel,\n790     \"builtins.PermissionError\": OSErrorInstanceModel,\n791     \"builtins.ProcessLookupError\": OSErrorInstanceModel,\n792     \"builtins.TimeoutError\": OSErrorInstanceModel,\n793 }\n794 \n795 \n796 class DictModel(ObjectModel):\n797     @property\n798     def attr___class__(self):\n799         return self._instance._proxied\n800 \n801     def _generic_dict_attribute(self, obj, name):\n802         \"\"\"Generate a bound method that can infer the given *obj*.\"\"\"\n803 \n804         class DictMethodBoundMethod(astroid.BoundMethod):\n805             def infer_call_result(\n806                 self, caller, context: InferenceContext | None = None\n807             ):\n808                 yield obj\n809 \n810         meth = next(self._instance._proxied.igetattr(name), None)\n811         return DictMethodBoundMethod(proxy=meth, bound=self._instance)\n812 \n813     @property\n814     def attr_items(self):\n815         elems = []\n816         obj = node_classes.List(parent=self._instance)\n817         for key, value in self._instance.items:\n818             elem = node_classes.Tuple(parent=obj)\n819             elem.postinit((key, value))\n820             elems.append(elem)\n821         obj.postinit(elts=elems)\n822 \n823         obj = objects.DictItems(obj)\n824         return self._generic_dict_attribute(obj, \"items\")\n825 \n826     @property\n827     def attr_keys(self):\n828         keys = [key for (key, _) in self._instance.items]\n829         obj = node_classes.List(parent=self._instance)\n830         obj.postinit(elts=keys)\n831 \n832         obj = objects.DictKeys(obj)\n833         return self._generic_dict_attribute(obj, \"keys\")\n834 \n835     @property\n836     def attr_values(self):\n837         values = [value for (_, value) in self._instance.items]\n838         obj = node_classes.List(parent=self._instance)\n839         obj.postinit(values)\n840 \n841         obj = objects.DictValues(obj)\n842         return self._generic_dict_attribute(obj, \"values\")\n843 \n844 \n845 class PropertyModel(ObjectModel):\n846     \"\"\"Model for a builtin property.\"\"\"\n847 \n848     def _init_function(self, name):\n849         function = nodes.FunctionDef(name=name, parent=self._instance)\n850 \n851         args = nodes.Arguments(parent=function)\n852         args.postinit(\n853             args=[],\n854             defaults=[],\n855             kwonlyargs=[],\n856             kw_defaults=[],\n857             annotations=[],\n858             posonlyargs=[],\n859             posonlyargs_annotations=[],\n860             kwonlyargs_annotations=[],\n861         )\n862 \n863         function.postinit(args=args, body=[])\n864         return function\n865 \n866     @property\n867     def attr_fget(self):\n868         func = self._instance\n869 \n870         class PropertyFuncAccessor(nodes.FunctionDef):\n871             def infer_call_result(\n872                 self, caller=None, context: InferenceContext | None = None\n873             ):\n874                 nonlocal func\n875                 if caller and len(caller.args) != 1:\n876                     raise InferenceError(\n877                         \"fget() needs a single argument\", target=self, context=context\n878                     )\n879 \n880                 yield from func.function.infer_call_result(\n881                     caller=caller, context=context\n882                 )\n883 \n884         property_accessor = PropertyFuncAccessor(name=\"fget\", parent=self._instance)\n885         property_accessor.postinit(args=func.args, body=func.body)\n886         return property_accessor\n887 \n888     @property\n889     def attr_fset(self):\n890         func = self._instance\n891 \n892         def find_setter(func: Property) -> astroid.FunctionDef | None:\n893             \"\"\"\n894             Given a property, find the corresponding setter function and returns it.\n895 \n896             :param func: property for which the setter has to be found\n897             :return: the setter function or None\n898             \"\"\"\n899             for target in [\n900                 t for t in func.parent.get_children() if t.name == func.function.name\n901             ]:\n902                 for dec_name in target.decoratornames():\n903                     if dec_name.endswith(func.function.name + \".setter\"):\n904                         return target\n905             return None\n906 \n907         func_setter = find_setter(func)\n908         if not func_setter:\n909             raise InferenceError(\n910                 f\"Unable to find the setter of property {func.function.name}\"\n911             )\n912 \n913         class PropertyFuncAccessor(nodes.FunctionDef):\n914             def infer_call_result(\n915                 self, caller=None, context: InferenceContext | None = None\n916             ):\n917                 nonlocal func_setter\n918                 if caller and len(caller.args) != 2:\n919                     raise InferenceError(\n920                         \"fset() needs two arguments\", target=self, context=context\n921                     )\n922                 yield from func_setter.infer_call_result(caller=caller, context=context)\n923 \n924         property_accessor = PropertyFuncAccessor(name=\"fset\", parent=self._instance)\n925         property_accessor.postinit(args=func_setter.args, body=func_setter.body)\n926         return property_accessor\n927 \n928     @property\n929     def attr_setter(self):\n930         return self._init_function(\"setter\")\n931 \n932     @property\n933     def attr_deleter(self):\n934         return self._init_function(\"deleter\")\n935 \n936     @property\n937     def attr_getter(self):\n938         return self._init_function(\"getter\")\n939 \n940     # pylint: enable=import-outside-toplevel\n941 \n[end of astroid/interpreter/objectmodel.py]\n[start of astroid/modutils.py]\n1 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n2 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"Python modules manipulation utility functions.\n6 \n7 :type PY_SOURCE_EXTS: tuple(str)\n8 :var PY_SOURCE_EXTS: list of possible python source file extension\n9 \n10 :type STD_LIB_DIRS: set of str\n11 :var STD_LIB_DIRS: directories where standard modules are located\n12 \n13 :type BUILTIN_MODULES: dict\n14 :var BUILTIN_MODULES: dictionary with builtin module names has key\n15 \"\"\"\n16 \n17 from __future__ import annotations\n18 \n19 import importlib\n20 import importlib.machinery\n21 import importlib.util\n22 import io\n23 import itertools\n24 import logging\n25 import os\n26 import sys\n27 import sysconfig\n28 import types\n29 import warnings\n30 from collections.abc import Callable, Iterable, Sequence\n31 from contextlib import redirect_stderr, redirect_stdout\n32 from functools import lru_cache\n33 from pathlib import Path\n34 \n35 from astroid.const import IS_JYTHON, IS_PYPY, PY310_PLUS\n36 from astroid.interpreter._import import spec, util\n37 \n38 if PY310_PLUS:\n39     from sys import stdlib_module_names\n40 else:\n41     from astroid._backport_stdlib_names import stdlib_module_names\n42 \n43 logger = logging.getLogger(__name__)\n44 \n45 \n46 if sys.platform.startswith(\"win\"):\n47     PY_SOURCE_EXTS = (\"py\", \"pyw\")\n48     PY_COMPILED_EXTS = (\"dll\", \"pyd\")\n49 else:\n50     PY_SOURCE_EXTS = (\"py\",)\n51     PY_COMPILED_EXTS = (\"so\",)\n52 \n53 \n54 # TODO: Adding `platstdlib` is a fix for a workaround in virtualenv. At some point we should\n55 # revisit whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1323.\n56 STD_LIB_DIRS = {sysconfig.get_path(\"stdlib\"), sysconfig.get_path(\"platstdlib\")}\n57 \n58 if os.name == \"nt\":\n59     STD_LIB_DIRS.add(os.path.join(sys.prefix, \"dlls\"))\n60     try:\n61         # real_prefix is defined when running inside virtual environments,\n62         # created with the **virtualenv** library.\n63         # Deprecated in virtualenv==16.7.9\n64         # See: https://github.com/pypa/virtualenv/issues/1622\n65         STD_LIB_DIRS.add(os.path.join(sys.real_prefix, \"dlls\"))  # type: ignore[attr-defined]\n66     except AttributeError:\n67         # sys.base_exec_prefix is always defined, but in a virtual environment\n68         # created with the stdlib **venv** module, it points to the original\n69         # installation, if the virtual env is activated.\n70         try:\n71             STD_LIB_DIRS.add(os.path.join(sys.base_exec_prefix, \"dlls\"))\n72         except AttributeError:\n73             pass\n74 \n75 if IS_PYPY and sys.version_info < (3, 8):\n76     # PyPy stores the stdlib in two places: sys.prefix/lib_pypy and sys.prefix/lib-python/3\n77     # sysconfig.get_path on PyPy returns the first, but without an underscore so we patch this manually.\n78     # Beginning with 3.8 the stdlib is only stored in: sys.prefix/pypy{py_version_short}\n79     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib_pypy\"))\n80     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"stdlib\")).parent / \"lib-python/3\"))\n81 \n82     # TODO: This is a fix for a workaround in virtualenv. At some point we should revisit\n83     # whether this is still necessary. See https://github.com/PyCQA/astroid/pull/1324.\n84     STD_LIB_DIRS.add(str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib_pypy\"))\n85     STD_LIB_DIRS.add(\n86         str(Path(sysconfig.get_path(\"platstdlib\")).parent / \"lib-python/3\")\n87     )\n88 \n89 if os.name == \"posix\":\n90     # Need the real prefix if we're in a virtualenv, otherwise\n91     # the usual one will do.\n92     # Deprecated in virtualenv==16.7.9\n93     # See: https://github.com/pypa/virtualenv/issues/1622\n94     try:\n95         prefix: str = sys.real_prefix  # type: ignore[attr-defined]\n96     except AttributeError:\n97         prefix = sys.prefix\n98 \n99     def _posix_path(path: str) -> str:\n100         base_python = \"python%d.%d\" % sys.version_info[:2]\n101         return os.path.join(prefix, path, base_python)\n102 \n103     STD_LIB_DIRS.add(_posix_path(\"lib\"))\n104     if sys.maxsize > 2**32:\n105         # This tries to fix a problem with /usr/lib64 builds,\n106         # where systems are running both 32-bit and 64-bit code\n107         # on the same machine, which reflects into the places where\n108         # standard library could be found. More details can be found\n109         # here http://bugs.python.org/issue1294959.\n110         # An easy reproducing case would be\n111         # https://github.com/PyCQA/pylint/issues/712#issuecomment-163178753\n112         STD_LIB_DIRS.add(_posix_path(\"lib64\"))\n113 \n114 EXT_LIB_DIRS = {sysconfig.get_path(\"purelib\"), sysconfig.get_path(\"platlib\")}\n115 BUILTIN_MODULES = dict.fromkeys(sys.builtin_module_names, True)\n116 \n117 \n118 class NoSourceFile(Exception):\n119     \"\"\"Exception raised when we are not able to get a python\n120     source file for a precompiled file.\n121     \"\"\"\n122 \n123 \n124 def _normalize_path(path: str) -> str:\n125     \"\"\"Resolve symlinks in path and convert to absolute path.\n126 \n127     Note that environment variables and ~ in the path need to be expanded in\n128     advance.\n129 \n130     This can be cached by using _cache_normalize_path.\n131     \"\"\"\n132     return os.path.normcase(os.path.realpath(path))\n133 \n134 \n135 def _path_from_filename(filename: str, is_jython: bool = IS_JYTHON) -> str:\n136     if not is_jython:\n137         return filename\n138     head, has_pyclass, _ = filename.partition(\"$py.class\")\n139     if has_pyclass:\n140         return head + \".py\"\n141     return filename\n142 \n143 \n144 def _handle_blacklist(\n145     blacklist: Sequence[str], dirnames: list[str], filenames: list[str]\n146 ) -> None:\n147     \"\"\"Remove files/directories in the black list.\n148 \n149     dirnames/filenames are usually from os.walk\n150     \"\"\"\n151     for norecurs in blacklist:\n152         if norecurs in dirnames:\n153             dirnames.remove(norecurs)\n154         elif norecurs in filenames:\n155             filenames.remove(norecurs)\n156 \n157 \n158 @lru_cache()\n159 def _cache_normalize_path_(path: str) -> str:\n160     return _normalize_path(path)\n161 \n162 \n163 def _cache_normalize_path(path: str) -> str:\n164     \"\"\"Normalize path with caching.\"\"\"\n165     # _module_file calls abspath on every path in sys.path every time it's\n166     # called; on a larger codebase this easily adds up to half a second just\n167     # assembling path components. This cache alleviates that.\n168     if not path:  # don't cache result for ''\n169         return _normalize_path(path)\n170     return _cache_normalize_path_(path)\n171 \n172 \n173 def load_module_from_name(dotted_name: str) -> types.ModuleType:\n174     \"\"\"Load a Python module from its name.\n175 \n176     :type dotted_name: str\n177     :param dotted_name: python name of a module or package\n178 \n179     :raise ImportError: if the module or package is not found\n180 \n181     :rtype: module\n182     :return: the loaded module\n183     \"\"\"\n184     try:\n185         return sys.modules[dotted_name]\n186     except KeyError:\n187         pass\n188 \n189     # Capture and log anything emitted during import to avoid\n190     # contaminating JSON reports in pylint\n191     with redirect_stderr(io.StringIO()) as stderr, redirect_stdout(\n192         io.StringIO()\n193     ) as stdout:\n194         module = importlib.import_module(dotted_name)\n195 \n196     stderr_value = stderr.getvalue()\n197     if stderr_value:\n198         logger.error(\n199             \"Captured stderr while importing %s:\\n%s\", dotted_name, stderr_value\n200         )\n201     stdout_value = stdout.getvalue()\n202     if stdout_value:\n203         logger.info(\n204             \"Captured stdout while importing %s:\\n%s\", dotted_name, stdout_value\n205         )\n206 \n207     return module\n208 \n209 \n210 def load_module_from_modpath(parts: Sequence[str]) -> types.ModuleType:\n211     \"\"\"Load a python module from its split name.\n212 \n213     :param parts:\n214       python name of a module or package split on '.'\n215 \n216     :raise ImportError: if the module or package is not found\n217 \n218     :return: the loaded module\n219     \"\"\"\n220     return load_module_from_name(\".\".join(parts))\n221 \n222 \n223 def load_module_from_file(filepath: str) -> types.ModuleType:\n224     \"\"\"Load a Python module from it's path.\n225 \n226     :type filepath: str\n227     :param filepath: path to the python module or package\n228 \n229     :raise ImportError: if the module or package is not found\n230 \n231     :rtype: module\n232     :return: the loaded module\n233     \"\"\"\n234     modpath = modpath_from_file(filepath)\n235     return load_module_from_modpath(modpath)\n236 \n237 \n238 def check_modpath_has_init(path: str, mod_path: list[str]) -> bool:\n239     \"\"\"Check there are some __init__.py all along the way.\"\"\"\n240     modpath: list[str] = []\n241     for part in mod_path:\n242         modpath.append(part)\n243         path = os.path.join(path, part)\n244         if not _has_init(path):\n245             old_namespace = util.is_namespace(\".\".join(modpath))\n246             if not old_namespace:\n247                 return False\n248     return True\n249 \n250 \n251 def _get_relative_base_path(filename: str, path_to_check: str) -> list[str] | None:\n252     \"\"\"Extracts the relative mod path of the file to import from.\n253 \n254     Check if a file is within the passed in path and if so, returns the\n255     relative mod path from the one passed in.\n256 \n257     If the filename is no in path_to_check, returns None\n258 \n259     Note this function will look for both abs and realpath of the file,\n260     this allows to find the relative base path even if the file is a\n261     symlink of a file in the passed in path\n262 \n263     Examples:\n264         _get_relative_base_path(\"/a/b/c/d.py\", \"/a/b\") ->  [\"c\",\"d\"]\n265         _get_relative_base_path(\"/a/b/c/d.py\", \"/dev\") ->  None\n266     \"\"\"\n267     importable_path = None\n268     path_to_check = os.path.normcase(path_to_check)\n269     abs_filename = os.path.abspath(filename)\n270     if os.path.normcase(abs_filename).startswith(path_to_check):\n271         importable_path = abs_filename\n272 \n273     real_filename = os.path.realpath(filename)\n274     if os.path.normcase(real_filename).startswith(path_to_check):\n275         importable_path = real_filename\n276 \n277     # if \"var\" in path_to_check:\n278     #     breakpoint()\n279 \n280     if importable_path:\n281         base_path = os.path.splitext(importable_path)[0]\n282         relative_base_path = base_path[len(path_to_check) :]\n283         return [pkg for pkg in relative_base_path.split(os.sep) if pkg]\n284 \n285     return None\n286 \n287 \n288 def modpath_from_file_with_callback(\n289     filename: str,\n290     path: Sequence[str] | None = None,\n291     is_package_cb: Callable[[str, list[str]], bool] | None = None,\n292 ) -> list[str]:\n293     filename = os.path.expanduser(_path_from_filename(filename))\n294     paths_to_check = sys.path.copy()\n295     if path:\n296         paths_to_check += path\n297     for pathname in itertools.chain(\n298         paths_to_check, map(_cache_normalize_path, paths_to_check)\n299     ):\n300         if not pathname:\n301             continue\n302         modpath = _get_relative_base_path(filename, pathname)\n303         if not modpath:\n304             continue\n305         assert is_package_cb is not None\n306         if is_package_cb(pathname, modpath[:-1]):\n307             return modpath\n308 \n309     raise ImportError(\n310         \"Unable to find module for {} in {}\".format(filename, \", \\n\".join(sys.path))\n311     )\n312 \n313 \n314 def modpath_from_file(filename: str, path: Sequence[str] | None = None) -> list[str]:\n315     \"\"\"Get the corresponding split module's name from a filename.\n316 \n317     This function will return the name of a module or package split on `.`.\n318 \n319     :type filename: str\n320     :param filename: file's path for which we want the module's name\n321 \n322     :type Optional[List[str]] path:\n323       Optional list of path where the module or package should be\n324       searched (use sys.path if nothing or None is given)\n325 \n326     :raise ImportError:\n327       if the corresponding module's name has not been found\n328 \n329     :rtype: list(str)\n330     :return: the corresponding split module's name\n331     \"\"\"\n332     return modpath_from_file_with_callback(filename, path, check_modpath_has_init)\n333 \n334 \n335 def file_from_modpath(\n336     modpath: list[str],\n337     path: Sequence[str] | None = None,\n338     context_file: str | None = None,\n339 ) -> str | None:\n340     return file_info_from_modpath(modpath, path, context_file).location\n341 \n342 \n343 def file_info_from_modpath(\n344     modpath: list[str],\n345     path: Sequence[str] | None = None,\n346     context_file: str | None = None,\n347 ) -> spec.ModuleSpec:\n348     \"\"\"Given a mod path (i.e. split module / package name), return the\n349     corresponding file.\n350 \n351     Giving priority to source file over precompiled file if it exists.\n352 \n353     :param modpath:\n354       split module's name (i.e name of a module or package split\n355       on '.')\n356       (this means explicit relative imports that start with dots have\n357       empty strings in this list!)\n358 \n359     :param path:\n360       optional list of path where the module or package should be\n361       searched (use sys.path if nothing or None is given)\n362 \n363     :param context_file:\n364       context file to consider, necessary if the identifier has been\n365       introduced using a relative import unresolvable in the actual\n366       context (i.e. modutils)\n367 \n368     :raise ImportError: if there is no such module in the directory\n369 \n370     :return:\n371       the path to the module's file or None if it's an integrated\n372       builtin module such as 'sys'\n373     \"\"\"\n374     if context_file is not None:\n375         context: str | None = os.path.dirname(context_file)\n376     else:\n377         context = context_file\n378     if modpath[0] == \"xml\":\n379         # handle _xmlplus\n380         try:\n381             return _spec_from_modpath([\"_xmlplus\"] + modpath[1:], path, context)\n382         except ImportError:\n383             return _spec_from_modpath(modpath, path, context)\n384     elif modpath == [\"os\", \"path\"]:\n385         # FIXME: currently ignoring search_path...\n386         return spec.ModuleSpec(\n387             name=\"os.path\",\n388             location=os.path.__file__,\n389             type=spec.ModuleType.PY_SOURCE,\n390         )\n391     return _spec_from_modpath(modpath, path, context)\n392 \n393 \n394 def get_module_part(dotted_name: str, context_file: str | None = None) -> str:\n395     \"\"\"Given a dotted name return the module part of the name :\n396 \n397     >>> get_module_part('astroid.as_string.dump')\n398     'astroid.as_string'\n399 \n400     :param dotted_name: full name of the identifier we are interested in\n401 \n402     :param context_file:\n403       context file to consider, necessary if the identifier has been\n404       introduced using a relative import unresolvable in the actual\n405       context (i.e. modutils)\n406 \n407     :raise ImportError: if there is no such module in the directory\n408 \n409     :return:\n410       the module part of the name or None if we have not been able at\n411       all to import the given name\n412 \n413     XXX: deprecated, since it doesn't handle package precedence over module\n414     (see #10066)\n415     \"\"\"\n416     # os.path trick\n417     if dotted_name.startswith(\"os.path\"):\n418         return \"os.path\"\n419     parts = dotted_name.split(\".\")\n420     if context_file is not None:\n421         # first check for builtin module which won't be considered latter\n422         # in that case (path != None)\n423         if parts[0] in BUILTIN_MODULES:\n424             if len(parts) > 2:\n425                 raise ImportError(dotted_name)\n426             return parts[0]\n427         # don't use += or insert, we want a new list to be created !\n428     path: list[str] | None = None\n429     starti = 0\n430     if parts[0] == \"\":\n431         assert (\n432             context_file is not None\n433         ), \"explicit relative import, but no context_file?\"\n434         path = []  # prevent resolving the import non-relatively\n435         starti = 1\n436     # for all further dots: change context\n437     while starti < len(parts) and parts[starti] == \"\":\n438         starti += 1\n439         assert (\n440             context_file is not None\n441         ), \"explicit relative import, but no context_file?\"\n442         context_file = os.path.dirname(context_file)\n443     for i in range(starti, len(parts)):\n444         try:\n445             file_from_modpath(\n446                 parts[starti : i + 1], path=path, context_file=context_file\n447             )\n448         except ImportError:\n449             if i < max(1, len(parts) - 2):\n450                 raise\n451             return \".\".join(parts[:i])\n452     return dotted_name\n453 \n454 \n455 def get_module_files(\n456     src_directory: str, blacklist: Sequence[str], list_all: bool = False\n457 ) -> list[str]:\n458     \"\"\"Given a package directory return a list of all available python\n459     module's files in the package and its subpackages.\n460 \n461     :param src_directory:\n462       path of the directory corresponding to the package\n463 \n464     :param blacklist: iterable\n465       list of files or directories to ignore.\n466 \n467     :param list_all:\n468         get files from all paths, including ones without __init__.py\n469 \n470     :return:\n471       the list of all available python module's files in the package and\n472       its subpackages\n473     \"\"\"\n474     files: list[str] = []\n475     for directory, dirnames, filenames in os.walk(src_directory):\n476         if directory in blacklist:\n477             continue\n478         _handle_blacklist(blacklist, dirnames, filenames)\n479         # check for __init__.py\n480         if not list_all and \"__init__.py\" not in filenames:\n481             dirnames[:] = ()\n482             continue\n483         for filename in filenames:\n484             if _is_python_file(filename):\n485                 src = os.path.join(directory, filename)\n486                 files.append(src)\n487     return files\n488 \n489 \n490 def get_source_file(filename: str, include_no_ext: bool = False) -> str:\n491     \"\"\"Given a python module's file name return the matching source file\n492     name (the filename will be returned identically if it's already an.\n493 \n494     absolute path to a python source file...)\n495 \n496     :param filename: python module's file name\n497 \n498     :raise NoSourceFile: if no source file exists on the file system\n499 \n500     :return: the absolute path of the source file if it exists\n501     \"\"\"\n502     filename = os.path.abspath(_path_from_filename(filename))\n503     base, orig_ext = os.path.splitext(filename)\n504     for ext in PY_SOURCE_EXTS:\n505         source_path = f\"{base}.{ext}\"\n506         if os.path.exists(source_path):\n507             return source_path\n508     if include_no_ext and not orig_ext and os.path.exists(base):\n509         return base\n510     raise NoSourceFile(filename)\n511 \n512 \n513 def is_python_source(filename: str | None) -> bool:\n514     \"\"\"Return: True if the filename is a python source file.\"\"\"\n515     if not filename:\n516         return False\n517     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n518 \n519 \n520 def is_stdlib_module(modname: str) -> bool:\n521     \"\"\"Return: True if the modname is in the standard library\"\"\"\n522     return modname.split(\".\")[0] in stdlib_module_names\n523 \n524 \n525 def module_in_path(modname: str, path: str | Iterable[str]) -> bool:\n526     \"\"\"Try to determine if a module is imported from one of the specified paths\n527 \n528     :param modname: name of the module\n529 \n530     :param path: paths to consider\n531 \n532     :return:\n533       true if the module:\n534       - is located on the path listed in one of the directory in `paths`\n535     \"\"\"\n536 \n537     modname = modname.split(\".\")[0]\n538     try:\n539         filename = file_from_modpath([modname])\n540     except ImportError:\n541         # Import failed, we can't check path if we don't know it\n542         return False\n543 \n544     if filename is None:\n545         # No filename likely means it's compiled in, or potentially a namespace\n546         return False\n547     filename = _normalize_path(filename)\n548 \n549     if isinstance(path, str):\n550         return filename.startswith(_cache_normalize_path(path))\n551 \n552     return any(filename.startswith(_cache_normalize_path(entry)) for entry in path)\n553 \n554 \n555 def is_standard_module(modname: str, std_path: Iterable[str] | None = None) -> bool:\n556     \"\"\"Try to guess if a module is a standard python module (by default,\n557     see `std_path` parameter's description).\n558 \n559     :param modname: name of the module we are interested in\n560 \n561     :param std_path: list of path considered has standard\n562 \n563     :return:\n564       true if the module:\n565       - is located on the path listed in one of the directory in `std_path`\n566       - is a built-in module\n567     \"\"\"\n568     warnings.warn(\n569         \"is_standard_module() is deprecated. Use, is_stdlib_module() or module_in_path() instead\",\n570         DeprecationWarning,\n571         stacklevel=2,\n572     )\n573 \n574     modname = modname.split(\".\")[0]\n575     try:\n576         filename = file_from_modpath([modname])\n577     except ImportError:\n578         # import failed, i'm probably not so wrong by supposing it's\n579         # not standard...\n580         return False\n581     # modules which are not living in a file are considered standard\n582     # (sys and __builtin__ for instance)\n583     if filename is None:\n584         # we assume there are no namespaces in stdlib\n585         return not util.is_namespace(modname)\n586     filename = _normalize_path(filename)\n587     for path in EXT_LIB_DIRS:\n588         if filename.startswith(_cache_normalize_path(path)):\n589             return False\n590     if std_path is None:\n591         std_path = STD_LIB_DIRS\n592 \n593     return any(filename.startswith(_cache_normalize_path(path)) for path in std_path)\n594 \n595 \n596 def is_relative(modname: str, from_file: str) -> bool:\n597     \"\"\"Return true if the given module name is relative to the given\n598     file name.\n599 \n600     :param modname: name of the module we are interested in\n601 \n602     :param from_file:\n603       path of the module from which modname has been imported\n604 \n605     :return:\n606       true if the module has been imported relatively to `from_file`\n607     \"\"\"\n608     if not os.path.isdir(from_file):\n609         from_file = os.path.dirname(from_file)\n610     if from_file in sys.path:\n611         return False\n612     return bool(\n613         importlib.machinery.PathFinder.find_spec(\n614             modname.split(\".\", maxsplit=1)[0], [from_file]\n615         )\n616     )\n617 \n618 \n619 # internal only functions #####################################################\n620 \n621 \n622 def _spec_from_modpath(\n623     modpath: list[str],\n624     path: Sequence[str] | None = None,\n625     context: str | None = None,\n626 ) -> spec.ModuleSpec:\n627     \"\"\"Given a mod path (i.e. split module / package name), return the\n628     corresponding spec.\n629 \n630     this function is used internally, see `file_from_modpath`'s\n631     documentation for more information\n632     \"\"\"\n633     assert modpath\n634     location = None\n635     if context is not None:\n636         try:\n637             found_spec = spec.find_spec(modpath, [context])\n638             location = found_spec.location\n639         except ImportError:\n640             found_spec = spec.find_spec(modpath, path)\n641             location = found_spec.location\n642     else:\n643         found_spec = spec.find_spec(modpath, path)\n644     if found_spec.type == spec.ModuleType.PY_COMPILED:\n645         try:\n646             assert found_spec.location is not None\n647             location = get_source_file(found_spec.location)\n648             return found_spec._replace(\n649                 location=location, type=spec.ModuleType.PY_SOURCE\n650             )\n651         except NoSourceFile:\n652             return found_spec._replace(location=location)\n653     elif found_spec.type == spec.ModuleType.C_BUILTIN:\n654         # integrated builtin module\n655         return found_spec._replace(location=None)\n656     elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n657         assert found_spec.location is not None\n658         location = _has_init(found_spec.location)\n659         return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n660     return found_spec\n661 \n662 \n663 def _is_python_file(filename: str) -> bool:\n664     \"\"\"Return true if the given filename should be considered as a python file.\n665 \n666     .pyc and .pyo are ignored\n667     \"\"\"\n668     return filename.endswith((\".py\", \".so\", \".pyd\", \".pyw\"))\n669 \n670 \n671 def _has_init(directory: str) -> str | None:\n672     \"\"\"If the given directory has a valid __init__ file, return its path,\n673     else return None.\n674     \"\"\"\n675     mod_or_pack = os.path.join(directory, \"__init__\")\n676     for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n677         if os.path.exists(mod_or_pack + \".\" + ext):\n678             return mod_or_pack + \".\" + ext\n679     return None\n680 \n681 \n682 def is_namespace(specobj: spec.ModuleSpec) -> bool:\n683     return specobj.type == spec.ModuleType.PY_NAMESPACE\n684 \n685 \n686 def is_directory(specobj: spec.ModuleSpec) -> bool:\n687     return specobj.type == spec.ModuleType.PKG_DIRECTORY\n688 \n689 \n690 def is_module_name_part_of_extension_package_whitelist(\n691     module_name: str, package_whitelist: set[str]\n692 ) -> bool:\n693     \"\"\"\n694     Returns True if one part of the module name is in the package whitelist.\n695 \n696     >>> is_module_name_part_of_extension_package_whitelist('numpy.core.umath', {'numpy'})\n697     True\n698     \"\"\"\n699     parts = module_name.split(\".\")\n700     return any(\n701         \".\".join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)\n702     )\n703 \n[end of astroid/modutils.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/astroid/brain/brain_typing.py\n+++ b/astroid/brain/brain_typing.py\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n```", "model_patch": "--- a/astroid/brain/brain_typing.py\n+++ b/astroid/brain/brain_typing.py\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n@@ -1,6 +1,7 @@\n # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n # Copyright (c) https://github.com/PyCQA/astroid/blob/main/CONTRIBUTORS.txt\n \n+from collections.abc import Mapping\n from __future__ import annotations\n \n import sys\n"}
{"instance_id": "pylint-dev__astroid-1262", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n``nodes.Module`` don't have a ``end_lineno`` and ``end_col_offset``\n### Steps to reproduce\r\n\r\n```python\r\nimport astroid\r\n\r\ncode = \"\"\"\r\n    print(\"a module\")\r\n    \"\"\"\r\n\r\nmodule = astroid.parse(code)\r\nprint(module.end_lineno)\r\nprint(module.end_col_offset)\r\n```\r\n\r\n### Current behavior\r\n\r\n`AttributeError` on both of the last lines.\r\n\r\n### Expected behavior\r\n\r\n@cdce8p Let me know if I misunderstood you, but I thought we wanted these to be accessible on all nodes, just initialised as `None`.\r\nIf that was not the case, I would make the case to do so as it allows you to do `node.end_lineno` without running in to `AttributeError`'s.\r\n\r\n### Version\r\n\r\nLatest `main`.\r\n\n\n</issue>\n<code>\n[start of README.rst]\n1 Astroid\n2 =======\n3 \n4 .. image:: https://coveralls.io/repos/github/PyCQA/astroid/badge.svg?branch=main\n5     :target: https://coveralls.io/github/PyCQA/astroid?branch=main\n6     :alt: Coverage badge from coveralls.io\n7 \n8 .. image:: https://readthedocs.org/projects/astroid/badge/?version=latest\n9     :target: http://astroid.readthedocs.io/en/latest/?badge=latest\n10     :alt: Documentation Status\n11 \n12 .. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n13     :target: https://github.com/ambv/black\n14 \n15 .. image:: https://results.pre-commit.ci/badge/github/PyCQA/astroid/main.svg\n16    :target: https://results.pre-commit.ci/latest/github/PyCQA/astroid/main\n17    :alt: pre-commit.ci status\n18 \n19 .. |tidelift_logo| image:: https://raw.githubusercontent.com/PyCQA/astroid/main/doc/media/Tidelift_Logos_RGB_Tidelift_Shorthand_On-White.png\n20    :width: 75\n21    :height: 60\n22    :alt: Tidelift\n23 \n24 .. list-table::\n25    :widths: 10 100\n26 \n27    * - |tidelift_logo|\n28      - Professional support for astroid is available as part of the\n29        `Tidelift Subscription`_.  Tidelift gives software development teams a single source for\n30        purchasing and maintaining their software, with professional grade assurances\n31        from the experts who know it best, while seamlessly integrating with existing\n32        tools.\n33 \n34 .. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-astroid?utm_source=pypi-astroid&utm_medium=referral&utm_campaign=readme\n35 \n36 \n37 \n38 What's this?\n39 ------------\n40 \n41 The aim of this module is to provide a common base representation of\n42 python source code. It is currently the library powering pylint's capabilities.\n43 \n44 It provides a compatible representation which comes from the `_ast`\n45 module.  It rebuilds the tree generated by the builtin _ast module by\n46 recursively walking down the AST and building an extended ast. The new\n47 node classes have additional methods and attributes for different\n48 usages. They include some support for static inference and local name\n49 scopes. Furthermore, astroid can also build partial trees by inspecting living\n50 objects.\n51 \n52 \n53 Installation\n54 ------------\n55 \n56 Extract the tarball, jump into the created directory and run::\n57 \n58     pip install .\n59 \n60 \n61 If you want to do an editable installation, you can run::\n62 \n63     pip install -e .\n64 \n65 \n66 If you have any questions, please mail the code-quality@python.org\n67 mailing list for support. See\n68 http://mail.python.org/mailman/listinfo/code-quality for subscription\n69 information and archives.\n70 \n71 Documentation\n72 -------------\n73 http://astroid.readthedocs.io/en/latest/\n74 \n75 \n76 Python Versions\n77 ---------------\n78 \n79 astroid 2.0 is currently available for Python 3 only. If you want Python 2\n80 support, use an older version of astroid (though note that these versions\n81 are no longer supported).\n82 \n83 Test\n84 ----\n85 \n86 Tests are in the 'test' subdirectory. To launch the whole tests suite, you can use\n87 either `tox` or `pytest`::\n88 \n89     tox\n90     pytest astroid\n91 \n[end of README.rst]\n[start of astroid/builder.py]\n1 # Copyright (c) 2006-2011, 2013-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n2 # Copyright (c) 2013 Phil Schaf <flying-sheep@web.de>\n3 # Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n4 # Copyright (c) 2014-2015 Google, Inc.\n5 # Copyright (c) 2014 Alexander Presnyakov <flagist0@gmail.com>\n6 # Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n7 # Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n8 # Copyright (c) 2017 \u0141ukasz Rogalski <rogalski.91@gmail.com>\n9 # Copyright (c) 2018 Anthony Sottile <asottile@umich.edu>\n10 # Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n11 # Copyright (c) 2021 Dani\u00ebl van Noord <13665637+DanielNoord@users.noreply.github.com>\n12 # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n13 # Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n14 # Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n15 \n16 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n17 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n18 \n19 \"\"\"The AstroidBuilder makes astroid from living object and / or from _ast\n20 \n21 The builder is not thread safe and can't be used to parse different sources\n22 at the same time.\n23 \"\"\"\n24 import os\n25 import textwrap\n26 import types\n27 from tokenize import detect_encoding\n28 from typing import List, Union\n29 \n30 from astroid import bases, modutils, nodes, raw_building, rebuilder, util\n31 from astroid._ast import get_parser_module\n32 from astroid.exceptions import AstroidBuildingError, AstroidSyntaxError, InferenceError\n33 from astroid.manager import AstroidManager\n34 from astroid.nodes.node_classes import NodeNG\n35 \n36 objects = util.lazy_import(\"objects\")\n37 \n38 # The name of the transient function that is used to\n39 # wrap expressions to be extracted when calling\n40 # extract_node.\n41 _TRANSIENT_FUNCTION = \"__\"\n42 \n43 # The comment used to select a statement to be extracted\n44 # when calling extract_node.\n45 _STATEMENT_SELECTOR = \"#@\"\n46 MISPLACED_TYPE_ANNOTATION_ERROR = \"misplaced type annotation\"\n47 \n48 \n49 def open_source_file(filename):\n50     # pylint: disable=consider-using-with\n51     with open(filename, \"rb\") as byte_stream:\n52         encoding = detect_encoding(byte_stream.readline)[0]\n53     stream = open(filename, newline=None, encoding=encoding)\n54     data = stream.read()\n55     return stream, encoding, data\n56 \n57 \n58 def _can_assign_attr(node, attrname):\n59     try:\n60         slots = node.slots()\n61     except NotImplementedError:\n62         pass\n63     else:\n64         if slots and attrname not in {slot.value for slot in slots}:\n65             return False\n66     return node.qname() != \"builtins.object\"\n67 \n68 \n69 class AstroidBuilder(raw_building.InspectBuilder):\n70     \"\"\"Class for building an astroid tree from source code or from a live module.\n71 \n72     The param *manager* specifies the manager class which should be used.\n73     If no manager is given, then the default one will be used. The\n74     param *apply_transforms* determines if the transforms should be\n75     applied after the tree was built from source or from a live object,\n76     by default being True.\n77     \"\"\"\n78 \n79     # pylint: disable=redefined-outer-name\n80     def __init__(self, manager=None, apply_transforms=True):\n81         super().__init__(manager)\n82         self._apply_transforms = apply_transforms\n83 \n84     def module_build(\n85         self, module: types.ModuleType, modname: str = None\n86     ) -> nodes.Module:\n87         \"\"\"Build an astroid from a living module instance.\"\"\"\n88         node = None\n89         path = getattr(module, \"__file__\", None)\n90         if path is not None:\n91             path_, ext = os.path.splitext(modutils._path_from_filename(path))\n92             if ext in {\".py\", \".pyc\", \".pyo\"} and os.path.exists(path_ + \".py\"):\n93                 node = self.file_build(path_ + \".py\", modname)\n94         if node is None:\n95             # this is a built-in module\n96             # get a partial representation by introspection\n97             node = self.inspect_build(module, modname=modname, path=path)\n98             if self._apply_transforms:\n99                 # We have to handle transformation by ourselves since the\n100                 # rebuilder isn't called for builtin nodes\n101                 node = self._manager.visit_transforms(node)\n102         return node\n103 \n104     def file_build(self, path, modname=None):\n105         \"\"\"Build astroid from a source code file (i.e. from an ast)\n106 \n107         *path* is expected to be a python source file\n108         \"\"\"\n109         try:\n110             stream, encoding, data = open_source_file(path)\n111         except OSError as exc:\n112             raise AstroidBuildingError(\n113                 \"Unable to load file {path}:\\n{error}\",\n114                 modname=modname,\n115                 path=path,\n116                 error=exc,\n117             ) from exc\n118         except (SyntaxError, LookupError) as exc:\n119             raise AstroidSyntaxError(\n120                 \"Python 3 encoding specification error or unknown encoding:\\n\"\n121                 \"{error}\",\n122                 modname=modname,\n123                 path=path,\n124                 error=exc,\n125             ) from exc\n126         except UnicodeError as exc:  # wrong encoding\n127             # detect_encoding returns utf-8 if no encoding specified\n128             raise AstroidBuildingError(\n129                 \"Wrong or no encoding specified for {filename}.\", filename=path\n130             ) from exc\n131         with stream:\n132             # get module name if necessary\n133             if modname is None:\n134                 try:\n135                     modname = \".\".join(modutils.modpath_from_file(path))\n136                 except ImportError:\n137                     modname = os.path.splitext(os.path.basename(path))[0]\n138             # build astroid representation\n139             module = self._data_build(data, modname, path)\n140             return self._post_build(module, encoding)\n141 \n142     def string_build(self, data, modname=\"\", path=None):\n143         \"\"\"Build astroid from source code string.\"\"\"\n144         module = self._data_build(data, modname, path)\n145         module.file_bytes = data.encode(\"utf-8\")\n146         return self._post_build(module, \"utf-8\")\n147 \n148     def _post_build(self, module, encoding):\n149         \"\"\"Handles encoding and delayed nodes after a module has been built\"\"\"\n150         module.file_encoding = encoding\n151         self._manager.cache_module(module)\n152         # post tree building steps after we stored the module in the cache:\n153         for from_node in module._import_from_nodes:\n154             if from_node.modname == \"__future__\":\n155                 for symbol, _ in from_node.names:\n156                     module.future_imports.add(symbol)\n157             self.add_from_names_to_locals(from_node)\n158         # handle delayed assattr nodes\n159         for delayed in module._delayed_assattr:\n160             self.delayed_assattr(delayed)\n161 \n162         # Visit the transforms\n163         if self._apply_transforms:\n164             module = self._manager.visit_transforms(module)\n165         return module\n166 \n167     def _data_build(self, data, modname, path):\n168         \"\"\"Build tree node from data and add some information\"\"\"\n169         try:\n170             node, parser_module = _parse_string(data, type_comments=True)\n171         except (TypeError, ValueError, SyntaxError) as exc:\n172             raise AstroidSyntaxError(\n173                 \"Parsing Python code failed:\\n{error}\",\n174                 source=data,\n175                 modname=modname,\n176                 path=path,\n177                 error=exc,\n178             ) from exc\n179 \n180         if path is not None:\n181             node_file = os.path.abspath(path)\n182         else:\n183             node_file = \"<?>\"\n184         if modname.endswith(\".__init__\"):\n185             modname = modname[:-9]\n186             package = True\n187         else:\n188             package = (\n189                 path is not None\n190                 and os.path.splitext(os.path.basename(path))[0] == \"__init__\"\n191             )\n192         builder = rebuilder.TreeRebuilder(self._manager, parser_module)\n193         module = builder.visit_module(node, modname, node_file, package)\n194         module._import_from_nodes = builder._import_from_nodes\n195         module._delayed_assattr = builder._delayed_assattr\n196         return module\n197 \n198     def add_from_names_to_locals(self, node):\n199         \"\"\"Store imported names to the locals\n200 \n201         Resort the locals if coming from a delayed node\n202         \"\"\"\n203 \n204         def _key_func(node):\n205             return node.fromlineno\n206 \n207         def sort_locals(my_list):\n208             my_list.sort(key=_key_func)\n209 \n210         for (name, asname) in node.names:\n211             if name == \"*\":\n212                 try:\n213                     imported = node.do_import_module()\n214                 except AstroidBuildingError:\n215                     continue\n216                 for name in imported.public_names():\n217                     node.parent.set_local(name, node)\n218                     sort_locals(node.parent.scope().locals[name])\n219             else:\n220                 node.parent.set_local(asname or name, node)\n221                 sort_locals(node.parent.scope().locals[asname or name])\n222 \n223     def delayed_assattr(self, node):\n224         \"\"\"Visit a AssAttr node\n225 \n226         This adds name to locals and handle members definition.\n227         \"\"\"\n228         try:\n229             frame = node.frame()\n230             for inferred in node.expr.infer():\n231                 if inferred is util.Uninferable:\n232                     continue\n233                 try:\n234                     cls = inferred.__class__\n235                     if cls is bases.Instance or cls is objects.ExceptionInstance:\n236                         inferred = inferred._proxied\n237                         iattrs = inferred.instance_attrs\n238                         if not _can_assign_attr(inferred, node.attrname):\n239                             continue\n240                     elif isinstance(inferred, bases.Instance):\n241                         # Const, Tuple or other containers that inherit from\n242                         # `Instance`\n243                         continue\n244                     elif inferred.is_function:\n245                         iattrs = inferred.instance_attrs\n246                     else:\n247                         iattrs = inferred.locals\n248                 except AttributeError:\n249                     # XXX log error\n250                     continue\n251                 values = iattrs.setdefault(node.attrname, [])\n252                 if node in values:\n253                     continue\n254                 # get assign in __init__ first XXX useful ?\n255                 if (\n256                     frame.name == \"__init__\"\n257                     and values\n258                     and values[0].frame().name != \"__init__\"\n259                 ):\n260                     values.insert(0, node)\n261                 else:\n262                     values.append(node)\n263         except InferenceError:\n264             pass\n265 \n266 \n267 def build_namespace_package_module(name: str, path: List[str]) -> nodes.Module:\n268     return nodes.Module(name, doc=\"\", path=path, package=True)\n269 \n270 \n271 def parse(code, module_name=\"\", path=None, apply_transforms=True):\n272     \"\"\"Parses a source string in order to obtain an astroid AST from it\n273 \n274     :param str code: The code for the module.\n275     :param str module_name: The name for the module, if any\n276     :param str path: The path for the module\n277     :param bool apply_transforms:\n278         Apply the transforms for the give code. Use it if you\n279         don't want the default transforms to be applied.\n280     \"\"\"\n281     code = textwrap.dedent(code)\n282     builder = AstroidBuilder(\n283         manager=AstroidManager(), apply_transforms=apply_transforms\n284     )\n285     return builder.string_build(code, modname=module_name, path=path)\n286 \n287 \n288 def _extract_expressions(node):\n289     \"\"\"Find expressions in a call to _TRANSIENT_FUNCTION and extract them.\n290 \n291     The function walks the AST recursively to search for expressions that\n292     are wrapped into a call to _TRANSIENT_FUNCTION. If it finds such an\n293     expression, it completely removes the function call node from the tree,\n294     replacing it by the wrapped expression inside the parent.\n295 \n296     :param node: An astroid node.\n297     :type node:  astroid.bases.NodeNG\n298     :yields: The sequence of wrapped expressions on the modified tree\n299     expression can be found.\n300     \"\"\"\n301     if (\n302         isinstance(node, nodes.Call)\n303         and isinstance(node.func, nodes.Name)\n304         and node.func.name == _TRANSIENT_FUNCTION\n305     ):\n306         real_expr = node.args[0]\n307         real_expr.parent = node.parent\n308         # Search for node in all _astng_fields (the fields checked when\n309         # get_children is called) of its parent. Some of those fields may\n310         # be lists or tuples, in which case the elements need to be checked.\n311         # When we find it, replace it by real_expr, so that the AST looks\n312         # like no call to _TRANSIENT_FUNCTION ever took place.\n313         for name in node.parent._astroid_fields:\n314             child = getattr(node.parent, name)\n315             if isinstance(child, (list, tuple)):\n316                 for idx, compound_child in enumerate(child):\n317                     if compound_child is node:\n318                         child[idx] = real_expr\n319             elif child is node:\n320                 setattr(node.parent, name, real_expr)\n321         yield real_expr\n322     else:\n323         for child in node.get_children():\n324             yield from _extract_expressions(child)\n325 \n326 \n327 def _find_statement_by_line(node, line):\n328     \"\"\"Extracts the statement on a specific line from an AST.\n329 \n330     If the line number of node matches line, it will be returned;\n331     otherwise its children are iterated and the function is called\n332     recursively.\n333 \n334     :param node: An astroid node.\n335     :type node: astroid.bases.NodeNG\n336     :param line: The line number of the statement to extract.\n337     :type line: int\n338     :returns: The statement on the line, or None if no statement for the line\n339       can be found.\n340     :rtype:  astroid.bases.NodeNG or None\n341     \"\"\"\n342     if isinstance(node, (nodes.ClassDef, nodes.FunctionDef, nodes.MatchCase)):\n343         # This is an inaccuracy in the AST: the nodes that can be\n344         # decorated do not carry explicit information on which line\n345         # the actual definition (class/def), but .fromline seems to\n346         # be close enough.\n347         node_line = node.fromlineno\n348     else:\n349         node_line = node.lineno\n350 \n351     if node_line == line:\n352         return node\n353 \n354     for child in node.get_children():\n355         result = _find_statement_by_line(child, line)\n356         if result:\n357             return result\n358 \n359     return None\n360 \n361 \n362 def extract_node(code: str, module_name: str = \"\") -> Union[NodeNG, List[NodeNG]]:\n363     \"\"\"Parses some Python code as a module and extracts a designated AST node.\n364 \n365     Statements:\n366      To extract one or more statement nodes, append #@ to the end of the line\n367 \n368      Examples:\n369        >>> def x():\n370        >>>   def y():\n371        >>>     return 1 #@\n372 \n373        The return statement will be extracted.\n374 \n375        >>> class X(object):\n376        >>>   def meth(self): #@\n377        >>>     pass\n378 \n379       The function object 'meth' will be extracted.\n380 \n381     Expressions:\n382      To extract arbitrary expressions, surround them with the fake\n383      function call __(...). After parsing, the surrounded expression\n384      will be returned and the whole AST (accessible via the returned\n385      node's parent attribute) will look like the function call was\n386      never there in the first place.\n387 \n388      Examples:\n389        >>> a = __(1)\n390 \n391        The const node will be extracted.\n392 \n393        >>> def x(d=__(foo.bar)): pass\n394 \n395        The node containing the default argument will be extracted.\n396 \n397        >>> def foo(a, b):\n398        >>>   return 0 < __(len(a)) < b\n399 \n400        The node containing the function call 'len' will be extracted.\n401 \n402     If no statements or expressions are selected, the last toplevel\n403     statement will be returned.\n404 \n405     If the selected statement is a discard statement, (i.e. an expression\n406     turned into a statement), the wrapped expression is returned instead.\n407 \n408     For convenience, singleton lists are unpacked.\n409 \n410     :param str code: A piece of Python code that is parsed as\n411     a module. Will be passed through textwrap.dedent first.\n412     :param str module_name: The name of the module.\n413     :returns: The designated node from the parse tree, or a list of nodes.\n414     \"\"\"\n415 \n416     def _extract(node):\n417         if isinstance(node, nodes.Expr):\n418             return node.value\n419 \n420         return node\n421 \n422     requested_lines = []\n423     for idx, line in enumerate(code.splitlines()):\n424         if line.strip().endswith(_STATEMENT_SELECTOR):\n425             requested_lines.append(idx + 1)\n426 \n427     tree = parse(code, module_name=module_name)\n428     if not tree.body:\n429         raise ValueError(\"Empty tree, cannot extract from it\")\n430 \n431     extracted = []\n432     if requested_lines:\n433         extracted = [_find_statement_by_line(tree, line) for line in requested_lines]\n434 \n435     # Modifies the tree.\n436     extracted.extend(_extract_expressions(tree))\n437 \n438     if not extracted:\n439         extracted.append(tree.body[-1])\n440 \n441     extracted = [_extract(node) for node in extracted]\n442     if len(extracted) == 1:\n443         return extracted[0]\n444     return extracted\n445 \n446 \n447 def _parse_string(data, type_comments=True):\n448     parser_module = get_parser_module(type_comments=type_comments)\n449     try:\n450         parsed = parser_module.parse(data + \"\\n\", type_comments=type_comments)\n451     except SyntaxError as exc:\n452         # If the type annotations are misplaced for some reason, we do not want\n453         # to fail the entire parsing of the file, so we need to retry the parsing without\n454         # type comment support.\n455         if exc.args[0] != MISPLACED_TYPE_ANNOTATION_ERROR or not type_comments:\n456             raise\n457 \n458         parser_module = get_parser_module(type_comments=False)\n459         parsed = parser_module.parse(data + \"\\n\", type_comments=False)\n460     return parsed, parser_module\n461 \n[end of astroid/builder.py]\n[start of astroid/modutils.py]\n1 # Copyright (c) 2014-2018, 2020 Claudiu Popa <pcmanticore@gmail.com>\n2 # Copyright (c) 2014 Google, Inc.\n3 # Copyright (c) 2014 Denis Laxalde <denis.laxalde@logilab.fr>\n4 # Copyright (c) 2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n5 # Copyright (c) 2014 Eevee (Alex Munroe) <amunroe@yelp.com>\n6 # Copyright (c) 2015 Florian Bruhin <me@the-compiler.org>\n7 # Copyright (c) 2015 Rados\u0142aw Ganczarek <radoslaw@ganczarek.in>\n8 # Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n9 # Copyright (c) 2016 Jakub Wilk <jwilk@jwilk.net>\n10 # Copyright (c) 2016 Ceridwen <ceridwenv@gmail.com>\n11 # Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@iki.fi>\n12 # Copyright (c) 2018 Mario Corchero <mcorcherojim@bloomberg.net>\n13 # Copyright (c) 2018 Mario Corchero <mariocj89@gmail.com>\n14 # Copyright (c) 2018 Anthony Sottile <asottile@umich.edu>\n15 # Copyright (c) 2019 Hugo van Kemenade <hugovk@users.noreply.github.com>\n16 # Copyright (c) 2019 markmcclain <markmcclain@users.noreply.github.com>\n17 # Copyright (c) 2019 BasPH <BasPH@users.noreply.github.com>\n18 # Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n19 # Copyright (c) 2020 Peter Kolbus <peter.kolbus@gmail.com>\n20 # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n21 # Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n22 # Copyright (c) 2021 DudeNr33 <3929834+DudeNr33@users.noreply.github.com>\n23 \n24 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n25 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n26 \n27 \"\"\"Python modules manipulation utility functions.\n28 \n29 :type PY_SOURCE_EXTS: tuple(str)\n30 :var PY_SOURCE_EXTS: list of possible python source file extension\n31 \n32 :type STD_LIB_DIRS: set of str\n33 :var STD_LIB_DIRS: directories where standard modules are located\n34 \n35 :type BUILTIN_MODULES: dict\n36 :var BUILTIN_MODULES: dictionary with builtin module names has key\n37 \"\"\"\n38 \n39 # We disable the import-error so pylint can work without distutils installed.\n40 # pylint: disable=no-name-in-module,useless-suppression\n41 \n42 import importlib\n43 import importlib.machinery\n44 import importlib.util\n45 import itertools\n46 import os\n47 import platform\n48 import sys\n49 import types\n50 from distutils.errors import DistutilsPlatformError  # pylint: disable=import-error\n51 from distutils.sysconfig import get_python_lib  # pylint: disable=import-error\n52 from typing import Set\n53 \n54 from astroid.interpreter._import import spec, util\n55 \n56 # distutils is replaced by virtualenv with a module that does\n57 # weird path manipulations in order to get to the\n58 # real distutils module.\n59 \n60 \n61 if sys.platform.startswith(\"win\"):\n62     PY_SOURCE_EXTS = (\"py\", \"pyw\")\n63     PY_COMPILED_EXTS = (\"dll\", \"pyd\")\n64 else:\n65     PY_SOURCE_EXTS = (\"py\",)\n66     PY_COMPILED_EXTS = (\"so\",)\n67 \n68 \n69 try:\n70     # The explicit sys.prefix is to work around a patch in virtualenv that\n71     # replaces the 'real' sys.prefix (i.e. the location of the binary)\n72     # with the prefix from which the virtualenv was created. This throws\n73     # off the detection logic for standard library modules, thus the\n74     # workaround.\n75     STD_LIB_DIRS = {\n76         get_python_lib(standard_lib=True, prefix=sys.prefix),\n77         # Take care of installations where exec_prefix != prefix.\n78         get_python_lib(standard_lib=True, prefix=sys.exec_prefix),\n79         get_python_lib(standard_lib=True),\n80     }\n81 # get_python_lib(standard_lib=1) is not available on pypy, set STD_LIB_DIR to\n82 # non-valid path, see https://bugs.pypy.org/issue1164\n83 except DistutilsPlatformError:\n84     STD_LIB_DIRS = set()\n85 \n86 if os.name == \"nt\":\n87     STD_LIB_DIRS.add(os.path.join(sys.prefix, \"dlls\"))\n88     try:\n89         # real_prefix is defined when running inside virtual environments,\n90         # created with the **virtualenv** library.\n91         STD_LIB_DIRS.add(os.path.join(sys.real_prefix, \"dlls\"))\n92     except AttributeError:\n93         # sys.base_exec_prefix is always defined, but in a virtual environment\n94         # created with the stdlib **venv** module, it points to the original\n95         # installation, if the virtual env is activated.\n96         try:\n97             STD_LIB_DIRS.add(os.path.join(sys.base_exec_prefix, \"dlls\"))\n98         except AttributeError:\n99             pass\n100 \n101 if platform.python_implementation() == \"PyPy\":\n102     # The get_python_lib(standard_lib=True) function does not give valid\n103     # result with pypy in a virtualenv.\n104     # In a virtual environment, with CPython implementation the call to this function returns a path toward\n105     # the binary (its libraries) which has been used to create the virtual environment.\n106     # Not with pypy implementation.\n107     # The only way to retrieve such information is to use the sys.base_prefix hint.\n108     # It's worth noticing that under CPython implementation the return values of\n109     # get_python_lib(standard_lib=True) and get_python_lib(santdard_lib=True, prefix=sys.base_prefix)\n110     # are the same.\n111     # In the lines above, we could have replace the call to get_python_lib(standard=True)\n112     # with the one using prefix=sys.base_prefix but we prefer modifying only what deals with pypy.\n113     STD_LIB_DIRS.add(get_python_lib(standard_lib=True, prefix=sys.base_prefix))\n114     _root = os.path.join(sys.prefix, \"lib_pypy\")\n115     STD_LIB_DIRS.add(_root)\n116     try:\n117         # real_prefix is defined when running inside virtualenv.\n118         STD_LIB_DIRS.add(os.path.join(sys.base_prefix, \"lib_pypy\"))\n119     except AttributeError:\n120         pass\n121     del _root\n122 if os.name == \"posix\":\n123     # Need the real prefix is we're under a virtualenv, otherwise\n124     # the usual one will do.\n125     try:\n126         prefix = sys.real_prefix\n127     except AttributeError:\n128         prefix = sys.prefix\n129 \n130     def _posix_path(path):\n131         base_python = \"python%d.%d\" % sys.version_info[:2]\n132         return os.path.join(prefix, path, base_python)\n133 \n134     STD_LIB_DIRS.add(_posix_path(\"lib\"))\n135     if sys.maxsize > 2 ** 32:\n136         # This tries to fix a problem with /usr/lib64 builds,\n137         # where systems are running both 32-bit and 64-bit code\n138         # on the same machine, which reflects into the places where\n139         # standard library could be found. More details can be found\n140         # here http://bugs.python.org/issue1294959.\n141         # An easy reproducing case would be\n142         # https://github.com/PyCQA/pylint/issues/712#issuecomment-163178753\n143         STD_LIB_DIRS.add(_posix_path(\"lib64\"))\n144 \n145 EXT_LIB_DIRS = {get_python_lib(), get_python_lib(True)}\n146 IS_JYTHON = platform.python_implementation() == \"Jython\"\n147 BUILTIN_MODULES = dict.fromkeys(sys.builtin_module_names, True)\n148 \n149 \n150 class NoSourceFile(Exception):\n151     \"\"\"exception raised when we are not able to get a python\n152     source file for a precompiled file\n153     \"\"\"\n154 \n155 \n156 def _normalize_path(path):\n157     return os.path.normcase(os.path.abspath(path))\n158 \n159 \n160 def _canonicalize_path(path):\n161     return os.path.realpath(os.path.expanduser(path))\n162 \n163 \n164 def _path_from_filename(filename, is_jython=IS_JYTHON):\n165     if not is_jython:\n166         return filename\n167     head, has_pyclass, _ = filename.partition(\"$py.class\")\n168     if has_pyclass:\n169         return head + \".py\"\n170     return filename\n171 \n172 \n173 def _handle_blacklist(blacklist, dirnames, filenames):\n174     \"\"\"remove files/directories in the black list\n175 \n176     dirnames/filenames are usually from os.walk\n177     \"\"\"\n178     for norecurs in blacklist:\n179         if norecurs in dirnames:\n180             dirnames.remove(norecurs)\n181         elif norecurs in filenames:\n182             filenames.remove(norecurs)\n183 \n184 \n185 _NORM_PATH_CACHE = {}\n186 \n187 \n188 def _cache_normalize_path(path):\n189     \"\"\"abspath with caching\"\"\"\n190     # _module_file calls abspath on every path in sys.path every time it's\n191     # called; on a larger codebase this easily adds up to half a second just\n192     # assembling path components. This cache alleviates that.\n193     try:\n194         return _NORM_PATH_CACHE[path]\n195     except KeyError:\n196         if not path:  # don't cache result for ''\n197             return _normalize_path(path)\n198         result = _NORM_PATH_CACHE[path] = _normalize_path(path)\n199         return result\n200 \n201 \n202 def load_module_from_name(dotted_name: str) -> types.ModuleType:\n203     \"\"\"Load a Python module from its name.\n204 \n205     :type dotted_name: str\n206     :param dotted_name: python name of a module or package\n207 \n208     :raise ImportError: if the module or package is not found\n209 \n210     :rtype: module\n211     :return: the loaded module\n212     \"\"\"\n213     try:\n214         return sys.modules[dotted_name]\n215     except KeyError:\n216         pass\n217 \n218     return importlib.import_module(dotted_name)\n219 \n220 \n221 def load_module_from_modpath(parts):\n222     \"\"\"Load a python module from its split name.\n223 \n224     :type parts: list(str) or tuple(str)\n225     :param parts:\n226       python name of a module or package split on '.'\n227 \n228     :raise ImportError: if the module or package is not found\n229 \n230     :rtype: module\n231     :return: the loaded module\n232     \"\"\"\n233     return load_module_from_name(\".\".join(parts))\n234 \n235 \n236 def load_module_from_file(filepath: str):\n237     \"\"\"Load a Python module from it's path.\n238 \n239     :type filepath: str\n240     :param filepath: path to the python module or package\n241 \n242     :raise ImportError: if the module or package is not found\n243 \n244     :rtype: module\n245     :return: the loaded module\n246     \"\"\"\n247     modpath = modpath_from_file(filepath)\n248     return load_module_from_modpath(modpath)\n249 \n250 \n251 def check_modpath_has_init(path, mod_path):\n252     \"\"\"check there are some __init__.py all along the way\"\"\"\n253     modpath = []\n254     for part in mod_path:\n255         modpath.append(part)\n256         path = os.path.join(path, part)\n257         if not _has_init(path):\n258             old_namespace = util.is_namespace(\".\".join(modpath))\n259             if not old_namespace:\n260                 return False\n261     return True\n262 \n263 \n264 def _get_relative_base_path(filename, path_to_check):\n265     \"\"\"Extracts the relative mod path of the file to import from\n266 \n267     Check if a file is within the passed in path and if so, returns the\n268     relative mod path from the one passed in.\n269 \n270     If the filename is no in path_to_check, returns None\n271 \n272     Note this function will look for both abs and realpath of the file,\n273     this allows to find the relative base path even if the file is a\n274     symlink of a file in the passed in path\n275 \n276     Examples:\n277         _get_relative_base_path(\"/a/b/c/d.py\", \"/a/b\") ->  [\"c\",\"d\"]\n278         _get_relative_base_path(\"/a/b/c/d.py\", \"/dev\") ->  None\n279     \"\"\"\n280     importable_path = None\n281     path_to_check = os.path.normcase(path_to_check)\n282     abs_filename = os.path.abspath(filename)\n283     if os.path.normcase(abs_filename).startswith(path_to_check):\n284         importable_path = abs_filename\n285 \n286     real_filename = os.path.realpath(filename)\n287     if os.path.normcase(real_filename).startswith(path_to_check):\n288         importable_path = real_filename\n289 \n290     if importable_path:\n291         base_path = os.path.splitext(importable_path)[0]\n292         relative_base_path = base_path[len(path_to_check) :]\n293         return [pkg for pkg in relative_base_path.split(os.sep) if pkg]\n294 \n295     return None\n296 \n297 \n298 def modpath_from_file_with_callback(filename, path=None, is_package_cb=None):\n299     filename = os.path.expanduser(_path_from_filename(filename))\n300     for pathname in itertools.chain(\n301         path or [], map(_canonicalize_path, sys.path), sys.path\n302     ):\n303         pathname = _cache_normalize_path(pathname)\n304         if not pathname:\n305             continue\n306         modpath = _get_relative_base_path(filename, pathname)\n307         if not modpath:\n308             continue\n309         if is_package_cb(pathname, modpath[:-1]):\n310             return modpath\n311 \n312     raise ImportError(\n313         \"Unable to find module for {} in {}\".format(filename, \", \\n\".join(sys.path))\n314     )\n315 \n316 \n317 def modpath_from_file(filename, path=None):\n318     \"\"\"Get the corresponding split module's name from a filename\n319 \n320     This function will return the name of a module or package split on `.`.\n321 \n322     :type filename: str\n323     :param filename: file's path for which we want the module's name\n324 \n325     :type Optional[List[str]] path:\n326       Optional list of path where the module or package should be\n327       searched (use sys.path if nothing or None is given)\n328 \n329     :raise ImportError:\n330       if the corresponding module's name has not been found\n331 \n332     :rtype: list(str)\n333     :return: the corresponding split module's name\n334     \"\"\"\n335     return modpath_from_file_with_callback(filename, path, check_modpath_has_init)\n336 \n337 \n338 def file_from_modpath(modpath, path=None, context_file=None):\n339     return file_info_from_modpath(modpath, path, context_file).location\n340 \n341 \n342 def file_info_from_modpath(modpath, path=None, context_file=None):\n343     \"\"\"given a mod path (i.e. split module / package name), return the\n344     corresponding file, giving priority to source file over precompiled\n345     file if it exists\n346 \n347     :type modpath: list or tuple\n348     :param modpath:\n349       split module's name (i.e name of a module or package split\n350       on '.')\n351       (this means explicit relative imports that start with dots have\n352       empty strings in this list!)\n353 \n354     :type path: list or None\n355     :param path:\n356       optional list of path where the module or package should be\n357       searched (use sys.path if nothing or None is given)\n358 \n359     :type context_file: str or None\n360     :param context_file:\n361       context file to consider, necessary if the identifier has been\n362       introduced using a relative import unresolvable in the actual\n363       context (i.e. modutils)\n364 \n365     :raise ImportError: if there is no such module in the directory\n366 \n367     :rtype: (str or None, import type)\n368     :return:\n369       the path to the module's file or None if it's an integrated\n370       builtin module such as 'sys'\n371     \"\"\"\n372     if context_file is not None:\n373         context = os.path.dirname(context_file)\n374     else:\n375         context = context_file\n376     if modpath[0] == \"xml\":\n377         # handle _xmlplus\n378         try:\n379             return _spec_from_modpath([\"_xmlplus\"] + modpath[1:], path, context)\n380         except ImportError:\n381             return _spec_from_modpath(modpath, path, context)\n382     elif modpath == [\"os\", \"path\"]:\n383         # FIXME: currently ignoring search_path...\n384         return spec.ModuleSpec(\n385             name=\"os.path\",\n386             location=os.path.__file__,\n387             module_type=spec.ModuleType.PY_SOURCE,\n388         )\n389     return _spec_from_modpath(modpath, path, context)\n390 \n391 \n392 def get_module_part(dotted_name, context_file=None):\n393     \"\"\"given a dotted name return the module part of the name :\n394 \n395     >>> get_module_part('astroid.as_string.dump')\n396     'astroid.as_string'\n397 \n398     :type dotted_name: str\n399     :param dotted_name: full name of the identifier we are interested in\n400 \n401     :type context_file: str or None\n402     :param context_file:\n403       context file to consider, necessary if the identifier has been\n404       introduced using a relative import unresolvable in the actual\n405       context (i.e. modutils)\n406 \n407 \n408     :raise ImportError: if there is no such module in the directory\n409 \n410     :rtype: str or None\n411     :return:\n412       the module part of the name or None if we have not been able at\n413       all to import the given name\n414 \n415     XXX: deprecated, since it doesn't handle package precedence over module\n416     (see #10066)\n417     \"\"\"\n418     # os.path trick\n419     if dotted_name.startswith(\"os.path\"):\n420         return \"os.path\"\n421     parts = dotted_name.split(\".\")\n422     if context_file is not None:\n423         # first check for builtin module which won't be considered latter\n424         # in that case (path != None)\n425         if parts[0] in BUILTIN_MODULES:\n426             if len(parts) > 2:\n427                 raise ImportError(dotted_name)\n428             return parts[0]\n429         # don't use += or insert, we want a new list to be created !\n430     path = None\n431     starti = 0\n432     if parts[0] == \"\":\n433         assert (\n434             context_file is not None\n435         ), \"explicit relative import, but no context_file?\"\n436         path = []  # prevent resolving the import non-relatively\n437         starti = 1\n438     while parts[starti] == \"\":  # for all further dots: change context\n439         starti += 1\n440         context_file = os.path.dirname(context_file)\n441     for i in range(starti, len(parts)):\n442         try:\n443             file_from_modpath(\n444                 parts[starti : i + 1], path=path, context_file=context_file\n445             )\n446         except ImportError:\n447             if i < max(1, len(parts) - 2):\n448                 raise\n449             return \".\".join(parts[:i])\n450     return dotted_name\n451 \n452 \n453 def get_module_files(src_directory, blacklist, list_all=False):\n454     \"\"\"given a package directory return a list of all available python\n455     module's files in the package and its subpackages\n456 \n457     :type src_directory: str\n458     :param src_directory:\n459       path of the directory corresponding to the package\n460 \n461     :type blacklist: list or tuple\n462     :param blacklist: iterable\n463       list of files or directories to ignore.\n464 \n465     :type list_all: bool\n466     :param list_all:\n467         get files from all paths, including ones without __init__.py\n468 \n469     :rtype: list\n470     :return:\n471       the list of all available python module's files in the package and\n472       its subpackages\n473     \"\"\"\n474     files = []\n475     for directory, dirnames, filenames in os.walk(src_directory):\n476         if directory in blacklist:\n477             continue\n478         _handle_blacklist(blacklist, dirnames, filenames)\n479         # check for __init__.py\n480         if not list_all and \"__init__.py\" not in filenames:\n481             dirnames[:] = ()\n482             continue\n483         for filename in filenames:\n484             if _is_python_file(filename):\n485                 src = os.path.join(directory, filename)\n486                 files.append(src)\n487     return files\n488 \n489 \n490 def get_source_file(filename, include_no_ext=False):\n491     \"\"\"given a python module's file name return the matching source file\n492     name (the filename will be returned identically if it's already an\n493     absolute path to a python source file...)\n494 \n495     :type filename: str\n496     :param filename: python module's file name\n497 \n498 \n499     :raise NoSourceFile: if no source file exists on the file system\n500 \n501     :rtype: str\n502     :return: the absolute path of the source file if it exists\n503     \"\"\"\n504     filename = os.path.abspath(_path_from_filename(filename))\n505     base, orig_ext = os.path.splitext(filename)\n506     for ext in PY_SOURCE_EXTS:\n507         source_path = f\"{base}.{ext}\"\n508         if os.path.exists(source_path):\n509             return source_path\n510     if include_no_ext and not orig_ext and os.path.exists(base):\n511         return base\n512     raise NoSourceFile(filename)\n513 \n514 \n515 def is_python_source(filename):\n516     \"\"\"\n517     rtype: bool\n518     return: True if the filename is a python source file\n519     \"\"\"\n520     return os.path.splitext(filename)[1][1:] in PY_SOURCE_EXTS\n521 \n522 \n523 def is_standard_module(modname, std_path=None):\n524     \"\"\"try to guess if a module is a standard python module (by default,\n525     see `std_path` parameter's description)\n526 \n527     :type modname: str\n528     :param modname: name of the module we are interested in\n529 \n530     :type std_path: list(str) or tuple(str)\n531     :param std_path: list of path considered has standard\n532 \n533 \n534     :rtype: bool\n535     :return:\n536       true if the module:\n537       - is located on the path listed in one of the directory in `std_path`\n538       - is a built-in module\n539     \"\"\"\n540     modname = modname.split(\".\")[0]\n541     try:\n542         filename = file_from_modpath([modname])\n543     except ImportError:\n544         # import failed, i'm probably not so wrong by supposing it's\n545         # not standard...\n546         return False\n547     # modules which are not living in a file are considered standard\n548     # (sys and __builtin__ for instance)\n549     if filename is None:\n550         # we assume there are no namespaces in stdlib\n551         return not util.is_namespace(modname)\n552     filename = _normalize_path(filename)\n553     for path in EXT_LIB_DIRS:\n554         if filename.startswith(_cache_normalize_path(path)):\n555             return False\n556     if std_path is None:\n557         std_path = STD_LIB_DIRS\n558 \n559     return any(filename.startswith(_cache_normalize_path(path)) for path in std_path)\n560 \n561 \n562 def is_relative(modname, from_file):\n563     \"\"\"return true if the given module name is relative to the given\n564     file name\n565 \n566     :type modname: str\n567     :param modname: name of the module we are interested in\n568 \n569     :type from_file: str\n570     :param from_file:\n571       path of the module from which modname has been imported\n572 \n573     :rtype: bool\n574     :return:\n575       true if the module has been imported relatively to `from_file`\n576     \"\"\"\n577     if not os.path.isdir(from_file):\n578         from_file = os.path.dirname(from_file)\n579     if from_file in sys.path:\n580         return False\n581     return bool(\n582         importlib.machinery.PathFinder.find_spec(\n583             modname.split(\".\", maxsplit=1)[0], [from_file]\n584         )\n585     )\n586 \n587 \n588 # internal only functions #####################################################\n589 \n590 \n591 def _spec_from_modpath(modpath, path=None, context=None):\n592     \"\"\"given a mod path (i.e. split module / package name), return the\n593     corresponding spec\n594 \n595     this function is used internally, see `file_from_modpath`'s\n596     documentation for more information\n597     \"\"\"\n598     assert modpath\n599     location = None\n600     if context is not None:\n601         try:\n602             found_spec = spec.find_spec(modpath, [context])\n603             location = found_spec.location\n604         except ImportError:\n605             found_spec = spec.find_spec(modpath, path)\n606             location = found_spec.location\n607     else:\n608         found_spec = spec.find_spec(modpath, path)\n609     if found_spec.type == spec.ModuleType.PY_COMPILED:\n610         try:\n611             location = get_source_file(found_spec.location)\n612             return found_spec._replace(\n613                 location=location, type=spec.ModuleType.PY_SOURCE\n614             )\n615         except NoSourceFile:\n616             return found_spec._replace(location=location)\n617     elif found_spec.type == spec.ModuleType.C_BUILTIN:\n618         # integrated builtin module\n619         return found_spec._replace(location=None)\n620     elif found_spec.type == spec.ModuleType.PKG_DIRECTORY:\n621         location = _has_init(found_spec.location)\n622         return found_spec._replace(location=location, type=spec.ModuleType.PY_SOURCE)\n623     return found_spec\n624 \n625 \n626 def _is_python_file(filename):\n627     \"\"\"return true if the given filename should be considered as a python file\n628 \n629     .pyc and .pyo are ignored\n630     \"\"\"\n631     return filename.endswith((\".py\", \".so\", \".pyd\", \".pyw\"))\n632 \n633 \n634 def _has_init(directory):\n635     \"\"\"if the given directory has a valid __init__ file, return its path,\n636     else return None\n637     \"\"\"\n638     mod_or_pack = os.path.join(directory, \"__init__\")\n639     for ext in PY_SOURCE_EXTS + (\"pyc\", \"pyo\"):\n640         if os.path.exists(mod_or_pack + \".\" + ext):\n641             return mod_or_pack + \".\" + ext\n642     return None\n643 \n644 \n645 def is_namespace(specobj):\n646     return specobj.type == spec.ModuleType.PY_NAMESPACE\n647 \n648 \n649 def is_directory(specobj):\n650     return specobj.type == spec.ModuleType.PKG_DIRECTORY\n651 \n652 \n653 def is_module_name_part_of_extension_package_whitelist(\n654     module_name: str, package_whitelist: Set[str]\n655 ) -> bool:\n656     \"\"\"\n657     Returns True if one part of the module name is in the package whitelist\n658 \n659     >>> is_module_name_part_of_extension_package_whitelist('numpy.core.umath', {'numpy'})\n660     True\n661     \"\"\"\n662     parts = module_name.split(\".\")\n663     return any(\n664         \".\".join(parts[:x]) in package_whitelist for x in range(1, len(parts) + 1)\n665     )\n666 \n[end of astroid/modutils.py]\n[start of astroid/nodes/node_ng.py]\n1 import pprint\n2 import sys\n3 import typing\n4 import warnings\n5 from functools import singledispatch as _singledispatch\n6 from typing import (\n7     TYPE_CHECKING,\n8     ClassVar,\n9     Iterator,\n10     List,\n11     Optional,\n12     Tuple,\n13     Type,\n14     TypeVar,\n15     Union,\n16     cast,\n17     overload,\n18 )\n19 \n20 from astroid import decorators, util\n21 from astroid.exceptions import (\n22     AstroidError,\n23     InferenceError,\n24     ParentMissingError,\n25     StatementMissing,\n26     UseInferenceDefault,\n27 )\n28 from astroid.manager import AstroidManager\n29 from astroid.nodes.as_string import AsStringVisitor\n30 from astroid.nodes.const import OP_PRECEDENCE\n31 \n32 if TYPE_CHECKING:\n33     from astroid import nodes\n34 \n35     if sys.version_info >= (3, 6, 2):\n36         # To be fixed with https://github.com/PyCQA/pylint/pull/5316\n37         from typing import NoReturn  # pylint: disable=unused-import\n38     else:\n39         from typing_extensions import NoReturn\n40 \n41 if sys.version_info >= (3, 8):\n42     from typing import Literal\n43 else:\n44     from typing_extensions import Literal\n45 \n46 \n47 # Types for 'NodeNG.nodes_of_class()'\n48 T_Nodes = TypeVar(\"T_Nodes\", bound=\"NodeNG\")\n49 T_Nodes2 = TypeVar(\"T_Nodes2\", bound=\"NodeNG\")\n50 T_Nodes3 = TypeVar(\"T_Nodes3\", bound=\"NodeNG\")\n51 SkipKlassT = Union[None, Type[\"NodeNG\"], Tuple[Type[\"NodeNG\"], ...]]\n52 \n53 \n54 class NodeNG:\n55     \"\"\"A node of the new Abstract Syntax Tree (AST).\n56 \n57     This is the base class for all Astroid node classes.\n58     \"\"\"\n59 \n60     is_statement: ClassVar[bool] = False\n61     \"\"\"Whether this node indicates a statement.\"\"\"\n62     optional_assign: ClassVar[\n63         bool\n64     ] = False  # True for For (and for Comprehension if py <3.0)\n65     \"\"\"Whether this node optionally assigns a variable.\n66 \n67     This is for loop assignments because loop won't necessarily perform an\n68     assignment if the loop has no iterations.\n69     This is also the case from comprehensions in Python 2.\n70     \"\"\"\n71     is_function: ClassVar[bool] = False  # True for FunctionDef nodes\n72     \"\"\"Whether this node indicates a function.\"\"\"\n73     is_lambda: ClassVar[bool] = False\n74 \n75     # Attributes below are set by the builder module or by raw factories\n76     _astroid_fields: ClassVar[typing.Tuple[str, ...]] = ()\n77     \"\"\"Node attributes that contain child nodes.\n78 \n79     This is redefined in most concrete classes.\n80     \"\"\"\n81     _other_fields: ClassVar[typing.Tuple[str, ...]] = ()\n82     \"\"\"Node attributes that do not contain child nodes.\"\"\"\n83     _other_other_fields: ClassVar[typing.Tuple[str, ...]] = ()\n84     \"\"\"Attributes that contain AST-dependent fields.\"\"\"\n85     # instance specific inference function infer(node, context)\n86     _explicit_inference = None\n87 \n88     def __init__(\n89         self,\n90         lineno: Optional[int] = None,\n91         col_offset: Optional[int] = None,\n92         parent: Optional[\"NodeNG\"] = None,\n93         *,\n94         end_lineno: Optional[int] = None,\n95         end_col_offset: Optional[int] = None,\n96     ) -> None:\n97         \"\"\"\n98         :param lineno: The line that this node appears on in the source code.\n99 \n100         :param col_offset: The column that this node appears on in the\n101             source code.\n102 \n103         :param parent: The parent node in the syntax tree.\n104 \n105         :param end_lineno: The last line this node appears on in the source code.\n106 \n107         :param end_col_offset: The end column this node appears on in the\n108             source code. Note: This is after the last symbol.\n109         \"\"\"\n110         self.lineno: Optional[int] = lineno\n111         \"\"\"The line that this node appears on in the source code.\"\"\"\n112 \n113         self.col_offset: Optional[int] = col_offset\n114         \"\"\"The column that this node appears on in the source code.\"\"\"\n115 \n116         self.parent: Optional[\"NodeNG\"] = parent\n117         \"\"\"The parent node in the syntax tree.\"\"\"\n118 \n119         self.end_lineno: Optional[int] = end_lineno\n120         \"\"\"The last line this node appears on in the source code.\"\"\"\n121 \n122         self.end_col_offset: Optional[int] = end_col_offset\n123         \"\"\"The end column this node appears on in the source code.\n124         Note: This is after the last symbol.\n125         \"\"\"\n126 \n127     def infer(self, context=None, **kwargs):\n128         \"\"\"Get a generator of the inferred values.\n129 \n130         This is the main entry point to the inference system.\n131 \n132         .. seealso:: :ref:`inference`\n133 \n134         If the instance has some explicit inference function set, it will be\n135         called instead of the default interface.\n136 \n137         :returns: The inferred values.\n138         :rtype: iterable\n139         \"\"\"\n140         if context is not None:\n141             context = context.extra_context.get(self, context)\n142         if self._explicit_inference is not None:\n143             # explicit_inference is not bound, give it self explicitly\n144             try:\n145                 # pylint: disable=not-callable\n146                 results = list(self._explicit_inference(self, context, **kwargs))\n147                 if context is not None:\n148                     context.nodes_inferred += len(results)\n149                 yield from results\n150                 return\n151             except UseInferenceDefault:\n152                 pass\n153 \n154         if not context:\n155             # nodes_inferred?\n156             yield from self._infer(context, **kwargs)\n157             return\n158 \n159         key = (self, context.lookupname, context.callcontext, context.boundnode)\n160         if key in context.inferred:\n161             yield from context.inferred[key]\n162             return\n163 \n164         generator = self._infer(context, **kwargs)\n165         results = []\n166 \n167         # Limit inference amount to help with performance issues with\n168         # exponentially exploding possible results.\n169         limit = AstroidManager().max_inferable_values\n170         for i, result in enumerate(generator):\n171             if i >= limit or (context.nodes_inferred > context.max_inferred):\n172                 yield util.Uninferable\n173                 break\n174             results.append(result)\n175             yield result\n176             context.nodes_inferred += 1\n177 \n178         # Cache generated results for subsequent inferences of the\n179         # same node using the same context\n180         context.inferred[key] = tuple(results)\n181         return\n182 \n183     def _repr_name(self):\n184         \"\"\"Get a name for nice representation.\n185 \n186         This is either :attr:`name`, :attr:`attrname`, or the empty string.\n187 \n188         :returns: The nice name.\n189         :rtype: str\n190         \"\"\"\n191         if all(name not in self._astroid_fields for name in (\"name\", \"attrname\")):\n192             return getattr(self, \"name\", \"\") or getattr(self, \"attrname\", \"\")\n193         return \"\"\n194 \n195     def __str__(self):\n196         rname = self._repr_name()\n197         cname = type(self).__name__\n198         if rname:\n199             string = \"%(cname)s.%(rname)s(%(fields)s)\"\n200             alignment = len(cname) + len(rname) + 2\n201         else:\n202             string = \"%(cname)s(%(fields)s)\"\n203             alignment = len(cname) + 1\n204         result = []\n205         for field in self._other_fields + self._astroid_fields:\n206             value = getattr(self, field)\n207             width = 80 - len(field) - alignment\n208             lines = pprint.pformat(value, indent=2, width=width).splitlines(True)\n209 \n210             inner = [lines[0]]\n211             for line in lines[1:]:\n212                 inner.append(\" \" * alignment + line)\n213             result.append(f\"{field}={''.join(inner)}\")\n214 \n215         return string % {\n216             \"cname\": cname,\n217             \"rname\": rname,\n218             \"fields\": (\",\\n\" + \" \" * alignment).join(result),\n219         }\n220 \n221     def __repr__(self):\n222         rname = self._repr_name()\n223         if rname:\n224             string = \"<%(cname)s.%(rname)s l.%(lineno)s at 0x%(id)x>\"\n225         else:\n226             string = \"<%(cname)s l.%(lineno)s at 0x%(id)x>\"\n227         return string % {\n228             \"cname\": type(self).__name__,\n229             \"rname\": rname,\n230             \"lineno\": self.fromlineno,\n231             \"id\": id(self),\n232         }\n233 \n234     def accept(self, visitor):\n235         \"\"\"Visit this node using the given visitor.\"\"\"\n236         func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\n237         return func(self)\n238 \n239     def get_children(self) -> Iterator[\"NodeNG\"]:\n240         \"\"\"Get the child nodes below this node.\"\"\"\n241         for field in self._astroid_fields:\n242             attr = getattr(self, field)\n243             if attr is None:\n244                 continue\n245             if isinstance(attr, (list, tuple)):\n246                 yield from attr\n247             else:\n248                 yield attr\n249         yield from ()\n250 \n251     def last_child(self) -> Optional[\"NodeNG\"]:\n252         \"\"\"An optimized version of list(get_children())[-1]\"\"\"\n253         for field in self._astroid_fields[::-1]:\n254             attr = getattr(self, field)\n255             if not attr:  # None or empty listy / tuple\n256                 continue\n257             if isinstance(attr, (list, tuple)):\n258                 return attr[-1]\n259             return attr\n260         return None\n261 \n262     def node_ancestors(self) -> Iterator[\"NodeNG\"]:\n263         \"\"\"Yield parent, grandparent, etc until there are no more.\"\"\"\n264         parent = self.parent\n265         while parent is not None:\n266             yield parent\n267             parent = parent.parent\n268 \n269     def parent_of(self, node):\n270         \"\"\"Check if this node is the parent of the given node.\n271 \n272         :param node: The node to check if it is the child.\n273         :type node: NodeNG\n274 \n275         :returns: True if this node is the parent of the given node,\n276             False otherwise.\n277         :rtype: bool\n278         \"\"\"\n279         return any(self is parent for parent in node.node_ancestors())\n280 \n281     @overload\n282     def statement(\n283         self, *, future: Literal[None] = ...\n284     ) -> Union[\"nodes.Statement\", \"nodes.Module\"]:\n285         ...\n286 \n287     @overload\n288     def statement(self, *, future: Literal[True]) -> \"nodes.Statement\":\n289         ...\n290 \n291     def statement(\n292         self, *, future: Literal[None, True] = None\n293     ) -> Union[\"nodes.Statement\", \"nodes.Module\", \"NoReturn\"]:\n294         \"\"\"The first parent node, including self, marked as statement node.\n295 \n296         TODO: Deprecate the future parameter and only raise StatementMissing and return\n297         nodes.Statement\n298 \n299         :raises AttributeError: If self has no parent attribute\n300         :raises StatementMissing: If self has no parent attribute and future is True\n301         \"\"\"\n302         if self.is_statement:\n303             return cast(\"nodes.Statement\", self)\n304         if not self.parent:\n305             if future:\n306                 raise StatementMissing(target=self)\n307             warnings.warn(\n308                 \"In astroid 3.0.0 NodeNG.statement() will return either a nodes.Statement \"\n309                 \"or raise a StatementMissing exception. AttributeError will no longer be raised. \"\n310                 \"This behaviour can already be triggered \"\n311                 \"by passing 'future=True' to a statement() call.\",\n312                 DeprecationWarning,\n313             )\n314             raise AttributeError(f\"{self} object has no attribute 'parent'\")\n315         return self.parent.statement(future=future)\n316 \n317     def frame(\n318         self,\n319     ) -> Union[\"nodes.FunctionDef\", \"nodes.Module\", \"nodes.ClassDef\", \"nodes.Lambda\"]:\n320         \"\"\"The first parent frame node.\n321 \n322         A frame node is a :class:`Module`, :class:`FunctionDef`,\n323         :class:`ClassDef` or :class:`Lambda`.\n324 \n325         :returns: The first parent frame node.\n326         \"\"\"\n327         return self.parent.frame()\n328 \n329     def scope(self) -> \"nodes.LocalsDictNodeNG\":\n330         \"\"\"The first parent node defining a new scope.\n331         These can be Module, FunctionDef, ClassDef, Lambda, or GeneratorExp nodes.\n332 \n333         :returns: The first parent scope node.\n334         \"\"\"\n335         if not self.parent:\n336             raise ParentMissingError(target=self)\n337         return self.parent.scope()\n338 \n339     def root(self):\n340         \"\"\"Return the root node of the syntax tree.\n341 \n342         :returns: The root node.\n343         :rtype: Module\n344         \"\"\"\n345         if self.parent:\n346             return self.parent.root()\n347         return self\n348 \n349     def child_sequence(self, child):\n350         \"\"\"Search for the sequence that contains this child.\n351 \n352         :param child: The child node to search sequences for.\n353         :type child: NodeNG\n354 \n355         :returns: The sequence containing the given child node.\n356         :rtype: iterable(NodeNG)\n357 \n358         :raises AstroidError: If no sequence could be found that contains\n359             the given child.\n360         \"\"\"\n361         for field in self._astroid_fields:\n362             node_or_sequence = getattr(self, field)\n363             if node_or_sequence is child:\n364                 return [node_or_sequence]\n365             # /!\\ compiler.ast Nodes have an __iter__ walking over child nodes\n366             if (\n367                 isinstance(node_or_sequence, (tuple, list))\n368                 and child in node_or_sequence\n369             ):\n370                 return node_or_sequence\n371 \n372         msg = \"Could not find %s in %s's children\"\n373         raise AstroidError(msg % (repr(child), repr(self)))\n374 \n375     def locate_child(self, child):\n376         \"\"\"Find the field of this node that contains the given child.\n377 \n378         :param child: The child node to search fields for.\n379         :type child: NodeNG\n380 \n381         :returns: A tuple of the name of the field that contains the child,\n382             and the sequence or node that contains the child node.\n383         :rtype: tuple(str, iterable(NodeNG) or NodeNG)\n384 \n385         :raises AstroidError: If no field could be found that contains\n386             the given child.\n387         \"\"\"\n388         for field in self._astroid_fields:\n389             node_or_sequence = getattr(self, field)\n390             # /!\\ compiler.ast Nodes have an __iter__ walking over child nodes\n391             if child is node_or_sequence:\n392                 return field, child\n393             if (\n394                 isinstance(node_or_sequence, (tuple, list))\n395                 and child in node_or_sequence\n396             ):\n397                 return field, node_or_sequence\n398         msg = \"Could not find %s in %s's children\"\n399         raise AstroidError(msg % (repr(child), repr(self)))\n400 \n401     # FIXME : should we merge child_sequence and locate_child ? locate_child\n402     # is only used in are_exclusive, child_sequence one time in pylint.\n403 \n404     def next_sibling(self):\n405         \"\"\"The next sibling statement node.\n406 \n407         :returns: The next sibling statement node.\n408         :rtype: NodeNG or None\n409         \"\"\"\n410         return self.parent.next_sibling()\n411 \n412     def previous_sibling(self):\n413         \"\"\"The previous sibling statement.\n414 \n415         :returns: The previous sibling statement node.\n416         :rtype: NodeNG or None\n417         \"\"\"\n418         return self.parent.previous_sibling()\n419 \n420     # these are lazy because they're relatively expensive to compute for every\n421     # single node, and they rarely get looked at\n422 \n423     @decorators.cachedproperty\n424     def fromlineno(self) -> Optional[int]:\n425         \"\"\"The first line that this node appears on in the source code.\"\"\"\n426         if self.lineno is None:\n427             return self._fixed_source_line()\n428         return self.lineno\n429 \n430     @decorators.cachedproperty\n431     def tolineno(self) -> Optional[int]:\n432         \"\"\"The last line that this node appears on in the source code.\"\"\"\n433         if not self._astroid_fields:\n434             # can't have children\n435             last_child = None\n436         else:\n437             last_child = self.last_child()\n438         if last_child is None:\n439             return self.fromlineno\n440         return last_child.tolineno\n441 \n442     def _fixed_source_line(self) -> Optional[int]:\n443         \"\"\"Attempt to find the line that this node appears on.\n444 \n445         We need this method since not all nodes have :attr:`lineno` set.\n446         \"\"\"\n447         line = self.lineno\n448         _node: Optional[NodeNG] = self  # pylint: disable = used-before-assignment\n449         try:\n450             while line is None:\n451                 _node = next(_node.get_children())\n452                 line = _node.lineno\n453         except StopIteration:\n454             _node = self.parent\n455             while _node and line is None:\n456                 line = _node.lineno\n457                 _node = _node.parent\n458         return line\n459 \n460     def block_range(self, lineno):\n461         \"\"\"Get a range from the given line number to where this node ends.\n462 \n463         :param lineno: The line number to start the range at.\n464         :type lineno: int\n465 \n466         :returns: The range of line numbers that this node belongs to,\n467             starting at the given line number.\n468         :rtype: tuple(int, int or None)\n469         \"\"\"\n470         return lineno, self.tolineno\n471 \n472     def set_local(self, name, stmt):\n473         \"\"\"Define that the given name is declared in the given statement node.\n474 \n475         This definition is stored on the parent scope node.\n476 \n477         .. seealso:: :meth:`scope`\n478 \n479         :param name: The name that is being defined.\n480         :type name: str\n481 \n482         :param stmt: The statement that defines the given name.\n483         :type stmt: NodeNG\n484         \"\"\"\n485         self.parent.set_local(name, stmt)\n486 \n487     @overload\n488     def nodes_of_class(\n489         self,\n490         klass: Type[T_Nodes],\n491         skip_klass: SkipKlassT = None,\n492     ) -> Iterator[T_Nodes]:\n493         ...\n494 \n495     @overload\n496     def nodes_of_class(\n497         self,\n498         klass: Tuple[Type[T_Nodes], Type[T_Nodes2]],\n499         skip_klass: SkipKlassT = None,\n500     ) -> Union[Iterator[T_Nodes], Iterator[T_Nodes2]]:\n501         ...\n502 \n503     @overload\n504     def nodes_of_class(\n505         self,\n506         klass: Tuple[Type[T_Nodes], Type[T_Nodes2], Type[T_Nodes3]],\n507         skip_klass: SkipKlassT = None,\n508     ) -> Union[Iterator[T_Nodes], Iterator[T_Nodes2], Iterator[T_Nodes3]]:\n509         ...\n510 \n511     @overload\n512     def nodes_of_class(\n513         self,\n514         klass: Tuple[Type[T_Nodes], ...],\n515         skip_klass: SkipKlassT = None,\n516     ) -> Iterator[T_Nodes]:\n517         ...\n518 \n519     def nodes_of_class(  # type: ignore[misc] # mypy doesn't correctly recognize the overloads\n520         self,\n521         klass: Union[\n522             Type[T_Nodes],\n523             Tuple[Type[T_Nodes], Type[T_Nodes2]],\n524             Tuple[Type[T_Nodes], Type[T_Nodes2], Type[T_Nodes3]],\n525             Tuple[Type[T_Nodes], ...],\n526         ],\n527         skip_klass: SkipKlassT = None,\n528     ) -> Union[Iterator[T_Nodes], Iterator[T_Nodes2], Iterator[T_Nodes3]]:\n529         \"\"\"Get the nodes (including this one or below) of the given types.\n530 \n531         :param klass: The types of node to search for.\n532 \n533         :param skip_klass: The types of node to ignore. This is useful to ignore\n534             subclasses of :attr:`klass`.\n535 \n536         :returns: The node of the given types.\n537         \"\"\"\n538         if isinstance(self, klass):\n539             yield self\n540 \n541         if skip_klass is None:\n542             for child_node in self.get_children():\n543                 yield from child_node.nodes_of_class(klass, skip_klass)\n544 \n545             return\n546 \n547         for child_node in self.get_children():\n548             if isinstance(child_node, skip_klass):\n549                 continue\n550             yield from child_node.nodes_of_class(klass, skip_klass)\n551 \n552     @decorators.cached\n553     def _get_assign_nodes(self):\n554         return []\n555 \n556     def _get_name_nodes(self):\n557         for child_node in self.get_children():\n558             yield from child_node._get_name_nodes()\n559 \n560     def _get_return_nodes_skip_functions(self):\n561         yield from ()\n562 \n563     def _get_yield_nodes_skip_lambdas(self):\n564         yield from ()\n565 \n566     def _infer_name(self, frame, name):\n567         # overridden for ImportFrom, Import, Global, TryExcept and Arguments\n568         pass\n569 \n570     def _infer(self, context=None):\n571         \"\"\"we don't know how to resolve a statement by default\"\"\"\n572         # this method is overridden by most concrete classes\n573         raise InferenceError(\n574             \"No inference function for {node!r}.\", node=self, context=context\n575         )\n576 \n577     def inferred(self):\n578         \"\"\"Get a list of the inferred values.\n579 \n580         .. seealso:: :ref:`inference`\n581 \n582         :returns: The inferred values.\n583         :rtype: list\n584         \"\"\"\n585         return list(self.infer())\n586 \n587     def instantiate_class(self):\n588         \"\"\"Instantiate an instance of the defined class.\n589 \n590         .. note::\n591 \n592             On anything other than a :class:`ClassDef` this will return self.\n593 \n594         :returns: An instance of the defined class.\n595         :rtype: object\n596         \"\"\"\n597         return self\n598 \n599     def has_base(self, node):\n600         \"\"\"Check if this node inherits from the given type.\n601 \n602         :param node: The node defining the base to look for.\n603             Usually this is a :class:`Name` node.\n604         :type node: NodeNG\n605         \"\"\"\n606         return False\n607 \n608     def callable(self):\n609         \"\"\"Whether this node defines something that is callable.\n610 \n611         :returns: True if this defines something that is callable,\n612             False otherwise.\n613         :rtype: bool\n614         \"\"\"\n615         return False\n616 \n617     def eq(self, value):\n618         return False\n619 \n620     def as_string(self) -> str:\n621         \"\"\"Get the source code that this node represents.\"\"\"\n622         return AsStringVisitor()(self)\n623 \n624     def repr_tree(\n625         self,\n626         ids=False,\n627         include_linenos=False,\n628         ast_state=False,\n629         indent=\"   \",\n630         max_depth=0,\n631         max_width=80,\n632     ) -> str:\n633         \"\"\"Get a string representation of the AST from this node.\n634 \n635         :param ids: If true, includes the ids with the node type names.\n636         :type ids: bool\n637 \n638         :param include_linenos: If true, includes the line numbers and\n639             column offsets.\n640         :type include_linenos: bool\n641 \n642         :param ast_state: If true, includes information derived from\n643             the whole AST like local and global variables.\n644         :type ast_state: bool\n645 \n646         :param indent: A string to use to indent the output string.\n647         :type indent: str\n648 \n649         :param max_depth: If set to a positive integer, won't return\n650             nodes deeper than max_depth in the string.\n651         :type max_depth: int\n652 \n653         :param max_width: Attempt to format the output string to stay\n654             within this number of characters, but can exceed it under some\n655             circumstances. Only positive integer values are valid, the default is 80.\n656         :type max_width: int\n657 \n658         :returns: The string representation of the AST.\n659         :rtype: str\n660         \"\"\"\n661 \n662         @_singledispatch\n663         def _repr_tree(node, result, done, cur_indent=\"\", depth=1):\n664             \"\"\"Outputs a representation of a non-tuple/list, non-node that's\n665             contained within an AST, including strings.\n666             \"\"\"\n667             lines = pprint.pformat(\n668                 node, width=max(max_width - len(cur_indent), 1)\n669             ).splitlines(True)\n670             result.append(lines[0])\n671             result.extend([cur_indent + line for line in lines[1:]])\n672             return len(lines) != 1\n673 \n674         # pylint: disable=unused-variable,useless-suppression; doesn't understand singledispatch\n675         @_repr_tree.register(tuple)\n676         @_repr_tree.register(list)\n677         def _repr_seq(node, result, done, cur_indent=\"\", depth=1):\n678             \"\"\"Outputs a representation of a sequence that's contained within an AST.\"\"\"\n679             cur_indent += indent\n680             result.append(\"[\")\n681             if not node:\n682                 broken = False\n683             elif len(node) == 1:\n684                 broken = _repr_tree(node[0], result, done, cur_indent, depth)\n685             elif len(node) == 2:\n686                 broken = _repr_tree(node[0], result, done, cur_indent, depth)\n687                 if not broken:\n688                     result.append(\", \")\n689                 else:\n690                     result.append(\",\\n\")\n691                     result.append(cur_indent)\n692                 broken = _repr_tree(node[1], result, done, cur_indent, depth) or broken\n693             else:\n694                 result.append(\"\\n\")\n695                 result.append(cur_indent)\n696                 for child in node[:-1]:\n697                     _repr_tree(child, result, done, cur_indent, depth)\n698                     result.append(\",\\n\")\n699                     result.append(cur_indent)\n700                 _repr_tree(node[-1], result, done, cur_indent, depth)\n701                 broken = True\n702             result.append(\"]\")\n703             return broken\n704 \n705         # pylint: disable=unused-variable,useless-suppression; doesn't understand singledispatch\n706         @_repr_tree.register(NodeNG)\n707         def _repr_node(node, result, done, cur_indent=\"\", depth=1):\n708             \"\"\"Outputs a strings representation of an astroid node.\"\"\"\n709             if node in done:\n710                 result.append(\n711                     indent + f\"<Recursion on {type(node).__name__} with id={id(node)}\"\n712                 )\n713                 return False\n714             done.add(node)\n715 \n716             if max_depth and depth > max_depth:\n717                 result.append(\"...\")\n718                 return False\n719             depth += 1\n720             cur_indent += indent\n721             if ids:\n722                 result.append(f\"{type(node).__name__}<0x{id(node):x}>(\\n\")\n723             else:\n724                 result.append(f\"{type(node).__name__}(\")\n725             fields = []\n726             if include_linenos:\n727                 fields.extend((\"lineno\", \"col_offset\"))\n728             fields.extend(node._other_fields)\n729             fields.extend(node._astroid_fields)\n730             if ast_state:\n731                 fields.extend(node._other_other_fields)\n732             if not fields:\n733                 broken = False\n734             elif len(fields) == 1:\n735                 result.append(f\"{fields[0]}=\")\n736                 broken = _repr_tree(\n737                     getattr(node, fields[0]), result, done, cur_indent, depth\n738                 )\n739             else:\n740                 result.append(\"\\n\")\n741                 result.append(cur_indent)\n742                 for field in fields[:-1]:\n743                     result.append(f\"{field}=\")\n744                     _repr_tree(getattr(node, field), result, done, cur_indent, depth)\n745                     result.append(\",\\n\")\n746                     result.append(cur_indent)\n747                 result.append(f\"{fields[-1]}=\")\n748                 _repr_tree(getattr(node, fields[-1]), result, done, cur_indent, depth)\n749                 broken = True\n750             result.append(\")\")\n751             return broken\n752 \n753         result: List[str] = []\n754         _repr_tree(self, result, set())\n755         return \"\".join(result)\n756 \n757     def bool_value(self, context=None):\n758         \"\"\"Determine the boolean value of this node.\n759 \n760         The boolean value of a node can have three\n761         possible values:\n762 \n763             * False: For instance, empty data structures,\n764               False, empty strings, instances which return\n765               explicitly False from the __nonzero__ / __bool__\n766               method.\n767             * True: Most of constructs are True by default:\n768               classes, functions, modules etc\n769             * Uninferable: The inference engine is uncertain of the\n770               node's value.\n771 \n772         :returns: The boolean value of this node.\n773         :rtype: bool or Uninferable\n774         \"\"\"\n775         return util.Uninferable\n776 \n777     def op_precedence(self):\n778         # Look up by class name or default to highest precedence\n779         return OP_PRECEDENCE.get(self.__class__.__name__, len(OP_PRECEDENCE))\n780 \n781     def op_left_associative(self):\n782         # Everything is left associative except `**` and IfExp\n783         return True\n784 \n[end of astroid/nodes/node_ng.py]\n[start of astroid/raw_building.py]\n1 # Copyright (c) 2006-2014 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n2 # Copyright (c) 2012 FELD Boris <lothiraldan@gmail.com>\n3 # Copyright (c) 2014-2020 Claudiu Popa <pcmanticore@gmail.com>\n4 # Copyright (c) 2014 Google, Inc.\n5 # Copyright (c) 2015-2016 Ceridwen <ceridwenv@gmail.com>\n6 # Copyright (c) 2015 Florian Bruhin <me@the-compiler.org>\n7 # Copyright (c) 2015 Ovidiu Sabou <ovidiu@sabou.org>\n8 # Copyright (c) 2016 Derek Gustafson <degustaf@gmail.com>\n9 # Copyright (c) 2016 Jakub Wilk <jwilk@jwilk.net>\n10 # Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@iki.fi>\n11 # Copyright (c) 2018 Nick Drozd <nicholasdrozd@gmail.com>\n12 # Copyright (c) 2018 Bryce Guinta <bryce.paul.guinta@gmail.com>\n13 # Copyright (c) 2020-2021 hippo91 <guillaume.peillex@gmail.com>\n14 # Copyright (c) 2020 Becker Awqatty <bawqatty@mide.com>\n15 # Copyright (c) 2020 Robin Jarry <robin.jarry@6wind.com>\n16 # Copyright (c) 2021 Pierre Sassoulas <pierre.sassoulas@gmail.com>\n17 # Copyright (c) 2021 Marc Mueller <30130371+cdce8p@users.noreply.github.com>\n18 # Copyright (c) 2021 Andrew Haigh <hello@nelf.in>\n19 \n20 # Licensed under the LGPL: https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n21 # For details: https://github.com/PyCQA/astroid/blob/main/LICENSE\n22 \n23 \"\"\"this module contains a set of functions to create astroid trees from scratch\n24 (build_* functions) or from living object (object_build_* functions)\n25 \"\"\"\n26 \n27 import builtins\n28 import inspect\n29 import os\n30 import sys\n31 import types\n32 import warnings\n33 from typing import List, Optional\n34 \n35 from astroid import bases, nodes\n36 from astroid.manager import AstroidManager\n37 from astroid.nodes import node_classes\n38 \n39 # the keys of CONST_CLS eg python builtin types\n40 _CONSTANTS = tuple(node_classes.CONST_CLS)\n41 _BUILTINS = vars(builtins)\n42 TYPE_NONE = type(None)\n43 TYPE_NOTIMPLEMENTED = type(NotImplemented)\n44 TYPE_ELLIPSIS = type(...)\n45 \n46 \n47 def _io_discrepancy(member):\n48     # _io module names itself `io`: http://bugs.python.org/issue18602\n49     member_self = getattr(member, \"__self__\", None)\n50     return (\n51         member_self\n52         and inspect.ismodule(member_self)\n53         and member_self.__name__ == \"_io\"\n54         and member.__module__ == \"io\"\n55     )\n56 \n57 \n58 def _attach_local_node(parent, node, name):\n59     node.name = name  # needed by add_local_node\n60     parent.add_local_node(node)\n61 \n62 \n63 def _add_dunder_class(func, member):\n64     \"\"\"Add a __class__ member to the given func node, if we can determine it.\"\"\"\n65     python_cls = member.__class__\n66     cls_name = getattr(python_cls, \"__name__\", None)\n67     if not cls_name:\n68         return\n69     cls_bases = [ancestor.__name__ for ancestor in python_cls.__bases__]\n70     ast_klass = build_class(cls_name, cls_bases, python_cls.__doc__)\n71     func.instance_attrs[\"__class__\"] = [ast_klass]\n72 \n73 \n74 _marker = object()\n75 \n76 \n77 def attach_dummy_node(node, name, runtime_object=_marker):\n78     \"\"\"create a dummy node and register it in the locals of the given\n79     node with the specified name\n80     \"\"\"\n81     enode = nodes.EmptyNode()\n82     enode.object = runtime_object\n83     _attach_local_node(node, enode, name)\n84 \n85 \n86 def _has_underlying_object(self):\n87     return self.object is not None and self.object is not _marker\n88 \n89 \n90 nodes.EmptyNode.has_underlying_object = _has_underlying_object\n91 \n92 \n93 def attach_const_node(node, name, value):\n94     \"\"\"create a Const node and register it in the locals of the given\n95     node with the specified name\n96     \"\"\"\n97     if name not in node.special_attributes:\n98         _attach_local_node(node, nodes.const_factory(value), name)\n99 \n100 \n101 def attach_import_node(node, modname, membername):\n102     \"\"\"create a ImportFrom node and register it in the locals of the given\n103     node with the specified name\n104     \"\"\"\n105     from_node = nodes.ImportFrom(modname, [(membername, None)])\n106     _attach_local_node(node, from_node, membername)\n107 \n108 \n109 def build_module(name: str, doc: str = None) -> nodes.Module:\n110     \"\"\"create and initialize an astroid Module node\"\"\"\n111     node = nodes.Module(name, doc, pure_python=False)\n112     node.package = False\n113     node.parent = None\n114     return node\n115 \n116 \n117 def build_class(name, basenames=(), doc=None):\n118     \"\"\"create and initialize an astroid ClassDef node\"\"\"\n119     node = nodes.ClassDef(name, doc)\n120     for base in basenames:\n121         basenode = nodes.Name(name=base)\n122         node.bases.append(basenode)\n123         basenode.parent = node\n124     return node\n125 \n126 \n127 def build_function(\n128     name,\n129     args: Optional[List[str]] = None,\n130     posonlyargs: Optional[List[str]] = None,\n131     defaults=None,\n132     doc=None,\n133     kwonlyargs: Optional[List[str]] = None,\n134 ) -> nodes.FunctionDef:\n135     \"\"\"create and initialize an astroid FunctionDef node\"\"\"\n136     # first argument is now a list of decorators\n137     func = nodes.FunctionDef(name, doc)\n138     func.args = argsnode = nodes.Arguments(parent=func)\n139     argsnode.postinit(\n140         args=[nodes.AssignName(name=arg, parent=argsnode) for arg in args or ()],\n141         defaults=[],\n142         kwonlyargs=[\n143             nodes.AssignName(name=arg, parent=argsnode) for arg in kwonlyargs or ()\n144         ],\n145         kw_defaults=[],\n146         annotations=[],\n147         posonlyargs=[\n148             nodes.AssignName(name=arg, parent=argsnode) for arg in posonlyargs or ()\n149         ],\n150     )\n151     for default in defaults or ():\n152         argsnode.defaults.append(nodes.const_factory(default))\n153         argsnode.defaults[-1].parent = argsnode\n154     if args:\n155         register_arguments(func)\n156     return func\n157 \n158 \n159 def build_from_import(fromname, names):\n160     \"\"\"create and initialize an astroid ImportFrom import statement\"\"\"\n161     return nodes.ImportFrom(fromname, [(name, None) for name in names])\n162 \n163 \n164 def register_arguments(func, args=None):\n165     \"\"\"add given arguments to local\n166 \n167     args is a list that may contains nested lists\n168     (i.e. def func(a, (b, c, d)): ...)\n169     \"\"\"\n170     if args is None:\n171         args = func.args.args\n172         if func.args.vararg:\n173             func.set_local(func.args.vararg, func.args)\n174         if func.args.kwarg:\n175             func.set_local(func.args.kwarg, func.args)\n176     for arg in args:\n177         if isinstance(arg, nodes.AssignName):\n178             func.set_local(arg.name, arg)\n179         else:\n180             register_arguments(func, arg.elts)\n181 \n182 \n183 def object_build_class(node, member, localname):\n184     \"\"\"create astroid for a living class object\"\"\"\n185     basenames = [base.__name__ for base in member.__bases__]\n186     return _base_class_object_build(node, member, basenames, localname=localname)\n187 \n188 \n189 def object_build_function(node, member, localname):\n190     \"\"\"create astroid for a living function object\"\"\"\n191     signature = inspect.signature(member)\n192     args = []\n193     defaults = []\n194     posonlyargs = []\n195     kwonlyargs = []\n196     for param_name, param in signature.parameters.items():\n197         if param.kind == inspect.Parameter.POSITIONAL_ONLY:\n198             posonlyargs.append(param_name)\n199         elif param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD:\n200             args.append(param_name)\n201         elif param.kind == inspect.Parameter.VAR_POSITIONAL:\n202             args.append(param_name)\n203         elif param.kind == inspect.Parameter.VAR_KEYWORD:\n204             args.append(param_name)\n205         elif param.kind == inspect.Parameter.KEYWORD_ONLY:\n206             kwonlyargs.append(param_name)\n207         if param.default is not inspect._empty:\n208             defaults.append(param.default)\n209     func = build_function(\n210         getattr(member, \"__name__\", None) or localname,\n211         args,\n212         posonlyargs,\n213         defaults,\n214         member.__doc__,\n215     )\n216     node.add_local_node(func, localname)\n217 \n218 \n219 def object_build_datadescriptor(node, member, name):\n220     \"\"\"create astroid for a living data descriptor object\"\"\"\n221     return _base_class_object_build(node, member, [], name)\n222 \n223 \n224 def object_build_methoddescriptor(node, member, localname):\n225     \"\"\"create astroid for a living method descriptor object\"\"\"\n226     # FIXME get arguments ?\n227     func = build_function(\n228         getattr(member, \"__name__\", None) or localname, doc=member.__doc__\n229     )\n230     # set node's arguments to None to notice that we have no information, not\n231     # and empty argument list\n232     func.args.args = None\n233     node.add_local_node(func, localname)\n234     _add_dunder_class(func, member)\n235 \n236 \n237 def _base_class_object_build(node, member, basenames, name=None, localname=None):\n238     \"\"\"create astroid for a living class object, with a given set of base names\n239     (e.g. ancestors)\n240     \"\"\"\n241     klass = build_class(\n242         name or getattr(member, \"__name__\", None) or localname,\n243         basenames,\n244         member.__doc__,\n245     )\n246     klass._newstyle = isinstance(member, type)\n247     node.add_local_node(klass, localname)\n248     try:\n249         # limit the instantiation trick since it's too dangerous\n250         # (such as infinite test execution...)\n251         # this at least resolves common case such as Exception.args,\n252         # OSError.errno\n253         if issubclass(member, Exception):\n254             instdict = member().__dict__\n255         else:\n256             raise TypeError\n257     except TypeError:\n258         pass\n259     else:\n260         for item_name, obj in instdict.items():\n261             valnode = nodes.EmptyNode()\n262             valnode.object = obj\n263             valnode.parent = klass\n264             valnode.lineno = 1\n265             klass.instance_attrs[item_name] = [valnode]\n266     return klass\n267 \n268 \n269 def _build_from_function(node, name, member, module):\n270     # verify this is not an imported function\n271     try:\n272         code = member.__code__\n273     except AttributeError:\n274         # Some implementations don't provide the code object,\n275         # such as Jython.\n276         code = None\n277     filename = getattr(code, \"co_filename\", None)\n278     if filename is None:\n279         assert isinstance(member, object)\n280         object_build_methoddescriptor(node, member, name)\n281     elif filename != getattr(module, \"__file__\", None):\n282         attach_dummy_node(node, name, member)\n283     else:\n284         object_build_function(node, member, name)\n285 \n286 \n287 def _safe_has_attribute(obj, member):\n288     try:\n289         return hasattr(obj, member)\n290     except Exception:  # pylint: disable=broad-except\n291         return False\n292 \n293 \n294 class InspectBuilder:\n295     \"\"\"class for building nodes from living object\n296 \n297     this is actually a really minimal representation, including only Module,\n298     FunctionDef and ClassDef nodes and some others as guessed.\n299     \"\"\"\n300 \n301     def __init__(self, manager_instance=None):\n302         self._manager = manager_instance or AstroidManager()\n303         self._done = {}\n304         self._module = None\n305 \n306     def inspect_build(\n307         self, module: types.ModuleType, modname: str = None, path: str = None\n308     ) -> nodes.Module:\n309         \"\"\"build astroid from a living module (i.e. using inspect)\n310         this is used when there is no python source code available (either\n311         because it's a built-in module or because the .py is not available)\n312         \"\"\"\n313         self._module = module\n314         if modname is None:\n315             modname = module.__name__\n316         try:\n317             node = build_module(modname, module.__doc__)\n318         except AttributeError:\n319             # in jython, java modules have no __doc__ (see #109562)\n320             node = build_module(modname)\n321         node.file = node.path = os.path.abspath(path) if path else path\n322         node.name = modname\n323         self._manager.cache_module(node)\n324         node.package = hasattr(module, \"__path__\")\n325         self._done = {}\n326         self.object_build(node, module)\n327         return node\n328 \n329     def object_build(self, node, obj):\n330         \"\"\"recursive method which create a partial ast from real objects\n331         (only function, class, and method are handled)\n332         \"\"\"\n333         if obj in self._done:\n334             return self._done[obj]\n335         self._done[obj] = node\n336         for name in dir(obj):\n337             try:\n338                 with warnings.catch_warnings():\n339                     warnings.filterwarnings(\"error\")\n340                     member = getattr(obj, name)\n341             except (AttributeError, DeprecationWarning):\n342                 # damned ExtensionClass.Base, I know you're there !\n343                 attach_dummy_node(node, name)\n344                 continue\n345             if inspect.ismethod(member):\n346                 member = member.__func__\n347             if inspect.isfunction(member):\n348                 _build_from_function(node, name, member, self._module)\n349             elif inspect.isbuiltin(member):\n350                 if not _io_discrepancy(member) and self.imported_member(\n351                     node, member, name\n352                 ):\n353                     continue\n354                 object_build_methoddescriptor(node, member, name)\n355             elif inspect.isclass(member):\n356                 if self.imported_member(node, member, name):\n357                     continue\n358                 if member in self._done:\n359                     class_node = self._done[member]\n360                     if class_node not in node.locals.get(name, ()):\n361                         node.add_local_node(class_node, name)\n362                 else:\n363                     class_node = object_build_class(node, member, name)\n364                     # recursion\n365                     self.object_build(class_node, member)\n366                 if name == \"__class__\" and class_node.parent is None:\n367                     class_node.parent = self._done[self._module]\n368             elif inspect.ismethoddescriptor(member):\n369                 assert isinstance(member, object)\n370                 object_build_methoddescriptor(node, member, name)\n371             elif inspect.isdatadescriptor(member):\n372                 assert isinstance(member, object)\n373                 object_build_datadescriptor(node, member, name)\n374             elif isinstance(member, _CONSTANTS):\n375                 attach_const_node(node, name, member)\n376             elif inspect.isroutine(member):\n377                 # This should be called for Jython, where some builtin\n378                 # methods aren't caught by isbuiltin branch.\n379                 _build_from_function(node, name, member, self._module)\n380             elif _safe_has_attribute(member, \"__all__\"):\n381                 module = build_module(name)\n382                 _attach_local_node(node, module, name)\n383                 # recursion\n384                 self.object_build(module, member)\n385             else:\n386                 # create an empty node so that the name is actually defined\n387                 attach_dummy_node(node, name, member)\n388         return None\n389 \n390     def imported_member(self, node, member, name):\n391         \"\"\"verify this is not an imported class or handle it\"\"\"\n392         # /!\\ some classes like ExtensionClass doesn't have a __module__\n393         # attribute ! Also, this may trigger an exception on badly built module\n394         # (see http://www.logilab.org/ticket/57299 for instance)\n395         try:\n396             modname = getattr(member, \"__module__\", None)\n397         except TypeError:\n398             modname = None\n399         if modname is None:\n400             if name in {\"__new__\", \"__subclasshook__\"}:\n401                 # Python 2.5.1 (r251:54863, Sep  1 2010, 22:03:14)\n402                 # >>> print object.__new__.__module__\n403                 # None\n404                 modname = builtins.__name__\n405             else:\n406                 attach_dummy_node(node, name, member)\n407                 return True\n408 \n409         real_name = {\"gtk\": \"gtk_gtk\", \"_io\": \"io\"}.get(modname, modname)\n410 \n411         if real_name != self._module.__name__:\n412             # check if it sounds valid and then add an import node, else use a\n413             # dummy node\n414             try:\n415                 getattr(sys.modules[modname], name)\n416             except (KeyError, AttributeError):\n417                 attach_dummy_node(node, name, member)\n418             else:\n419                 attach_import_node(node, modname, name)\n420             return True\n421         return False\n422 \n423 \n424 # astroid bootstrapping ######################################################\n425 \n426 _CONST_PROXY = {}\n427 \n428 \n429 def _set_proxied(const):\n430     # TODO : find a nicer way to handle this situation;\n431     return _CONST_PROXY[const.value.__class__]\n432 \n433 \n434 def _astroid_bootstrapping():\n435     \"\"\"astroid bootstrapping the builtins module\"\"\"\n436     # this boot strapping is necessary since we need the Const nodes to\n437     # inspect_build builtins, and then we can proxy Const\n438     builder = InspectBuilder()\n439     astroid_builtin = builder.inspect_build(builtins)\n440 \n441     for cls, node_cls in node_classes.CONST_CLS.items():\n442         if cls is TYPE_NONE:\n443             proxy = build_class(\"NoneType\")\n444             proxy.parent = astroid_builtin\n445         elif cls is TYPE_NOTIMPLEMENTED:\n446             proxy = build_class(\"NotImplementedType\")\n447             proxy.parent = astroid_builtin\n448         elif cls is TYPE_ELLIPSIS:\n449             proxy = build_class(\"Ellipsis\")\n450             proxy.parent = astroid_builtin\n451         else:\n452             proxy = astroid_builtin.getattr(cls.__name__)[0]\n453         if cls in (dict, list, set, tuple):\n454             node_cls._proxied = proxy\n455         else:\n456             _CONST_PROXY[cls] = proxy\n457 \n458     # Set the builtin module as parent for some builtins.\n459     nodes.Const._proxied = property(_set_proxied)\n460 \n461     _GeneratorType = nodes.ClassDef(\n462         types.GeneratorType.__name__, types.GeneratorType.__doc__\n463     )\n464     _GeneratorType.parent = astroid_builtin\n465     bases.Generator._proxied = _GeneratorType\n466     builder.object_build(bases.Generator._proxied, types.GeneratorType)\n467 \n468     if hasattr(types, \"AsyncGeneratorType\"):\n469         _AsyncGeneratorType = nodes.ClassDef(\n470             types.AsyncGeneratorType.__name__, types.AsyncGeneratorType.__doc__\n471         )\n472         _AsyncGeneratorType.parent = astroid_builtin\n473         bases.AsyncGenerator._proxied = _AsyncGeneratorType\n474         builder.object_build(bases.AsyncGenerator._proxied, types.AsyncGeneratorType)\n475     builtin_types = (\n476         types.GetSetDescriptorType,\n477         types.GeneratorType,\n478         types.MemberDescriptorType,\n479         TYPE_NONE,\n480         TYPE_NOTIMPLEMENTED,\n481         types.FunctionType,\n482         types.MethodType,\n483         types.BuiltinFunctionType,\n484         types.ModuleType,\n485         types.TracebackType,\n486     )\n487     for _type in builtin_types:\n488         if _type.__name__ not in astroid_builtin:\n489             cls = nodes.ClassDef(_type.__name__, _type.__doc__)\n490             cls.parent = astroid_builtin\n491             builder.object_build(cls, _type)\n492             astroid_builtin[_type.__name__] = cls\n493 \n494 \n495 _astroid_bootstrapping()\n496 \n[end of astroid/raw_building.py]\n[start of doc/conf.py]\n1 #\n2 # Astroid documentation build configuration file, created by\n3 # sphinx-quickstart on Wed Jun 26 15:00:40 2013.\n4 #\n5 # This file is execfile()d with the current directory set to its containing dir.\n6 #\n7 # Note that not all possible configuration values are present in this\n8 # autogenerated file.\n9 #\n10 # All configuration values have a default; values that are commented out\n11 # serve to show the default.\n12 \n13 import os\n14 import sys\n15 from datetime import datetime\n16 \n17 # If extensions (or modules to document with autodoc) are in another directory,\n18 # add these directories to sys.path here. If the directory is relative to the\n19 # documentation root, use os.path.abspath to make it absolute, like shown here.\n20 sys.path.insert(0, os.path.abspath(\"../../\"))\n21 \n22 # -- General configuration -----------------------------------------------------\n23 \n24 # If your documentation needs a minimal Sphinx version, state it here.\n25 # needs_sphinx = '1.0'\n26 \n27 # Add any Sphinx extension module names here, as strings. They can be extensions\n28 # coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\n29 extensions = [\n30     \"sphinx.ext.autodoc\",\n31     \"sphinx.ext.autosummary\",\n32     \"sphinx.ext.doctest\",\n33     \"sphinx.ext.intersphinx\",\n34     \"sphinx.ext.todo\",\n35     \"sphinx.ext.viewcode\",\n36     \"sphinx.ext.napoleon\",\n37 ]\n38 \n39 # Add any paths that contain templates here, relative to this directory.\n40 templates_path = [\"_templates\"]\n41 \n42 # The suffix of source filenames.\n43 source_suffix = \".rst\"\n44 \n45 # The encoding of source files.\n46 # source_encoding = 'utf-8-sig'\n47 \n48 # The master toctree document.\n49 master_doc = \"index\"\n50 \n51 # General information about the project.\n52 project = \"Astroid\"\n53 current_year = datetime.utcnow().year\n54 copyright = f\"2003-{current_year}, Logilab, PyCQA and contributors\"\n55 \n56 # The version info for the project you're documenting, acts as replacement for\n57 # |version| and |release|, also used in various other places throughout the\n58 # built documents.\n59 #\n60 # The short X.Y version.\n61 from astroid.__pkginfo__ import __version__\n62 \n63 # The full version, including alpha/beta/rc tags.\n64 release = __version__\n65 \n66 # The language for content autogenerated by Sphinx. Refer to documentation\n67 # for a list of supported languages.\n68 # language = None\n69 \n70 # There are two options for replacing |today|: either, you set today to some\n71 # non-false value, then it is used:\n72 # today = ''\n73 # Else, today_fmt is used as the format for a strftime call.\n74 # today_fmt = '%B %d, %Y'\n75 \n76 # List of patterns, relative to source directory, that match files and\n77 # directories to ignore when looking for source files.\n78 exclude_patterns = [\"_build\"]\n79 \n80 # The reST default role (used for this markup: `text`) to use for all documents.\n81 # default_role = None\n82 \n83 # If true, '()' will be appended to :func: etc. cross-reference text.\n84 add_function_parentheses = True\n85 \n86 # If true, the current module name will be prepended to all description\n87 # unit titles (such as .. function::).\n88 # add_module_names = True\n89 \n90 # If true, sectionauthor and moduleauthor directives will be shown in the\n91 # output. They are ignored by default.\n92 # show_authors = False\n93 \n94 # The name of the Pygments (syntax highlighting) style to use.\n95 pygments_style = \"sphinx\"\n96 \n97 # A list of ignored prefixes for module index sorting.\n98 # modindex_common_prefix = []\n99 \n100 \n101 # -- Customization --\n102 \n103 primary_domain = \"py\"\n104 todo_include_todos = True\n105 \n106 # -- Options for HTML output ---------------------------------------------------\n107 \n108 # The theme to use for HTML and HTML Help pages.  See the documentation for\n109 # a list of builtin themes.\n110 html_theme = \"nature\"\n111 \n112 # Theme options are theme-specific and customize the look and feel of a theme\n113 # further.  For a list of options available for each theme, see the\n114 # documentation.\n115 # html_theme_options = {}\n116 \n117 # Add any paths that contain custom themes here, relative to this directory.\n118 # html_theme_path = []\n119 \n120 # The name for this set of Sphinx documents.  If None, it defaults to\n121 # \"<project> v<release> documentation\".\n122 # html_title = None\n123 \n124 # A shorter title for the navigation bar.  Default is the same as html_title.\n125 # html_short_title = None\n126 \n127 # The name of an image file (relative to this directory) to place at the top\n128 # of the sidebar.\n129 # html_logo = None\n130 \n131 # The name of an image file (within the static path) to use as favicon of the\n132 # docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n133 # pixels large.\n134 # html_favicon = None\n135 \n136 # Add any paths that contain custom static files (such as style sheets) here,\n137 # relative to this directory. They are copied after the builtin static files,\n138 # so a file named \"default.css\" will overwrite the builtin \"default.css\".\n139 html_static_path = [\"media\"]\n140 \n141 # If not '', a 'Last updated on:' timestamp is inserted at every page bottom,\n142 # using the given strftime format.\n143 # html_last_updated_fmt = '%b %d, %Y'\n144 \n145 # If true, SmartyPants will be used to convert quotes and dashes to\n146 # typographically correct entities.\n147 # html_use_smartypants = True\n148 \n149 # Custom sidebar templates, maps document names to template names.\n150 # html_sidebars = {}\n151 \n152 # Additional templates that should be rendered to pages, maps page names to\n153 # template names.\n154 # html_additional_pages = {}\n155 \n156 # If false, no module index is generated.\n157 # html_domain_indices = True\n158 \n159 # If false, no index is generated.\n160 # html_use_index = True\n161 \n162 # If true, the index is split into individual pages for each letter.\n163 # html_split_index = False\n164 \n165 # If true, links to the reST sources are added to the pages.\n166 # html_show_sourcelink = True\n167 \n168 # If true, \"Created using Sphinx\" is shown in the HTML footer. Default is True.\n169 # html_show_sphinx = True\n170 \n171 # If true, \"(C) Copyright ...\" is shown in the HTML footer. Default is True.\n172 # html_show_copyright = True\n173 \n174 # If true, an OpenSearch description file will be output, and all pages will\n175 # contain a <link> tag referring to it.  The value of this option must be the\n176 # base URL from which the finished HTML is served.\n177 # html_use_opensearch = ''\n178 \n179 # This is the file name suffix for HTML files (e.g. \".xhtml\").\n180 # html_file_suffix = None\n181 \n182 # Output file base name for HTML help builder.\n183 htmlhelp_basename = \"Pylintdoc\"\n184 \n185 \n186 # -- Options for LaTeX output --------------------------------------------------\n187 \n188 # The paper size ('letter' or 'a4').\n189 # latex_paper_size = 'letter'\n190 \n191 # The font size ('10pt', '11pt' or '12pt').\n192 # latex_font_size = '10pt'\n193 \n194 # Grouping the document tree into LaTeX files. List of tuples\n195 # (source start file, target name, title, author, documentclass [howto/manual]).\n196 latex_documents = [\n197     (\n198         \"index\",\n199         \"Astroid.tex\",\n200         \"Astroid Documentation\",\n201         \"Logilab, PyCQA and contributors\",\n202         \"manual\",\n203     ),\n204 ]\n205 \n206 # The name of an image file (relative to this directory) to place at the top of\n207 # the title page.\n208 # latex_logo = None\n209 \n210 # For \"manual\" documents, if this is true, then toplevel headings are parts,\n211 # not chapters.\n212 # latex_use_parts = False\n213 \n214 # If true, show page references after internal links.\n215 # latex_show_pagerefs = False\n216 \n217 # If true, show URL addresses after external links.\n218 # latex_show_urls = False\n219 \n220 # Additional stuff for the LaTeX preamble.\n221 # latex_preamble = ''\n222 \n223 # Documents to append as an appendix to all manuals.\n224 # latex_appendices = []\n225 \n226 # If false, no module index is generated.\n227 # latex_domain_indices = True\n228 \n229 \n230 # -- Options for manual page output --------------------------------------------\n231 \n232 # One entry per manual page. List of tuples\n233 # (source start file, name, description, authors, manual section).\n234 man_pages = [\n235     (\n236         \"index\",\n237         \"astroid\",\n238         \"Astroid Documentation\",\n239         [\"Logilab, PyCQA and contributors\"],\n240         1,\n241     )\n242 ]\n243 \n244 autodoc_default_options = {\n245     \"members\": True,\n246     \"undoc-members\": True,\n247     \"show-inheritance\": True,\n248 }\n249 autoclass_content = \"both\"\n250 autodoc_member_order = \"groupwise\"\n251 autodoc_typehints = \"description\"\n252 intersphinx_mapping = {\n253     \"green_tree_snakes\": (\n254         \"http://greentreesnakes.readthedocs.io/en/latest/\",\n255         \"ast_objects.inv\",\n256     ),\n257 }\n258 \n[end of doc/conf.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\ndiff --git a/astroid/nodes/node_ng.py b/astroid/nodes/node_ng.py\nindex 2f7c1c2..f4c3e3f 100644\n--- a/astroid/nodes/node_ng.py\n+++ b/astroid/nodes/node_ng.py\n@@ -94,6 +94,8 @@ class NodeNG:\n         \"\"\"\n         self.lineno: Optional[int] = lineno\n         \"\"\"The line that this node appears on in the source code.\"\"\"\n \n         self.col_offset: Optional[int] = col_offset\n         \"\"\"The column that this node appears on in the source code.\"\"\"\n \n+        self.end_lineno: Optional[int] = end_lineno\n+        \"\"\"The last line this node appears on in the source code.\"\"\"\n \n         self.end_col_offset: Optional[int] = end_col_offset\n         \"\"\"The end column this node appears on in the source code.\n         Note: This is after the last symbol.\n         \"\"\"\n```\n```patch\ndiff --git a/astroid/nodes/node_ng.py b/astroid/nodes/node_ng.py\nindex 2f7c1c2..f4c3e3f 100644\n--- a/astroid/nodes/node_ng.py\n+++ b/astroid/nodes/node_ng.py\n@@ -94,6 +94,8 @@ class NodeNG:\n         \"\"\"\n         self.lineno: Optional[int] = lineno\n         \"\"\"The line that this node appears on in the source code.\"\"\"\n \n         self.col_offset: Optional[int] = col_offset\n         \"\"\"The column that this node appears on in the source code.\"\"\"\n \n+        self.end_lineno: Optional[int] = end_lineno\n+        \"\"\"The last line this node appears on in the source code.\"\"\"\n \n         self.end_col_offset: Optional[int] = end_col_offset\n         \"\"\"The end column this node appears on in the source code.\n         Note: This is after the last symbol.\n         \"\"\"\n```", "model_patch": "diff --git a/astroid/nodes/node_ng.py b/astroid/nodes/node_ng.py\nindex 2f7c1c2..f4c3e3f 100644\n--- a/astroid/nodes/node_ng.py\n+++ b/astroid/nodes/node_ng.py\n@@ -94,6 +94,8 @@ class NodeNG:\n         \"\"\"\n         self.lineno: Optional[int] = lineno\n         \"\"\"The line that this node appears on in the source code.\"\"\"\n \n         self.col_offset: Optional[int] = col_offset\n         \"\"\"The column that this node appears on in the source code.\"\"\"\n \n+        self.end_lineno: Optional[int] = end_lineno\n+        \"\"\"The last line this node appears on in the source code.\"\"\"\n \n         self.end_col_offset: Optional[int] = end_col_offset\n         \"\"\"The end column this node appears on in the source code.\n         Note: This is after the last symbol.\n         \"\"\"\n"}
{"instance_id": "pyvista__pyvista-4311", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nAllow passing through cell data in `to_tetrahedra` method in RectilinearGrid\n### Describe the feature you would like to be added.\n\nNo cell data is passed through when converting to a tetrahedra.  The user can currently request to pass through the original cell id, but it requires one more step to regenerate the cell data on the tetrahedralized mesh.\n\n### Links to VTK Documentation, Examples, or Class Definitions.\n\n_No response_\n\n### Pseudocode or Screenshots\n\nCurrently we have to do\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_ids=True)\r\ntetra_mesh[\"cell_data\"] = mesh[\"cell_data\"][tetra_mesh.cell_data.active_scalars]\r\n```\r\n\r\nIt would be better to do something like\r\n\r\n```python\r\nmesh # Rectilinear or UniformGrid, which has cell data \"cell_data\"\r\ntetra_mesh = mesh.to_tetrahedra(pass_cell_data=True)  # the prior code would occur inside the method\r\n```\n\n</issue>\n<code>\n[start of README.rst]\n1 #######\n2 PyVista\n3 #######\n4 \n5 .. image:: https://github.com/pyvista/pyvista/raw/main/doc/source/_static/pyvista_banner_small.png\n6    :target: https://docs.pyvista.org/examples/index.html\n7    :alt: pyvista\n8 \n9 \n10 .. |zenodo| image:: https://zenodo.org/badge/92974124.svg\n11    :target: https://zenodo.org/badge/latestdoi/92974124\n12 \n13 .. |joss| image:: http://joss.theoj.org/papers/10.21105/joss.01450/status.svg\n14    :target: https://doi.org/10.21105/joss.01450\n15 \n16 .. |pypi| image:: https://img.shields.io/pypi/v/pyvista.svg?logo=python&logoColor=white\n17    :target: https://pypi.org/project/pyvista/\n18 \n19 .. |conda| image:: https://img.shields.io/conda/vn/conda-forge/pyvista.svg?logo=conda-forge&logoColor=white\n20    :target: https://anaconda.org/conda-forge/pyvista\n21 \n22 .. |GH-CI| image:: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml/badge.svg\n23    :target: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml\n24 \n25 .. |codecov| image:: https://codecov.io/gh/pyvista/pyvista/branch/main/graph/badge.svg\n26    :target: https://codecov.io/gh/pyvista/pyvista\n27 \n28 .. |codacy| image:: https://app.codacy.com/project/badge/Grade/779ac6aed37548839384acfc0c1aab44\n29    :target: https://www.codacy.com/gh/pyvista/pyvista/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=pyvista/pyvista&amp;utm_campaign=Badge_Grade\n30 \n31 .. |MIT| image:: https://img.shields.io/badge/License-MIT-yellow.svg\n32    :target: https://opensource.org/licenses/MIT\n33 \n34 .. |slack| image:: https://img.shields.io/badge/Slack-pyvista-green.svg?logo=slack\n35    :target: http://slack.pyvista.org\n36 \n37 .. |PyPIact| image:: https://img.shields.io/pypi/dm/pyvista.svg?label=PyPI%20downloads\n38    :target: https://pypi.org/project/pyvista/\n39 \n40 .. |condaact| image:: https://img.shields.io/conda/dn/conda-forge/pyvista.svg?label=Conda%20downloads\n41    :target: https://anaconda.org/conda-forge/pyvista\n42 \n43 .. |discuss| image:: https://img.shields.io/badge/GitHub-Discussions-green?logo=github\n44    :target: https://github.com/pyvista/pyvista/discussions\n45 \n46 .. |isort| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat\n47   :target: https://timothycrosley.github.io/isort\n48   :alt: isort\n49 \n50 .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n51   :target: https://github.com/psf/black\n52   :alt: black\n53 \n54 .. |python| image:: https://img.shields.io/badge/python-3.8+-blue.svg\n55    :target: https://www.python.org/downloads/\n56 \n57 .. |NumFOCUS Affiliated| image:: https://img.shields.io/badge/affiliated-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n58    :target: https://numfocus.org/sponsored-projects/affiliated-projects\n59 \n60 .. |pre-commit.ci status| image:: https://results.pre-commit.ci/badge/github/pyvista/pyvista/main.svg\n61    :target: https://results.pre-commit.ci/latest/github/pyvista/pyvista/main\n62 \n63 \n64 +----------------------+------------------------+-------------+\n65 | Deployment           | |pypi|                 | |conda|     |\n66 +----------------------+------------------------+-------------+\n67 | Build Status         | |GH-CI|                | |python|    |\n68 |                      +------------------------+-------------+\n69 |                      | |pre-commit.ci status| |             |\n70 +----------------------+------------------------+-------------+\n71 | Metrics              | |codacy|               | |codecov|   |\n72 +----------------------+------------------------+-------------+\n73 | Activity             | |PyPIact|              | |condaact|  |\n74 +----------------------+------------------------+-------------+\n75 | Citation             | |joss|                 | |zenodo|    |\n76 +----------------------+------------------------+-------------+\n77 | License              | |MIT|                  |             |\n78 +----------------------+------------------------+-------------+\n79 | Community            | |slack|                | |discuss|   |\n80 +----------------------+------------------------+-------------+\n81 | Formatter            | |black|                | |isort|     |\n82 +----------------------+------------------------+-------------+\n83 | Affiliated           | |NumFOCUS Affiliated|                |\n84 +----------------------+------------------------+-------------+\n85 \n86 \n87     3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)\n88 \n89 PyVista is:\n90 \n91 * *Pythonic VTK*: a high-level API to the `Visualization Toolkit`_ (VTK)\n92 * mesh data structures and filtering methods for spatial datasets\n93 * 3D plotting made simple and built for large/complex data geometries\n94 \n95 .. _Visualization Toolkit: https://vtk.org\n96 \n97 PyVista is a helper module for the Visualization Toolkit (VTK) that wraps the VTK library\n98 through NumPy and direct array access through a variety of methods and classes.\n99 This package provides a Pythonic, well-documented interface exposing\n100 VTK's powerful visualization backend to facilitate rapid prototyping, analysis,\n101 and visual integration of spatially referenced datasets.\n102 \n103 This module can be used for scientific plotting for presentations and research\n104 papers as well as a supporting module for other mesh 3D rendering dependent\n105 Python modules; see Connections for a list of projects that leverage\n106 PyVista.\n107 \n108 \n109 .. |tweet| image:: https://img.shields.io/twitter/url.svg?style=social&url=http%3A%2F%2Fshields.io\n110    :target: https://twitter.com/intent/tweet?text=Check%20out%20this%20project%20for%203D%20visualization%20in%20Python&url=https://github.com/pyvista/pyvista&hashtags=3D,visualization,Python,vtk,mesh,plotting,PyVista\n111 \n112 Share this project on Twitter: |tweet|\n113 \n114 \n115 PyVista is a NumFOCUS affiliated project\n116 \n117 .. image:: https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\n118    :target: https://numfocus.org/sponsored-projects/affiliated-projects\n119    :alt: NumFOCUS affiliated projects\n120    :height: 60px\n121 \n122 \n123 Highlights\n124 ==========\n125 \n126 .. |binder| image:: https://static.mybinder.org/badge_logo.svg\n127    :target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master\n128    :alt: Launch on Binder\n129 \n130 Head over to the `Quick Examples`_ page in the docs to explore our gallery of\n131 examples showcasing what PyVista can do. Want to test-drive PyVista?\n132 All of the examples from the gallery are live on MyBinder for you to test\n133 drive without installing anything locally: |binder|\n134 \n135 .. _Quick Examples: http://docs.pyvista.org/examples/index.html\n136 \n137 \n138 Overview of Features\n139 --------------------\n140 \n141 * Extensive gallery of examples (see `Quick Examples`_)\n142 * Interactive plotting in Jupyter Notebooks with server-side and client-side\n143   rendering with `trame`_.\n144 * Filtering/plotting tools built for interactivity (see `Widgets`_)\n145 * Direct access to mesh analysis and transformation routines (see Filters_)\n146 * Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)\n147 * Import meshes from many common formats (use ``pyvista.read()``). Support for all formats handled by `meshio`_ is built-in.\n148 * Export meshes as VTK, STL, OBJ, or PLY (``mesh.save()``) file types or any formats supported by meshio_ (``pyvista.save_meshio()``)\n149 \n150 .. _trame: https://github.com/Kitware/trame\n151 .. _Widgets: https://docs.pyvista.org/api/plotting/index.html#widget-api\n152 .. _Filters: https://docs.pyvista.org/api/core/filters.html\n153 .. _Plotting: https://docs.pyvista.org/api/plotting/index.html\n154 .. _meshio: https://github.com/nschloe/meshio\n155 \n156 \n157 Documentation\n158 =============\n159 \n160 Refer to the `documentation <http://docs.pyvista.org/>`_ for detailed\n161 installation and usage details.\n162 \n163 For general questions about the project, its applications, or about software\n164 usage, please create a discussion in `pyvista/discussions`_\n165 where the community can collectively address your questions. You are also\n166 welcome to join us on Slack_ or send one of the developers an email.\n167 The project support team can be reached at `info@pyvista.org`_.\n168 \n169 .. _pyvista/discussions: https://github.com/pyvista/pyvista/discussions\n170 .. _Slack: http://slack.pyvista.org\n171 .. _info@pyvista.org: mailto:info@pyvista.org\n172 \n173 \n174 Installation\n175 ============\n176 \n177 PyVista can be installed from `PyPI <https://pypi.org/project/pyvista/>`_\n178 using ``pip`` on Python >= 3.8::\n179 \n180     pip install pyvista\n181 \n182 You can also visit `PyPI <https://pypi.org/project/pyvista/>`_,\n183 `Anaconda <https://anaconda.org/conda-forge/pyvista>`_, or\n184 `GitHub <https://github.com/pyvista/pyvista>`_ to download the source.\n185 \n186 See the `Installation <http://docs.pyvista.org/getting-started/installation.html#install-ref.>`_\n187 for more details regarding optional dependencies or if the installation through pip doesn't work out.\n188 \n189 \n190 Connections\n191 ===========\n192 \n193 PyVista is a powerful tool that researchers can harness to create compelling,\n194 integrated visualizations of large datasets in an intuitive, Pythonic manner.\n195 \n196 Learn more about how PyVista is used across science and engineering disciplines\n197 by a diverse community of users on our `Connections page`_.\n198 \n199 .. _Connections page: https://docs.pyvista.org/getting-started/connections.html\n200 \n201 \n202 Authors\n203 =======\n204 \n205 Please take a look at the `contributors page`_ and the active `list of authors`_\n206 to learn more about the developers of PyVista.\n207 \n208 .. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/\n209 .. _list of authors: https://docs.pyvista.org/getting-started/authors.html#authors\n210 \n211 \n212 Contributing\n213 ============\n214 \n215 .. |Contributor Covenant| image:: https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg\n216    :target: CODE_OF_CONDUCT.md\n217 \n218 .. |codetriage| image:: https://www.codetriage.com/pyvista/pyvista/badges/users.svg\n219    :target: https://www.codetriage.com/pyvista/pyvista\n220    :alt: Code Triage\n221 \n222 |Contributor Covenant|\n223 |codetriage|\n224 \n225 We absolutely welcome contributions and we hope that our `Contributing Guide`_\n226 will facilitate your ability to make PyVista better. PyVista is mostly\n227 maintained on a volunteer basis and thus we need to foster a community that can\n228 support user questions and develop new features to make this software a useful\n229 tool for all users while encouraging every member of the community to share\n230 their ideas. To learn more about contributing to PyVista, please see the\n231 `Contributing Guide`_ and our `Code of Conduct`_.\n232 \n233 .. _Contributing Guide: https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst\n234 .. _Code of Conduct: https://github.com/pyvista/pyvista/blob/main/CODE_OF_CONDUCT.md\n235 \n236 \n237 Citing PyVista\n238 ==============\n239 \n240 There is a `paper about PyVista <https://doi.org/10.21105/joss.01450>`_.\n241 \n242 If you are using PyVista in your scientific research, please help our scientific\n243 visibility by citing our work.\n244 \n245 \n246     Sullivan and Kaszynski, (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450\n247 \n248 \n249 BibTex:\n250 \n251 .. code::\n252 \n253     @article{sullivan2019pyvista,\n254       doi = {10.21105/joss.01450},\n255       url = {https://doi.org/10.21105/joss.01450},\n256       year = {2019},\n257       month = {May},\n258       publisher = {The Open Journal},\n259       volume = {4},\n260       number = {37},\n261       pages = {1450},\n262       author = {Bane Sullivan and Alexander Kaszynski},\n263       title = {{PyVista}: {3D} plotting and mesh analysis through a streamlined interface for the {Visualization Toolkit} ({VTK})},\n264       journal = {Journal of Open Source Software}\n265     }\n266 \n[end of README.rst]\n[start of examples/99-advanced/add-example.py]\n1 \"\"\"\n2 .. _add_example_example:\n3 \n4 Adding a New Gallery Example\n5 ----------------------------\n6 This example demonstrates how to add a new PyVista `Sphinx Gallery\n7 <https://sphinx-gallery.github.io/>`_ example as well as being a template that\n8 can be used in their creation.\n9 \n10 Each example should have a reference tag/key in the form:\n11 \n12 ``.. _<example-name>_example:``\n13 \n14 The ``.. _`` is necessary. Everything that follows is your reference tag, which\n15 can potentially be used within a docstring. As convention, we keep all\n16 references all in ``snake_case``.\n17 \n18 This section should give a brief overview of what the example is about and/or\n19 demonstrates.  The title should be changed to reflect the topic your example\n20 covers.\n21 \n22 New examples should be added as python scripts to:\n23 \n24 ``examples/<index>-<directory-name>/<some-example>.py``\n25 \n26 .. note::\n27    Avoid creating new directories unless absolutely necessary.If you *must*\n28    create a new folder, make sure to add a ``README.txt`` containing a\n29    reference, a title and a single sentence description of the folder.\n30    Otherwise the new folder will be ignored by Sphinx.\n31 \n32 Example file names should be hyphen separated snake case:\n33 \n34 ``some-example.py``\n35 \n36 After this preamble is complete, the first code block begins. This is where you\n37 typically set up your imports.\n38 \n39 \"\"\"\n40 \n41 import pyvista as pv\n42 from pyvista import examples\n43 \n44 ###############################################################################\n45 # Section Title\n46 # ~~~~~~~~~~~~~\n47 # Code blocks can be broken up with text \"sections\" which are interpreted as\n48 # restructured text.\n49 #\n50 # This will also be translated into a markdown cell in the generated jupyter\n51 # notebook or the HTML page.\n52 #\n53 # Sections can contain any information you may have regarding the example\n54 # such as step-by-step comments or notes regarding motivations etc.\n55 #\n56 # As in Jupyter notebooks, if a statement is unassigned at the end of a code\n57 # block, output will be generated and printed to the screen according to its\n58 # ``__repr__`` method.  Otherwise, you can use ``print()`` to output text.\n59 \n60 # Create a dataset and exercise it's repr method\n61 dataset = pv.Sphere()\n62 dataset\n63 \n64 \n65 ###############################################################################\n66 # Plots and images\n67 # ~~~~~~~~~~~~~~~~\n68 # If you use anything that outputs an image (for example,\n69 # :func:`pyvista.Plotter.show`) the resulting image will be rendered within the\n70 # output HTML.\n71 #\n72 # .. note::\n73 #    Unless ``sphinx_gallery_thumbnail_number = <int>`` is included at the top\n74 #    of the example script, first figure (this one) will be used for the\n75 #    gallery thumbnail image.\n76 #\n77 #    Also note that this image number uses one based indexing.\n78 \n79 dataset.plot(text='Example Figure')\n80 \n81 \n82 ###############################################################################\n83 # Caveat - Plotter must be within One Cell\n84 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n85 # It's not possible for a single :class:`pyvista.Plotter` object across\n86 # multiple cells because these are closed out automatically at the end of a\n87 # cell.\n88 #\n89 # Here we just exercise the :class:`pyvista.Actor` ``repr`` for demonstrating\n90 # why you might want to instantiate a plotter without showing it in the same\n91 # cell.\n92 \n93 pl = pv.Plotter()\n94 actor = pl.add_mesh(dataset)\n95 actor\n96 \n97 \n98 ###############################################################################\n99 # This Cell Cannot Run the Plotter\n100 # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n101 # The plotter will already be closed by ``sphinx_gallery``.\n102 \n103 # This cannot be run here because the plotter is already closed and would raise\n104 # an error:\n105 # >>> pl.show()\n106 \n107 # You can, however, close out the plotter or access other attributes.\n108 pl.close()\n109 \n110 \n111 ###############################################################################\n112 # Animations\n113 # ~~~~~~~~~~\n114 # You can even create animations, and while there is a full example in\n115 # :ref:`movie_example`, this cell explains how you can create an animation\n116 # within a single cell.\n117 #\n118 # Here, we explode a simple sphere.\n119 \n120 pl = pv.Plotter(off_screen=True)\n121 \n122 # optimize for size\n123 pl.open_gif('example_movie.gif', palettesize=16)\n124 \n125 sphere = pv.Sphere(theta_resolution=10, phi_resolution=10)\n126 \n127 # Add initial mesh to setup the camera\n128 actor = pl.add_mesh(sphere)\n129 pl.background_color = 'w'\n130 \n131 # clear and overwrite the mesh on each frame\n132 n_frames = 20\n133 for i in range(n_frames):\n134     exploded = sphere.explode(factor=i / (n_frames * 2)).extract_surface()\n135     actor.mapper.dataset.copy_from(exploded)\n136     pl.camera.reset_clipping_range()\n137     pl.write_frame()  # Write this frame\n138 \n139 # Be sure to close the plotter when finished\n140 pl.close()\n141 \n142 \n143 ###############################################################################\n144 # Adding Example Files\n145 # ~~~~~~~~~~~~~~~~~~~~\n146 # PyVista has a variety of example files all stored at `pyvista/vtk_data\n147 # <https://github.com/pyvista/vtk-data>`_, and you can add the file by\n148 # following the directions there.\n149 #\n150 # Under the hood, PyVista uses `pooch <https://github.com/fatiando/pooch>`_,\n151 # and you can easily access any files added with\n152 # :func:`pyvista.examples.downloads.download_file`.\n153 \n154 filename = examples.download_file('bunny.ply')\n155 filename\n156 \n157 \n158 ###############################################################################\n159 # Adding a Wrapped Example\n160 # ~~~~~~~~~~~~~~~~~~~~~~~~\n161 # While it's possible to simply download a file and then read it in, it's\n162 # better for you to write a wrapped ``download_<example-dataset>()`` within\n163 # ``/pyvista/examples/downloads.py``. For example :func:`download_bunny()\n164 # <pyvista.examples.downloads.download_bunny>` downloads and reads with\n165 # :func:`pyvista.read`.\n166 #\n167 # If you intend on adding an example file, you should add a new function in\n168 # ``downloads.py`` to make it easy for users to add example files.\n169 \n170 dataset = examples.download_bunny()\n171 dataset\n172 \n173 \n174 # Making a Pull Request\n175 # ~~~~~~~~~~~~~~~~~~~~~\n176 # Once your example is complete and you've verified it builds locally, you can\n177 # make a pull request (PR).\n178 #\n179 # Branches containing examples should be prefixed with `docs/` as per the branch\n180 # naming conventions found in out `Contributing Guidelines\n181 # <https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst>`_.\n182 #\n183 # .. note::\n184 #    You only need to create the Python source example (``*.py``).  The jupyter\n185 #    notebook and the example HTML will be auto-generated via `sphinx-gallery\n186 #    <https://sphinx-gallery.github.io/>`_.\n187 \n[end of examples/99-advanced/add-example.py]\n[start of pyvista/core/dataobject.py]\n1 \"\"\"Attributes common to PolyData and Grid Objects.\"\"\"\n2 \n3 from abc import abstractmethod\n4 import collections.abc\n5 from pathlib import Path\n6 from typing import Any, DefaultDict, Dict, Type, Union\n7 \n8 import numpy as np\n9 \n10 import pyvista\n11 from pyvista import _vtk\n12 from pyvista.utilities import FieldAssociation, abstract_class, fileio\n13 \n14 from .datasetattributes import DataSetAttributes\n15 \n16 # vector array names\n17 DEFAULT_VECTOR_KEY = '_vectors'\n18 \n19 \n20 @abstract_class\n21 class DataObject:\n22     \"\"\"Methods common to all wrapped data objects.\"\"\"\n23 \n24     _WRITERS: Dict[str, Union[Type[_vtk.vtkXMLWriter], Type[_vtk.vtkDataWriter]]] = {}\n25 \n26     def __init__(self, *args, **kwargs) -> None:\n27         \"\"\"Initialize the data object.\"\"\"\n28         super().__init__()\n29         # Remember which arrays come from numpy.bool arrays, because there is no direct\n30         # conversion from bool to vtkBitArray, such arrays are stored as vtkCharArray.\n31         self._association_bitarray_names: DefaultDict = collections.defaultdict(set)\n32 \n33         # view these arrays as complex128 as VTK doesn't support complex types\n34         self._association_complex_names: DefaultDict = collections.defaultdict(set)\n35 \n36     def __getattr__(self, item: str) -> Any:\n37         \"\"\"Get attribute from base class if not found.\"\"\"\n38         return super().__getattribute__(item)\n39 \n40     def shallow_copy(self, to_copy: _vtk.vtkDataObject) -> _vtk.vtkDataObject:\n41         \"\"\"Shallow copy the given mesh to this mesh.\n42 \n43         Parameters\n44         ----------\n45         to_copy : pyvista.DataObject or vtk.vtkDataObject\n46             Data object to perform a shallow copy from.\n47 \n48         \"\"\"\n49         self.ShallowCopy(to_copy)\n50 \n51     def deep_copy(self, to_copy: _vtk.vtkDataObject) -> _vtk.vtkDataObject:\n52         \"\"\"Overwrite this data object with another data object as a deep copy.\n53 \n54         Parameters\n55         ----------\n56         to_copy : pyvista.DataObject or vtk.vtkDataObject\n57             Data object to perform a deep copy from.\n58 \n59         \"\"\"\n60         self.DeepCopy(to_copy)\n61 \n62     def _from_file(self, filename: Union[str, Path], **kwargs):\n63         data = pyvista.read(filename, **kwargs)\n64         if not isinstance(self, type(data)):\n65             raise ValueError(\n66                 f'Reading file returned data of `{type(data).__name__}`, '\n67                 f'but `{type(self).__name__}` was expected.'\n68             )\n69         self.shallow_copy(data)\n70         self._post_file_load_processing()\n71 \n72     def _post_file_load_processing(self):\n73         \"\"\"Execute after loading a dataset from file, to be optionally overridden by subclasses.\"\"\"\n74         pass\n75 \n76     def save(self, filename: str, binary=True, texture=None):\n77         \"\"\"Save this vtk object to file.\n78 \n79         Parameters\n80         ----------\n81         filename : str, pathlib.Path\n82             Filename of output file. Writer type is inferred from\n83             the extension of the filename.\n84 \n85         binary : bool, default: True\n86             If ``True``, write as binary.  Otherwise, write as ASCII.\n87 \n88         texture : str, np.ndarray, optional\n89             Write a single texture array to file when using a PLY\n90             file.  Texture array must be a 3 or 4 component array with\n91             the datatype ``np.uint8``.  Array may be a cell array or a\n92             point array, and may also be a string if the array already\n93             exists in the PolyData.\n94 \n95             If a string is provided, the texture array will be saved\n96             to disk as that name.  If an array is provided, the\n97             texture array will be saved as ``'RGBA'``\n98 \n99             .. note::\n100                This feature is only available when saving PLY files.\n101 \n102         Notes\n103         -----\n104         Binary files write much faster than ASCII and have a smaller\n105         file size.\n106 \n107         \"\"\"\n108         if self._WRITERS is None:\n109             raise NotImplementedError(\n110                 f'{self.__class__.__name__} writers are not specified,'\n111                 ' this should be a dict of (file extension: vtkWriter type)'\n112             )\n113 \n114         file_path = Path(filename)\n115         file_path = file_path.expanduser()\n116         file_path = file_path.resolve()\n117         file_ext = file_path.suffix\n118         if file_ext not in self._WRITERS:\n119             raise ValueError(\n120                 'Invalid file extension for this data type.'\n121                 f' Must be one of: {self._WRITERS.keys()}'\n122             )\n123 \n124         # store complex and bitarray types as field data\n125         self._store_metadata()\n126 \n127         writer = self._WRITERS[file_ext]()\n128         fileio.set_vtkwriter_mode(vtk_writer=writer, use_binary=binary)\n129         writer.SetFileName(str(file_path))\n130         writer.SetInputData(self)\n131         if file_ext == '.ply' and texture is not None:\n132             if isinstance(texture, str):\n133                 writer.SetArrayName(texture)\n134                 array_name = texture\n135             elif isinstance(texture, np.ndarray):\n136                 array_name = '_color_array'\n137                 self[array_name] = texture\n138                 writer.SetArrayName(array_name)\n139 \n140             # enable alpha channel if applicable\n141             if self[array_name].shape[-1] == 4:  # type: ignore\n142                 writer.SetEnableAlpha(True)\n143         writer.Write()\n144 \n145     def _store_metadata(self):\n146         \"\"\"Store metadata as field data.\"\"\"\n147         fdata = self.field_data\n148         for assoc_name in ('bitarray', 'complex'):\n149             for assoc_type in ('POINT', 'CELL'):\n150                 assoc_data = getattr(self, f'_association_{assoc_name}_names')\n151                 array_names = assoc_data.get(assoc_type)\n152                 if array_names:\n153                     key = f'_PYVISTA_{assoc_name}_{assoc_type}_'.upper()\n154                     fdata[key] = list(array_names)\n155 \n156     def _restore_metadata(self):\n157         \"\"\"Restore PyVista metadata from field data.\n158 \n159         Metadata is stored using ``_store_metadata`` and contains entries in\n160         the format of f'_PYVISTA_{assoc_name}_{assoc_type}_'. These entries are\n161         removed when calling this method.\n162 \n163         \"\"\"\n164         fdata = self.field_data\n165         for assoc_name in ('bitarray', 'complex'):\n166             for assoc_type in ('POINT', 'CELL'):\n167                 key = f'_PYVISTA_{assoc_name}_{assoc_type}_'.upper()\n168                 if key in fdata:\n169                     assoc_data = getattr(self, f'_association_{assoc_name}_names')\n170                     assoc_data[assoc_type] = set(fdata[key])\n171                     del fdata[key]\n172 \n173     @abstractmethod\n174     def get_data_range(self):  # pragma: no cover\n175         \"\"\"Get the non-NaN min and max of a named array.\"\"\"\n176         raise NotImplementedError(\n177             f'{type(self)} mesh type does not have a `get_data_range` method.'\n178         )\n179 \n180     def _get_attrs(self):  # pragma: no cover\n181         \"\"\"Return the representation methods (internal helper).\"\"\"\n182         raise NotImplementedError('Called only by the inherited class')\n183 \n184     def head(self, display=True, html=None):\n185         \"\"\"Return the header stats of this dataset.\n186 \n187         If in IPython, this will be formatted to HTML. Otherwise\n188         returns a console friendly string.\n189 \n190         Parameters\n191         ----------\n192         display : bool, default: True\n193             Display this header in iPython.\n194 \n195         html : bool, optional\n196             Generate the output as HTML.\n197 \n198         Returns\n199         -------\n200         str\n201             Header statistics.\n202 \n203         \"\"\"\n204         # Generate the output\n205         if html:\n206             fmt = \"\"\n207             # HTML version\n208             fmt += \"\\n\"\n209             fmt += \"<table>\\n\"\n210             fmt += f\"<tr><th>{type(self).__name__}</th><th>Information</th></tr>\\n\"\n211             row = \"<tr><td>{}</td><td>{}</td></tr>\\n\"\n212             # now make a call on the object to get its attributes as a list of len 2 tuples\n213             for attr in self._get_attrs():\n214                 try:\n215                     fmt += row.format(attr[0], attr[2].format(*attr[1]))\n216                 except:\n217                     fmt += row.format(attr[0], attr[2].format(attr[1]))\n218             if hasattr(self, 'n_arrays'):\n219                 fmt += row.format('N Arrays', self.n_arrays)\n220             fmt += \"</table>\\n\"\n221             fmt += \"\\n\"\n222             if display:\n223                 from IPython.display import HTML, display as _display\n224 \n225                 _display(HTML(fmt))\n226                 return\n227             return fmt\n228         # Otherwise return a string that is Python console friendly\n229         fmt = f\"{type(self).__name__} ({hex(id(self))})\\n\"\n230         # now make a call on the object to get its attributes as a list of len 2 tuples\n231         # get longest row header\n232         max_len = max(len(attr[0]) for attr in self._get_attrs()) + 4\n233 \n234         # now make a call on the object to get its attributes as a list of len\n235         # 2 tuples\n236         row = \"  {:%ds}{}\\n\" % max_len\n237         for attr in self._get_attrs():\n238             try:\n239                 fmt += row.format(attr[0] + ':', attr[2].format(*attr[1]))\n240             except:\n241                 fmt += row.format(attr[0] + ':', attr[2].format(attr[1]))\n242         if hasattr(self, 'n_arrays'):\n243             fmt += row.format('N Arrays:', self.n_arrays)\n244         return fmt\n245 \n246     def _repr_html_(self):  # pragma: no cover\n247         \"\"\"Return a pretty representation for Jupyter notebooks.\n248 \n249         This includes header details and information about all arrays.\n250 \n251         \"\"\"\n252         raise NotImplementedError('Called only by the inherited class')\n253 \n254     def copy_meta_from(self, *args, **kwargs):  # pragma: no cover\n255         \"\"\"Copy pyvista meta data onto this object from another object.\n256 \n257         Intended to be overridden by subclasses.\n258 \n259         Parameters\n260         ----------\n261         *args : tuple\n262             Positional arguments.\n263 \n264         **kwargs : dict, optional\n265             Keyword arguments.\n266 \n267         \"\"\"\n268         pass  # called only by the inherited class\n269 \n270     def copy(self, deep=True):\n271         \"\"\"Return a copy of the object.\n272 \n273         Parameters\n274         ----------\n275         deep : bool, default: True\n276             When ``True`` makes a full copy of the object.  When\n277             ``False``, performs a shallow copy where the points, cell,\n278             and data arrays are references to the original object.\n279 \n280         Returns\n281         -------\n282         pyvista.DataSet\n283             Deep or shallow copy of the input.  Type is identical to\n284             the input.\n285 \n286         Examples\n287         --------\n288         Create and make a deep copy of a PolyData object.\n289 \n290         >>> import pyvista\n291         >>> mesh_a = pyvista.Sphere()\n292         >>> mesh_b = mesh_a.copy()\n293         >>> mesh_a == mesh_b\n294         True\n295 \n296         \"\"\"\n297         thistype = type(self)\n298         newobject = thistype()\n299 \n300         if deep:\n301             newobject.deep_copy(self)\n302         else:\n303             newobject.shallow_copy(self)\n304         newobject.copy_meta_from(self, deep)\n305         return newobject\n306 \n307     def __eq__(self, other):\n308         \"\"\"Test equivalency between data objects.\"\"\"\n309         if not isinstance(self, type(other)):\n310             return False\n311 \n312         if self is other:\n313             return True\n314 \n315         # these attrs use numpy.array_equal\n316         equal_attrs = [\n317             'verts',  # DataObject\n318             'points',  # DataObject\n319             'lines',  # DataObject\n320             'faces',  # DataObject\n321             'cells',  # UnstructuredGrid\n322             'celltypes',\n323         ]  # UnstructuredGrid\n324         for attr in equal_attrs:\n325             if hasattr(self, attr):\n326                 if not np.array_equal(getattr(self, attr), getattr(other, attr)):\n327                     return False\n328 \n329         # these attrs can be directly compared\n330         attrs = ['field_data', 'point_data', 'cell_data']\n331         for attr in attrs:\n332             if hasattr(self, attr):\n333                 if getattr(self, attr) != getattr(other, attr):\n334                     return False\n335 \n336         return True\n337 \n338     def add_field_data(self, array: np.ndarray, name: str, deep=True):\n339         \"\"\"Add field data.\n340 \n341         Use field data when size of the data you wish to associate\n342         with the dataset does not match the number of points or cells\n343         of the dataset.\n344 \n345         Parameters\n346         ----------\n347         array : sequence\n348             Array of data to add to the dataset as a field array.\n349 \n350         name : str\n351             Name to assign the field array.\n352 \n353         deep : bool, default: True\n354             Perform a deep copy of the data when adding it to the\n355             dataset.\n356 \n357         Examples\n358         --------\n359         Add field data to a PolyData dataset.\n360 \n361         >>> import pyvista\n362         >>> import numpy as np\n363         >>> mesh = pyvista.Sphere()\n364         >>> mesh.add_field_data(np.arange(10), 'my-field-data')\n365         >>> mesh['my-field-data']\n366         pyvista_ndarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n367 \n368         Add field data to a UniformGrid dataset.\n369 \n370         >>> mesh = pyvista.UniformGrid(dimensions=(2, 2, 1))\n371         >>> mesh.add_field_data(\n372         ...     ['I could', 'write', 'notes', 'here'], 'my-field-data'\n373         ... )\n374         >>> mesh['my-field-data']\n375         pyvista_ndarray(['I could', 'write', 'notes', 'here'], dtype='<U7')\n376 \n377         Add field data to a MultiBlock dataset.\n378 \n379         >>> blocks = pyvista.MultiBlock()\n380         >>> blocks.append(pyvista.Sphere())\n381         >>> blocks[\"cube\"] = pyvista.Cube(center=(0, 0, -1))\n382         >>> blocks.add_field_data([1, 2, 3], 'my-field-data')\n383         >>> blocks.field_data['my-field-data']\n384         pyvista_ndarray([1, 2, 3])\n385 \n386         \"\"\"\n387         if not hasattr(self, 'field_data'):\n388             raise NotImplementedError(f'`{type(self)}` does not support field data')\n389 \n390         self.field_data.set_array(array, name, deep_copy=deep)\n391 \n392     @property\n393     def field_data(self) -> DataSetAttributes:\n394         \"\"\"Return FieldData as DataSetAttributes.\n395 \n396         Use field data when size of the data you wish to associate\n397         with the dataset does not match the number of points or cells\n398         of the dataset.\n399 \n400         Examples\n401         --------\n402         Add field data to a PolyData dataset and then return it.\n403 \n404         >>> import pyvista\n405         >>> import numpy as np\n406         >>> mesh = pyvista.Sphere()\n407         >>> mesh.field_data['my-field-data'] = np.arange(10)\n408         >>> mesh.field_data['my-field-data']\n409         pyvista_ndarray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n410 \n411         \"\"\"\n412         return DataSetAttributes(\n413             self.GetFieldData(), dataset=self, association=FieldAssociation.NONE\n414         )\n415 \n416     def clear_field_data(self):\n417         \"\"\"Remove all field data.\n418 \n419         Examples\n420         --------\n421         Add field data to a PolyData dataset and then remove it.\n422 \n423         >>> import pyvista\n424         >>> mesh = pyvista.Sphere()\n425         >>> mesh.field_data['my-field-data'] = range(10)\n426         >>> len(mesh.field_data)\n427         1\n428         >>> mesh.clear_field_data()\n429         >>> len(mesh.field_data)\n430         0\n431 \n432         \"\"\"\n433         if not hasattr(self, 'field_data'):\n434             raise NotImplementedError(f'`{type(self)}` does not support field data')\n435 \n436         self.field_data.clear()\n437 \n438     @property\n439     def memory_address(self) -> str:\n440         \"\"\"Get address of the underlying VTK C++ object.\n441 \n442         Returns\n443         -------\n444         str\n445             Memory address formatted as ``'Addr=%p'``.\n446 \n447         Examples\n448         --------\n449         >>> import pyvista\n450         >>> mesh = pyvista.Sphere()\n451         >>> mesh.memory_address\n452         'Addr=...'\n453 \n454         \"\"\"\n455         return self.GetInformation().GetAddressAsString(\"\")\n456 \n457     @property\n458     def actual_memory_size(self) -> int:\n459         \"\"\"Return the actual size of the dataset object.\n460 \n461         Returns\n462         -------\n463         int\n464             The actual size of the dataset object in kibibytes (1024\n465             bytes).\n466 \n467         Examples\n468         --------\n469         >>> from pyvista import examples\n470         >>> mesh = examples.load_airplane()\n471         >>> mesh.actual_memory_size  # doctest:+SKIP\n472         93\n473 \n474         \"\"\"\n475         return self.GetActualMemorySize()\n476 \n477     def copy_structure(self, dataset: _vtk.vtkDataSet):\n478         \"\"\"Copy the structure (geometry and topology) of the input dataset object.\n479 \n480         Parameters\n481         ----------\n482         dataset : vtk.vtkDataSet\n483             Dataset to copy the geometry and topology from.\n484 \n485         Examples\n486         --------\n487         >>> import pyvista as pv\n488         >>> source = pv.UniformGrid(dimensions=(10, 10, 5))\n489         >>> target = pv.UniformGrid()\n490         >>> target.copy_structure(source)\n491         >>> target.plot(show_edges=True)\n492 \n493         \"\"\"\n494         self.CopyStructure(dataset)\n495 \n496     def copy_attributes(self, dataset: _vtk.vtkDataSet):\n497         \"\"\"Copy the data attributes of the input dataset object.\n498 \n499         Parameters\n500         ----------\n501         dataset : pyvista.DataSet\n502             Dataset to copy the data attributes from.\n503 \n504         Examples\n505         --------\n506         >>> import pyvista as pv\n507         >>> source = pv.UniformGrid(dimensions=(10, 10, 5))\n508         >>> source = source.compute_cell_sizes()\n509         >>> target = pv.UniformGrid(dimensions=(10, 10, 5))\n510         >>> target.copy_attributes(source)\n511         >>> target.plot(scalars='Volume', show_edges=True)\n512 \n513         \"\"\"\n514         self.CopyAttributes(dataset)\n515 \n516     def __getstate__(self):\n517         \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n518 \n519         The format of the serialized VTK object data depends on `pyvista.PICKLE_FORMAT` (case-insensitive).\n520         - If `pyvista.PICKLE_FORMAT == 'xml'`, the data is serialized as an XML-formatted string.\n521         - If `pyvista.PICKLE_FORMAT == 'legacy'`, the data is serialized to bytes in VTK's binary format.\n522         \"\"\"\n523         state = self.__dict__.copy()\n524 \n525         if pyvista.PICKLE_FORMAT.lower() == 'xml':\n526             # the generic VTK XML writer `vtkXMLDataSetWriter` currently has a bug where it does not pass all\n527             # settings down to the sub-writers. Until this is fixed, use the dataset-specific writers\n528             # https://gitlab.kitware.com/vtk/vtk/-/issues/18661\n529             writers = {\n530                 _vtk.vtkImageData: _vtk.vtkXMLImageDataWriter,\n531                 _vtk.vtkStructuredGrid: _vtk.vtkXMLStructuredGridWriter,\n532                 _vtk.vtkRectilinearGrid: _vtk.vtkXMLRectilinearGridWriter,\n533                 _vtk.vtkUnstructuredGrid: _vtk.vtkXMLUnstructuredGridWriter,\n534                 _vtk.vtkPolyData: _vtk.vtkXMLPolyDataWriter,\n535                 _vtk.vtkTable: _vtk.vtkXMLTableWriter,\n536             }\n537 \n538             for parent_type, writer_type in writers.items():\n539                 if isinstance(self, parent_type):\n540                     writer = writer_type()\n541                     break\n542             else:\n543                 raise TypeError(f'Cannot pickle dataset of type {self.GetDataObjectType()}')\n544 \n545             writer.SetInputDataObject(self)\n546             writer.SetWriteToOutputString(True)\n547             writer.SetDataModeToBinary()\n548             writer.SetCompressorTypeToNone()\n549             writer.Write()\n550             to_serialize = writer.GetOutputString()\n551 \n552         elif pyvista.PICKLE_FORMAT.lower() == 'legacy':\n553             writer = _vtk.vtkDataSetWriter()\n554             writer.SetInputDataObject(self)\n555             writer.SetWriteToOutputString(True)\n556             writer.SetFileTypeToBinary()\n557             writer.Write()\n558             to_serialize = writer.GetOutputStdString()\n559 \n560         state['vtk_serialized'] = to_serialize\n561 \n562         # this needs to be here because in multiprocessing situations, `pyvista.PICKLE_FORMAT` is not shared between\n563         # processes\n564         state['PICKLE_FORMAT'] = pyvista.PICKLE_FORMAT\n565         return state\n566 \n567     def __setstate__(self, state):\n568         \"\"\"Support unpickle.\"\"\"\n569         vtk_serialized = state.pop('vtk_serialized')\n570         pickle_format = state.pop(\n571             'PICKLE_FORMAT', 'legacy'  # backwards compatibility - assume 'legacy'\n572         )\n573         self.__dict__.update(state)\n574 \n575         if pickle_format.lower() == 'xml':\n576             # the generic VTK XML reader `vtkXMLGenericDataObjectReader` currently has a bug where it does not pass all\n577             # settings down to the sub-readers. Until this is fixed, use the dataset-specific readers\n578             # https://gitlab.kitware.com/vtk/vtk/-/issues/18661\n579             readers = {\n580                 _vtk.vtkImageData: _vtk.vtkXMLImageDataReader,\n581                 _vtk.vtkStructuredGrid: _vtk.vtkXMLStructuredGridReader,\n582                 _vtk.vtkRectilinearGrid: _vtk.vtkXMLRectilinearGridReader,\n583                 _vtk.vtkUnstructuredGrid: _vtk.vtkXMLUnstructuredGridReader,\n584                 _vtk.vtkPolyData: _vtk.vtkXMLPolyDataReader,\n585                 _vtk.vtkTable: _vtk.vtkXMLTableReader,\n586             }\n587 \n588             for parent_type, reader_type in readers.items():\n589                 if isinstance(self, parent_type):\n590                     reader = reader_type()\n591                     break\n592             else:\n593                 raise TypeError(f'Cannot unpickle dataset of type {self.GetDataObjectType()}')\n594 \n595             reader.ReadFromInputStringOn()\n596             reader.SetInputString(vtk_serialized)\n597             reader.Update()\n598 \n599         elif pickle_format.lower() == 'legacy':\n600             reader = _vtk.vtkDataSetReader()\n601             reader.ReadFromInputStringOn()\n602             if isinstance(vtk_serialized, bytes):\n603                 reader.SetBinaryInputString(vtk_serialized, len(vtk_serialized))\n604             elif isinstance(vtk_serialized, str):\n605                 reader.SetInputString(vtk_serialized)\n606             reader.Update()\n607 \n608         mesh = pyvista.wrap(reader.GetOutput())\n609 \n610         # copy data\n611         self.copy_structure(mesh)\n612         self.copy_attributes(mesh)\n613 \n[end of pyvista/core/dataobject.py]\n[start of pyvista/utilities/helpers.py]\n1 \"\"\"Supporting functions for polydata and grid objects.\"\"\"\n2 \n3 import collections.abc\n4 import enum\n5 import logging\n6 import os\n7 import signal\n8 import sys\n9 import threading\n10 from threading import Thread\n11 import traceback\n12 from typing import Optional\n13 import warnings\n14 \n15 import numpy as np\n16 \n17 import pyvista\n18 from pyvista import _vtk\n19 from pyvista.errors import AmbiguousDataError, MissingDataError\n20 \n21 from . import transformations\n22 from .fileio import from_meshio\n23 \n24 \n25 class FieldAssociation(enum.Enum):\n26     \"\"\"Represents which type of vtk field a scalar or vector array is associated with.\"\"\"\n27 \n28     POINT = _vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS\n29     CELL = _vtk.vtkDataObject.FIELD_ASSOCIATION_CELLS\n30     NONE = _vtk.vtkDataObject.FIELD_ASSOCIATION_NONE\n31     ROW = _vtk.vtkDataObject.FIELD_ASSOCIATION_ROWS\n32 \n33 \n34 def get_vtk_type(typ):\n35     \"\"\"Look up the VTK type for a given numpy data type.\n36 \n37     Corrects for string type mapping issues.\n38 \n39     Parameters\n40     ----------\n41     typ : numpy.dtype\n42         Numpy data type.\n43 \n44     Returns\n45     -------\n46     int\n47         Integer type id specified in ``vtkType.h``\n48 \n49     \"\"\"\n50     typ = _vtk.get_vtk_array_type(typ)\n51     # This handles a silly string type bug\n52     if typ == 3:\n53         return 13\n54     return typ\n55 \n56 \n57 def vtk_bit_array_to_char(vtkarr_bint):\n58     \"\"\"Cast vtk bit array to a char array.\n59 \n60     Parameters\n61     ----------\n62     vtkarr_bint : vtk.vtkBitArray\n63         VTK binary array.\n64 \n65     Returns\n66     -------\n67     vtk.vtkCharArray\n68         VTK char array.\n69 \n70     Notes\n71     -----\n72     This performs a copy.\n73 \n74     \"\"\"\n75     vtkarr = _vtk.vtkCharArray()\n76     vtkarr.DeepCopy(vtkarr_bint)\n77     return vtkarr\n78 \n79 \n80 def vtk_id_list_to_array(vtk_id_list):\n81     \"\"\"Convert a vtkIdList to a NumPy array.\n82 \n83     Parameters\n84     ----------\n85     vtk_id_list : vtk.vtkIdList\n86         VTK ID list.\n87 \n88     Returns\n89     -------\n90     numpy.ndarray\n91         Array of IDs.\n92 \n93     \"\"\"\n94     return np.array([vtk_id_list.GetId(i) for i in range(vtk_id_list.GetNumberOfIds())])\n95 \n96 \n97 def convert_string_array(arr, name=None):\n98     \"\"\"Convert a numpy array of strings to a vtkStringArray or vice versa.\n99 \n100     Parameters\n101     ----------\n102     arr : numpy.ndarray\n103         Numpy string array to convert.\n104 \n105     name : str, optional\n106         Name to set the vtkStringArray to.\n107 \n108     Returns\n109     -------\n110     vtkStringArray\n111         VTK string array.\n112 \n113     Notes\n114     -----\n115     Note that this is terribly inefficient. If you have ideas on how\n116     to make this faster, please consider opening a pull request.\n117 \n118     \"\"\"\n119     if isinstance(arr, np.ndarray):\n120         # VTK default fonts only support ASCII. See https://gitlab.kitware.com/vtk/vtk/-/issues/16904\n121         if np.issubdtype(arr.dtype, np.str_) and not ''.join(arr).isascii():  # avoids segfault\n122             raise ValueError(\n123                 'String array contains non-ASCII characters that are not supported by VTK.'\n124             )\n125         vtkarr = _vtk.vtkStringArray()\n126         ########### OPTIMIZE ###########\n127         for val in arr:\n128             vtkarr.InsertNextValue(val)\n129         ################################\n130         if isinstance(name, str):\n131             vtkarr.SetName(name)\n132         return vtkarr\n133     # Otherwise it is a vtk array and needs to be converted back to numpy\n134     ############### OPTIMIZE ###############\n135     nvalues = arr.GetNumberOfValues()\n136     return np.array([arr.GetValue(i) for i in range(nvalues)], dtype='|U')\n137     ########################################\n138 \n139 \n140 def convert_array(arr, name=None, deep=False, array_type=None):\n141     \"\"\"Convert a NumPy array to a vtkDataArray or vice versa.\n142 \n143     Parameters\n144     ----------\n145     arr : np.ndarray | vtkDataArray\n146         A numpy array or vtkDataArry to convert.\n147     name : str, optional\n148         The name of the data array for VTK.\n149     deep : bool, default: False\n150         If input is numpy array then deep copy values.\n151     array_type : int, optional\n152         VTK array type ID as specified in specified in ``vtkType.h``.\n153 \n154     Returns\n155     -------\n156     vtkDataArray or numpy.ndarray\n157         The converted array.  If input is a :class:`numpy.ndarray` then\n158         returns ``vtkDataArray`` or is input is ``vtkDataArray`` then\n159         returns NumPy ``ndarray``.\n160 \n161     \"\"\"\n162     if arr is None:\n163         return\n164     if isinstance(arr, (list, tuple)):\n165         arr = np.array(arr)\n166     if isinstance(arr, np.ndarray):\n167         if arr.dtype == np.dtype('O'):\n168             arr = arr.astype('|S')\n169         arr = np.ascontiguousarray(arr)\n170         if arr.dtype.type in (np.str_, np.bytes_):\n171             # This handles strings\n172             vtk_data = convert_string_array(arr)\n173         else:\n174             # This will handle numerical data\n175             arr = np.ascontiguousarray(arr)\n176             vtk_data = _vtk.numpy_to_vtk(num_array=arr, deep=deep, array_type=array_type)\n177         if isinstance(name, str):\n178             vtk_data.SetName(name)\n179         return vtk_data\n180     # Otherwise input must be a vtkDataArray\n181     if not isinstance(arr, (_vtk.vtkDataArray, _vtk.vtkBitArray, _vtk.vtkStringArray)):\n182         raise TypeError(f'Invalid input array type ({type(arr)}).')\n183     # Handle booleans\n184     if isinstance(arr, _vtk.vtkBitArray):\n185         arr = vtk_bit_array_to_char(arr)\n186     # Handle string arrays\n187     if isinstance(arr, _vtk.vtkStringArray):\n188         return convert_string_array(arr)\n189     # Convert from vtkDataArry to NumPy\n190     return _vtk.vtk_to_numpy(arr)\n191 \n192 \n193 def is_pyvista_dataset(obj):\n194     \"\"\"Return ``True`` if the object is a PyVista wrapped dataset.\n195 \n196     Parameters\n197     ----------\n198     obj : Any\n199         Any object to test.\n200 \n201     Returns\n202     -------\n203     bool\n204         ``True`` when the object is a :class:`pyvista.DataSet`.\n205 \n206     \"\"\"\n207     return isinstance(obj, (pyvista.DataSet, pyvista.MultiBlock))\n208 \n209 \n210 def _assoc_array(obj, name, association='point'):\n211     \"\"\"Return a point, cell, or field array from a pyvista.DataSet or VTK object.\n212 \n213     If the array or index doesn't exist, return nothing. This matches VTK's\n214     behavior when using ``GetAbstractArray`` with an invalid key or index.\n215 \n216     \"\"\"\n217     vtk_attr = f'Get{association.title()}Data'\n218     python_attr = f'{association.lower()}_data'\n219 \n220     if isinstance(obj, pyvista.DataSet):\n221         try:\n222             return getattr(obj, python_attr).get_array(name)\n223         except KeyError:  # pragma: no cover\n224             return None\n225     abstract_array = getattr(obj, vtk_attr)().GetAbstractArray(name)\n226     if abstract_array is not None:\n227         return pyvista.pyvista_ndarray(abstract_array)\n228     return None\n229 \n230 \n231 def point_array(obj, name):\n232     \"\"\"Return point array of a pyvista or vtk object.\n233 \n234     Parameters\n235     ----------\n236     obj : pyvista.DataSet | vtk.vtkDataSet\n237         PyVista or VTK dataset.\n238 \n239     name : str | int\n240         Name or index of the array.\n241 \n242     Returns\n243     -------\n244     pyvista.pyvista_ndarray or None\n245         Wrapped array if the index or name is valid. Otherwise, ``None``.\n246 \n247     \"\"\"\n248     return _assoc_array(obj, name, 'point')\n249 \n250 \n251 def field_array(obj, name):\n252     \"\"\"Return field data of a pyvista or vtk object.\n253 \n254     Parameters\n255     ----------\n256     obj : pyvista.DataSet or vtk.vtkDataSet\n257         PyVista or VTK dataset.\n258 \n259     name : str | int\n260         Name or index of the array.\n261 \n262     Returns\n263     -------\n264     pyvista.pyvista_ndarray or None\n265         Wrapped array if the index or name is valid. Otherwise, ``None``.\n266 \n267     \"\"\"\n268     return _assoc_array(obj, name, 'field')\n269 \n270 \n271 def cell_array(obj, name):\n272     \"\"\"Return cell array of a pyvista or vtk object.\n273 \n274     Parameters\n275     ----------\n276     obj : pyvista.DataSet or vtk.vtkDataSet\n277         PyVista or VTK dataset.\n278 \n279     name : str | int\n280         Name or index of the array.\n281 \n282     Returns\n283     -------\n284     pyvista.pyvista_ndarray or None\n285         Wrapped array if the index or name is valid. Otherwise, ``None``.\n286 \n287     \"\"\"\n288     return _assoc_array(obj, name, 'cell')\n289 \n290 \n291 def row_array(obj, name):\n292     \"\"\"Return row array of a vtk object.\n293 \n294     Parameters\n295     ----------\n296     obj : vtk.vtkDataSet\n297         PyVista or VTK dataset.\n298 \n299     name : str\n300         Name of the array.\n301 \n302     Returns\n303     -------\n304     numpy.ndarray\n305         Wrapped array.\n306 \n307     \"\"\"\n308     vtkarr = obj.GetRowData().GetAbstractArray(name)\n309     return convert_array(vtkarr)\n310 \n311 \n312 def parse_field_choice(field):\n313     \"\"\"Return a field association object for a given field type string.\n314 \n315     Parameters\n316     ----------\n317     field : str, FieldAssociation\n318         Name of the field (e.g, ``'cell'``, ``'field'``, ``'point'``,\n319         ``'row'``).\n320 \n321     Returns\n322     -------\n323     pyvista.FieldAssociation\n324         Field association.\n325 \n326     \"\"\"\n327     if isinstance(field, str):\n328         field = field.strip().lower()\n329         if field in ['cell', 'c', 'cells']:\n330             field = FieldAssociation.CELL\n331         elif field in ['point', 'p', 'points']:\n332             field = FieldAssociation.POINT\n333         elif field in ['field', 'f', 'fields']:\n334             field = FieldAssociation.NONE\n335         elif field in ['row', 'r']:\n336             field = FieldAssociation.ROW\n337         else:\n338             raise ValueError(f'Data field ({field}) not supported.')\n339     elif isinstance(field, FieldAssociation):\n340         pass\n341     else:\n342         raise TypeError(f'Data field ({field}) not supported.')\n343     return field\n344 \n345 \n346 def get_array(mesh, name, preference='cell', err=False) -> Optional[np.ndarray]:\n347     \"\"\"Search point, cell and field data for an array.\n348 \n349     Parameters\n350     ----------\n351     mesh : pyvista.DataSet\n352         Dataset to get the array from.\n353 \n354     name : str\n355         The name of the array to get the range.\n356 \n357     preference : str, default: \"cell\"\n358         When scalars is specified, this is the preferred array type to\n359         search for in the dataset.  Must be either ``'point'``,\n360         ``'cell'``, or ``'field'``.\n361 \n362     err : bool, default: False\n363         Whether to throw an error if array is not present.\n364 \n365     Returns\n366     -------\n367     pyvista.pyvista_ndarray or ``None``\n368         Requested array.  Return ``None`` if there is no array\n369         matching the ``name`` and ``err=False``.\n370 \n371     \"\"\"\n372     if isinstance(mesh, _vtk.vtkTable):\n373         arr = row_array(mesh, name)\n374         if arr is None and err:\n375             raise KeyError(f'Data array ({name}) not present in this dataset.')\n376         return arr\n377 \n378     if not isinstance(preference, str):\n379         raise TypeError('`preference` must be a string')\n380     if preference not in ['cell', 'point', 'field']:\n381         raise ValueError(\n382             f'`preference` must be either \"cell\", \"point\", \"field\" for a '\n383             f'{type(mesh)}, not \"{preference}\".'\n384         )\n385 \n386     parr = point_array(mesh, name)\n387     carr = cell_array(mesh, name)\n388     farr = field_array(mesh, name)\n389     preference = parse_field_choice(preference)\n390     if sum([array is not None for array in (parr, carr, farr)]) > 1:\n391         if preference == FieldAssociation.CELL:\n392             return carr\n393         elif preference == FieldAssociation.POINT:\n394             return parr\n395         else:  # must be field\n396             return farr\n397 \n398     if parr is not None:\n399         return parr\n400     elif carr is not None:\n401         return carr\n402     elif farr is not None:\n403         return farr\n404     elif err:\n405         raise KeyError(f'Data array ({name}) not present in this dataset.')\n406     return None\n407 \n408 \n409 def get_array_association(mesh, name, preference='cell', err=False) -> FieldAssociation:\n410     \"\"\"Return the array association.\n411 \n412     Parameters\n413     ----------\n414     mesh : Dataset\n415         Dataset to get the array association from.\n416 \n417     name : str\n418         The name of the array.\n419 \n420     preference : str, default: \"cell\"\n421         When scalars is specified, this is the preferred array type to\n422         search for in the dataset.  Must be either ``'point'``,\n423         ``'cell'``, or ``'field'``.\n424 \n425     err : bool, default: False\n426         Boolean to control whether to throw an error if array is not\n427         present.\n428 \n429     Returns\n430     -------\n431     pyvista.utilities.helpers.FieldAssociation\n432         Association of the array. If array is not present and ``err`` is\n433         ``False``, ``FieldAssociation.NONE`` is returned.\n434 \n435     \"\"\"\n436     if isinstance(mesh, _vtk.vtkTable):\n437         arr = row_array(mesh, name)\n438         if arr is None and err:\n439             raise KeyError(f'Data array ({name}) not present in this dataset.')\n440         return FieldAssociation.ROW\n441 \n442     # with multiple arrays, return the array preference if possible\n443     parr = point_array(mesh, name)\n444     carr = cell_array(mesh, name)\n445     farr = field_array(mesh, name)\n446     arrays = [parr, carr, farr]\n447     preferences = [FieldAssociation.POINT, FieldAssociation.CELL, FieldAssociation.NONE]\n448     preference = parse_field_choice(preference)\n449     if preference not in preferences:\n450         raise ValueError(f'Data field ({preference}) not supported.')\n451 \n452     matches = [pref for pref, array in zip(preferences, arrays) if array is not None]\n453     # optionally raise if no match\n454     if not matches:\n455         if err:\n456             raise KeyError(f'Data array ({name}) not present in this dataset.')\n457         return FieldAssociation.NONE\n458     # use preference if it applies\n459     if preference in matches:\n460         return preference\n461     # otherwise return first in order of point -> cell -> field\n462     return matches[0]\n463 \n464 \n465 def vtk_points(points, deep=True, force_float=False):\n466     \"\"\"Convert numpy array or array-like to a ``vtkPoints`` object.\n467 \n468     Parameters\n469     ----------\n470     points : numpy.ndarray or sequence\n471         Points to convert.  Should be 1 or 2 dimensional.  Accepts a\n472         single point or several points.\n473 \n474     deep : bool, default: True\n475         Perform a deep copy of the array.  Only applicable if\n476         ``points`` is a :class:`numpy.ndarray`.\n477 \n478     force_float : bool, default: False\n479         Casts the datatype to ``float32`` if points datatype is\n480         non-float.  Set this to ``False`` to allow non-float types,\n481         though this may lead to truncation of intermediate floats\n482         when transforming datasets.\n483 \n484     Returns\n485     -------\n486     vtk.vtkPoints\n487         The vtkPoints object.\n488 \n489     Examples\n490     --------\n491     >>> import pyvista\n492     >>> import numpy as np\n493     >>> points = np.random.random((10, 3))\n494     >>> vpoints = pyvista.vtk_points(points)\n495     >>> vpoints  # doctest:+SKIP\n496     (vtkmodules.vtkCommonCore.vtkPoints)0x7f0c2e26af40\n497 \n498     \"\"\"\n499     points = np.asanyarray(points)\n500 \n501     # verify is numeric\n502     if not np.issubdtype(points.dtype, np.number):\n503         raise TypeError('Points must be a numeric type')\n504 \n505     if force_float:\n506         if not np.issubdtype(points.dtype, np.floating):\n507             warnings.warn(\n508                 'Points is not a float type. This can cause issues when '\n509                 'transforming or applying filters. Casting to '\n510                 '``np.float32``. Disable this by passing '\n511                 '``force_float=False``.'\n512             )\n513             points = points.astype(np.float32)\n514 \n515     # check dimensionality\n516     if points.ndim == 1:\n517         points = points.reshape(-1, 3)\n518     elif points.ndim > 2:\n519         raise ValueError(f'Dimension of ``points`` should be 1 or 2, not {points.ndim}')\n520 \n521     # verify shape\n522     if points.shape[1] != 3:\n523         raise ValueError(\n524             'Points array must contain three values per point. '\n525             f'Shape is {points.shape} and should be (X, 3)'\n526         )\n527 \n528     # use the underlying vtk data if present to avoid memory leaks\n529     if not deep and isinstance(points, pyvista.pyvista_ndarray):\n530         if points.VTKObject is not None:\n531             vtk_object = points.VTKObject\n532 \n533             # we can only use the underlying data if `points` is not a slice of\n534             # the VTK data object\n535             if vtk_object.GetSize() == points.size:\n536                 vtkpts = _vtk.vtkPoints()\n537                 vtkpts.SetData(points.VTKObject)\n538                 return vtkpts\n539             else:\n540                 deep = True\n541 \n542     # points must be contiguous\n543     points = np.require(points, requirements=['C'])\n544     vtkpts = _vtk.vtkPoints()\n545     vtk_arr = _vtk.numpy_to_vtk(points, deep=deep)\n546     vtkpts.SetData(vtk_arr)\n547 \n548     return vtkpts\n549 \n550 \n551 def line_segments_from_points(points):\n552     \"\"\"Generate non-connected line segments from points.\n553 \n554     Assumes points are ordered as line segments and an even number of\n555     points.\n556 \n557     Parameters\n558     ----------\n559     points : array_like[float]\n560         Points representing line segments. An even number must be\n561         given as every two vertices represent a single line\n562         segment. For example, two line segments would be represented\n563         as ``np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n564 \n565     Returns\n566     -------\n567     pyvista.PolyData\n568         PolyData with lines and cells.\n569 \n570     Examples\n571     --------\n572     This example plots two line segments at right angles to each other.\n573 \n574     >>> import pyvista\n575     >>> import numpy as np\n576     >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])\n577     >>> lines = pyvista.line_segments_from_points(points)\n578     >>> lines.plot()\n579 \n580     \"\"\"\n581     if len(points) % 2 != 0:\n582         raise ValueError(\"An even number of points must be given to define each segment.\")\n583     # Assuming ordered points, create array defining line order\n584     n_points = len(points)\n585     n_lines = n_points // 2\n586     lines = np.c_[\n587         (\n588             2 * np.ones(n_lines, np.int_),\n589             np.arange(0, n_points - 1, step=2),\n590             np.arange(1, n_points + 1, step=2),\n591         )\n592     ]\n593     poly = pyvista.PolyData()\n594     poly.points = points\n595     poly.lines = lines\n596     return poly\n597 \n598 \n599 def lines_from_points(points, close=False):\n600     \"\"\"Make a connected line set given an array of points.\n601 \n602     Parameters\n603     ----------\n604     points : array_like[float]\n605         Points representing the vertices of the connected\n606         segments. For example, two line segments would be represented\n607         as ``np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n608 \n609     close : bool, default: False\n610         If ``True``, close the line segments into a loop.\n611 \n612     Returns\n613     -------\n614     pyvista.PolyData\n615         PolyData with lines and cells.\n616 \n617     Examples\n618     --------\n619     >>> import numpy as np\n620     >>> import pyvista\n621     >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])\n622     >>> poly = pyvista.lines_from_points(points)\n623     >>> poly.plot(line_width=5)\n624 \n625     \"\"\"\n626     poly = pyvista.PolyData()\n627     poly.points = points\n628     cells = np.full((len(points) - 1, 3), 2, dtype=np.int_)\n629     cells[:, 1] = np.arange(0, len(points) - 1, dtype=np.int_)\n630     cells[:, 2] = np.arange(1, len(points), dtype=np.int_)\n631     if close:\n632         cells = np.append(cells, [[2, len(points) - 1, 0]], axis=0)\n633     poly.lines = cells\n634     return poly\n635 \n636 \n637 def make_tri_mesh(points, faces):\n638     \"\"\"Construct a ``pyvista.PolyData`` mesh using points and faces arrays.\n639 \n640     Construct a mesh from an Nx3 array of points and an Mx3 array of\n641     triangle indices, resulting in a mesh with N vertices and M\n642     triangles.  This function does not require the standard VTK\n643     \"padding\" column and simplifies mesh creation.\n644 \n645     Parameters\n646     ----------\n647     points : np.ndarray\n648         Array of points with shape ``(N, 3)`` storing the vertices of the\n649         triangle mesh.\n650 \n651     faces : np.ndarray\n652         Array of indices with shape ``(M, 3)`` containing the triangle\n653         indices.\n654 \n655     Returns\n656     -------\n657     pyvista.PolyData\n658         PolyData instance containing the triangle mesh.\n659 \n660     Examples\n661     --------\n662     This example discretizes the unit square into a triangle mesh with\n663     nine vertices and eight faces.\n664 \n665     >>> import numpy as np\n666     >>> import pyvista\n667     >>> points = np.array(\n668     ...     [\n669     ...         [0, 0, 0],\n670     ...         [0.5, 0, 0],\n671     ...         [1, 0, 0],\n672     ...         [0, 0.5, 0],\n673     ...         [0.5, 0.5, 0],\n674     ...         [1, 0.5, 0],\n675     ...         [0, 1, 0],\n676     ...         [0.5, 1, 0],\n677     ...         [1, 1, 0],\n678     ...     ]\n679     ... )\n680     >>> faces = np.array(\n681     ...     [\n682     ...         [0, 1, 4],\n683     ...         [4, 7, 6],\n684     ...         [2, 5, 4],\n685     ...         [4, 5, 8],\n686     ...         [0, 4, 3],\n687     ...         [3, 4, 6],\n688     ...         [1, 2, 4],\n689     ...         [4, 8, 7],\n690     ...     ]\n691     ... )\n692     >>> tri_mesh = pyvista.make_tri_mesh(points, faces)\n693     >>> tri_mesh.plot(show_edges=True, line_width=5)\n694 \n695     \"\"\"\n696     if points.shape[1] != 3:\n697         raise ValueError(\"Points array should have shape (N, 3).\")\n698     if faces.ndim != 2 or faces.shape[1] != 3:\n699         raise ValueError(\"Face array should have shape (M, 3).\")\n700     cells = np.empty((faces.shape[0], 4), dtype=faces.dtype)\n701     cells[:, 0] = 3\n702     cells[:, 1:] = faces\n703     return pyvista.PolyData(points, cells)\n704 \n705 \n706 def vector_poly_data(orig, vec):\n707     \"\"\"Create a pyvista.PolyData object composed of vectors.\n708 \n709     Parameters\n710     ----------\n711     orig : array_like[float]\n712         Array of vector origins.\n713 \n714     vec : array_like[float]\n715         Array of vectors.\n716 \n717     Returns\n718     -------\n719     pyvista.PolyData\n720         Mesh containing the ``orig`` points along with the\n721         ``'vectors'`` and ``'mag'`` point arrays representing the\n722         vectors and magnitude of the vectors at each point.\n723 \n724     Examples\n725     --------\n726     Create basic vector field.  This is a point cloud where each point\n727     has a vector and magnitude attached to it.\n728 \n729     >>> import pyvista\n730     >>> import numpy as np\n731     >>> x, y = np.meshgrid(np.linspace(-5, 5, 10), np.linspace(-5, 5, 10))\n732     >>> points = np.vstack((x.ravel(), y.ravel(), np.zeros(x.size))).T\n733     >>> u = x / np.sqrt(x**2 + y**2)\n734     >>> v = y / np.sqrt(x**2 + y**2)\n735     >>> vectors = np.vstack(\n736     ...     (u.ravel() ** 3, v.ravel() ** 3, np.zeros(u.size))\n737     ... ).T\n738     >>> pdata = pyvista.vector_poly_data(points, vectors)\n739     >>> pdata.point_data.keys()\n740     ['vectors', 'mag']\n741 \n742     Convert these to arrows and plot it.\n743 \n744     >>> pdata.glyph(orient='vectors', scale='mag').plot()\n745 \n746     \"\"\"\n747     # shape, dimension checking\n748     if not isinstance(orig, np.ndarray):\n749         orig = np.asarray(orig)\n750 \n751     if not isinstance(vec, np.ndarray):\n752         vec = np.asarray(vec)\n753 \n754     if orig.ndim != 2:\n755         orig = orig.reshape((-1, 3))\n756     elif orig.shape[1] != 3:\n757         raise ValueError('orig array must be 3D')\n758 \n759     if vec.ndim != 2:\n760         vec = vec.reshape((-1, 3))\n761     elif vec.shape[1] != 3:\n762         raise ValueError('vec array must be 3D')\n763 \n764     # Create vtk points and cells objects\n765     vpts = _vtk.vtkPoints()\n766     vpts.SetData(_vtk.numpy_to_vtk(np.ascontiguousarray(orig), deep=True))\n767 \n768     npts = orig.shape[0]\n769     cells = np.empty((npts, 2), dtype=pyvista.ID_TYPE)\n770     cells[:, 0] = 1\n771     cells[:, 1] = np.arange(npts, dtype=pyvista.ID_TYPE)\n772     vcells = pyvista.utilities.cells.CellArray(cells, npts)\n773 \n774     # Create vtkPolyData object\n775     pdata = _vtk.vtkPolyData()\n776     pdata.SetPoints(vpts)\n777     pdata.SetVerts(vcells)\n778 \n779     # Add vectors to polydata\n780     name = 'vectors'\n781     vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(vec), deep=True)\n782     vtkfloat.SetName(name)\n783     pdata.GetPointData().AddArray(vtkfloat)\n784     pdata.GetPointData().SetActiveVectors(name)\n785 \n786     # Add magnitude of vectors to polydata\n787     name = 'mag'\n788     scalars = (vec * vec).sum(1) ** 0.5\n789     vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(scalars), deep=True)\n790     vtkfloat.SetName(name)\n791     pdata.GetPointData().AddArray(vtkfloat)\n792     pdata.GetPointData().SetActiveScalars(name)\n793 \n794     return pyvista.PolyData(pdata)\n795 \n796 \n797 def trans_from_matrix(matrix):  # pragma: no cover\n798     \"\"\"Convert a vtk matrix to a numpy.ndarray.\n799 \n800     DEPRECATED: Please use ``array_from_vtkmatrix``.\n801 \n802     \"\"\"\n803     # import needs to happen here to prevent a circular import\n804     from pyvista.core.errors import DeprecationError\n805 \n806     raise DeprecationError('DEPRECATED: Please use ``array_from_vtkmatrix``.')\n807 \n808 \n809 def array_from_vtkmatrix(matrix):\n810     \"\"\"Convert a vtk matrix to an array.\n811 \n812     Parameters\n813     ----------\n814     matrix : vtk.vtkMatrix3x3 | vtk.vtkMatrix4x4\n815         The vtk matrix to be converted to a ``numpy.ndarray``.\n816         Returned ndarray has shape (3, 3) or (4, 4) as appropriate.\n817 \n818     Returns\n819     -------\n820     numpy.ndarray\n821         Numpy array containing the data from ``matrix``.\n822 \n823     \"\"\"\n824     if isinstance(matrix, _vtk.vtkMatrix3x3):\n825         shape = (3, 3)\n826     elif isinstance(matrix, _vtk.vtkMatrix4x4):\n827         shape = (4, 4)\n828     else:\n829         raise TypeError(\n830             'Expected vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4 input,'\n831             f' got {type(matrix).__name__} instead.'\n832         )\n833     array = np.zeros(shape)\n834     for i in range(shape[0]):\n835         for j in range(shape[1]):\n836             array[i, j] = matrix.GetElement(i, j)\n837     return array\n838 \n839 \n840 def vtkmatrix_from_array(array):\n841     \"\"\"Convert a ``numpy.ndarray`` or array-like to a vtk matrix.\n842 \n843     Parameters\n844     ----------\n845     array : array_like[float]\n846         The array or array-like to be converted to a vtk matrix.\n847         Shape (3, 3) gets converted to a ``vtk.vtkMatrix3x3``, shape (4, 4)\n848         gets converted to a ``vtk.vtkMatrix4x4``. No other shapes are valid.\n849 \n850     Returns\n851     -------\n852     vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4\n853         VTK matrix.\n854 \n855     \"\"\"\n856     array = np.asarray(array)\n857     if array.shape == (3, 3):\n858         matrix = _vtk.vtkMatrix3x3()\n859     elif array.shape == (4, 4):\n860         matrix = _vtk.vtkMatrix4x4()\n861     else:\n862         raise ValueError(f'Invalid shape {array.shape}, must be (3, 3) or (4, 4).')\n863     m, n = array.shape\n864     for i in range(m):\n865         for j in range(n):\n866             matrix.SetElement(i, j, array[i, j])\n867     return matrix\n868 \n869 \n870 def is_meshio_mesh(obj):\n871     \"\"\"Test if passed object is instance of ``meshio.Mesh``.\n872 \n873     Parameters\n874     ----------\n875     obj\n876         Any object.\n877 \n878     Returns\n879     -------\n880     bool\n881         ``True`` if ``obj`` is an ``meshio.Mesh``.\n882 \n883     \"\"\"\n884     try:\n885         import meshio\n886 \n887         return isinstance(obj, meshio.Mesh)\n888     except ImportError:\n889         return False\n890 \n891 \n892 def wrap(dataset):\n893     \"\"\"Wrap any given VTK data object to its appropriate PyVista data object.\n894 \n895     Other formats that are supported include:\n896 \n897     * 2D :class:`numpy.ndarray` of XYZ vertices\n898     * 3D :class:`numpy.ndarray` representing a volume. Values will be scalars.\n899     * 3D :class:`trimesh.Trimesh` mesh.\n900     * 3D :class:`meshio.Mesh` mesh.\n901 \n902     .. versionchanged:: 0.38.0\n903         If the passed object is already a wrapped PyVista object, then\n904         this is no-op and will return that object directly. In previous\n905         versions of PyVista, this would perform a shallow copy.\n906 \n907     Parameters\n908     ----------\n909     dataset : :class:`numpy.ndarray` | :class:`trimesh.Trimesh` | vtk.DataSet\n910         Dataset to wrap.\n911 \n912     Returns\n913     -------\n914     pyvista.DataSet\n915         The PyVista wrapped dataset.\n916 \n917     Examples\n918     --------\n919     Wrap a numpy array representing a random point cloud.\n920 \n921     >>> import numpy as np\n922     >>> import pyvista\n923     >>> points = np.random.random((10, 3))\n924     >>> cloud = pyvista.wrap(points)\n925     >>> cloud  # doctest:+SKIP\n926     PolyData (0x7fc52db83d70)\n927       N Cells:  10\n928       N Points: 10\n929       X Bounds: 1.123e-01, 7.457e-01\n930       Y Bounds: 1.009e-01, 9.877e-01\n931       Z Bounds: 2.346e-03, 9.640e-01\n932       N Arrays: 0\n933 \n934     Wrap a Trimesh object.\n935 \n936     >>> import trimesh\n937     >>> import pyvista\n938     >>> points = [[0, 0, 0], [0, 0, 1], [0, 1, 0]]\n939     >>> faces = [[0, 1, 2]]\n940     >>> tmesh = trimesh.Trimesh(points, faces=faces, process=False)\n941     >>> mesh = pyvista.wrap(tmesh)\n942     >>> mesh  # doctest:+SKIP\n943     PolyData (0x7fc55ff27ad0)\n944       N Cells:  1\n945       N Points: 3\n946       X Bounds: 0.000e+00, 0.000e+00\n947       Y Bounds: 0.000e+00, 1.000e+00\n948       Z Bounds: 0.000e+00, 1.000e+00\n949       N Arrays: 0\n950 \n951     Wrap a VTK object.\n952 \n953     >>> import pyvista\n954     >>> import vtk\n955     >>> points = vtk.vtkPoints()\n956     >>> p = [1.0, 2.0, 3.0]\n957     >>> vertices = vtk.vtkCellArray()\n958     >>> pid = points.InsertNextPoint(p)\n959     >>> _ = vertices.InsertNextCell(1)\n960     >>> _ = vertices.InsertCellPoint(pid)\n961     >>> point = vtk.vtkPolyData()\n962     >>> _ = point.SetPoints(points)\n963     >>> _ = point.SetVerts(vertices)\n964     >>> mesh = pyvista.wrap(point)\n965     >>> mesh  # doctest:+SKIP\n966     PolyData (0x7fc55ff27ad0)\n967       N Cells:  1\n968       N Points: 3\n969       X Bounds: 0.000e+00, 0.000e+00\n970       Y Bounds: 0.000e+00, 1.000e+00\n971       Z Bounds: 0.000e+00, 1.000e+00\n972       N Arrays: 0\n973 \n974     \"\"\"\n975     # Return if None\n976     if dataset is None:\n977         return\n978 \n979     if isinstance(dataset, tuple(pyvista._wrappers.values())):\n980         # Return object if it is already wrapped\n981         return dataset\n982 \n983     # Check if dataset is a numpy array.  We do this first since\n984     # pyvista_ndarray contains a VTK type that we don't want to\n985     # directly wrap.\n986     if isinstance(dataset, (np.ndarray, pyvista.pyvista_ndarray)):\n987         if dataset.ndim == 1 and dataset.shape[0] == 3:\n988             return pyvista.PolyData(dataset)\n989         if dataset.ndim > 1 and dataset.ndim < 3 and dataset.shape[1] == 3:\n990             return pyvista.PolyData(dataset)\n991         elif dataset.ndim == 3:\n992             mesh = pyvista.UniformGrid(dimensions=dataset.shape)\n993             if isinstance(dataset, pyvista.pyvista_ndarray):\n994                 # this gets rid of pesky VTK reference since we're raveling this\n995                 dataset = np.array(dataset, copy=False)\n996             mesh['values'] = dataset.ravel(order='F')\n997             mesh.active_scalars_name = 'values'\n998             return mesh\n999         else:\n1000             raise NotImplementedError('NumPy array could not be wrapped pyvista.')\n1001 \n1002     # wrap VTK arrays as pyvista_ndarray\n1003     if isinstance(dataset, _vtk.vtkDataArray):\n1004         return pyvista.pyvista_ndarray(dataset)\n1005 \n1006     # Check if a dataset is a VTK type\n1007     if hasattr(dataset, 'GetClassName'):\n1008         key = dataset.GetClassName()\n1009         try:\n1010             return pyvista._wrappers[key](dataset)\n1011         except KeyError:\n1012             raise TypeError(f'VTK data type ({key}) is not currently supported by pyvista.')\n1013         return\n1014 \n1015     # wrap meshio\n1016     if is_meshio_mesh(dataset):\n1017         return from_meshio(dataset)\n1018 \n1019     # wrap trimesh\n1020     if dataset.__class__.__name__ == 'Trimesh':\n1021         # trimesh doesn't pad faces\n1022         n_face = dataset.faces.shape[0]\n1023         faces = np.empty((n_face, 4), dataset.faces.dtype)\n1024         faces[:, 1:] = dataset.faces\n1025         faces[:, 0] = 3\n1026         polydata = pyvista.PolyData(np.asarray(dataset.vertices), faces)\n1027         # If the Trimesh object has uv, pass them to the PolyData\n1028         if hasattr(dataset.visual, 'uv'):\n1029             polydata.active_t_coords = np.asarray(dataset.visual.uv)\n1030         return polydata\n1031 \n1032     # otherwise, flag tell the user we can't wrap this object\n1033     raise NotImplementedError(f'Unable to wrap ({type(dataset)}) into a pyvista type.')\n1034 \n1035 \n1036 def image_to_texture(image):\n1037     \"\"\"Convert ``vtkImageData`` (:class:`pyvista.UniformGrid`) to a ``vtkTexture``.\n1038 \n1039     Parameters\n1040     ----------\n1041     image : pyvista.UniformGrid | vtkImageData\n1042         Image to convert.\n1043 \n1044     Returns\n1045     -------\n1046     vtkTexture\n1047         VTK texture.\n1048 \n1049     \"\"\"\n1050     return pyvista.Texture(image)\n1051 \n1052 \n1053 def numpy_to_texture(image):\n1054     \"\"\"Convert a NumPy image array to a vtk.vtkTexture.\n1055 \n1056     Parameters\n1057     ----------\n1058     image : numpy.ndarray\n1059         Numpy image array. Texture datatype expected to be ``np.uint8``.\n1060 \n1061     Returns\n1062     -------\n1063     pyvista.Texture\n1064         PyVista texture.\n1065 \n1066     Examples\n1067     --------\n1068     Create an all white texture.\n1069 \n1070     >>> import pyvista as pv\n1071     >>> import numpy as np\n1072     >>> tex_arr = np.ones((1024, 1024, 3), dtype=np.uint8) * 255\n1073     >>> tex = pv.numpy_to_texture(tex_arr)\n1074 \n1075     \"\"\"\n1076     if image.dtype != np.uint8:\n1077         image = image.astype(np.uint8)\n1078         warnings.warn(\n1079             'Expected `image` dtype to be ``np.uint8``. `image` has been copied '\n1080             'and converted to np.uint8.',\n1081             UserWarning,\n1082         )\n1083 \n1084     return pyvista.Texture(image)\n1085 \n1086 \n1087 def is_inside_bounds(point, bounds):\n1088     \"\"\"Check if a point is inside a set of bounds.\n1089 \n1090     This is implemented through recursion so that this is N-dimensional.\n1091 \n1092     Parameters\n1093     ----------\n1094     point : sequence[float]\n1095         Three item cartesian point (i.e. ``[x, y, z]``).\n1096 \n1097     bounds : sequence[float]\n1098         Six item bounds in the form of ``(xMin, xMax, yMin, yMax, zMin, zMax)``.\n1099 \n1100     Returns\n1101     -------\n1102     bool\n1103         ``True`` when ``point`` is inside ``bounds``.\n1104 \n1105     \"\"\"\n1106     if isinstance(point, (int, float)):\n1107         point = [point]\n1108     if isinstance(point, (np.ndarray, collections.abc.Sequence)) and not isinstance(\n1109         point, collections.deque\n1110     ):\n1111         if len(bounds) < 2 * len(point) or len(bounds) % 2 != 0:\n1112             raise ValueError('Bounds mismatch point dimensionality')\n1113         point = collections.deque(point)\n1114         bounds = collections.deque(bounds)\n1115         return is_inside_bounds(point, bounds)\n1116     if not isinstance(point, collections.deque):\n1117         raise TypeError(f'Unknown input data type ({type(point)}).')\n1118     if len(point) < 1:\n1119         return True\n1120     p = point.popleft()\n1121     lower, upper = bounds.popleft(), bounds.popleft()\n1122     if lower <= p <= upper:\n1123         return is_inside_bounds(point, bounds)\n1124     return False\n1125 \n1126 \n1127 def fit_plane_to_points(points, return_meta=False):\n1128     \"\"\"Fit a plane to a set of points using the SVD algorithm.\n1129 \n1130     Parameters\n1131     ----------\n1132     points : array_like[float]\n1133         Size ``[N x 3]`` sequence of points to fit a plane through.\n1134 \n1135     return_meta : bool, default: False\n1136         If ``True``, also returns the center and normal used to\n1137         generate the plane.\n1138 \n1139     Returns\n1140     -------\n1141     pyvista.PolyData\n1142         Plane mesh.\n1143 \n1144     numpy.ndarray\n1145         Plane center if ``return_meta=True``.\n1146 \n1147     numpy.ndarray\n1148         Plane normal if ``return_meta=True``.\n1149 \n1150     Examples\n1151     --------\n1152     Fit a plane to a random point cloud.\n1153 \n1154     >>> import pyvista\n1155     >>> import numpy as np\n1156     >>> cloud = np.random.random((10, 3))\n1157     >>> cloud[:, 2] *= 0.1\n1158     >>> plane, center, normal = pyvista.fit_plane_to_points(\n1159     ...     cloud, return_meta=True\n1160     ... )\n1161 \n1162     Plot the fitted plane.\n1163 \n1164     >>> pl = pyvista.Plotter()\n1165     >>> _ = pl.add_mesh(\n1166     ...     plane, color='tan', style='wireframe', line_width=4\n1167     ... )\n1168     >>> _ = pl.add_points(\n1169     ...     cloud,\n1170     ...     render_points_as_spheres=True,\n1171     ...     color='r',\n1172     ...     point_size=30,\n1173     ... )\n1174     >>> pl.show()\n1175 \n1176     \"\"\"\n1177     data = np.array(points)\n1178     center = data.mean(axis=0)\n1179     result = np.linalg.svd(data - center)\n1180     normal = np.cross(result[2][0], result[2][1])\n1181     plane = pyvista.Plane(center=center, direction=normal)\n1182     if return_meta:\n1183         return plane, center, normal\n1184     return plane\n1185 \n1186 \n1187 def raise_not_matching(scalars, dataset):\n1188     \"\"\"Raise exception about inconsistencies.\n1189 \n1190     Parameters\n1191     ----------\n1192     scalars : numpy.ndarray\n1193         Array of scalars.\n1194 \n1195     dataset : pyvista.DataSet\n1196         Dataset to check against.\n1197 \n1198     Raises\n1199     ------\n1200     ValueError\n1201         Raises a ValueError if the size of scalars does not the dataset.\n1202     \"\"\"\n1203     if isinstance(dataset, _vtk.vtkTable):\n1204         raise ValueError(\n1205             f'Number of scalars ({scalars.shape[0]}) must match number of rows ({dataset.n_rows}).'\n1206         )\n1207     raise ValueError(\n1208         f'Number of scalars ({scalars.shape[0]}) '\n1209         f'must match either the number of points ({dataset.n_points}) '\n1210         f'or the number of cells ({dataset.n_cells}).'\n1211     )\n1212 \n1213 \n1214 def generate_plane(normal, origin):\n1215     \"\"\"Return a _vtk.vtkPlane.\n1216 \n1217     Parameters\n1218     ----------\n1219     normal : sequence[float]\n1220         Three item sequence representing the normal of the plane.\n1221 \n1222     origin : sequence[float]\n1223         Three item sequence representing the origin of the plane.\n1224 \n1225     Returns\n1226     -------\n1227     vtk.vtkPlane\n1228         VTK plane.\n1229 \n1230     \"\"\"\n1231     plane = _vtk.vtkPlane()\n1232     # NORMAL MUST HAVE MAGNITUDE OF 1\n1233     normal = normal / np.linalg.norm(normal)\n1234     plane.SetNormal(normal)\n1235     plane.SetOrigin(origin)\n1236     return plane\n1237 \n1238 \n1239 def try_callback(func, *args):\n1240     \"\"\"Wrap a given callback in a try statement.\n1241 \n1242     Parameters\n1243     ----------\n1244     func : callable\n1245         Callable object.\n1246 \n1247     *args\n1248         Any arguments.\n1249 \n1250     \"\"\"\n1251     try:\n1252         func(*args)\n1253     except Exception:\n1254         etype, exc, tb = sys.exc_info()\n1255         stack = traceback.extract_tb(tb)[1:]\n1256         formatted_exception = 'Encountered issue in callback (most recent call last):\\n' + ''.join(\n1257             traceback.format_list(stack) + traceback.format_exception_only(etype, exc)\n1258         ).rstrip('\\n')\n1259         warnings.warn(formatted_exception)\n1260 \n1261 \n1262 def check_depth_peeling(number_of_peels=100, occlusion_ratio=0.0):\n1263     \"\"\"Check if depth peeling is available.\n1264 \n1265     Attempts to use depth peeling to see if it is available for the\n1266     current environment. Returns ``True`` if depth peeling is\n1267     available and has been successfully leveraged, otherwise\n1268     ``False``.\n1269 \n1270     Parameters\n1271     ----------\n1272     number_of_peels : int, default: 100\n1273         Maximum number of depth peels.\n1274 \n1275     occlusion_ratio : float, default: 0.0\n1276         Occlusion ratio.\n1277 \n1278     Returns\n1279     -------\n1280     bool\n1281         ``True`` when system supports depth peeling with the specified\n1282         settings.\n1283 \n1284     \"\"\"\n1285     # Try Depth Peeling with a basic scene\n1286     source = _vtk.vtkSphereSource()\n1287     mapper = _vtk.vtkPolyDataMapper()\n1288     mapper.SetInputConnection(source.GetOutputPort())\n1289     actor = _vtk.vtkActor()\n1290     actor.SetMapper(mapper)\n1291     # requires opacity < 1\n1292     actor.GetProperty().SetOpacity(0.5)\n1293     renderer = _vtk.vtkRenderer()\n1294     renderWindow = _vtk.vtkRenderWindow()\n1295     renderWindow.AddRenderer(renderer)\n1296     renderWindow.SetOffScreenRendering(True)\n1297     renderWindow.SetAlphaBitPlanes(True)\n1298     renderWindow.SetMultiSamples(0)\n1299     renderer.AddActor(actor)\n1300     renderer.SetUseDepthPeeling(True)\n1301     renderer.SetMaximumNumberOfPeels(number_of_peels)\n1302     renderer.SetOcclusionRatio(occlusion_ratio)\n1303     renderWindow.Render()\n1304     return renderer.GetLastRenderingUsedDepthPeeling() == 1\n1305 \n1306 \n1307 def threaded(fn):\n1308     \"\"\"Call a function using a thread.\n1309 \n1310     Parameters\n1311     ----------\n1312     fn : callable\n1313         Callable object.\n1314 \n1315     Returns\n1316     -------\n1317     function\n1318         Wrapped function.\n1319 \n1320     \"\"\"\n1321 \n1322     def wrapper(*args, **kwargs):\n1323         thread = Thread(target=fn, args=args, kwargs=kwargs)\n1324         thread.start()\n1325         return thread\n1326 \n1327     return wrapper\n1328 \n1329 \n1330 class conditional_decorator:\n1331     \"\"\"Conditional decorator for methods.\n1332 \n1333     Parameters\n1334     ----------\n1335     dec\n1336         Decorator\n1337     condition\n1338         Condition to match.\n1339 \n1340     \"\"\"\n1341 \n1342     def __init__(self, dec, condition):\n1343         \"\"\"Initialize.\"\"\"\n1344         self.decorator = dec\n1345         self.condition = condition\n1346 \n1347     def __call__(self, func):\n1348         \"\"\"Call the decorated function if condition is matched.\"\"\"\n1349         if not self.condition:\n1350             # Return the function unchanged, not decorated.\n1351             return func\n1352         return self.decorator(func)\n1353 \n1354 \n1355 class ProgressMonitor:\n1356     \"\"\"A standard class for monitoring the progress of a VTK algorithm.\n1357 \n1358     This must be use in a ``with`` context and it will block keyboard\n1359     interrupts from happening until the exit event as interrupts will crash\n1360     the kernel if the VTK algorithm is still executing.\n1361 \n1362     Parameters\n1363     ----------\n1364     algorithm\n1365         VTK algorithm or filter.\n1366 \n1367     message : str, default: \"\"\n1368         Message to display in the progress bar.\n1369 \n1370     scaling : float, optional\n1371         Unused keyword argument.\n1372 \n1373     \"\"\"\n1374 \n1375     def __init__(self, algorithm, message=\"\", scaling=None):\n1376         \"\"\"Initialize observer.\"\"\"\n1377         try:\n1378             from tqdm import tqdm  # noqa\n1379         except ImportError:\n1380             raise ImportError(\"Please install `tqdm` to monitor algorithms.\")\n1381         self.event_type = _vtk.vtkCommand.ProgressEvent\n1382         self.progress = 0.0\n1383         self._last_progress = self.progress\n1384         self.algorithm = algorithm\n1385         self.message = message\n1386         self._interrupt_signal_received = False\n1387         self._old_progress = 0\n1388         self._old_handler = None\n1389         self._progress_bar = None\n1390 \n1391     def handler(self, sig, frame):\n1392         \"\"\"Pass signal to custom interrupt handler.\"\"\"\n1393         self._interrupt_signal_received = (sig, frame)\n1394         logging.debug('SIGINT received. Delaying KeyboardInterrupt until VTK algorithm finishes.')\n1395 \n1396     def __call__(self, obj, event, *args):\n1397         \"\"\"Call progress update callback.\n1398 \n1399         On an event occurrence, this function executes.\n1400         \"\"\"\n1401         if self._interrupt_signal_received:\n1402             obj.AbortExecuteOn()\n1403         else:\n1404             progress = obj.GetProgress()\n1405             step = progress - self._old_progress\n1406             self._progress_bar.update(step)\n1407             self._old_progress = progress\n1408 \n1409     def __enter__(self):\n1410         \"\"\"Enter event for ``with`` context.\"\"\"\n1411         from tqdm import tqdm\n1412 \n1413         # check if in main thread\n1414         if threading.current_thread().__class__.__name__ == '_MainThread':\n1415             self._old_handler = signal.signal(signal.SIGINT, self.handler)\n1416         self._progress_bar = tqdm(\n1417             total=1, leave=True, bar_format='{l_bar}{bar}[{elapsed}<{remaining}]'\n1418         )\n1419         self._progress_bar.set_description(self.message)\n1420         self.algorithm.AddObserver(self.event_type, self)\n1421         return self._progress_bar\n1422 \n1423     def __exit__(self, type, value, traceback):\n1424         \"\"\"Exit event for ``with`` context.\"\"\"\n1425         self._progress_bar.total = 1\n1426         self._progress_bar.refresh()\n1427         self._progress_bar.close()\n1428         self.algorithm.RemoveObservers(self.event_type)\n1429         if threading.current_thread().__class__.__name__ == '_MainThread':\n1430             signal.signal(signal.SIGINT, self._old_handler)\n1431 \n1432 \n1433 def abstract_class(cls_):\n1434     \"\"\"Decorate a class, overriding __new__.\n1435 \n1436     Preventing a class from being instantiated similar to abc.ABCMeta\n1437     but does not require an abstract method.\n1438     \"\"\"\n1439 \n1440     def __new__(cls, *args, **kwargs):\n1441         if cls is cls_:\n1442             raise TypeError(f'{cls.__name__} is an abstract class and may not be instantiated.')\n1443         return object.__new__(cls)\n1444 \n1445     cls_.__new__ = __new__\n1446     return cls_\n1447 \n1448 \n1449 def axis_rotation(points, angle, inplace=False, deg=True, axis='z'):\n1450     \"\"\"Rotate points by angle about an axis.\n1451 \n1452     Parameters\n1453     ----------\n1454     points : numpy.ndarray\n1455         Array of points with shape ``(N, 3)``.\n1456 \n1457     angle : float\n1458         Rotation angle.\n1459 \n1460     inplace : bool, default: False\n1461         Updates points in-place while returning nothing.\n1462 \n1463     deg : bool, default: True\n1464         If ``True``, the angle is interpreted as degrees instead of\n1465         radians.\n1466 \n1467     axis : str, default: \"z\"\n1468         Name of axis to rotate about. Valid options are ``'x'``, ``'y'``,\n1469         and ``'z'``.\n1470 \n1471     Returns\n1472     -------\n1473     numpy.ndarray\n1474         Rotated points.\n1475 \n1476     Examples\n1477     --------\n1478     Rotate a set of points by 90 degrees about the x-axis in-place.\n1479 \n1480     >>> import numpy as np\n1481     >>> import pyvista\n1482     >>> from pyvista import examples\n1483     >>> points = examples.load_airplane().points\n1484     >>> points_orig = points.copy()\n1485     >>> pyvista.axis_rotation(points, 90, axis='x', deg=True, inplace=True)\n1486     >>> assert np.all(np.isclose(points[:, 0], points_orig[:, 0]))\n1487     >>> assert np.all(np.isclose(points[:, 1], -points_orig[:, 2]))\n1488     >>> assert np.all(np.isclose(points[:, 2], points_orig[:, 1]))\n1489     \"\"\"\n1490     axis = axis.lower()\n1491     axis_to_vec = {'x': (1, 0, 0), 'y': (0, 1, 0), 'z': (0, 0, 1)}\n1492 \n1493     if axis not in axis_to_vec:\n1494         raise ValueError('Invalid axis. Must be either \"x\", \"y\", or \"z\"')\n1495 \n1496     rot_mat = transformations.axis_angle_rotation(axis_to_vec[axis], angle, deg=deg)\n1497     return transformations.apply_transformation_to_points(rot_mat, points, inplace=inplace)\n1498 \n1499 \n1500 def cubemap(path='', prefix='', ext='.jpg'):\n1501     \"\"\"Construct a cubemap from 6 images from a directory.\n1502 \n1503     Each of the 6 images must be in the following format:\n1504 \n1505     - <prefix>negx<ext>\n1506     - <prefix>negy<ext>\n1507     - <prefix>negz<ext>\n1508     - <prefix>posx<ext>\n1509     - <prefix>posy<ext>\n1510     - <prefix>posz<ext>\n1511 \n1512     Prefix may be empty, and extension will default to ``'.jpg'``\n1513 \n1514     For example, if you have 6 images with the skybox2 prefix:\n1515 \n1516     - ``'skybox2-negx.jpg'``\n1517     - ``'skybox2-negy.jpg'``\n1518     - ``'skybox2-negz.jpg'``\n1519     - ``'skybox2-posx.jpg'``\n1520     - ``'skybox2-posy.jpg'``\n1521     - ``'skybox2-posz.jpg'``\n1522 \n1523     Parameters\n1524     ----------\n1525     path : str, default: \"\"\n1526         Directory containing the cubemap images.\n1527 \n1528     prefix : str, default: \"\"\n1529         Prefix to the filename.\n1530 \n1531     ext : str, default: \".jpg\"\n1532         The filename extension.  For example ``'.jpg'``.\n1533 \n1534     Returns\n1535     -------\n1536     pyvista.Texture\n1537         Texture with cubemap.\n1538 \n1539     Notes\n1540     -----\n1541     Cubemap will appear flipped relative to the XY plane between VTK v9.1 and\n1542     VTK v9.2.\n1543 \n1544     Examples\n1545     --------\n1546     Load a skybox given a directory, prefix, and file extension.\n1547 \n1548     >>> import pyvista\n1549     >>> skybox = pyvista.cubemap(\n1550     ...     'my_directory', 'skybox', '.jpeg'\n1551     ... )  # doctest:+SKIP\n1552 \n1553     \"\"\"\n1554     sets = ['posx', 'negx', 'posy', 'negy', 'posz', 'negz']\n1555     image_paths = [os.path.join(path, f'{prefix}{suffix}{ext}') for suffix in sets]\n1556     return _cubemap_from_paths(image_paths)\n1557 \n1558 \n1559 def cubemap_from_filenames(image_paths):\n1560     \"\"\"Construct a cubemap from 6 images.\n1561 \n1562     Images must be in the following order:\n1563 \n1564     - Positive X\n1565     - Negative X\n1566     - Positive Y\n1567     - Negative Y\n1568     - Positive Z\n1569     - Negative Z\n1570 \n1571     Parameters\n1572     ----------\n1573     image_paths : sequence[str]\n1574         Paths of the individual cubemap images.\n1575 \n1576     Returns\n1577     -------\n1578     pyvista.Texture\n1579         Texture with cubemap.\n1580 \n1581     Examples\n1582     --------\n1583     Load a skybox given a list of image paths.\n1584 \n1585     >>> image_paths = [\n1586     ...     '/home/user/_px.jpg',\n1587     ...     '/home/user/_nx.jpg',\n1588     ...     '/home/user/_py.jpg',\n1589     ...     '/home/user/_ny.jpg',\n1590     ...     '/home/user/_pz.jpg',\n1591     ...     '/home/user/_nz.jpg',\n1592     ... ]\n1593     >>> skybox = pyvista.cubemap(image_paths=image_paths)  # doctest:+SKIP\n1594 \n1595     \"\"\"\n1596     if len(image_paths) != 6:\n1597         raise ValueError(\"image_paths must contain 6 paths\")\n1598 \n1599     return _cubemap_from_paths(image_paths)\n1600 \n1601 \n1602 def _cubemap_from_paths(image_paths):\n1603     \"\"\"Construct a cubemap from image paths.\"\"\"\n1604     for image_path in image_paths:\n1605         if not os.path.isfile(image_path):\n1606             file_str = '\\n'.join(image_paths)\n1607             raise FileNotFoundError(\n1608                 f'Unable to locate {image_path}\\n'\n1609                 'Expected to find the following files:\\n'\n1610                 f'{file_str}'\n1611             )\n1612 \n1613     texture = pyvista.Texture()\n1614     texture.SetMipmap(True)\n1615     texture.SetInterpolate(True)\n1616     texture.cube_map = True  # Must be set prior to setting images\n1617 \n1618     # add each image to the cubemap\n1619     for i, fn in enumerate(image_paths):\n1620         image = pyvista.read(fn)\n1621         flip = _vtk.vtkImageFlip()\n1622         flip.SetInputDataObject(image)\n1623         flip.SetFilteredAxis(1)  # flip y axis\n1624         flip.Update()\n1625         texture.SetInputDataObject(i, flip.GetOutput())\n1626 \n1627     return texture\n1628 \n1629 \n1630 def set_default_active_vectors(mesh: 'pyvista.DataSet') -> None:\n1631     \"\"\"Set a default vectors array on mesh, if not already set.\n1632 \n1633     If an active vector already exists, no changes are made.\n1634 \n1635     If an active vectors does not exist, it checks for possibly cell\n1636     or point arrays with shape ``(n, 3)``.  If only one exists, then\n1637     it is set as the active vectors.  Otherwise, an error is raised.\n1638 \n1639     Parameters\n1640     ----------\n1641     mesh : pyvista.DataSet\n1642         Dataset to set default active vectors.\n1643 \n1644     Raises\n1645     ------\n1646     MissingDataError\n1647         If no vector-like arrays exist.\n1648 \n1649     AmbiguousDataError\n1650         If more than one vector-like arrays exist.\n1651 \n1652     \"\"\"\n1653     if mesh.active_vectors_name is not None:\n1654         return\n1655 \n1656     point_data = mesh.point_data\n1657     cell_data = mesh.cell_data\n1658 \n1659     possible_vectors_point = [\n1660         name for name, value in point_data.items() if value.ndim == 2 and value.shape[1] == 3\n1661     ]\n1662     possible_vectors_cell = [\n1663         name for name, value in cell_data.items() if value.ndim == 2 and value.shape[1] == 3\n1664     ]\n1665 \n1666     possible_vectors = possible_vectors_point + possible_vectors_cell\n1667     n_possible_vectors = len(possible_vectors)\n1668 \n1669     if n_possible_vectors == 1:\n1670         if len(possible_vectors_point) == 1:\n1671             preference = 'point'\n1672         else:\n1673             preference = 'cell'\n1674         mesh.set_active_vectors(possible_vectors[0], preference=preference)\n1675     elif n_possible_vectors < 1:\n1676         raise MissingDataError(\"No vector-like data available.\")\n1677     elif n_possible_vectors > 1:\n1678         raise AmbiguousDataError(\n1679             \"Multiple vector-like data available\\n\"\n1680             f\"cell data: {possible_vectors_cell}.\\n\"\n1681             f\"point data: {possible_vectors_point}.\\n\"\n1682             \"Set one as active using DataSet.set_active_vectors(name, preference=type)\"\n1683         )\n1684 \n1685 \n1686 def set_default_active_scalars(mesh: 'pyvista.DataSet') -> None:\n1687     \"\"\"Set a default scalars array on mesh, if not already set.\n1688 \n1689     If an active scalars already exists, no changes are made.\n1690 \n1691     If an active scalars does not exist, it checks for point or cell\n1692     arrays.  If only one exists, then it is set as the active scalars.\n1693     Otherwise, an error is raised.\n1694 \n1695     Parameters\n1696     ----------\n1697     mesh : pyvista.DataSet\n1698         Dataset to set default active scalars.\n1699 \n1700     Raises\n1701     ------\n1702     MissingDataError\n1703         If no arrays exist.\n1704 \n1705     AmbiguousDataError\n1706         If more than one array exists.\n1707 \n1708     \"\"\"\n1709     if mesh.active_scalars_name is not None:\n1710         return\n1711 \n1712     point_data = mesh.point_data\n1713     cell_data = mesh.cell_data\n1714 \n1715     possible_scalars_point = point_data.keys()\n1716     possible_scalars_cell = cell_data.keys()\n1717 \n1718     possible_scalars = possible_scalars_point + possible_scalars_cell\n1719     n_possible_scalars = len(possible_scalars)\n1720 \n1721     if n_possible_scalars == 1:\n1722         if len(possible_scalars_point) == 1:\n1723             preference = 'point'\n1724         else:\n1725             preference = 'cell'\n1726         mesh.set_active_scalars(possible_scalars[0], preference=preference)\n1727     elif n_possible_scalars < 1:\n1728         raise MissingDataError(\"No data available.\")\n1729     elif n_possible_scalars > 1:\n1730         raise AmbiguousDataError(\n1731             \"Multiple data available\\n\"\n1732             f\"cell data: {possible_scalars_cell}.\\n\"\n1733             f\"point data: {possible_scalars_point}.\\n\"\n1734             \"Set one as active using DataSet.set_active_scalars(name, preference=type)\"\n1735         )\n1736 \n[end of pyvista/utilities/helpers.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pyvista/core/dataobject.py\n+++ b/pyvista/core/dataobject.py\n@@ -1,6 +1,7 @@\n \n from abc import abstractmethod\n import collections.abc\n+from typing import Optional\n from pathlib import Path\n from typing import Any, DefaultDict, Dict, Type, Union\n \n@@ -62,6 +63,7 @@\n \n     def _post_file_load_processing(self):\n         \"\"\"Execute after loading a dataset from file, to be optionally overridden by subclasses.\"\"\"\n+        self._restore_metadata()\n         pass\n \n     def save(self, filename: str, binary=True, texture=None):\n@@ -270,6 +272,7 @@\n \n     def copy_attributes(self, dataset: _vtk.vtkDataSet):\n         \"\"\"Copy the data attributes of the input dataset object.\n \n+        This method is now updated to also copy cell data if present.\n         \"\"\"\n         self.CopyAttributes(dataset)\n         # Copy cell data if available\n@@ -307,6 +310,7 @@\n \n     def __getstate__(self):\n         \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n \n+        This method is now updated to also handle cell data during serialization.\n         \"\"\"\n         state = self.__dict__.copy()\n \n@@ -610,6 +614,7 @@\n         # copy data\n         self.copy_structure(mesh)\n         self.copy_attributes(mesh)\n+        self.copy_cell_data(mesh)\n \n     def copy_cell_data(self, dataset: _vtk.vtkDataSet):\n         \"\"\"Copy the cell data attributes of the input dataset object.\n@@ -640,6 +645,7 @@\n \n         Examples\n         --------\n         >>> import pyvista as pv\n         >>> source = pv.UniformGrid(dimensions=(10, 10, 5))\n         >>> source = source.compute_cell_sizes()\n         >>> target = pv.UniformGrid(dimensions=(10, 10, 5))\n         >>> target.copy_cell_data(source)\n         >>> target.plot(scalars='Volume', show_edges=True)\n         \"\"\"\n         self.CopyCellData(dataset)\n \n     def __getstate__(self):\n         \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n \n+        This method is now updated to also handle cell data during serialization.\n         \"\"\"\n         state = self.__dict__.copy()\n \n```\n", "model_patch": "--- a/pyvista/core/dataobject.py\n+++ b/pyvista/core/dataobject.py\n@@ -1,6 +1,7 @@\n \n from abc import abstractmethod\n import collections.abc\n+from typing import Optional\n from pathlib import Path\n from typing import Any, DefaultDict, Dict, Type, Union\n \n@@ -62,6 +63,7 @@\n \n     def _post_file_load_processing(self):\n         \"\"\"Execute after loading a dataset from file, to be optionally overridden by subclasses.\"\"\"\n+        self._restore_metadata()\n         pass\n \n     def save(self, filename: str, binary=True, texture=None):\n@@ -270,6 +272,7 @@\n \n     def copy_attributes(self, dataset: _vtk.vtkDataSet):\n         \"\"\"Copy the data attributes of the input dataset object.\n \n+        This method is now updated to also copy cell data if present.\n         \"\"\"\n         self.CopyAttributes(dataset)\n         # Copy cell data if available\n@@ -307,6 +310,7 @@\n \n     def __getstate__(self):\n         \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n \n+        This method is now updated to also handle cell data during serialization.\n         \"\"\"\n         state = self.__dict__.copy()\n \n@@ -610,6 +614,7 @@\n         # copy data\n         self.copy_structure(mesh)\n         self.copy_attributes(mesh)\n+        self.copy_cell_data(mesh)\n \n     def copy_cell_data(self, dataset: _vtk.vtkDataSet):\n         \"\"\"Copy the cell data attributes of the input dataset object.\n@@ -640,6 +645,7 @@\n \n         Examples\n         --------\n         >>> import pyvista as pv\n         >>> source = pv.UniformGrid(dimensions=(10, 10, 5))\n         >>> source = source.compute_cell_sizes()\n         >>> target = pv.UniformGrid(dimensions=(10, 10, 5))\n         >>> target.copy_cell_data(source)\n         >>> target.plot(scalars='Volume', show_edges=True)\n         \"\"\"\n         self.CopyCellData(dataset)\n \n     def __getstate__(self):\n         \"\"\"Support pickle by serializing the VTK object data to something which can be pickled natively.\n \n+        This method is now updated to also handle cell data during serialization.\n         \"\"\"\n         state = self.__dict__.copy()\n \n"}
{"instance_id": "pyvista__pyvista-3747", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n`bounds` property return type is mutable from `MultiBlock`\nThe `bounds` property has a different return type for meshes and `MultiBlock` objects:\r\n\r\n```\r\n>>> import pyvista as pv\r\n>>> slices = pv.Sphere().slice_orthogonal()\r\n# MultiBlock returns list (mutable)\r\n>>> slices.bounds\r\n[-0.49926671385765076, 0.49926671385765076, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5]\r\n# Mesh returns tuple (immutable)\r\n>>> slices[0].bounds\r\n(-6.162975822039155e-33, 0.0, -0.4965316653251648, 0.4965316653251648, -0.5, 0.5)\r\n```\r\n\r\nIMO, the return value should be immutable and the `bounds` property should  be cast to a tuple before returning.\n\n</issue>\n<code>\n[start of README.rst]\n1 #######\n2 PyVista\n3 #######\n4 \n5 .. image:: https://github.com/pyvista/pyvista/raw/main/doc/_static/pyvista_banner_small.png\n6    :target: https://docs.pyvista.org/examples/index.html\n7    :alt: pyvista\n8 \n9 \n10 .. |zenodo| image:: https://zenodo.org/badge/92974124.svg\n11    :target: https://zenodo.org/badge/latestdoi/92974124\n12 \n13 .. |joss| image:: http://joss.theoj.org/papers/10.21105/joss.01450/status.svg\n14    :target: https://doi.org/10.21105/joss.01450\n15 \n16 .. |pypi| image:: https://img.shields.io/pypi/v/pyvista.svg?logo=python&logoColor=white\n17    :target: https://pypi.org/project/pyvista/\n18 \n19 .. |conda| image:: https://img.shields.io/conda/vn/conda-forge/pyvista.svg?logo=conda-forge&logoColor=white\n20    :target: https://anaconda.org/conda-forge/pyvista\n21 \n22 .. |GH-CI| image:: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml/badge.svg\n23    :target: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml\n24 \n25 .. |codecov| image:: https://codecov.io/gh/pyvista/pyvista/branch/main/graph/badge.svg\n26    :target: https://codecov.io/gh/pyvista/pyvista\n27 \n28 .. |codacy| image:: https://app.codacy.com/project/badge/Grade/779ac6aed37548839384acfc0c1aab44\n29    :target: https://www.codacy.com/gh/pyvista/pyvista/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=pyvista/pyvista&amp;utm_campaign=Badge_Grade\n30 \n31 .. |MIT| image:: https://img.shields.io/badge/License-MIT-yellow.svg\n32    :target: https://opensource.org/licenses/MIT\n33 \n34 .. |slack| image:: https://img.shields.io/badge/Slack-pyvista-green.svg?logo=slack\n35    :target: http://slack.pyvista.org\n36 \n37 .. |PyPIact| image:: https://img.shields.io/pypi/dm/pyvista.svg?label=PyPI%20downloads\n38    :target: https://pypi.org/project/pyvista/\n39 \n40 .. |condaact| image:: https://img.shields.io/conda/dn/conda-forge/pyvista.svg?label=Conda%20downloads\n41    :target: https://anaconda.org/conda-forge/pyvista\n42 \n43 .. |discuss| image:: https://img.shields.io/badge/GitHub-Discussions-green?logo=github\n44    :target: https://github.com/pyvista/pyvista/discussions\n45 \n46 .. |isort| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat\n47   :target: https://timothycrosley.github.io/isort\n48   :alt: isort\n49 \n50 .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n51   :target: https://github.com/psf/black\n52   :alt: black\n53 \n54 .. |python| image:: https://img.shields.io/badge/python-3.7+-blue.svg\n55    :target: https://www.python.org/downloads/\n56 \n57 \n58 +----------------------+-----------+------------+\n59 | Deployment           | |pypi|    |   |conda|  |\n60 +----------------------+-----------+------------+\n61 | Build Status         | |GH-CI|   |  |python|  |\n62 +----------------------+-----------+------------+\n63 | Metrics              | |codacy|  |  |codecov| |\n64 +----------------------+-----------+------------+\n65 | Activity             | |PyPIact| | |condaact| |\n66 +----------------------+-----------+------------+\n67 | Citation             | |joss|    |  |zenodo|  |\n68 +----------------------+-----------+------------+\n69 | License              | |MIT|     |            |\n70 +----------------------+-----------+------------+\n71 | Community            | |slack|   |  |discuss| |\n72 +----------------------+-----------+------------+\n73 | Formatter            | |black|   |  |isort|   |\n74 +----------------------+-----------+------------+\n75 \n76 \n77     3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)\n78 \n79 PyVista is...\n80 \n81 * *Pythonic VTK*: a high-level API to the `Visualization Toolkit`_ (VTK)\n82 * mesh data structures and filtering methods for spatial datasets\n83 * 3D plotting made simple and built for large/complex data geometries\n84 \n85 .. _Visualization Toolkit: https://vtk.org\n86 \n87 PyVista is a helper module for the Visualization Toolkit (VTK) that wraps the VTK library\n88 through NumPy and direct array access through a variety of methods and classes.\n89 This package provides a Pythonic, well-documented interface exposing\n90 VTK's powerful visualization backend to facilitate rapid prototyping, analysis,\n91 and visual integration of spatially referenced datasets.\n92 \n93 This module can be used for scientific plotting for presentations and research\n94 papers as well as a supporting module for other mesh 3D rendering dependent\n95 Python modules; see Connections for a list of projects that leverage\n96 PyVista.\n97 \n98 \n99 .. |tweet| image:: https://img.shields.io/twitter/url.svg?style=social&url=http%3A%2F%2Fshields.io\n100    :target: https://twitter.com/intent/tweet?text=Check%20out%20this%20project%20for%203D%20visualization%20in%20Python&url=https://github.com/pyvista/pyvista&hashtags=3D,visualization,Python,vtk,mesh,plotting,PyVista\n101 \n102 Share this project on Twitter: |tweet|\n103 \n104 \n105 Highlights\n106 ==========\n107 \n108 .. |binder| image:: https://static.mybinder.org/badge_logo.svg\n109    :target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master\n110    :alt: Launch on Binder\n111 \n112 Head over to the `Quick Examples`_ page in the docs to explore our gallery of\n113 examples showcasing what PyVista can do! Want to test-drive PyVista?\n114 All of the examples from the gallery are live on MyBinder for you to test\n115 drive without installing anything locally: |binder|\n116 \n117 .. _Quick Examples: http://docs.pyvista.org/examples/index.html\n118 \n119 \n120 Overview of Features\n121 --------------------\n122 \n123 * Extensive gallery of examples (see `Quick Examples`_)\n124 * Interactive plotting in Jupyter Notebooks using server-side rendering\n125   with `ipyvtklink`_ or client-side rendering with ``panel`` or ``ipygany``.\n126 * Filtering/plotting tools built for interactivity (see `Widgets`_)\n127 * Direct access to mesh analysis and transformation routines (see Filters_)\n128 * Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)\n129 * Import meshes from many common formats (use ``pyvista.read()``). Support for all formats handled by `meshio`_ is built-in!\n130 * Export meshes as VTK, STL, OBJ, or PLY (``mesh.save()``) file types or any formats supported by meshio_ (``pyvista.save_meshio()``)\n131 \n132 .. _ipyvtklink: https://github.com/Kitware/ipyvtklink\n133 .. _Widgets: https://docs.pyvista.org/api/plotting/index.html#widget-api\n134 .. _Filters: https://docs.pyvista.org/api/core/filters.html\n135 .. _Plotting: https://docs.pyvista.org/api/plotting/index.html\n136 .. _meshio: https://github.com/nschloe/meshio\n137 \n138 \n139 Documentation\n140 =============\n141 \n142 Refer to the `documentation <http://docs.pyvista.org/>`_ for detailed\n143 installation and usage details.\n144 \n145 For general questions about the project, its applications, or about software\n146 usage, please create a discussion in `pyvista/discussions`_\n147 where the community can collectively address your questions. You are also\n148 welcome to join us on Slack_ or send one of the developers an email.\n149 The project support team can be reached at `info@pyvista.org`_.\n150 \n151 .. _pyvista/discussions: https://github.com/pyvista/pyvista/discussions\n152 .. _Slack: http://slack.pyvista.org\n153 .. _info@pyvista.org: mailto:info@pyvista.org\n154 \n155 \n156 Installation\n157 ============\n158 \n159 PyVista can be installed from `PyPI <https://pypi.org/project/pyvista/>`_\n160 using ``pip`` on Python >= 3.7::\n161 \n162     pip install pyvista\n163 \n164 You can also visit `PyPI <https://pypi.org/project/pyvista/>`_,\n165 `Anaconda <https://anaconda.org/conda-forge/pyvista>`_, or\n166 `GitHub <https://github.com/pyvista/pyvista>`_ to download the source.\n167 \n168 See the `Installation <http://docs.pyvista.org/getting-started/installation.html#install-ref.>`_\n169 for more details regarding optional dependencies or if the installation through pip doesn't work out.\n170 \n171 \n172 Connections\n173 ===========\n174 \n175 PyVista is a powerful tool that researchers can harness to create compelling,\n176 integrated visualizations of large datasets in an intuitive, Pythonic manner.\n177 \n178 Learn more about how PyVista is used across science and engineering disciplines\n179 by a diverse community of users on our `Connections page`_.\n180 \n181 .. _Connections page: https://docs.pyvista.org/getting-started/connections.html\n182 \n183 \n184 Authors\n185 =======\n186 \n187 Please take a look at the `contributors page`_ and the active `list of authors`_\n188 to learn more about the developers of PyVista.\n189 \n190 .. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/\n191 .. _list of authors: https://docs.pyvista.org/getting-started/authors.html#authors\n192 \n193 \n194 Contributing\n195 ============\n196 \n197 .. |Contributor Covenant| image:: https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg\n198    :target: CODE_OF_CONDUCT.md\n199 \n200 .. |codetriage| image:: https://www.codetriage.com/pyvista/pyvista/badges/users.svg\n201    :target: https://www.codetriage.com/pyvista/pyvista\n202    :alt: Code Triage\n203 \n204 |Contributor Covenant|\n205 |codetriage|\n206 \n207 We absolutely welcome contributions and we hope that our `Contributing Guide`_\n208 will facilitate your ability to make PyVista better. PyVista is mostly\n209 maintained on a volunteer basis and thus we need to foster a community that can\n210 support user questions and develop new features to make this software a useful\n211 tool for all users while encouraging every member of the commutinity to share\n212 their ideas. To learn more about contributing to PyVista, please see the\n213 `Contributing Guide`_ and our `Code of Conduct`_.\n214 \n215 .. _Contributing Guide: https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst\n216 .. _Code of Conduct: https://github.com/pyvista/pyvista/blob/main/CODE_OF_CONDUCT.md\n217 \n218 \n219 Citing PyVista\n220 ==============\n221 \n222 There is a `paper about PyVista <https://doi.org/10.21105/joss.01450>`_!\n223 \n224 If you are using PyVista in your scientific research, please help our scientific\n225 visibility by citing our work!\n226 \n227 \n228     Sullivan and Kaszynski, (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450\n229 \n230 \n231 BibTex:\n232 \n233 .. code::\n234 \n235     @article{sullivan2019pyvista,\n236       doi = {10.21105/joss.01450},\n237       url = {https://doi.org/10.21105/joss.01450},\n238       year = {2019},\n239       month = {May},\n240       publisher = {The Open Journal},\n241       volume = {4},\n242       number = {37},\n243       pages = {1450},\n244       author = {Bane Sullivan and Alexander Kaszynski},\n245       title = {{PyVista}: {3D} plotting and mesh analysis through a streamlined interface for the {Visualization Toolkit} ({VTK})},\n246       journal = {Journal of Open Source Software}\n247     }\n248 \n[end of README.rst]\n[start of pyvista/core/composite.py]\n1 \"\"\"Container to mimic ``vtkMultiBlockDataSet`` objects.\n2 \n3 These classes hold many VTK datasets in one object that can be passed\n4 to VTK algorithms and PyVista filtering/plotting routines.\n5 \"\"\"\n6 import collections.abc\n7 from itertools import zip_longest\n8 import pathlib\n9 from typing import Any, Iterable, List, Optional, Set, Tuple, Union, cast, overload\n10 \n11 import numpy as np\n12 \n13 import pyvista\n14 from pyvista import _vtk\n15 from pyvista.utilities import FieldAssociation, is_pyvista_dataset, wrap\n16 \n17 from .dataset import DataObject, DataSet\n18 from .filters import CompositeFilters\n19 from .pyvista_ndarray import pyvista_ndarray\n20 \n21 _TypeMultiBlockLeaf = Union['MultiBlock', DataSet]\n22 \n23 \n24 class MultiBlock(\n25     _vtk.vtkMultiBlockDataSet, CompositeFilters, DataObject, collections.abc.MutableSequence\n26 ):\n27     \"\"\"A composite class to hold many data sets which can be iterated over.\n28 \n29     This wraps/extends the `vtkMultiBlockDataSet\n30     <https://vtk.org/doc/nightly/html/classvtkMultiBlockDataSet.html>`_ class\n31     so that we can easily plot these data sets and use the composite in a\n32     Pythonic manner.\n33 \n34     You can think of ``MultiBlock`` like a list as we\n35     can iterate over this data structure by index.  It has some dictionary\n36     features as we can also access blocks by their string name.\n37 \n38     .. versionchanged:: 0.36.0\n39        ``MultiBlock`` adheres more closely to being list like, and inherits\n40        from :class:`collections.abc.MutableSequence`.  Multiple nonconforming\n41        behaviors were removed or modified.\n42 \n43     Examples\n44     --------\n45     >>> import pyvista as pv\n46 \n47     Create an empty composite dataset.\n48 \n49     >>> blocks = pv.MultiBlock()\n50 \n51     Add a dataset to the collection.\n52 \n53     >>> sphere = pv.Sphere()\n54     >>> blocks.append(sphere)\n55 \n56     Add a named block.\n57 \n58     >>> blocks[\"cube\"] = pv.Cube()\n59 \n60     Instantiate from a list of objects.\n61 \n62     >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)),\n63     ...         pv.Cone()]\n64     >>> blocks = pv.MultiBlock(data)\n65     >>> blocks.plot()\n66 \n67     Instantiate from a dictionary.\n68 \n69     >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n70     >>> blocks = pv.MultiBlock(data)\n71     >>> blocks.plot()\n72 \n73     Iterate over the collection.\n74 \n75     >>> for name in blocks.keys():\n76     ...     block = blocks[name]\n77 \n78     >>> for block in blocks:\n79     ...     surf = block.extract_surface()  # Do something with each dataset\n80 \n81     \"\"\"\n82 \n83     # Bind pyvista.plotting.plot to the object\n84     plot = pyvista.plot\n85     _WRITERS = dict.fromkeys(['.vtm', '.vtmb'], _vtk.vtkXMLMultiBlockDataWriter)\n86 \n87     def __init__(self, *args, **kwargs) -> None:\n88         \"\"\"Initialize multi block.\"\"\"\n89         super().__init__()\n90         deep = kwargs.pop('deep', False)\n91 \n92         # keep a python reference to the dataset to avoid\n93         # unintentional garbage collections since python does not\n94         # add a reference to the dataset when it's added here in\n95         # MultiBlock.  See https://github.com/pyvista/pyvista/pull/1805\n96         self._refs: Any = {}\n97 \n98         if len(args) == 1:\n99             if isinstance(args[0], _vtk.vtkMultiBlockDataSet):\n100                 if deep:\n101                     self.deep_copy(args[0])\n102                 else:\n103                     self.shallow_copy(args[0])\n104             elif isinstance(args[0], (list, tuple)):\n105                 for block in args[0]:\n106                     self.append(block)\n107             elif isinstance(args[0], (str, pathlib.Path)):\n108                 self._from_file(args[0], **kwargs)\n109             elif isinstance(args[0], dict):\n110                 for key, block in args[0].items():\n111                     self.append(block, key)\n112             else:\n113                 raise TypeError(f'Type {type(args[0])} is not supported by pyvista.MultiBlock')\n114 \n115         elif len(args) > 1:\n116             raise ValueError(\n117                 'Invalid number of arguments:\\n``pyvista.MultiBlock``' 'supports 0 or 1 arguments.'\n118             )\n119 \n120         # Upon creation make sure all nested structures are wrapped\n121         self.wrap_nested()\n122 \n123     def wrap_nested(self):\n124         \"\"\"Ensure that all nested data structures are wrapped as PyVista datasets.\n125 \n126         This is performed in place.\n127 \n128         \"\"\"\n129         for i in range(self.n_blocks):\n130             block = self.GetBlock(i)\n131             if not is_pyvista_dataset(block):\n132                 self.SetBlock(i, pyvista.wrap(block))\n133 \n134     @property\n135     def bounds(self) -> List[float]:\n136         \"\"\"Find min/max for bounds across blocks.\n137 \n138         Returns\n139         -------\n140         tuple(float)\n141             length 6 tuple of floats containing min/max along each axis\n142 \n143         Examples\n144         --------\n145         Return the bounds across blocks.\n146 \n147         >>> import pyvista as pv\n148         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n149         >>> blocks = pv.MultiBlock(data)\n150         >>> blocks.bounds\n151         [-0.5, 2.5, -0.5, 2.5, -0.5, 0.5]\n152 \n153         \"\"\"\n154         # apply reduction of min and max over each block\n155         all_bounds = [block.bounds for block in self if block]\n156         # edge case where block has no bounds\n157         if not all_bounds:  # pragma: no cover\n158             minima = np.array([0, 0, 0])\n159             maxima = np.array([0, 0, 0])\n160         else:\n161             minima = np.minimum.reduce(all_bounds)[::2]\n162             maxima = np.maximum.reduce(all_bounds)[1::2]\n163 \n164         # interleave minima and maxima for bounds\n165         return np.stack([minima, maxima]).ravel('F').tolist()\n166 \n167     @property\n168     def center(self) -> Any:\n169         \"\"\"Return the center of the bounding box.\n170 \n171         Examples\n172         --------\n173         >>> import pyvista as pv\n174         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n175         >>> blocks = pv.MultiBlock(data)\n176         >>> blocks.center  # doctest:+SKIP\n177         array([1., 1., 0.])\n178 \n179         \"\"\"\n180         return np.reshape(self.bounds, (3, 2)).mean(axis=1)\n181 \n182     @property\n183     def length(self) -> float:\n184         \"\"\"Return the length of the diagonal of the bounding box.\n185 \n186         Examples\n187         --------\n188         >>> import pyvista as pv\n189         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n190         >>> blocks = pv.MultiBlock(data)\n191         >>> blocks.length\n192         4.3584\n193 \n194         \"\"\"\n195         return pyvista.Box(self.bounds).length\n196 \n197     @property\n198     def n_blocks(self) -> int:\n199         \"\"\"Return the total number of blocks set.\n200 \n201         Examples\n202         --------\n203         >>> import pyvista as pv\n204         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n205         >>> blocks = pv.MultiBlock(data)\n206         >>> blocks.n_blocks\n207         3\n208 \n209         \"\"\"\n210         return self.GetNumberOfBlocks()\n211 \n212     @n_blocks.setter\n213     def n_blocks(self, n):\n214         \"\"\"Change the total number of blocks set.\"\"\"\n215         self.SetNumberOfBlocks(n)\n216         self.Modified()\n217 \n218     @property\n219     def volume(self) -> float:\n220         \"\"\"Return the total volume of all meshes in this dataset.\n221 \n222         Returns\n223         -------\n224         float\n225             Total volume of the mesh.\n226 \n227         Examples\n228         --------\n229         >>> import pyvista as pv\n230         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n231         >>> blocks = pv.MultiBlock(data)\n232         >>> blocks.volume\n233         1.7348\n234 \n235         \"\"\"\n236         return sum(block.volume for block in self if block)\n237 \n238     def get_data_range(self, name: str, allow_missing: bool = False) -> Tuple[float, float]:  # type: ignore\n239         \"\"\"Get the min/max of an array given its name across all blocks.\n240 \n241         Parameters\n242         ----------\n243         name : str\n244             Name of the array.\n245 \n246         allow_missing : bool, optional\n247             Allow a block to be missing the named array.\n248 \n249         Returns\n250         -------\n251         tuple\n252             ``(min, max)`` of the named array.\n253 \n254         \"\"\"\n255         mini, maxi = np.inf, -np.inf\n256         for i in range(self.n_blocks):\n257             data = self[i]\n258             if data is None:\n259                 continue\n260             # get the scalars if available - recursive\n261             try:\n262                 tmi, tma = data.get_data_range(name)\n263             except KeyError as err:\n264                 if allow_missing:\n265                     continue\n266                 else:\n267                     raise err\n268             if not np.isnan(tmi) and tmi < mini:\n269                 mini = tmi\n270             if not np.isnan(tma) and tma > maxi:\n271                 maxi = tma\n272         return mini, maxi\n273 \n274     def get_index_by_name(self, name: str) -> int:\n275         \"\"\"Find the index number by block name.\n276 \n277         Parameters\n278         ----------\n279         name : str\n280             Name of the block.\n281 \n282         Returns\n283         -------\n284         int\n285             Index of the block.\n286 \n287         Examples\n288         --------\n289         >>> import pyvista as pv\n290         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n291         >>> blocks = pv.MultiBlock(data)\n292         >>> blocks.get_index_by_name('sphere')\n293         1\n294 \n295         \"\"\"\n296         for i in range(self.n_blocks):\n297             if self.get_block_name(i) == name:\n298                 return i\n299         raise KeyError(f'Block name ({name}) not found')\n300 \n301     @overload\n302     def __getitem__(self, index: Union[int, str]) -> Optional[_TypeMultiBlockLeaf]:  # noqa: D105\n303         ...  # pragma: no cover\n304 \n305     @overload\n306     def __getitem__(self, index: slice) -> 'MultiBlock':  # noqa: D105\n307         ...  # pragma: no cover\n308 \n309     def __getitem__(self, index):\n310         \"\"\"Get a block by its index or name.\n311 \n312         If the name is non-unique then returns the first occurrence.\n313 \n314         \"\"\"\n315         if isinstance(index, slice):\n316             multi = MultiBlock()\n317             for i in range(self.n_blocks)[index]:\n318                 multi.append(self[i], self.get_block_name(i))\n319             return multi\n320         elif isinstance(index, str):\n321             index = self.get_index_by_name(index)\n322         ############################\n323         if index < -self.n_blocks or index >= self.n_blocks:\n324             raise IndexError(f'index ({index}) out of range for this dataset.')\n325         if index < 0:\n326             index = self.n_blocks + index\n327 \n328         data = self.GetBlock(index)\n329         if data is None:\n330             return data\n331         if data is not None and not is_pyvista_dataset(data):\n332             data = wrap(data)\n333         return data\n334 \n335     def append(self, dataset: Optional[_TypeMultiBlockLeaf], name: Optional[str] = None):\n336         \"\"\"Add a data set to the next block index.\n337 \n338         Parameters\n339         ----------\n340         dataset : pyvista.DataSet or pyvista.MultiBlock\n341             Dataset to append to this multi-block.\n342 \n343         name : str, optional\n344             Block name to give to dataset.  A default name is given\n345             depending on the block index as 'Block-{i:02}'.\n346 \n347         Examples\n348         --------\n349         >>> import pyvista as pv\n350         >>> from pyvista import examples\n351         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n352         >>> blocks = pv.MultiBlock(data)\n353         >>> blocks.append(pv.Cone())\n354         >>> len(blocks)\n355         3\n356         >>> blocks.append(examples.load_uniform(), \"uniform\")\n357         >>> blocks.keys()\n358         ['cube', 'sphere', 'Block-02', 'uniform']\n359 \n360         \"\"\"\n361         # do not allow to add self\n362         if dataset is self:\n363             raise ValueError(\"Cannot nest a composite dataset in itself.\")\n364 \n365         index = self.n_blocks  # note off by one so use as index\n366         # always wrap since we may need to reference the VTK memory address\n367         if not pyvista.is_pyvista_dataset(dataset):\n368             dataset = pyvista.wrap(dataset)\n369         self.n_blocks += 1\n370         self[index] = dataset\n371         # No overwrite if name is None\n372         self.set_block_name(index, name)\n373 \n374     def extend(self, datasets: Iterable[_TypeMultiBlockLeaf]) -> None:\n375         \"\"\"Extend MultiBlock with an Iterable.\n376 \n377         If another MultiBlock object is supplied, the key names will\n378         be preserved.\n379 \n380         Parameters\n381         ----------\n382         datasets : Iterable[pyvista.DataSet or pyvista.MultiBlock]\n383             Datasets to extend.\n384 \n385         Examples\n386         --------\n387         >>> import pyvista as pv\n388         >>> from pyvista import examples\n389         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n390         >>> blocks = pv.MultiBlock(data)\n391         >>> blocks_uniform = pv.MultiBlock({\"uniform\": examples.load_uniform()})\n392         >>> blocks.extend(blocks_uniform)\n393         >>> len(blocks)\n394         3\n395         >>> blocks.keys()\n396         ['cube', 'sphere', 'uniform']\n397 \n398         \"\"\"\n399         # Code based on collections.abc\n400         if isinstance(datasets, MultiBlock):\n401             for key, data in zip(datasets.keys(), datasets):\n402                 self.append(data, key)\n403         else:\n404             for v in datasets:\n405                 self.append(v)\n406 \n407     def get(\n408         self, index: str, default: Optional[_TypeMultiBlockLeaf] = None\n409     ) -> Optional[_TypeMultiBlockLeaf]:\n410         \"\"\"Get a block by its name.\n411 \n412         If the name is non-unique then returns the first occurrence.\n413         Returns ``default`` if name isn't in the dataset.\n414 \n415         Parameters\n416         ----------\n417         index : str\n418             Index or name of the dataset within the multiblock.\n419 \n420         default : pyvista.DataSet or pyvista.MultiBlock, optional\n421             Default to return if index is not in the multiblock.\n422 \n423         Returns\n424         -------\n425         pyvista.DataSet or pyvista.MultiBlock or None\n426             Dataset from the given index if it exists.\n427 \n428         Examples\n429         --------\n430         >>> import pyvista as pv\n431         >>> from pyvista import examples\n432         >>> data = {\"poly\": pv.PolyData(), \"uni\": pv.UniformGrid()}\n433         >>> blocks = pv.MultiBlock(data)\n434         >>> blocks.get(\"poly\")\n435         PolyData ...\n436         >>> blocks.get(\"cone\")\n437 \n438         \"\"\"\n439         try:\n440             return self[index]\n441         except KeyError:\n442             return default\n443 \n444     def set_block_name(self, index: int, name: Optional[str]):\n445         \"\"\"Set a block's string name at the specified index.\n446 \n447         Parameters\n448         ----------\n449         index : int\n450             Index or the dataset within the multiblock.\n451 \n452         name : str\n453             Name to assign to the block at ``index``.\n454 \n455         Examples\n456         --------\n457         >>> import pyvista as pv\n458         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n459         >>> blocks = pv.MultiBlock(data)\n460         >>> blocks.append(pv.Cone())\n461         >>> blocks.set_block_name(2, 'cone')\n462         >>> blocks.keys()\n463         ['cube', 'sphere', 'cone']\n464 \n465         \"\"\"\n466         index = range(self.n_blocks)[index]\n467         if name is None:\n468             return\n469         self.GetMetaData(index).Set(_vtk.vtkCompositeDataSet.NAME(), name)\n470         self.Modified()\n471 \n472     def get_block_name(self, index: int) -> Optional[str]:\n473         \"\"\"Return the string name of the block at the given index.\n474 \n475         Parameters\n476         ----------\n477         index : int\n478             Index of the block to get the name of.\n479 \n480         Returns\n481         -------\n482         str\n483             Name of the block at the given index.\n484 \n485         Examples\n486         --------\n487         >>> import pyvista as pv\n488         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n489         >>> blocks = pv.MultiBlock(data)\n490         >>> blocks.get_block_name(0)\n491         'cube'\n492 \n493         \"\"\"\n494         index = range(self.n_blocks)[index]\n495         meta = self.GetMetaData(index)\n496         if meta is not None:\n497             return meta.Get(_vtk.vtkCompositeDataSet.NAME())\n498         return None\n499 \n500     def keys(self) -> List[Optional[str]]:\n501         \"\"\"Get all the block names in the dataset.\n502 \n503         Returns\n504         -------\n505         list\n506             List of block names.\n507 \n508         Examples\n509         --------\n510         >>> import pyvista as pv\n511         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n512         >>> blocks = pv.MultiBlock(data)\n513         >>> blocks.keys()\n514         ['cube', 'sphere']\n515 \n516         \"\"\"\n517         return [self.get_block_name(i) for i in range(self.n_blocks)]\n518 \n519     def _ipython_key_completions_(self) -> List[Optional[str]]:\n520         return self.keys()\n521 \n522     def replace(self, index: int, dataset: Optional[_TypeMultiBlockLeaf]) -> None:\n523         \"\"\"Replace dataset at index while preserving key name.\n524 \n525         Parameters\n526         ----------\n527         index : int\n528             Index of the block to replace.\n529         dataset : pyvista.DataSet or pyvista.MultiBlock\n530             Dataset for replacing the one at index.\n531 \n532         Examples\n533         --------\n534         >>> import pyvista as pv\n535         >>> import numpy as np\n536         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n537         >>> blocks = pv.MultiBlock(data)\n538         >>> blocks.replace(1, pv.Sphere(center=(10, 10, 10)))\n539         >>> blocks.keys()\n540         ['cube', 'sphere']\n541         >>> np.allclose(blocks[1].center, [10., 10., 10.])\n542         True\n543 \n544         \"\"\"\n545         name = self.get_block_name(index)\n546         self[index] = dataset\n547         self.set_block_name(index, name)\n548 \n549     @overload\n550     def __setitem__(\n551         self, index: Union[int, str], data: Optional[_TypeMultiBlockLeaf]\n552     ):  # noqa: D105\n553         ...  # pragma: no cover\n554 \n555     @overload\n556     def __setitem__(\n557         self, index: slice, data: Iterable[Optional[_TypeMultiBlockLeaf]]\n558     ):  # noqa: D105\n559         ...  # pragma: no cover\n560 \n561     def __setitem__(\n562         self,\n563         index,\n564         data,\n565     ):\n566         \"\"\"Set a block with a VTK data object.\n567 \n568         To set the name simultaneously, pass a string name as the 2nd index.\n569 \n570         Example\n571         -------\n572         >>> import pyvista\n573         >>> multi = pyvista.MultiBlock()\n574         >>> multi.append(pyvista.PolyData())\n575         >>> multi[0] = pyvista.UnstructuredGrid()\n576         >>> multi.append(pyvista.PolyData(), 'poly')\n577         >>> multi.keys()\n578         ['Block-00', 'poly']\n579         >>> multi['bar'] = pyvista.PolyData()\n580         >>> multi.n_blocks\n581         3\n582 \n583         \"\"\"\n584         i: int = 0\n585         name: Optional[str] = None\n586         if isinstance(index, str):\n587             try:\n588                 i = self.get_index_by_name(index)\n589             except KeyError:\n590                 self.append(data, index)\n591                 return\n592             name = index\n593         elif isinstance(index, slice):\n594             index_iter = range(self.n_blocks)[index]\n595             for i, (idx, d) in enumerate(zip_longest(index_iter, data)):\n596                 if idx is None:\n597                     self.insert(\n598                         index_iter[-1] + 1 + (i - len(index_iter)), d\n599                     )  # insert after last entry, increasing\n600                 elif d is None:\n601                     del self[index_iter[-1] + 1]  # delete next entry\n602                 else:\n603                     self[idx] = d  #\n604             return\n605         else:\n606             i = index\n607 \n608         # data, i, and name are a single value now\n609         if data is not None and not is_pyvista_dataset(data):\n610             data = wrap(data)\n611         data = cast(pyvista.DataSet, data)\n612 \n613         i = range(self.n_blocks)[i]\n614 \n615         # this is the only spot in the class where we actually add\n616         # data to the MultiBlock\n617 \n618         # check if we are overwriting a block\n619         existing_dataset = self.GetBlock(i)\n620         if existing_dataset is not None:\n621             self._remove_ref(i)\n622         self.SetBlock(i, data)\n623         if data is not None:\n624             self._refs[data.memory_address] = data\n625 \n626         if name is None:\n627             name = f'Block-{i:02}'\n628         self.set_block_name(i, name)  # Note that this calls self.Modified()\n629 \n630     def __delitem__(self, index: Union[int, str, slice]) -> None:\n631         \"\"\"Remove a block at the specified index.\"\"\"\n632         if isinstance(index, slice):\n633             if index.indices(self.n_blocks)[2] > 0:\n634                 for i in reversed(range(*index.indices(self.n_blocks))):\n635                     self.__delitem__(i)\n636             else:\n637                 for i in range(*index.indices(self.n_blocks)):\n638                     self.__delitem__(i)\n639             return\n640         if isinstance(index, str):\n641             index = self.get_index_by_name(index)\n642         self._remove_ref(index)\n643         self.RemoveBlock(index)\n644 \n645     def _remove_ref(self, index: int):\n646         \"\"\"Remove python reference to the dataset.\"\"\"\n647         dataset = self[index]\n648         if hasattr(dataset, 'memory_address'):\n649             self._refs.pop(dataset.memory_address, None)  # type: ignore\n650 \n651     def __iter__(self) -> 'MultiBlock':\n652         \"\"\"Return the iterator across all blocks.\"\"\"\n653         self._iter_n = 0\n654         return self\n655 \n656     def __eq__(self, other):\n657         \"\"\"Equality comparison.\"\"\"\n658         if not isinstance(other, MultiBlock):\n659             return False\n660 \n661         if self is other:\n662             return True\n663 \n664         if len(self) != len(other):\n665             return False\n666 \n667         if not self.keys() == other.keys():\n668             return False\n669 \n670         if any(self_mesh != other_mesh for self_mesh, other_mesh in zip(self, other)):\n671             return False\n672 \n673         return True\n674 \n675     def __next__(self) -> Optional[_TypeMultiBlockLeaf]:\n676         \"\"\"Get the next block from the iterator.\"\"\"\n677         if self._iter_n < self.n_blocks:\n678             result = self[self._iter_n]\n679             self._iter_n += 1\n680             return result\n681         raise StopIteration\n682 \n683     def insert(self, index: int, dataset: _TypeMultiBlockLeaf, name: Optional[str] = None) -> None:\n684         \"\"\"Insert data before index.\n685 \n686         Parameters\n687         ----------\n688         index : int\n689             Index before which to insert data.\n690         dataset : pyvista.DataSet or pyvista.MultiBlock\n691             Data to insert.\n692         name : str, optional\n693             Name for key to give dataset.  A default name is given\n694             depending on the block index as ``'Block-{i:02}'``.\n695 \n696         Examples\n697         --------\n698         Insert a new :class:`pyvista.PolyData` at the start of the multiblock.\n699 \n700         >>> import pyvista as pv\n701         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n702         >>> blocks = pv.MultiBlock(data)\n703         >>> blocks.keys()\n704         ['cube', 'sphere']\n705         >>> blocks.insert(0, pv.Plane(), \"plane\")\n706         >>> blocks.keys()\n707         ['plane', 'cube', 'sphere']\n708 \n709         \"\"\"\n710         index = range(self.n_blocks)[index]\n711 \n712         self.n_blocks += 1\n713         for i in reversed(range(index, self.n_blocks - 1)):\n714             self[i + 1] = self[i]\n715             self.set_block_name(i + 1, self.get_block_name(i))\n716 \n717         self[index] = dataset\n718         self.set_block_name(index, name)\n719 \n720     def pop(self, index: Union[int, str] = -1) -> Optional[_TypeMultiBlockLeaf]:\n721         \"\"\"Pop off a block at the specified index.\n722 \n723         Parameters\n724         ----------\n725         index : int or str, optional\n726             Index or name of the dataset within the multiblock.  Defaults to\n727             last dataset.\n728 \n729         Returns\n730         -------\n731         pyvista.DataSet or pyvista.MultiBlock\n732             Dataset from the given index that was removed.\n733 \n734         Examples\n735         --------\n736         Pop the ``\"cube\"`` multiblock.\n737 \n738         >>> import pyvista as pv\n739         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n740         >>> blocks = pv.MultiBlock(data)\n741         >>> blocks.keys()\n742         ['cube', 'sphere']\n743         >>> cube = blocks.pop(\"cube\")\n744         >>> blocks.keys()\n745         ['sphere']\n746 \n747         \"\"\"\n748         if isinstance(index, int):\n749             index = range(self.n_blocks)[index]\n750         data = self[index]\n751         del self[index]\n752         return data\n753 \n754     def reverse(self):\n755         \"\"\"Reverse MultiBlock in-place.\n756 \n757         Examples\n758         --------\n759         Reverse a multiblock.\n760 \n761         >>> import pyvista as pv\n762         >>> data = {\"cube\": pv.Cube(), \"sphere\": pv.Sphere(center=(2, 2, 0))}\n763         >>> blocks = pv.MultiBlock(data)\n764         >>> blocks.keys()\n765         ['cube', 'sphere']\n766         >>> blocks.reverse()\n767         >>> blocks.keys()\n768         ['sphere', 'cube']\n769 \n770         \"\"\"\n771         # Taken from implementation in collections.abc.MutableSequence\n772         names = self.keys()\n773         n = len(self)\n774         for i in range(n // 2):\n775             self[i], self[n - i - 1] = self[n - i - 1], self[i]\n776         for i, name in enumerate(reversed(names)):\n777             self.set_block_name(i, name)\n778 \n779     def clean(self, empty=True):\n780         \"\"\"Remove any null blocks in place.\n781 \n782         Parameters\n783         ----------\n784         empty : bool, default: True\n785             Remove any meshes that are empty as well (have zero points).\n786 \n787         Examples\n788         --------\n789         >>> import pyvista as pv\n790         >>> data = {\"cube\": pv.Cube(), \"empty\": pv.PolyData()}\n791         >>> blocks = pv.MultiBlock(data)\n792         >>> blocks.clean(empty=True)\n793         >>> blocks.keys()\n794         ['cube']\n795 \n796         \"\"\"\n797         null_blocks = []\n798         for i in range(self.n_blocks):\n799             if isinstance(self[i], MultiBlock):\n800                 # Recursively move through nested structures\n801                 self[i].clean()\n802                 if self[i].n_blocks < 1:\n803                     null_blocks.append(i)\n804             elif self[i] is None:\n805                 null_blocks.append(i)\n806             elif empty and self[i].n_points < 1:\n807                 null_blocks.append(i)\n808         # Now remove the null/empty meshes\n809         null_blocks = np.array(null_blocks, dtype=int)\n810         for i in range(len(null_blocks)):\n811             # Cast as int because windows is super annoying\n812             del self[int(null_blocks[i])]\n813             null_blocks -= 1\n814 \n815     def _get_attrs(self):\n816         \"\"\"Return the representation methods (internal helper).\"\"\"\n817         attrs = []\n818         attrs.append((\"N Blocks\", self.n_blocks, \"{}\"))\n819         bds = self.bounds\n820         attrs.append((\"X Bounds\", (bds[0], bds[1]), \"{:.3f}, {:.3f}\"))\n821         attrs.append((\"Y Bounds\", (bds[2], bds[3]), \"{:.3f}, {:.3f}\"))\n822         attrs.append((\"Z Bounds\", (bds[4], bds[5]), \"{:.3f}, {:.3f}\"))\n823         return attrs\n824 \n825     def _repr_html_(self) -> str:\n826         \"\"\"Define a pretty representation for Jupyter notebooks.\"\"\"\n827         fmt = \"\"\n828         fmt += \"<table>\"\n829         fmt += \"<tr><th>Information</th><th>Blocks</th></tr>\"\n830         fmt += \"<tr><td>\"\n831         fmt += \"\\n\"\n832         fmt += \"<table>\\n\"\n833         fmt += f\"<tr><th>{type(self).__name__}</th><th>Values</th></tr>\\n\"\n834         row = \"<tr><td>{}</td><td>{}</td></tr>\\n\"\n835 \n836         # now make a call on the object to get its attributes as a list of len 2 tuples\n837         for attr in self._get_attrs():\n838             try:\n839                 fmt += row.format(attr[0], attr[2].format(*attr[1]))\n840             except:\n841                 fmt += row.format(attr[0], attr[2].format(attr[1]))\n842 \n843         fmt += \"</table>\\n\"\n844         fmt += \"\\n\"\n845         fmt += \"</td><td>\"\n846         fmt += \"\\n\"\n847         fmt += \"<table>\\n\"\n848         row = \"<tr><th>{}</th><th>{}</th><th>{}</th></tr>\\n\"\n849         fmt += row.format(\"Index\", \"Name\", \"Type\")\n850 \n851         for i in range(self.n_blocks):\n852             data = self[i]\n853             fmt += row.format(i, self.get_block_name(i), type(data).__name__)\n854 \n855         fmt += \"</table>\\n\"\n856         fmt += \"\\n\"\n857         fmt += \"</td></tr> </table>\"\n858         return fmt\n859 \n860     def __repr__(self) -> str:\n861         \"\"\"Define an adequate representation.\"\"\"\n862         # return a string that is Python console friendly\n863         fmt = f\"{type(self).__name__} ({hex(id(self))})\\n\"\n864         # now make a call on the object to get its attributes as a list of len 2 tuples\n865         row = \"  {}:\\t{}\\n\"\n866         for attr in self._get_attrs():\n867             try:\n868                 fmt += row.format(attr[0], attr[2].format(*attr[1]))\n869             except:\n870                 fmt += row.format(attr[0], attr[2].format(attr[1]))\n871         return fmt\n872 \n873     def __str__(self) -> str:\n874         \"\"\"Return the str representation of the multi block.\"\"\"\n875         return MultiBlock.__repr__(self)\n876 \n877     def __len__(self) -> int:\n878         \"\"\"Return the number of blocks.\"\"\"\n879         return self.n_blocks\n880 \n881     def copy_meta_from(self, ido, deep):\n882         \"\"\"Copy pyvista meta data onto this object from another object.\"\"\"\n883         # Note that `pyvista.MultiBlock` datasets currently don't have any meta.\n884         # This method is here for consistency with the rest of the API and\n885         # in case we add meta data to this pbject down the road.\n886         pass\n887 \n888     def copy(self, deep=True):\n889         \"\"\"Return a copy of the multiblock.\n890 \n891         Parameters\n892         ----------\n893         deep : bool, optional\n894             When ``True``, make a full copy of the object.\n895 \n896         Returns\n897         -------\n898         pyvista.MultiBlock\n899            Deep or shallow copy of the ``MultiBlock``.\n900 \n901         Examples\n902         --------\n903         >>> import pyvista as pv\n904         >>> data = [pv.Sphere(center=(2, 0, 0)), pv.Cube(center=(0, 2, 0)), pv.Cone()]\n905         >>> blocks = pv.MultiBlock(data)\n906         >>> new_blocks = blocks.copy()\n907         >>> len(new_blocks)\n908         3\n909 \n910         \"\"\"\n911         thistype = type(self)\n912         newobject = thistype()\n913         if deep:\n914             newobject.deep_copy(self)\n915         else:\n916             newobject.shallow_copy(self)\n917         newobject.copy_meta_from(self, deep)\n918         newobject.wrap_nested()\n919         return newobject\n920 \n921     def set_active_scalars(\n922         self, name: Optional[str], preference: str = 'cell', allow_missing: bool = False\n923     ) -> Tuple[FieldAssociation, np.ndarray]:  # type: ignore\n924         \"\"\"Find the scalars by name and appropriately set it as active.\n925 \n926         To deactivate any active scalars, pass ``None`` as the ``name``.\n927 \n928         Parameters\n929         ----------\n930         name : str or None\n931             Name of the scalars array to assign as active.  If\n932             ``None``, deactivates active scalars for both point and\n933             cell data.\n934 \n935         preference : str, optional\n936             If there are two arrays of the same name associated with\n937             points or cells, it will prioritize an array matching this\n938             type.  Can be either ``'cell'`` or ``'point'``.\n939 \n940         allow_missing : bool, optional\n941             Allow missing scalars in part of the composite dataset. If all\n942             blocks are missing the array, it will raise a ``KeyError``.\n943 \n944         Returns\n945         -------\n946         pyvista.FieldAssociation\n947             Field association of the scalars activated.\n948 \n949         numpy.ndarray\n950             An array from the dataset matching ``name``.\n951 \n952         Notes\n953         -----\n954         The number of components of the data must match.\n955 \n956         \"\"\"\n957         data_assoc: List[Tuple[FieldAssociation, np.ndarray, _TypeMultiBlockLeaf]] = []\n958         for block in self:\n959             if block is not None:\n960                 if isinstance(block, MultiBlock):\n961                     field, scalars = block.set_active_scalars(\n962                         name, preference, allow_missing=allow_missing\n963                     )\n964                 else:\n965                     try:\n966                         field, scalars = block.set_active_scalars(name, preference)\n967                     except KeyError as err:\n968                         if not allow_missing:\n969                             raise err\n970                         block.set_active_scalars(None, preference)\n971                         field, scalars = FieldAssociation.NONE, pyvista_ndarray([])\n972 \n973                 if field != FieldAssociation.NONE:\n974                     data_assoc.append((field, scalars, block))\n975 \n976         if name is None:\n977             return FieldAssociation.NONE, pyvista_ndarray([])\n978 \n979         if not data_assoc:\n980             raise KeyError(f'\"{name}\" is missing from all the blocks of this composite dataset.')\n981 \n982         field_asc = data_assoc[0][0]\n983         # set the field association to the preference if at least one occurrence\n984         # of it exists\n985         if field_asc.name.lower() != preference.lower():\n986             for field, _, _ in data_assoc:\n987                 if field.name.lower() == preference:\n988                     field_asc = getattr(FieldAssociation, preference.upper())\n989                     break\n990 \n991         # Verify array consistency\n992         dims: Set[int] = set()\n993         dtypes: Set[np.dtype] = set()\n994         for block in self:\n995             for field, scalars, _ in data_assoc:\n996                 # only check for the active field association\n997                 if field != field_asc:\n998                     continue\n999                 dims.add(scalars.ndim)\n1000                 dtypes.add(scalars.dtype)\n1001 \n1002         if len(dims) > 1:\n1003             raise ValueError(f'Inconsistent dimensions {dims} in active scalars.')\n1004 \n1005         # check complex mismatch\n1006         is_complex = [np.issubdtype(dtype, np.complexfloating) for dtype in dtypes]\n1007         if any(is_complex) and not all(is_complex):\n1008             raise ValueError('Inconsistent complex and real data types in active scalars.')\n1009 \n1010         return field_asc, scalars\n1011 \n1012     def as_polydata_blocks(self, copy=False):\n1013         \"\"\"Convert all the datasets within this MultiBlock to :class:`pyvista.PolyData`.\n1014 \n1015         Parameters\n1016         ----------\n1017         copy : bool, optional\n1018             Option to create a shallow copy of any datasets that are already a\n1019             :class:`pyvista.PolyData`. When ``False``, any datasets that are\n1020             already PolyData will not be copied.\n1021 \n1022         Returns\n1023         -------\n1024         pyvista.MultiBlock\n1025             MultiBlock containing only :class:`pyvista.PolyData` datasets.\n1026 \n1027         Notes\n1028         -----\n1029         Null blocks are converted to empty :class:`pyvista.PolyData`\n1030         objects. Downstream filters that operate on PolyData cannot accept\n1031         MultiBlocks with null blocks.\n1032 \n1033         \"\"\"\n1034         # we make a shallow copy here to avoid modifying the original dataset\n1035         dataset = self.copy(deep=False)\n1036 \n1037         # Loop through the multiblock and convert to polydata\n1038         for i, block in enumerate(dataset):\n1039             if block is not None:\n1040                 if isinstance(block, MultiBlock):\n1041                     dataset.replace(i, block.as_polydata_blocks(copy=copy))\n1042                 elif not isinstance(block, pyvista.PolyData):\n1043                     dataset.replace(i, block.extract_surface())\n1044                 elif copy:\n1045                     # dataset is a PolyData\n1046                     dataset.replace(i, block.copy(deep=False))\n1047             else:\n1048                 # must have empty polydata within these datasets as some\n1049                 # downstream filters don't work on null pointers (i.e. None)\n1050                 dataset[i] = pyvista.PolyData()\n1051 \n1052         return dataset\n1053 \n1054     @property\n1055     def is_all_polydata(self) -> bool:\n1056         \"\"\"Return ``True`` when all the blocks are :class:`pyvista.PolyData`.\n1057 \n1058         This method will recursively check if any internal blocks are also\n1059         :class:`pyvista.PolyData`.\n1060 \n1061         Returns\n1062         -------\n1063         bool\n1064             Return ``True`` when all blocks are :class:`pyvista.PolyData`.\n1065 \n1066         \"\"\"\n1067         for block in self:\n1068             if isinstance(block, MultiBlock):\n1069                 if not block.is_all_polydata:\n1070                     return False\n1071             else:\n1072                 if not isinstance(block, pyvista.PolyData):\n1073                     return False\n1074 \n1075         return True\n1076 \n1077     def _activate_plotting_scalars(self, scalars_name, preference, component, rgb):\n1078         \"\"\"Active a scalars for an instance of :class:`pyvista.Plotter`.\"\"\"\n1079         # set the active scalars\n1080         field, scalars = self.set_active_scalars(\n1081             scalars_name,\n1082             preference,\n1083             allow_missing=True,\n1084         )\n1085 \n1086         data_attr = f'{field.name.lower()}_data'\n1087         dtype = scalars.dtype\n1088         if rgb:\n1089             if scalars.ndim != 2 or scalars.shape[1] not in (3, 4):\n1090                 raise ValueError('RGB array must be n_points/n_cells by 3/4 in shape.')\n1091         elif np.issubdtype(scalars.dtype, np.complexfloating):\n1092             # Use only the real component if an array is complex\n1093             scalars_name = self._convert_to_real_scalars(data_attr, scalars_name)\n1094         elif scalars.dtype in (np.bool_, np.uint8):\n1095             # bool and uint8 do not display properly, must convert to float\n1096             self._convert_to_real_scalars(data_attr, scalars_name)\n1097             if scalars.dtype == np.bool_:\n1098                 dtype = np.bool_\n1099         elif scalars.ndim > 1:\n1100             # multi-component\n1101             if not isinstance(component, (int, type(None))):\n1102                 raise TypeError('`component` must be either None or an integer')\n1103             if component is not None:\n1104                 if component >= scalars.shape[1] or component < 0:\n1105                     raise ValueError(\n1106                         'Component must be nonnegative and less than the '\n1107                         f'dimensionality of the scalars array: {scalars.shape[1]}'\n1108                     )\n1109             scalars_name = self._convert_to_single_component(data_attr, scalars_name, component)\n1110 \n1111         return field, scalars_name, dtype\n1112 \n1113     def _convert_to_real_scalars(self, data_attr: str, scalars_name: str):\n1114         \"\"\"Extract the real component of the active scalars of this dataset.\"\"\"\n1115         for block in self:\n1116             if isinstance(block, MultiBlock):\n1117                 block._convert_to_real_scalars(data_attr, scalars_name)\n1118             elif block is not None:\n1119                 scalars = getattr(block, data_attr).get(scalars_name, None)\n1120                 if scalars is not None:\n1121                     scalars = np.array(scalars.astype(float))\n1122                     dattr = getattr(block, data_attr)\n1123                     dattr[f'{scalars_name}-real'] = scalars\n1124                     dattr.active_scalars_name = f'{scalars_name}-real'\n1125         return f'{scalars_name}-real'\n1126 \n1127     def _convert_to_single_component(\n1128         self, data_attr: str, scalars_name: str, component: Union[None, str]\n1129     ) -> str:\n1130         \"\"\"Convert multi-component scalars to a single component.\"\"\"\n1131         if component is None:\n1132             for block in self:\n1133                 if isinstance(block, MultiBlock):\n1134                     block._convert_to_single_component(data_attr, scalars_name, component)\n1135                 elif block is not None:\n1136                     scalars = getattr(block, data_attr).get(scalars_name, None)\n1137                     if scalars is not None:\n1138                         scalars = np.linalg.norm(scalars, axis=1)\n1139                         dattr = getattr(block, data_attr)\n1140                         dattr[f'{scalars_name}-normed'] = scalars\n1141                         dattr.active_scalars_name = f'{scalars_name}-normed'\n1142             return f'{scalars_name}-normed'\n1143 \n1144         for block in self:\n1145             if isinstance(block, MultiBlock):\n1146                 block._convert_to_single_component(data_attr, scalars_name, component)\n1147             elif block is not None:\n1148                 scalars = getattr(block, data_attr).get(scalars_name, None)\n1149                 if scalars is not None:\n1150                     dattr = getattr(block, data_attr)\n1151                     dattr[f'{scalars_name}-{component}'] = scalars[:, component]\n1152                     dattr.active_scalars_name = f'{scalars_name}-{component}'\n1153         return f'{scalars_name}-{component}'\n1154 \n1155     def _get_consistent_active_scalars(self):\n1156         \"\"\"Check if there are any consistent active scalars.\"\"\"\n1157         point_names = set()\n1158         cell_names = set()\n1159         for block in self:\n1160             if isinstance(block, MultiBlock):\n1161                 point_name, cell_name = block._get_consistent_active_scalars()\n1162             elif block is not None:\n1163                 point_name = block.point_data.active_scalars_name\n1164                 cell_name = block.cell_data.active_scalars_name\n1165             point_names.add(point_name)\n1166             cell_names.add(cell_name)\n1167 \n1168         point_name = point_names.pop() if len(point_names) == 1 else None\n1169         cell_name = cell_names.pop() if len(cell_names) == 1 else None\n1170         return point_name, cell_name\n1171 \n[end of pyvista/core/composite.py]\n[start of pyvista/core/filters/composite.py]\n1 \"\"\"Filters module with a class to manage filters/algorithms for composite datasets.\"\"\"\n2 \n3 import numpy as np\n4 \n5 import pyvista\n6 from pyvista import _vtk, abstract_class, wrap\n7 from pyvista.core.filters import _get_output, _update_alg\n8 from pyvista.core.filters.data_set import DataSetFilters\n9 \n10 \n11 @abstract_class\n12 class CompositeFilters:\n13     \"\"\"An internal class to manage filters/algorithms for composite datasets.\"\"\"\n14 \n15     def extract_geometry(self):\n16         \"\"\"Extract the surface the geometry of all blocks.\n17 \n18         Place this filter at the end of a pipeline before a polydata\n19         consumer such as a polydata mapper to extract geometry from\n20         all blocks and append them to one polydata object.\n21 \n22         Returns\n23         -------\n24         pyvista.PolyData\n25             Surface of the composite dataset.\n26 \n27         \"\"\"\n28         gf = _vtk.vtkCompositeDataGeometryFilter()\n29         gf.SetInputData(self)\n30         gf.Update()\n31         return wrap(gf.GetOutputDataObject(0))\n32 \n33     def combine(self, merge_points=False, tolerance=0.0):\n34         \"\"\"Combine all blocks into a single unstructured grid.\n35 \n36         Parameters\n37         ----------\n38         merge_points : bool, default: False\n39             Merge coincidental points.\n40 \n41         tolerance : float, default: 0.0\n42             The absolute tolerance to use to find coincident points when\n43             ``merge_points=True``. Note, this was added in VTK v9.\n44 \n45         Returns\n46         -------\n47         pyvista.UnstructuredGrid\n48             Combined blocks.\n49 \n50         Examples\n51         --------\n52         Combine blocks within a multiblock without merging points.\n53 \n54         >>> import pyvista\n55         >>> block = pyvista.MultiBlock([\n56         ...     pyvista.Cube(clean=False),\n57         ...     pyvista.Cube(center=(1, 0, 0), clean=False)\n58         ... ])\n59         >>> merged = block.combine()\n60         >>> merged.n_points\n61         48\n62 \n63         Combine blocks and merge points\n64 \n65         >>> merged = block.combine(merge_points=True)\n66         >>> merged.n_points\n67         12\n68 \n69         \"\"\"\n70         alg = _vtk.vtkAppendFilter()\n71         for block in self:\n72             if isinstance(block, _vtk.vtkMultiBlockDataSet):\n73                 block = CompositeFilters.combine(\n74                     block, merge_points=merge_points, tolerance=tolerance\n75                 )\n76             alg.AddInputData(block)\n77         alg.SetMergePoints(merge_points)\n78         if pyvista.vtk_version_info >= (9,):\n79             alg.SetTolerance(tolerance)\n80         alg.Update()\n81         return wrap(alg.GetOutputDataObject(0))\n82 \n83     clip = DataSetFilters.clip\n84 \n85     clip_box = DataSetFilters.clip_box\n86 \n87     slice = DataSetFilters.slice\n88 \n89     slice_orthogonal = DataSetFilters.slice_orthogonal\n90 \n91     slice_along_axis = DataSetFilters.slice_along_axis\n92 \n93     slice_along_line = DataSetFilters.slice_along_line\n94 \n95     extract_all_edges = DataSetFilters.extract_all_edges\n96 \n97     elevation = DataSetFilters.elevation\n98 \n99     compute_cell_sizes = DataSetFilters.compute_cell_sizes\n100 \n101     cell_centers = DataSetFilters.cell_centers\n102 \n103     cell_data_to_point_data = DataSetFilters.cell_data_to_point_data\n104 \n105     point_data_to_cell_data = DataSetFilters.point_data_to_cell_data\n106 \n107     triangulate = DataSetFilters.triangulate\n108 \n109     def outline(self, generate_faces=False, nested=False, progress_bar=False):\n110         \"\"\"Produce an outline of the full extent for the all blocks in this composite dataset.\n111 \n112         Parameters\n113         ----------\n114         generate_faces : bool, default: False\n115             Generate solid faces for the box.\n116 \n117         nested : bool, default: False\n118             If ``True``, these creates individual outlines for each nested dataset.\n119 \n120         progress_bar : bool, default: False\n121             Display a progress bar to indicate progress.\n122 \n123         Returns\n124         -------\n125         pyvista.PolyData\n126             Mesh containing the outline.\n127 \n128         \"\"\"\n129         if nested:\n130             return DataSetFilters.outline(\n131                 self, generate_faces=generate_faces, progress_bar=progress_bar\n132             )\n133         box = pyvista.Box(bounds=self.bounds)\n134         return box.outline(generate_faces=generate_faces, progress_bar=progress_bar)\n135 \n136     def outline_corners(self, factor=0.2, nested=False, progress_bar=False):\n137         \"\"\"Produce an outline of the corners for the all blocks in this composite dataset.\n138 \n139         Parameters\n140         ----------\n141         factor : float, default: 0.2\n142             Controls the relative size of the corners to the length of\n143             the corresponding bounds.\n144 \n145         nested : bool, default: False\n146             If ``True``, these creates individual outlines for each nested dataset.\n147 \n148         progress_bar : bool, default: False\n149             Display a progress bar to indicate progress.\n150 \n151         Returns\n152         -------\n153         pyvista.PolyData\n154             Mesh containing outlined corners.\n155 \n156         \"\"\"\n157         if nested:\n158             return DataSetFilters.outline_corners(self, factor=factor, progress_bar=progress_bar)\n159         box = pyvista.Box(bounds=self.bounds)\n160         return box.outline_corners(factor=factor, progress_bar=progress_bar)\n161 \n162     def _compute_normals(\n163         self,\n164         cell_normals=True,\n165         point_normals=True,\n166         split_vertices=False,\n167         flip_normals=False,\n168         consistent_normals=True,\n169         auto_orient_normals=False,\n170         non_manifold_traversal=True,\n171         feature_angle=30.0,\n172         track_vertices=False,\n173         progress_bar=False,\n174     ):\n175         \"\"\"Compute point and/or cell normals for a multi-block dataset.\"\"\"\n176         if not self.is_all_polydata:\n177             raise RuntimeError(\n178                 'This multiblock contains non-PolyData datasets. Convert all the '\n179                 'datasets to PolyData with `as_polydata`'\n180             )\n181 \n182         # track original point indices\n183         if split_vertices and track_vertices:\n184             for block in self:\n185                 ids = np.arange(block.n_points, dtype=pyvista.ID_TYPE)\n186                 block.point_data.set_array(ids, 'pyvistaOriginalPointIds')\n187 \n188         alg = _vtk.vtkPolyDataNormals()\n189         alg.SetComputeCellNormals(cell_normals)\n190         alg.SetComputePointNormals(point_normals)\n191         alg.SetSplitting(split_vertices)\n192         alg.SetFlipNormals(flip_normals)\n193         alg.SetConsistency(consistent_normals)\n194         alg.SetAutoOrientNormals(auto_orient_normals)\n195         alg.SetNonManifoldTraversal(non_manifold_traversal)\n196         alg.SetFeatureAngle(feature_angle)\n197         alg.SetInputData(self)\n198         _update_alg(alg, progress_bar, 'Computing Normals')\n199         return _get_output(alg)\n200 \n[end of pyvista/core/filters/composite.py]\n[start of pyvista/plotting/composite_mapper.py]\n1 \"\"\"Module containing composite data mapper.\"\"\"\n2 from itertools import cycle\n3 import sys\n4 from typing import Optional\n5 import warnings\n6 import weakref\n7 \n8 import numpy as np\n9 \n10 import pyvista as pv\n11 from pyvista import _vtk\n12 from pyvista.utilities import convert_array, convert_string_array\n13 \n14 from ..utilities.misc import has_module, vtk_version_info\n15 from .colors import Color\n16 from .mapper import _BaseMapper\n17 \n18 \n19 class BlockAttributes:\n20     \"\"\"Block attributes used to set the attributes of a block.\n21 \n22     Parameters\n23     ----------\n24     block : pyvista.DataObject\n25         PyVista data object.\n26 \n27     attr : pyvista.plotting.composite_mapper.CompositeAttributes\n28         Parent attributes.\n29 \n30     Notes\n31     -----\n32     This class employs VTK's flat indexing and allows for accessing both\n33     the blocks of a composite dataset as well as the entire composite\n34     dataset. If there is only one composite dataset, ``A``, which contains\n35     datasets ``[b, c]``, the indexing would be ``[A, b, c]``.\n36 \n37     If there are two composite datasets ``[B, C]`` in one composite\n38     dataset, ``A``, each of which containing three additional datasets\n39     ``[d, e, f]``, and ``[g, h, i]``, respectively, then the head node,\n40     ``A``, would be the zero index, followed by the first child, ``B``,\n41     followed by all the children of ``B``, ``[d, e, f]``. In data\n42     structures, this flat indexing would be known as \"Depth-first search\"\n43     and the entire indexing would be::\n44 \n45        [A, B, d, e, f, C, g, h, i]\n46 \n47     Note how the composite datasets themselves are capitalized and are\n48     accessible in the flat indexing, and not just the datasets.\n49 \n50     Examples\n51     --------\n52     Add a sphere and a cube as a multiblock dataset to a plotter and then\n53     change the visibility and color of the blocks. Note how the index of the\n54     cube is ``1`` as the index of the entire multiblock is ``0``.\n55 \n56     >>> import pyvista as pv\n57     >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n58     >>> pl = pv.Plotter()\n59     >>> actor, mapper = pl.add_composite(dataset)\n60     >>> mapper.block_attr[1].color = 'b'\n61     >>> mapper.block_attr[1].opacity = 0.1\n62     >>> mapper.block_attr[1]\n63     Composite Block Addr=... Attributes\n64     Visible:   None\n65     Opacity:   0.1\n66     Color:     Color(name='blue', hex='#0000ffff', opacity=255)\n67     Pickable   None\n68 \n69     \"\"\"\n70 \n71     def __init__(self, block, attr):\n72         \"\"\"Initialize the block attributes class.\"\"\"\n73         self._block = block\n74         self.__attr = weakref.ref(attr)\n75 \n76     @property\n77     def _attr(self):\n78         \"\"\"Return the CompositeAttributes.\"\"\"\n79         return self.__attr()\n80 \n81     @property\n82     def _has_color(self):\n83         \"\"\"Return if a block has its color set.\"\"\"\n84         return self._attr.HasBlockColor(self._block)\n85 \n86     @property\n87     def _has_visibility(self):\n88         \"\"\"Return if a block has its visibility set.\"\"\"\n89         return self._attr.HasBlockVisibility(self._block)\n90 \n91     @property\n92     def _has_opacity(self):\n93         \"\"\"Return if a block has its opacity set.\"\"\"\n94         return self._attr.HasBlockOpacity(self._block)\n95 \n96     @property\n97     def _has_pickable(self):\n98         \"\"\"Return if a block has its pickability set.\"\"\"\n99         return self._attr.HasBlockPickability(self._block)\n100 \n101     @property\n102     def color(self):\n103         \"\"\"Get or set the color of a block.\n104 \n105         Examples\n106         --------\n107         Set the colors of a composite dataset to red and blue.\n108 \n109         Note how the zero index is the entire multiblock, so we have to add 1\n110         to our indexing to access the right block.\n111 \n112         >>> import pyvista as pv\n113         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n114         >>> pl = pv.Plotter()\n115         >>> actor, mapper = pl.add_composite(dataset)\n116         >>> mapper.block_attr[1].color = 'r'\n117         >>> mapper.block_attr[2].color = 'b'\n118         >>> pl.show()\n119 \n120         \"\"\"\n121         if not self._has_color:\n122             return None\n123         return Color(tuple(self._attr.GetBlockColor(self._block)))\n124 \n125     @color.setter\n126     def color(self, new_color):\n127         if new_color is None:\n128             self._attr.RemoveBlockColor(self._block)\n129             self._attr.Modified()\n130             return\n131         self._attr.SetBlockColor(self._block, Color(new_color).float_rgb)\n132 \n133     @property\n134     def visible(self) -> Optional[bool]:\n135         \"\"\"Get or set the visibility of a block.\n136 \n137         Examples\n138         --------\n139         Hide the first block of a composite dataset.\n140 \n141         Note how the zero index is the entire multiblock, so we have to add 1\n142         to our indexing to access the right block.\n143 \n144         >>> import pyvista as pv\n145         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n146         >>> pl = pv.Plotter()\n147         >>> actor, mapper = pl.add_composite(dataset)\n148         >>> mapper.block_attr[1].visible = False\n149         >>> pl.show()\n150 \n151         \"\"\"\n152         if not self._has_visibility:\n153             return None\n154         return self._attr.GetBlockVisibility(self._block)\n155 \n156     @visible.setter\n157     def visible(self, new_visible: bool):\n158         if new_visible is None:\n159             self._attr.RemoveBlockVisibility(self._block)\n160             self._attr.Modified()\n161             return\n162         self._attr.SetBlockVisibility(self._block, new_visible)\n163 \n164     @property\n165     def opacity(self) -> Optional[float]:\n166         \"\"\"Get or set the opacity of a block.\n167 \n168         If opacity has not been set this will be ``None``.\n169 \n170         Warnings\n171         --------\n172         VTK 9.0.3 has a bug where changing the opacity to less than 1.0 also\n173         changes the edge visibility on the block that is partially transparent.\n174 \n175         Examples\n176         --------\n177         Change the opacity of the second block of the dataset.\n178 \n179         Note how the zero index is the entire multiblock, so we have to add 1\n180         to our indexing to access the right block.\n181 \n182         >>> import pyvista as pv\n183         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n184         >>> pl = pv.Plotter()\n185         >>> actor, mapper = pl.add_composite(dataset)\n186         >>> mapper.block_attr[2].opacity = 0.5\n187         >>> pl.show()\n188 \n189         \"\"\"\n190         if not self._has_opacity:\n191             return None\n192         return self._attr.GetBlockOpacity(self._block)\n193 \n194     @opacity.setter\n195     def opacity(self, new_opacity: float):\n196         \"\"\"Get or set the visibility of a block.\"\"\"\n197         if new_opacity is None:\n198             self._attr.RemoveBlockOpacity(self._block)\n199             self._attr.Modified()\n200             return\n201 \n202         self._attr.SetBlockOpacity(self._block, new_opacity)\n203 \n204     @property\n205     def pickable(self) -> Optional[bool]:\n206         \"\"\"Get or set the pickability of a block.\n207 \n208         Examples\n209         --------\n210         Make the cube of a multiblock dataset pickable and the sphere unpickable.\n211 \n212         Note how the zero index is the entire multiblock, so we have to add 1\n213         to our indexing to access the right block.\n214 \n215         >>> import pyvista as pv\n216         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n217         >>> pl = pv.Plotter()\n218         >>> actor, mapper = pl.add_composite(dataset)\n219         >>> mapper.block_attr[1].pickable = True\n220         >>> mapper.block_attr[2].pickable = False\n221         >>> pl.close()\n222 \n223         See :ref:`composite_picking_example` for a full example using block\n224         picking.\n225 \n226         \"\"\"\n227         if not self._has_pickable:\n228             return None\n229         return self._attr.GetBlockPickability(self._block)\n230 \n231     @pickable.setter\n232     def pickable(self, new_pickable: bool):\n233         if new_pickable is None:\n234             self._attr.RemoveBlockPickability(self._block)\n235             self._attr.Modified()\n236             return\n237         self._attr.SetBlockPickability(self._block, new_pickable)\n238 \n239     def __repr__(self):\n240         \"\"\"Representation of block properties.\"\"\"\n241         return '\\n'.join(\n242             [\n243                 f'Composite Block {self._block.memory_address} Attributes',\n244                 f'Visible:   {self.visible}',\n245                 f'Opacity:   {self.opacity}',\n246                 f'Color:     {self.color}',\n247                 f'Pickable   {self.pickable}',\n248             ]\n249         )\n250 \n251 \n252 class CompositeAttributes(_vtk.vtkCompositeDataDisplayAttributes):\n253     \"\"\"Block attributes.\n254 \n255     Parameters\n256     ----------\n257     mapper : pyvista.plotting.composite_mapper.CompositePolyDataMapper\n258         Parent mapper.\n259 \n260     dataset : pyvista.MultiBlock\n261         Multiblock dataset.\n262 \n263     Notes\n264     -----\n265     This class employs VTK's flat indexing and allows for accessing both\n266     the blocks of a composite dataset as well as the entire composite\n267     dataset. If there is only one composite dataset, ``A``, which contains\n268     datasets ``[b, c]``, the indexing would be ``[A, b, c]``.\n269 \n270     If there are two composite datasets ``[B, C]`` in one composite\n271     dataset, ``A``, each of which containing three additional datasets\n272     ``[d, e, f]``, and ``[g, h, i]``, respectively, then the head node,\n273     ``A``, would be the zero index, followed by the first child, ``B``,\n274     followed by all the children of ``B``, ``[d, e, f]``. In data\n275     structures, this flat indexing would be known as \"Depth-first search\"\n276     and the entire indexing would be::\n277 \n278        [A, B, d, e, f, C, g, h, i]\n279 \n280     Note how the composite datasets themselves are capitalized and are\n281     accessible in the flat indexing, and not just the datasets.\n282 \n283     Examples\n284     --------\n285     Add a sphere and a cube as a multiblock dataset to a plotter and then\n286     change the visibility and color of the blocks. Note how the index of the\n287     cube is ``1`` as the index of the entire multiblock is ``0``.\n288 \n289     >>> import pyvista as pv\n290     >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n291     >>> pl = pv.Plotter()\n292     >>> actor, mapper = pl.add_composite(dataset)\n293     >>> mapper.block_attr[1].color = 'b'\n294     >>> mapper.block_attr[1].opacity = 0.1\n295     >>> mapper.block_attr[1]\n296     Composite Block Addr=... Attributes\n297     Visible:   None\n298     Opacity:   0.1\n299     Color:     Color(name='blue', hex='#0000ffff', opacity=255)\n300     Pickable   None\n301 \n302     \"\"\"\n303 \n304     def __init__(self, mapper, dataset):\n305         \"\"\"Initialize CompositeAttributes.\"\"\"\n306         super().__init__()\n307         mapper.SetCompositeDataDisplayAttributes(self)\n308         self._dataset = dataset\n309 \n310     def reset_visibilities(self):\n311         \"\"\"Reset the visibility of all blocks.\n312 \n313         Examples\n314         --------\n315         Hide the first block of a composite dataset and then show all by\n316         resetting visibilities.\n317 \n318         Note how the zero index is the entire multiblock, so we have to add 1\n319         to our indexing to access the right block.\n320 \n321         >>> import pyvista as pv\n322         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n323         >>> pl = pv.Plotter()\n324         >>> actor, mapper = pl.add_composite(dataset)\n325         >>> mapper.block_attr[1].visible = False\n326         >>> mapper.block_attr.reset_visibilities()\n327         >>> pl.show()\n328 \n329         \"\"\"\n330         self.RemoveBlockVisibilities()\n331 \n332     def reset_pickabilities(self):\n333         \"\"\"Reset the pickability of all blocks.\n334 \n335         Examples\n336         --------\n337         Make the cube of a multiblock dataset pickable and the sphere\n338         unpickable, then reset it.\n339 \n340         Note how the zero index is the entire multiblock, so we have to add 1\n341         to our indexing to access the right block.\n342 \n343         >>> import pyvista as pv\n344         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n345         >>> pl = pv.Plotter()\n346         >>> actor, mapper = pl.add_composite(dataset)\n347         >>> mapper.block_attr[1].pickable = True\n348         >>> mapper.block_attr[2].pickable = False\n349         >>> mapper.block_attr.reset_pickabilities()\n350         >>> [mapper.block_attr[1].pickable, mapper.block_attr[2].pickable]\n351         [None, None]\n352         >>> pl.close()\n353 \n354         \"\"\"\n355         self.RemoveBlockPickabilities()\n356 \n357     def reset_colors(self):\n358         \"\"\"Reset the color of all blocks.\n359 \n360         Examples\n361         --------\n362         Set individual block colors and then reset them.\n363 \n364         >>> import pyvista as pv\n365         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n366         >>> pl = pv.Plotter()\n367         >>> actor, mapper = pl.add_composite(dataset, color='w')\n368         >>> mapper.block_attr[1].color = 'r'\n369         >>> mapper.block_attr[2].color = 'b'\n370         >>> mapper.block_attr.reset_colors()\n371         >>> pl.show()\n372 \n373         \"\"\"\n374         self.RemoveBlockColors()\n375 \n376     def reset_opacities(self):\n377         \"\"\"Reset the opacities of all blocks.\n378 \n379         Examples\n380         --------\n381         Change the opacity of the second block of the dataset then reset all\n382         opacities.\n383 \n384         Note how the zero index is the entire multiblock, so we have to add 1\n385         to our indexing to access the right block.\n386 \n387         >>> import pyvista as pv\n388         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n389         >>> pl = pv.Plotter()\n390         >>> actor, mapper = pl.add_composite(dataset)\n391         >>> mapper.block_attr[2].opacity = 0.5\n392         >>> mapper.block_attr.reset_opacities()\n393         >>> pl.show()\n394 \n395         \"\"\"\n396         self.RemoveBlockOpacities()\n397 \n398     def get_block(self, index):\n399         \"\"\"Return a block by its flat index.\n400 \n401         Parameters\n402         ----------\n403         index : int\n404             Flat index of the block to retrieve.\n405 \n406         Returns\n407         -------\n408         pyvista.DataObject\n409             PyVista data object.\n410 \n411         Notes\n412         -----\n413         This method employs VTK's flat indexing and allows for accessing both\n414         the blocks of a composite dataset as well as the entire composite\n415         dataset. If there is only one composite dataset, ``A``, which contains\n416         datasets ``[b, c]``, the indexing would be ``[A, b, c]``.\n417 \n418         If there are two composite datasets ``[B, C]`` in one composite\n419         dataset, ``A``, each of which containing three additional datasets\n420         ``[d, e, f]``, and ``[g, h, i]``, respectively, then the head node,\n421         ``A``, would be the zero index, followed by the first child, ``B``,\n422         followed by all the children of ``B``, ``[d, e, f]``. In data\n423         structures, this flat indexing would be known as \"Depth-first search\"\n424         and the entire indexing would be::\n425 \n426            [A, B, d, e, f, C, g, h, i]\n427 \n428         Note how the composite datasets themselves are capitalized and are\n429         accessible in the flat indexing, and not just the datasets.\n430 \n431         Examples\n432         --------\n433         Add a composite dataset to a plotter and access its block attributes.\n434         Note how the zero index is the entire multiblock and you can use ``1``\n435         and ``2`` to access the individual sub-blocks.\n436 \n437         >>> import pyvista as pv\n438         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n439         >>> pl = pv.Plotter()\n440         >>> actor, mapper = pl.add_composite(dataset)\n441         >>> mapper.block_attr.get_block(0)  # doctest:+SKIP\n442         MultiBlock (...)\n443           N Blocks:\t2\n444           X Bounds:\t-0.500, 0.500\n445           Y Bounds:\t-0.500, 0.500\n446           Z Bounds:\t-0.500, 1.500\n447 \n448         Note this is the same as using ``__getitem__``\n449 \n450         >>> mapper.block_attr[0]  # doctest:+SKIP\n451         MultiBlock (...)\n452           N Blocks:\t2\n453           X Bounds:\t-0.500, 0.500\n454           Y Bounds:\t-0.500, 0.500\n455           Z Bounds:\t-0.500, 1.500\n456 \n457         \"\"\"\n458         try:\n459             if vtk_version_info <= (9, 0, 3):  # pragma: no cover\n460                 vtk_ref = _vtk.reference(0)  # needed for <=9.0.3\n461                 block = self.DataObjectFromIndex(index, self._dataset, vtk_ref)\n462             else:\n463                 block = self.DataObjectFromIndex(index, self._dataset)\n464         except OverflowError:\n465             raise KeyError(f'Invalid block key: {index}') from None\n466         if block is None:\n467             if index > len(self) - 1:\n468                 raise KeyError(\n469                     f'index {index} is out of bounds. There are only {len(self)} blocks.'\n470                 ) from None\n471         return block\n472 \n473     def __getitem__(self, index):\n474         \"\"\"Return a block attribute by its flat index.\"\"\"\n475         return BlockAttributes(self.get_block(index), self)\n476 \n477     def __len__(self):\n478         \"\"\"Return the number of blocks in this dataset.\"\"\"\n479         from pyvista import MultiBlock  # avoid circular\n480 \n481         # start with 1 as there is always a composite dataset and this is the\n482         # root of the tree\n483         cc = 1\n484         for dataset in self._dataset:\n485             if isinstance(dataset, MultiBlock):\n486                 cc += len(dataset) + 1  # include the block itself\n487             else:\n488                 cc += 1\n489         return cc\n490 \n491     def __iter__(self):\n492         \"\"\"Return an iterator of all the block attributes.\"\"\"\n493         for ii in range(len(self)):\n494             yield self[ii]\n495 \n496 \n497 class CompositePolyDataMapper(_vtk.vtkCompositePolyDataMapper2, _BaseMapper):\n498     \"\"\"Composite PolyData mapper.\n499 \n500     Parameters\n501     ----------\n502     dataset : pyvista.MultiBlock\n503         Multiblock dataset.\n504 \n505     theme : pyvista.themes.DefaultTheme, optional\n506         Plot-specific theme.\n507 \n508     color_missing_with_nan : bool, optional\n509         Color any missing values with the ``nan_color``. This is useful\n510         when not all blocks of the composite dataset have the specified\n511         ``scalars``.\n512 \n513     interpolate_before_map : bool, optional\n514         Enabling makes for a smoother scalars display.  Default is\n515         ``True``.  When ``False``, OpenGL will interpolate the\n516         mapped colors which can result is showing colors that are\n517         not present in the color map.\n518 \n519     \"\"\"\n520 \n521     def __init__(\n522         self, dataset, theme=None, color_missing_with_nan=None, interpolate_before_map=None\n523     ):\n524         \"\"\"Initialize this composite mapper.\"\"\"\n525         super().__init__(theme=theme)\n526         self.SetInputDataObject(dataset)\n527 \n528         # this must be added to set the color, opacity, and visibility of\n529         # individual blocks\n530         self._attr = CompositeAttributes(self, dataset)\n531         self._dataset = dataset\n532 \n533         if color_missing_with_nan is not None:\n534             self.color_missing_with_nan = color_missing_with_nan\n535         if interpolate_before_map is not None:\n536             self.interpolate_before_map = interpolate_before_map\n537 \n538     @property\n539     def dataset(self) -> 'pv.MultiBlock':\n540         \"\"\"Return the composite dataset assigned to this mapper.\n541 \n542         Examples\n543         --------\n544         >>> import pyvista as pv\n545         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n546         >>> pl = pv.Plotter()\n547         >>> actor, mapper = pl.add_composite(dataset)\n548         >>> mapper.dataset   # doctest:+SKIP\n549         MultiBlock (...)\n550           N Blocks:     2\n551           X Bounds:     -0.500, 0.500\n552           Y Bounds:     -0.500, 0.500\n553           Z Bounds:     -0.500, 1.500\n554 \n555         \"\"\"\n556         return self._dataset\n557 \n558     @property\n559     def block_attr(self) -> CompositeAttributes:\n560         \"\"\"Return the block attributes.\n561 \n562         Notes\n563         -----\n564         ``block_attr`` employs VTK's flat indexing and allows for accessing\n565         both the blocks of a composite dataset as well as the entire composite\n566         dataset. If there is only one composite dataset, ``A``, which contains\n567         datasets ``[b, c]``, the indexing would be ``[A, b, c]``.\n568 \n569         If there are two composite datasets ``[B, C]`` in one composite\n570         dataset, ``A``, each of which containing three additional datasets\n571         ``[d, e, f]``, and ``[g, h, i]``, respectively, then the head node,\n572         ``A``, would be the zero index, followed by the first child, ``B``,\n573         followed by all the children of ``B``, ``[d, e, f]``. In data\n574         structures, this flat indexing would be known as \"Depth-first search\"\n575         and the entire indexing would be::\n576 \n577            [A, B, d, e, f, C, g, h, i]\n578 \n579         Examples\n580         --------\n581         Add a sphere and a cube as a multiblock dataset to a plotter and then\n582         change the visibility and color of the blocks.\n583 \n584         >>> import pyvista as pv\n585         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n586         >>> pl = pv.Plotter()\n587         >>> actor, mapper = pl.add_composite(dataset)\n588         >>> mapper.block_attr[1].color = 'b'\n589         >>> mapper.block_attr[1].opacity = 0.1\n590         >>> mapper.block_attr[1]\n591         Composite Block Addr=... Attributes\n592         Visible:   None\n593         Opacity:   0.1\n594         Color:     Color(name='blue', hex='#0000ffff', opacity=255)\n595         Pickable   None\n596 \n597         \"\"\"\n598         return self._attr\n599 \n600     @property\n601     def color_missing_with_nan(self) -> bool:\n602         \"\"\"Color missing arrays with the NaN color.\n603 \n604         Examples\n605         --------\n606         Enable coloring missing values with NaN.\n607 \n608         >>> import pyvista as pv\n609         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n610         >>> dataset[0].point_data['data'] = dataset[0].points[:, 2]\n611         >>> pl = pv.Plotter()\n612         >>> actor, mapper = pl.add_composite(dataset, scalars='data', show_scalar_bar=False)\n613         >>> mapper.nan_color = 'r'\n614         >>> mapper.color_missing_with_nan = True\n615         >>> pl.show()\n616 \n617         \"\"\"\n618         return self.GetColorMissingArraysWithNanColor()\n619 \n620     @color_missing_with_nan.setter\n621     def color_missing_with_nan(self, value: bool):\n622         self.SetColorMissingArraysWithNanColor(value)\n623 \n624     def set_unique_colors(self):\n625         \"\"\"Set each block of the dataset to a unique color.\n626 \n627         This uses ``matplotlib``'s color cycler.\n628 \n629         Examples\n630         --------\n631         Set each block of the composite dataset to a unique color.\n632 \n633         >>> import pyvista as pv\n634         >>> dataset = pv.MultiBlock([pv.Cube(), pv.Sphere(center=(0, 0, 1))])\n635         >>> pl = pv.Plotter()\n636         >>> actor, mapper = pl.add_composite(dataset)\n637         >>> mapper.set_unique_colors()\n638         >>> mapper.block_attr[1].color\n639         Color(name='tab:orange', hex='#ff7f0eff', opacity=255)\n640         >>> mapper.block_attr[2].color\n641         Color(name='tab:green', hex='#2ca02cff', opacity=255)\n642         \"\"\"\n643         self.scalar_visibility = False\n644         if has_module('matplotlib'):\n645             import matplotlib\n646 \n647             colors = cycle(matplotlib.rcParams['axes.prop_cycle'])\n648             for attr in self.block_attr:\n649                 attr.color = next(colors)['color']\n650 \n651         else:  # pragma: no cover\n652             warnings.warn('Please install matplotlib for color cycles.')\n653 \n654     def set_scalars(\n655         self,\n656         scalars_name,\n657         preference,\n658         component,\n659         annotations,\n660         rgb,\n661         scalar_bar_args,\n662         n_colors,\n663         nan_color,\n664         above_color,\n665         below_color,\n666         clim,\n667         cmap,\n668         flip_scalars,\n669         categories,\n670         log_scale,\n671     ):\n672         \"\"\"Set the scalars of the mapper.\n673 \n674         Parameters\n675         ----------\n676         scalars_name : str\n677             Name of the scalars in the dataset. Must already exist in at least\n678             of the blocks.\n679 \n680         preference : str\n681             For each block, when ``block.n_points == block.n_cells`` and\n682             setting scalars, this parameter sets how the scalars will be mapped\n683             to the mesh.  Default ``'point'``, causes the scalars will be\n684             associated with the mesh points.  Can be either ``'point'`` or\n685             ``'cell'``.\n686 \n687         component : int\n688             Set component of vector valued scalars to plot.  Must be\n689             nonnegative, if supplied. If ``None``, the magnitude of\n690             the vector is plotted.\n691 \n692         annotations : dict\n693             Pass a dictionary of annotations. Keys are the float\n694             values in the scalars range to annotate on the scalar bar\n695             and the values are the the string annotations.\n696 \n697         rgb : bool\n698             If the ``scalars_name`` corresponds to a 2 dimensional array, plot\n699             those values as RGB(A) colors.\n700 \n701         scalar_bar_args : dict\n702             Dictionary of keyword arguments to pass when adding the\n703             scalar bar to the scene. For options, see\n704             :func:`pyvista.Plotter.add_scalar_bar`.\n705 \n706         n_colors : int\n707             Number of colors to use when displaying scalars.\n708 \n709         nan_color : color_like\n710             The color to use for all ``NaN`` values in the plotted\n711             scalar array.\n712 \n713         above_color : color_like\n714             Solid color for values below the scalars range\n715             (``clim``). This will automatically set the scalar bar\n716             ``above_label`` to ``'Above'``.\n717 \n718         below_color : color_like\n719             Solid color for values below the scalars range\n720             (``clim``). This will automatically set the scalar bar\n721             ``below_label`` to ``'Below'``.\n722 \n723         clim : 2 item list\n724             Color bar range for scalars.  Defaults to minimum and\n725             maximum of scalars array.  Example: ``[-1, 2]``. ``rng``\n726             is also an accepted alias for this.\n727 \n728         cmap : str, list, or pyvista.LookupTable\n729             Name of the Matplotlib colormap to use when mapping the\n730             ``scalars``.  See available Matplotlib colormaps.  Only applicable\n731             for when displaying ``scalars``. Requires Matplotlib to be\n732             installed.  ``colormap`` is also an accepted alias for this. If\n733             ``colorcet`` or ``cmocean`` are installed, their colormaps can be\n734             specified by name.\n735 \n736             You can also specify a list of colors to override an existing\n737             colormap with a custom one.  For example, to create a three color\n738             colormap you might specify ``['green', 'red', 'blue']``.\n739 \n740             This parameter also accepts a :class:`pyvista.LookupTable`. If this\n741             is set, all parameters controlling the color map like ``n_colors``\n742             will be ignored.\n743             are installed, their colormaps can be specified by name.\n744 \n745         flip_scalars : bool\n746             Flip direction of cmap. Most colormaps allow ``*_r``\n747             suffix to do this as well.\n748 \n749         categories : bool\n750             If set to ``True``, then the number of unique values in\n751             the scalar array will be used as the ``n_colors``\n752             argument.\n753 \n754         log_scale : bool\n755             Use log scale when mapping data to colors. Scalars less\n756             than zero are mapped to the smallest representable\n757             positive float.\n758 \n759         Returns\n760         -------\n761         dict\n762             Dictionary of scalar bar arguments.\n763 \n764         \"\"\"\n765         self._orig_scalars_name = scalars_name\n766 \n767         field, scalars_name, dtype = self._dataset._activate_plotting_scalars(\n768             scalars_name, preference, component, rgb\n769         )\n770 \n771         self.scalar_visibility = True\n772         if rgb:\n773             self.scalar_mode = 'direct'\n774             return scalar_bar_args\n775         else:\n776             self.scalar_map_mode = field.name.lower()\n777 \n778         scalar_bar_args.setdefault('title', scalars_name)\n779 \n780         if clim is None:\n781             clim = self._dataset.get_data_range(scalars_name, allow_missing=True)\n782         self.scalar_range = clim\n783 \n784         if log_scale:\n785             if clim[0] <= 0:\n786                 clim = [sys.float_info.min, clim[1]]\n787 \n788         if isinstance(cmap, pv.LookupTable):\n789             self.lookup_table = cmap\n790         else:\n791             if dtype == np.bool_:\n792                 cats = np.array([b'False', b'True'], dtype='|S5')\n793                 values = np.array([0, 1])\n794                 n_colors = 2\n795                 scalar_bar_args.setdefault('n_labels', 0)\n796                 self.lookup_table.SetAnnotations(convert_array(values), convert_string_array(cats))\n797                 clim = [-0.5, 1.5]\n798 \n799             self.lookup_table.log_scale = log_scale\n800 \n801             if isinstance(annotations, dict):\n802                 self.lookup_table.annotations = annotations\n803 \n804             # self.lookup_table.SetNumberOfTableValues(n_colors)\n805             if nan_color:\n806                 self.lookup_table.nan_color = nan_color\n807             if above_color:\n808                 self.lookup_table.above_range_color = above_color\n809                 scalar_bar_args.setdefault('above_label', 'Above')\n810             if below_color:\n811                 self.lookup_table.below_range_color = below_color\n812                 scalar_bar_args.setdefault('below_label', 'Below')\n813 \n814             if cmap is None:  # Set default map if matplotlib is available\n815                 if has_module('matplotlib'):\n816                     if self._theme is None:\n817                         cmap = pv.global_theme.cmap\n818                     else:\n819                         cmap = self._theme.cmap\n820 \n821             if cmap is not None:\n822                 self.lookup_table.apply_cmap(cmap, n_colors, flip_scalars)\n823             else:  # pragma: no cover\n824                 if flip_scalars:\n825                     self.lookup_table.SetHueRange(0.0, 0.66667)\n826                 else:\n827                     self.lookup_table.SetHueRange(0.66667, 0.0)\n828 \n829         return scalar_bar_args\n830 \n[end of pyvista/plotting/composite_mapper.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pyvista/core/composite.py\n+++ b/pyvista/core/composite.py\n@@ -165,5 +165,5 @@\n         return np.stack([minima, maxima]).ravel('F').tolist()\n \n-        # interleave minima and maxima for bounds\n+        # interleave minima and maxima for bounds and cast to tuple\n         return tuple(np.stack([minima, maxima]).ravel('F'))\n```", "model_patch": "--- a/pyvista/core/composite.py\n+++ b/pyvista/core/composite.py\n@@ -165,5 +165,5 @@\n         return np.stack([minima, maxima]).ravel('F').tolist()\n \n-        # interleave minima and maxima for bounds\n+        # interleave minima and maxima for bounds and cast to tuple\n         return tuple(np.stack([minima, maxima]).ravel('F'))\n"}
{"instance_id": "pyvista__pyvista-4315", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nRectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n\n</issue>\n<code>\n[start of README.rst]\n1 #######\n2 PyVista\n3 #######\n4 \n5 .. image:: https://github.com/pyvista/pyvista/raw/main/doc/source/_static/pyvista_banner_small.png\n6    :target: https://docs.pyvista.org/examples/index.html\n7    :alt: pyvista\n8 \n9 \n10 .. |zenodo| image:: https://zenodo.org/badge/92974124.svg\n11    :target: https://zenodo.org/badge/latestdoi/92974124\n12 \n13 .. |joss| image:: http://joss.theoj.org/papers/10.21105/joss.01450/status.svg\n14    :target: https://doi.org/10.21105/joss.01450\n15 \n16 .. |pypi| image:: https://img.shields.io/pypi/v/pyvista.svg?logo=python&logoColor=white\n17    :target: https://pypi.org/project/pyvista/\n18 \n19 .. |conda| image:: https://img.shields.io/conda/vn/conda-forge/pyvista.svg?logo=conda-forge&logoColor=white\n20    :target: https://anaconda.org/conda-forge/pyvista\n21 \n22 .. |GH-CI| image:: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml/badge.svg\n23    :target: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml\n24 \n25 .. |codecov| image:: https://codecov.io/gh/pyvista/pyvista/branch/main/graph/badge.svg\n26    :target: https://codecov.io/gh/pyvista/pyvista\n27 \n28 .. |codacy| image:: https://app.codacy.com/project/badge/Grade/779ac6aed37548839384acfc0c1aab44\n29    :target: https://www.codacy.com/gh/pyvista/pyvista/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=pyvista/pyvista&amp;utm_campaign=Badge_Grade\n30 \n31 .. |MIT| image:: https://img.shields.io/badge/License-MIT-yellow.svg\n32    :target: https://opensource.org/licenses/MIT\n33 \n34 .. |slack| image:: https://img.shields.io/badge/Slack-pyvista-green.svg?logo=slack\n35    :target: http://slack.pyvista.org\n36 \n37 .. |PyPIact| image:: https://img.shields.io/pypi/dm/pyvista.svg?label=PyPI%20downloads\n38    :target: https://pypi.org/project/pyvista/\n39 \n40 .. |condaact| image:: https://img.shields.io/conda/dn/conda-forge/pyvista.svg?label=Conda%20downloads\n41    :target: https://anaconda.org/conda-forge/pyvista\n42 \n43 .. |discuss| image:: https://img.shields.io/badge/GitHub-Discussions-green?logo=github\n44    :target: https://github.com/pyvista/pyvista/discussions\n45 \n46 .. |isort| image:: https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat\n47   :target: https://timothycrosley.github.io/isort\n48   :alt: isort\n49 \n50 .. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg?style=flat\n51   :target: https://github.com/psf/black\n52   :alt: black\n53 \n54 .. |python| image:: https://img.shields.io/badge/python-3.8+-blue.svg\n55    :target: https://www.python.org/downloads/\n56 \n57 .. |NumFOCUS Affiliated| image:: https://img.shields.io/badge/affiliated-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A\n58    :target: https://numfocus.org/sponsored-projects/affiliated-projects\n59 \n60 .. |pre-commit.ci status| image:: https://results.pre-commit.ci/badge/github/pyvista/pyvista/main.svg\n61    :target: https://results.pre-commit.ci/latest/github/pyvista/pyvista/main\n62 \n63 \n64 +----------------------+------------------------+-------------+\n65 | Deployment           | |pypi|                 | |conda|     |\n66 +----------------------+------------------------+-------------+\n67 | Build Status         | |GH-CI|                | |python|    |\n68 |                      +------------------------+-------------+\n69 |                      | |pre-commit.ci status| |             |\n70 +----------------------+------------------------+-------------+\n71 | Metrics              | |codacy|               | |codecov|   |\n72 +----------------------+------------------------+-------------+\n73 | Activity             | |PyPIact|              | |condaact|  |\n74 +----------------------+------------------------+-------------+\n75 | Citation             | |joss|                 | |zenodo|    |\n76 +----------------------+------------------------+-------------+\n77 | License              | |MIT|                  |             |\n78 +----------------------+------------------------+-------------+\n79 | Community            | |slack|                | |discuss|   |\n80 +----------------------+------------------------+-------------+\n81 | Formatter            | |black|                | |isort|     |\n82 +----------------------+------------------------+-------------+\n83 | Affiliated           | |NumFOCUS Affiliated|                |\n84 +----------------------+------------------------+-------------+\n85 \n86 \n87     3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK)\n88 \n89 PyVista is:\n90 \n91 * *Pythonic VTK*: a high-level API to the `Visualization Toolkit`_ (VTK)\n92 * mesh data structures and filtering methods for spatial datasets\n93 * 3D plotting made simple and built for large/complex data geometries\n94 \n95 .. _Visualization Toolkit: https://vtk.org\n96 \n97 PyVista is a helper module for the Visualization Toolkit (VTK) that wraps the VTK library\n98 through NumPy and direct array access through a variety of methods and classes.\n99 This package provides a Pythonic, well-documented interface exposing\n100 VTK's powerful visualization backend to facilitate rapid prototyping, analysis,\n101 and visual integration of spatially referenced datasets.\n102 \n103 This module can be used for scientific plotting for presentations and research\n104 papers as well as a supporting module for other mesh 3D rendering dependent\n105 Python modules; see Connections for a list of projects that leverage\n106 PyVista.\n107 \n108 \n109 .. |tweet| image:: https://img.shields.io/twitter/url.svg?style=social&url=http%3A%2F%2Fshields.io\n110    :target: https://twitter.com/intent/tweet?text=Check%20out%20this%20project%20for%203D%20visualization%20in%20Python&url=https://github.com/pyvista/pyvista&hashtags=3D,visualization,Python,vtk,mesh,plotting,PyVista\n111 \n112 Share this project on Twitter: |tweet|\n113 \n114 \n115 PyVista is a NumFOCUS affiliated project\n116 \n117 .. image:: https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\n118    :target: https://numfocus.org/sponsored-projects/affiliated-projects\n119    :alt: NumFOCUS affiliated projects\n120    :height: 60px\n121 \n122 \n123 Highlights\n124 ==========\n125 \n126 .. |binder| image:: https://static.mybinder.org/badge_logo.svg\n127    :target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master\n128    :alt: Launch on Binder\n129 \n130 Head over to the `Quick Examples`_ page in the docs to explore our gallery of\n131 examples showcasing what PyVista can do. Want to test-drive PyVista?\n132 All of the examples from the gallery are live on MyBinder for you to test\n133 drive without installing anything locally: |binder|\n134 \n135 .. _Quick Examples: http://docs.pyvista.org/examples/index.html\n136 \n137 \n138 Overview of Features\n139 --------------------\n140 \n141 * Extensive gallery of examples (see `Quick Examples`_)\n142 * Interactive plotting in Jupyter Notebooks with server-side and client-side\n143   rendering with `trame`_.\n144 * Filtering/plotting tools built for interactivity (see `Widgets`_)\n145 * Direct access to mesh analysis and transformation routines (see Filters_)\n146 * Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)\n147 * Import meshes from many common formats (use ``pyvista.read()``). Support for all formats handled by `meshio`_ is built-in.\n148 * Export meshes as VTK, STL, OBJ, or PLY (``mesh.save()``) file types or any formats supported by meshio_ (``pyvista.save_meshio()``)\n149 \n150 .. _trame: https://github.com/Kitware/trame\n151 .. _Widgets: https://docs.pyvista.org/api/plotting/index.html#widget-api\n152 .. _Filters: https://docs.pyvista.org/api/core/filters.html\n153 .. _Plotting: https://docs.pyvista.org/api/plotting/index.html\n154 .. _meshio: https://github.com/nschloe/meshio\n155 \n156 \n157 Documentation\n158 =============\n159 \n160 Refer to the `documentation <http://docs.pyvista.org/>`_ for detailed\n161 installation and usage details.\n162 \n163 For general questions about the project, its applications, or about software\n164 usage, please create a discussion in `pyvista/discussions`_\n165 where the community can collectively address your questions. You are also\n166 welcome to join us on Slack_ or send one of the developers an email.\n167 The project support team can be reached at `info@pyvista.org`_.\n168 \n169 .. _pyvista/discussions: https://github.com/pyvista/pyvista/discussions\n170 .. _Slack: http://slack.pyvista.org\n171 .. _info@pyvista.org: mailto:info@pyvista.org\n172 \n173 \n174 Installation\n175 ============\n176 \n177 PyVista can be installed from `PyPI <https://pypi.org/project/pyvista/>`_\n178 using ``pip`` on Python >= 3.8::\n179 \n180     pip install pyvista\n181 \n182 You can also visit `PyPI <https://pypi.org/project/pyvista/>`_,\n183 `Anaconda <https://anaconda.org/conda-forge/pyvista>`_, or\n184 `GitHub <https://github.com/pyvista/pyvista>`_ to download the source.\n185 \n186 See the `Installation <http://docs.pyvista.org/getting-started/installation.html#install-ref.>`_\n187 for more details regarding optional dependencies or if the installation through pip doesn't work out.\n188 \n189 \n190 Connections\n191 ===========\n192 \n193 PyVista is a powerful tool that researchers can harness to create compelling,\n194 integrated visualizations of large datasets in an intuitive, Pythonic manner.\n195 \n196 Learn more about how PyVista is used across science and engineering disciplines\n197 by a diverse community of users on our `Connections page`_.\n198 \n199 .. _Connections page: https://docs.pyvista.org/getting-started/connections.html\n200 \n201 \n202 Authors\n203 =======\n204 \n205 Please take a look at the `contributors page`_ and the active `list of authors`_\n206 to learn more about the developers of PyVista.\n207 \n208 .. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/\n209 .. _list of authors: https://docs.pyvista.org/getting-started/authors.html#authors\n210 \n211 \n212 Contributing\n213 ============\n214 \n215 .. |Contributor Covenant| image:: https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg\n216    :target: CODE_OF_CONDUCT.md\n217 \n218 .. |codetriage| image:: https://www.codetriage.com/pyvista/pyvista/badges/users.svg\n219    :target: https://www.codetriage.com/pyvista/pyvista\n220    :alt: Code Triage\n221 \n222 |Contributor Covenant|\n223 |codetriage|\n224 \n225 We absolutely welcome contributions and we hope that our `Contributing Guide`_\n226 will facilitate your ability to make PyVista better. PyVista is mostly\n227 maintained on a volunteer basis and thus we need to foster a community that can\n228 support user questions and develop new features to make this software a useful\n229 tool for all users while encouraging every member of the community to share\n230 their ideas. To learn more about contributing to PyVista, please see the\n231 `Contributing Guide`_ and our `Code of Conduct`_.\n232 \n233 .. _Contributing Guide: https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst\n234 .. _Code of Conduct: https://github.com/pyvista/pyvista/blob/main/CODE_OF_CONDUCT.md\n235 \n236 \n237 Citing PyVista\n238 ==============\n239 \n240 There is a `paper about PyVista <https://doi.org/10.21105/joss.01450>`_.\n241 \n242 If you are using PyVista in your scientific research, please help our scientific\n243 visibility by citing our work.\n244 \n245 \n246     Sullivan and Kaszynski, (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software, 4(37), 1450, https://doi.org/10.21105/joss.01450\n247 \n248 \n249 BibTex:\n250 \n251 .. code::\n252 \n253     @article{sullivan2019pyvista,\n254       doi = {10.21105/joss.01450},\n255       url = {https://doi.org/10.21105/joss.01450},\n256       year = {2019},\n257       month = {May},\n258       publisher = {The Open Journal},\n259       volume = {4},\n260       number = {37},\n261       pages = {1450},\n262       author = {Bane Sullivan and Alexander Kaszynski},\n263       title = {{PyVista}: {3D} plotting and mesh analysis through a streamlined interface for the {Visualization Toolkit} ({VTK})},\n264       journal = {Journal of Open Source Software}\n265     }\n266 \n[end of README.rst]\n[start of examples/00-load/create-unstructured-surface.py]\n1 \"\"\"\n2 .. _create_unstructured_example:\n3 \n4 Creating an Unstructured Grid\n5 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n6 \n7 Create an irregular, unstructured grid from NumPy arrays.\n8 \"\"\"\n9 \n10 import numpy as np\n11 \n12 import pyvista as pv\n13 from pyvista import CellType\n14 \n15 ###############################################################################\n16 # An unstructured grid can be created directly from NumPy arrays.\n17 # This is useful when creating a grid from scratch or copying it from another\n18 # format.  See `vtkUnstructuredGrid <https://www.vtk.org/doc/nightly/html/classvtkUnstructuredGrid.html>`_\n19 # for available cell types and their descriptions.\n20 \n21 # Contains information on the points composing each cell.\n22 # Each cell begins with the number of points in the cell and then the points\n23 # composing the cell\n24 cells = np.array([8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 9, 10, 11, 12, 13, 14, 15])\n25 \n26 # cell type array. Contains the cell type of each cell\n27 cell_type = np.array([CellType.HEXAHEDRON, CellType.HEXAHEDRON])\n28 \n29 # in this example, each cell uses separate points\n30 cell1 = np.array(\n31     [\n32         [0, 0, 0],\n33         [1, 0, 0],\n34         [1, 1, 0],\n35         [0, 1, 0],\n36         [0, 0, 1],\n37         [1, 0, 1],\n38         [1, 1, 1],\n39         [0, 1, 1],\n40     ]\n41 )\n42 \n43 cell2 = np.array(\n44     [\n45         [0, 0, 2],\n46         [1, 0, 2],\n47         [1, 1, 2],\n48         [0, 1, 2],\n49         [0, 0, 3],\n50         [1, 0, 3],\n51         [1, 1, 3],\n52         [0, 1, 3],\n53     ]\n54 )\n55 \n56 # points of the cell array\n57 points = np.vstack((cell1, cell2)).astype(float)\n58 \n59 # create the unstructured grid directly from the numpy arrays\n60 grid = pv.UnstructuredGrid(cells, cell_type, points)\n61 \n62 # For cells of fixed sizes (like the mentioned Hexahedra), it is also possible to use the\n63 # simplified dictionary interface. This automatically calculates the cell array.\n64 # Note that for mixing with additional cell types, just the appropriate key needs to be\n65 # added to the dictionary.\n66 cells_hex = np.arange(16).reshape([2, 8])\n67 # = np.array([[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15]])\n68 grid = pv.UnstructuredGrid({CellType.HEXAHEDRON: cells_hex}, points)\n69 \n70 # plot the grid (and suppress the camera position output)\n71 _ = grid.plot(show_edges=True)\n72 \n73 ###############################################################################\n74 # UnstructuredGrid with Shared Points\n75 # -----------------------------------\n76 #\n77 # The next example again creates an unstructured grid containing\n78 # hexahedral cells, but using common points between the cells.\n79 \n80 # these points will all be shared between the cells\n81 points = np.array(\n82     [\n83         [0.0, 0.0, 0.0],\n84         [1.0, 0.0, 0.0],\n85         [0.5, 0.0, 0.0],\n86         [1.0, 1.0, 0.0],\n87         [1.0, 0.5, 0.0],\n88         [0.0, 1.0, 0.0],\n89         [0.5, 1.0, 0.0],\n90         [0.0, 0.5, 0.0],\n91         [0.5, 0.5, 0.0],\n92         [1.0, 0.0, 0.5],\n93         [1.0, 0.0, 1.0],\n94         [0.0, 0.0, 0.5],\n95         [0.0, 0.0, 1.0],\n96         [0.5, 0.0, 0.5],\n97         [0.5, 0.0, 1.0],\n98         [1.0, 1.0, 0.5],\n99         [1.0, 1.0, 1.0],\n100         [1.0, 0.5, 0.5],\n101         [1.0, 0.5, 1.0],\n102         [0.0, 1.0, 0.5],\n103         [0.0, 1.0, 1.0],\n104         [0.5, 1.0, 0.5],\n105         [0.5, 1.0, 1.0],\n106         [0.0, 0.5, 0.5],\n107         [0.0, 0.5, 1.0],\n108         [0.5, 0.5, 0.5],\n109         [0.5, 0.5, 1.0],\n110     ]\n111 )\n112 \n113 \n114 # Each cell in the cell array needs to include the size of the cell\n115 # and the points belonging to the cell.  In this example, there are 8\n116 # hexahedral cells that have common points between them.\n117 cells = np.array(\n118     [\n119         [8, 0, 2, 8, 7, 11, 13, 25, 23],\n120         [8, 2, 1, 4, 8, 13, 9, 17, 25],\n121         [8, 7, 8, 6, 5, 23, 25, 21, 19],\n122         [8, 8, 4, 3, 6, 25, 17, 15, 21],\n123         [8, 11, 13, 25, 23, 12, 14, 26, 24],\n124         [8, 13, 9, 17, 25, 14, 10, 18, 26],\n125         [8, 23, 25, 21, 19, 24, 26, 22, 20],\n126         [8, 25, 17, 15, 21, 26, 18, 16, 22],\n127     ]\n128 ).ravel()\n129 \n130 # each cell is a HEXAHEDRON\n131 celltypes = np.full(8, CellType.HEXAHEDRON, dtype=np.uint8)\n132 \n133 \n134 ###############################################################################\n135 # Finally, create the unstructured grid and plot it\n136 grid = pv.UnstructuredGrid(cells, celltypes, points)\n137 \n138 # Alternate versions:\n139 grid = pv.UnstructuredGrid({CellType.HEXAHEDRON: cells.reshape([-1, 9])[:, 1:]}, points)\n140 grid = pv.UnstructuredGrid(\n141     {CellType.HEXAHEDRON: np.delete(cells, np.arange(0, cells.size, 9))}, points\n142 )\n143 \n144 # plot the grid (and suppress the camera position output)\n145 _ = grid.plot(show_edges=True)\n146 \n147 \n148 ###############################################################################\n149 # Tetrahedral Grid\n150 # ~~~~~~~~~~~~~~~~\n151 # Here is how we can create an unstructured tetrahedral grid.\n152 \n153 # There are 10 cells here, each cell is [4, INDEX0, INDEX1, INDEX2, INDEX3]\n154 # where INDEX is one of the corners of the tetrahedron.\n155 #\n156 # Note that the array does not need to be shaped like this, we could have a\n157 # flat array, but it's easier to make out the structure of the array this way.\n158 cells = np.array(\n159     [\n160         [4, 6, 5, 8, 7],\n161         [4, 7, 3, 8, 9],\n162         [4, 7, 3, 1, 5],\n163         [4, 9, 3, 1, 7],\n164         [4, 2, 6, 5, 8],\n165         [4, 2, 6, 0, 4],\n166         [4, 6, 2, 0, 8],\n167         [4, 5, 2, 8, 3],\n168         [4, 5, 3, 8, 7],\n169         [4, 2, 6, 4, 5],\n170     ]\n171 )\n172 \n173 celltypes = np.full(10, fill_value=CellType.TETRA, dtype=np.uint8)\n174 \n175 # These are the 10 points. The number of cells does not need to match the\n176 # number of points, they just happen to in this example\n177 points = np.array(\n178     [\n179         [-0.0, 0.0, -0.5],\n180         [0.0, 0.0, 0.5],\n181         [-0.43, 0.0, -0.25],\n182         [-0.43, 0.0, 0.25],\n183         [-0.0, 0.43, -0.25],\n184         [0.0, 0.43, 0.25],\n185         [0.43, 0.0, -0.25],\n186         [0.43, 0.0, 0.25],\n187         [0.0, -0.43, -0.25],\n188         [0.0, -0.43, 0.25],\n189     ]\n190 )\n191 \n192 # Create and plot the unstructured grid\n193 grid = pv.UnstructuredGrid(cells, celltypes, points)\n194 grid.plot(show_edges=True)\n195 \n196 \n197 ###############################################################################\n198 # For fun, let's separate all the cells and plot out the individual cells. Shift\n199 # them a little bit from the center to create an \"exploded view\".\n200 \n201 split_cells = grid.explode(0.5)\n202 split_cells.plot(show_edges=True, ssao=True)\n203 \n[end of examples/00-load/create-unstructured-surface.py]\n[start of pyvista/core/filters/rectilinear_grid.py]\n1 \"\"\"Filters module with the class to manage filters/algorithms for rectilinear grid datasets.\"\"\"\n2 \n3 import collections\n4 from typing import Sequence, Union\n5 \n6 import numpy as np\n7 \n8 from pyvista import _vtk, abstract_class\n9 from pyvista.core.filters import _get_output, _update_alg\n10 \n11 \n12 @abstract_class\n13 class RectilinearGridFilters:\n14     \"\"\"An internal class to manage filters/algorithms for rectilinear grid datasets.\"\"\"\n15 \n16     def to_tetrahedra(\n17         self,\n18         tetra_per_cell: int = 5,\n19         mixed: Union[Sequence[int], bool] = False,\n20         pass_cell_ids: bool = False,\n21         progress_bar: bool = False,\n22     ):\n23         \"\"\"Create a tetrahedral mesh structured grid.\n24 \n25         Parameters\n26         ----------\n27         tetra_per_cell : int, default: 5\n28             The number of tetrahedrons to divide each cell into. Can be\n29             either ``5``, ``6``, or ``12``. If ``mixed=True``, this value is\n30             overridden.\n31 \n32         mixed : str, bool, sequence, default: False\n33             When set, subdivides some cells into 5 and some cells into 12. Set\n34             to ``True`` to use the active cell scalars of the\n35             :class:`pyvista.RectilinearGrid` to be either 5 or 12 to\n36             determining the number of tetrahedra to generate per cell.\n37 \n38             When a sequence, uses these values to subdivide the cells. When a\n39             string uses a cell array rather than the active array to determine\n40             the number of tetrahedra to generate per cell.\n41 \n42         pass_cell_ids : bool, default: False\n43             Set to ``True`` to make the tetrahedra have scalar data indicating\n44             which cell they came from in the original\n45             :class:`pyvista.RectilinearGrid`.\n46 \n47         progress_bar : bool, default: False\n48             Display a progress bar to indicate progress.\n49 \n50         Returns\n51         -------\n52         pyvista.UnstructuredGrid\n53             UnstructuredGrid containing the tetrahedral cells.\n54 \n55         Examples\n56         --------\n57         Divide a rectangular grid into tetrahedrons. Each cell contains by\n58         default 5 tetrahedrons.\n59 \n60         First, create and plot the grid.\n61 \n62         >>> import numpy as np\n63         >>> import pyvista as pv\n64         >>> xrng = np.linspace(0, 1, 2)\n65         >>> yrng = np.linspace(0, 1, 2)\n66         >>> zrng = np.linspace(0, 2, 3)\n67         >>> grid = pv.RectilinearGrid(xrng, yrng, zrng)\n68         >>> grid.plot()\n69 \n70         Now, generate the tetrahedra plot in the exploded view of the cell.\n71 \n72         >>> tet_grid = grid.to_tetrahedra()\n73         >>> tet_grid.explode(factor=0.5).plot(show_edges=True)\n74 \n75         Take the same grid but divide the first cell into 5 cells and the other\n76         cell into 12 tetrahedrons per cell.\n77 \n78         >>> tet_grid = grid.to_tetrahedra(mixed=[5, 12])\n79         >>> tet_grid.explode(factor=0.5).plot(show_edges=True)\n80 \n81         \"\"\"\n82         alg = _vtk.vtkRectilinearGridToTetrahedra()\n83         alg.SetRememberVoxelId(pass_cell_ids)\n84         if mixed is not False:\n85             if isinstance(mixed, str):\n86                 self.cell_data.active_scalars_name = mixed\n87             elif isinstance(mixed, (np.ndarray, collections.abc.Sequence)):\n88                 self.cell_data['_MIXED_CELLS_'] = mixed  # type: ignore\n89             elif not isinstance(mixed, bool):\n90                 raise TypeError('`mixed` must be either a sequence of ints or bool')\n91             alg.SetTetraPerCellTo5And12()\n92         else:\n93             if tetra_per_cell not in [5, 6, 12]:\n94                 raise ValueError(\n95                     f'`tetra_per_cell` should be either 5, 6, or 12, not {tetra_per_cell}'\n96                 )\n97 \n98             # Edge case causing a seg-fault where grid is flat in one dimension\n99             # See: https://gitlab.kitware.com/vtk/vtk/-/issues/18650\n100             if 1 in self.dimensions and tetra_per_cell == 12:  # type: ignore\n101                 raise RuntimeError(\n102                     'Cannot split cells into 12 tetrahedrals when at least '  # type: ignore\n103                     f'one dimension is 1. Dimensions are {self.dimensions}.'\n104                 )\n105 \n106             alg.SetTetraPerCell(tetra_per_cell)\n107 \n108         alg.SetInputData(self)\n109         _update_alg(alg, progress_bar, 'Converting to tetrahedra')\n110         return _get_output(alg)\n111 \n[end of pyvista/core/filters/rectilinear_grid.py]\n[start of pyvista/utilities/errors.py]\n1 \"\"\"Module managing errors.\"\"\"\n2 \n3 import collections\n4 from collections.abc import Iterable\n5 import logging\n6 import os\n7 import re\n8 import subprocess\n9 import sys\n10 import traceback\n11 \n12 import scooby\n13 \n14 from pyvista import _vtk\n15 \n16 \n17 def set_error_output_file(filename):\n18     \"\"\"Set a file to write out the VTK errors.\n19 \n20     Parameters\n21     ----------\n22     filename : str\n23         Path to the file to write VTK errors to.\n24 \n25     Returns\n26     -------\n27     vtkFileOutputWindow\n28         VTK file output window.\n29     vtkOutputWindow\n30         VTK output window.\n31 \n32     \"\"\"\n33     filename = os.path.abspath(os.path.expanduser(filename))\n34     fileOutputWindow = _vtk.vtkFileOutputWindow()\n35     fileOutputWindow.SetFileName(filename)\n36     outputWindow = _vtk.vtkOutputWindow()\n37     outputWindow.SetInstance(fileOutputWindow)\n38     return fileOutputWindow, outputWindow\n39 \n40 \n41 class VtkErrorCatcher:\n42     \"\"\"Context manager to temporarily catch VTK errors.\n43 \n44     Parameters\n45     ----------\n46     raise_errors : bool, default: False\n47         Raise a ``RuntimeError`` when a VTK error is encountered.\n48 \n49     send_to_logging : bool, default: True\n50         Determine whether VTK errors raised within the context should\n51         also be sent to logging.\n52 \n53     Examples\n54     --------\n55     Catch VTK errors using the context manager.\n56 \n57     >>> import pyvista\n58     >>> with pyvista.VtkErrorCatcher() as error_catcher:\n59     ...     sphere = pyvista.Sphere()\n60     ...\n61     \"\"\"\n62 \n63     def __init__(self, raise_errors=False, send_to_logging=True):\n64         \"\"\"Initialize context manager.\"\"\"\n65         self.raise_errors = raise_errors\n66         self.send_to_logging = send_to_logging\n67 \n68     def __enter__(self):\n69         \"\"\"Observe VTK string output window for errors.\"\"\"\n70         error_output = _vtk.vtkStringOutputWindow()\n71         error_win = _vtk.vtkOutputWindow()\n72         self._error_output_orig = error_win.GetInstance()\n73         error_win.SetInstance(error_output)\n74         obs = Observer(log=self.send_to_logging, store_history=True)\n75         obs.observe(error_output)\n76         self._observer = obs\n77 \n78     def __exit__(self, type, val, tb):\n79         \"\"\"Stop observing VTK string output window.\"\"\"\n80         error_win = _vtk.vtkOutputWindow()\n81         error_win.SetInstance(self._error_output_orig)\n82         self.events = self._observer.event_history\n83         if self.raise_errors and self.events:\n84             errors = [RuntimeError(f'{e.kind}: {e.alert}', e.path, e.address) for e in self.events]\n85             raise RuntimeError(errors)\n86 \n87 \n88 class Observer:\n89     \"\"\"A standard class for observing VTK objects.\"\"\"\n90 \n91     def __init__(self, event_type='ErrorEvent', log=True, store_history=False):\n92         \"\"\"Initialize observer.\"\"\"\n93         self.__event_occurred = False\n94         self.__message = None\n95         self.__message_etc = None\n96         self.CallDataType = 'string0'\n97         self.__observing = False\n98         self.event_type = event_type\n99         self.__log = log\n100 \n101         self.store_history = store_history\n102         self.event_history = []\n103 \n104     @staticmethod\n105     def parse_message(message):\n106         \"\"\"Parse the given message.\"\"\"\n107         # Message format\n108         regex = re.compile(r'([A-Z]+):\\sIn\\s(.+),\\sline\\s.+\\n\\w+\\s\\((.+)\\):\\s(.+)')\n109         try:\n110             kind, path, address, alert = regex.findall(message)[0]\n111             return kind, path, address, alert\n112         except:  # noqa: E722\n113             return '', '', '', message\n114 \n115     def log_message(self, kind, alert):\n116         \"\"\"Parse different event types and passes them to logging.\"\"\"\n117         if kind == 'ERROR':\n118             logging.error(alert)\n119         else:\n120             logging.warning(alert)\n121 \n122     def __call__(self, obj, event, message):\n123         \"\"\"Declare standard call function for the observer.\n124 \n125         On an event occurrence, this function executes.\n126 \n127         \"\"\"\n128         try:\n129             self.__event_occurred = True\n130             self.__message_etc = message\n131             kind, path, address, alert = self.parse_message(message)\n132             self.__message = alert\n133             if self.store_history:\n134                 VtkEvent = collections.namedtuple('VtkEvent', ['kind', 'path', 'address', 'alert'])\n135                 self.event_history.append(VtkEvent(kind, path, address, alert))\n136             if self.__log:\n137                 self.log_message(kind, alert)\n138         except Exception:  # pragma: no cover\n139             try:\n140                 if len(message) > 120:\n141                     message = f'{repr(message[:100])} ... ({len(message)} characters)'\n142                 else:\n143                     message = repr(message)\n144                 print(\n145                     f'PyVista error in handling VTK error message:\\n{message}',\n146                     file=sys.__stdout__,\n147                 )\n148                 traceback.print_tb(sys.last_traceback, file=sys.__stderr__)\n149             except Exception:\n150                 pass\n151 \n152     def has_event_occurred(self):\n153         \"\"\"Ask self if an error has occurred since last queried.\n154 \n155         This resets the observer's status.\n156 \n157         \"\"\"\n158         occ = self.__event_occurred\n159         self.__event_occurred = False\n160         return occ\n161 \n162     def get_message(self, etc=False):\n163         \"\"\"Get the last set error message.\n164 \n165         Returns\n166         -------\n167             str: the last set error message\n168 \n169         \"\"\"\n170         if etc:\n171             return self.__message_etc\n172         return self.__message\n173 \n174     def observe(self, algorithm):\n175         \"\"\"Make this an observer of an algorithm.\"\"\"\n176         if self.__observing:\n177             raise RuntimeError('This error observer is already observing an algorithm.')\n178         if hasattr(algorithm, 'GetExecutive') and algorithm.GetExecutive() is not None:\n179             algorithm.GetExecutive().AddObserver(self.event_type, self)\n180         algorithm.AddObserver(self.event_type, self)\n181         self.__observing = True\n182         return\n183 \n184 \n185 def send_errors_to_logging():\n186     \"\"\"Send all VTK error/warning messages to Python's logging module.\"\"\"\n187     error_output = _vtk.vtkStringOutputWindow()\n188     error_win = _vtk.vtkOutputWindow()\n189     error_win.SetInstance(error_output)\n190     obs = Observer()\n191     return obs.observe(error_output)\n192 \n193 \n194 _cmd = \"\"\"\\\n195 import pyvista; \\\n196 plotter = pyvista.Plotter(notebook=False, off_screen=True); \\\n197 plotter.add_mesh(pyvista.Sphere()); \\\n198 plotter.show(auto_close=False); \\\n199 gpu_info = plotter.render_window.ReportCapabilities(); \\\n200 print(gpu_info); \\\n201 plotter.close()\\\n202 \"\"\"\n203 \n204 \n205 def get_gpu_info():\n206     \"\"\"Get all information about the GPU.\"\"\"\n207     # an OpenGL context MUST be opened before trying to do this.\n208     proc = subprocess.run([sys.executable, '-c', _cmd], check=False, capture_output=True)\n209     gpu_info = '' if proc.returncode else proc.stdout.decode()\n210     return gpu_info\n211 \n212 \n213 class GPUInfo:\n214     \"\"\"A class to hold GPU details.\"\"\"\n215 \n216     def __init__(self):\n217         \"\"\"Instantiate a container for the GPU information.\"\"\"\n218         self._gpu_info = get_gpu_info()\n219 \n220     @property\n221     def renderer(self):\n222         \"\"\"GPU renderer name.\"\"\"\n223         regex = re.compile(\"OpenGL renderer string:(.+)\\n\")\n224         try:\n225             renderer = regex.findall(self._gpu_info)[0]\n226         except IndexError:\n227             raise RuntimeError(\"Unable to parse GPU information for the renderer.\") from None\n228         return renderer.strip()\n229 \n230     @property\n231     def version(self):\n232         \"\"\"GPU renderer version.\"\"\"\n233         regex = re.compile(\"OpenGL version string:(.+)\\n\")\n234         try:\n235             version = regex.findall(self._gpu_info)[0]\n236         except IndexError:\n237             raise RuntimeError(\"Unable to parse GPU information for the version.\") from None\n238         return version.strip()\n239 \n240     @property\n241     def vendor(self):\n242         \"\"\"GPU renderer vendor.\"\"\"\n243         regex = re.compile(\"OpenGL vendor string:(.+)\\n\")\n244         try:\n245             vendor = regex.findall(self._gpu_info)[0]\n246         except IndexError:\n247             raise RuntimeError(\"Unable to parse GPU information for the vendor.\") from None\n248         return vendor.strip()\n249 \n250     def get_info(self):\n251         \"\"\"All GPU information as tuple pairs.\"\"\"\n252         return (\n253             (\"GPU Vendor\", self.vendor),\n254             (\"GPU Renderer\", self.renderer),\n255             (\"GPU Version\", self.version),\n256         )\n257 \n258     def _repr_html_(self):\n259         \"\"\"HTML table representation.\"\"\"\n260         fmt = \"<table>\"\n261         row = \"<tr><th>{}</th><td>{}</td></tr>\\n\"\n262         for meta in self.get_info():\n263             fmt += row.format(*meta)\n264         fmt += \"</table>\"\n265         return fmt\n266 \n267     def __repr__(self):\n268         \"\"\"Representation method.\"\"\"\n269         content = \"\\n\"\n270         for k, v in self.get_info():\n271             content += f\"{k:>18} : {v}\\n\"\n272         content += \"\\n\"\n273         return content\n274 \n275 \n276 class Report(scooby.Report):\n277     \"\"\"Generate a PyVista software environment report.\n278 \n279     Parameters\n280     ----------\n281     additional : sequence[types.ModuleType], sequence[str]\n282         List of packages or package names to add to output information.\n283 \n284     ncol : int, default: 3\n285         Number of package-columns in html table; only has effect if\n286         ``mode='HTML'`` or ``mode='html'``.\n287 \n288     text_width : int, default: 80\n289         The text width for non-HTML display modes.\n290 \n291     sort : bool, default: False\n292         Alphabetically sort the packages.\n293 \n294     gpu : bool, default: True\n295         Gather information about the GPU. Defaults to ``True`` but if\n296         experiencing rendering issues, pass ``False`` to safely generate a\n297         report.\n298 \n299     Examples\n300     --------\n301     >>> import pyvista as pv\n302     >>> pv.Report()  # doctest:+SKIP\n303       Date: Fri Oct 28 15:54:11 2022 MDT\n304     <BLANKLINE>\n305                     OS : Linux\n306                 CPU(s) : 6\n307                Machine : x86_64\n308           Architecture : 64bit\n309                    RAM : 62.6 GiB\n310            Environment : IPython\n311            File system : ext4\n312             GPU Vendor : NVIDIA Corporation\n313           GPU Renderer : Quadro P2000/PCIe/SSE2\n314            GPU Version : 4.5.0 NVIDIA 470.141.03\n315     <BLANKLINE>\n316       Python 3.8.10 (default, Jun 22 2022, 20:18:18)  [GCC 9.4.0]\n317     <BLANKLINE>\n318                pyvista : 0.37.dev0\n319                    vtk : 9.1.0\n320                  numpy : 1.23.3\n321                imageio : 2.22.0\n322                 scooby : 0.7.1.dev1+gf097dad\n323                  pooch : v1.6.0\n324             matplotlib : 3.6.0\n325                IPython : 7.31.0\n326               colorcet : 3.0.1\n327                cmocean : 2.0\n328             ipyvtklink : 0.2.3\n329                  scipy : 1.9.1\n330                   tqdm : 4.64.1\n331                 meshio : 5.3.4\n332             jupyterlab : 3.4.7\n333              pythreejs : Version unknown\n334 \n335     \"\"\"\n336 \n337     def __init__(self, additional=None, ncol=3, text_width=80, sort=False, gpu=True):\n338         \"\"\"Generate a :class:`scooby.Report` instance.\"\"\"\n339         # Mandatory packages\n340         core = ['pyvista', 'vtk', 'numpy', 'matplotlib', 'imageio', 'scooby', 'pooch']\n341 \n342         # Optional packages.\n343         optional = [\n344             'pyvistaqt',\n345             'PyQt5',\n346             'IPython',\n347             'colorcet',\n348             'cmocean',\n349             'ipyvtklink',\n350             'ipywidgets',\n351             'scipy',\n352             'tqdm',\n353             'meshio',\n354             'jupyterlab',\n355             'pythreejs',\n356             'pytest_pyvista',\n357             'trame',\n358             'trame_client',\n359             'trame_server',\n360             'trame_vtk',\n361             'jupyter_server_proxy',\n362             'nest_asyncio',\n363         ]\n364 \n365         # Information about the GPU - bare except in case there is a rendering\n366         # bug that the user is trying to report.\n367         if gpu:\n368             try:\n369                 extra_meta = GPUInfo().get_info()\n370             except:\n371                 extra_meta = (\"GPU Details\", \"error\")\n372         else:\n373             extra_meta = (\"GPU Details\", \"None\")\n374 \n375         scooby.Report.__init__(\n376             self,\n377             additional=additional,\n378             core=core,\n379             optional=optional,\n380             ncol=ncol,\n381             text_width=text_width,\n382             sort=sort,\n383             extra_meta=extra_meta,\n384         )\n385 \n386 \n387 def assert_empty_kwargs(**kwargs):\n388     \"\"\"Assert that all keyword arguments have been used (internal helper).\n389 \n390     If any keyword arguments are passed, a ``TypeError`` is raised.\n391     \"\"\"\n392     n = len(kwargs)\n393     if n == 0:\n394         return True\n395     caller = sys._getframe(1).f_code.co_name\n396     keys = list(kwargs.keys())\n397     bad_arguments = ', '.join([f'\"{key}\"' for key in keys])\n398     if n == 1:\n399         grammar = \"is an invalid keyword argument\"\n400     else:\n401         grammar = \"are invalid keyword arguments\"\n402     message = f\"{bad_arguments} {grammar} for `{caller}`\"\n403     raise TypeError(message)\n404 \n405 \n406 def check_valid_vector(point, name=''):\n407     \"\"\"Check if a vector contains three components.\"\"\"\n408     if not isinstance(point, Iterable):\n409         raise TypeError(f'{name} must be a length three iterable of floats.')\n410     if len(point) != 3:\n411         if name == '':\n412             name = 'Vector'\n413         raise ValueError(f'{name} must be a length three iterable of floats.')\n414 \n[end of pyvista/utilities/errors.py]\n[start of pyvista/utilities/helpers.py]\n1 \"\"\"Supporting functions for polydata and grid objects.\"\"\"\n2 \n3 import collections.abc\n4 import enum\n5 import logging\n6 import os\n7 import signal\n8 import sys\n9 import threading\n10 from threading import Thread\n11 import traceback\n12 from typing import Optional\n13 import warnings\n14 \n15 import numpy as np\n16 \n17 import pyvista\n18 from pyvista import _vtk\n19 from pyvista.errors import AmbiguousDataError, MissingDataError\n20 \n21 from . import transformations\n22 from .fileio import from_meshio\n23 \n24 \n25 class FieldAssociation(enum.Enum):\n26     \"\"\"Represents which type of vtk field a scalar or vector array is associated with.\"\"\"\n27 \n28     POINT = _vtk.vtkDataObject.FIELD_ASSOCIATION_POINTS\n29     CELL = _vtk.vtkDataObject.FIELD_ASSOCIATION_CELLS\n30     NONE = _vtk.vtkDataObject.FIELD_ASSOCIATION_NONE\n31     ROW = _vtk.vtkDataObject.FIELD_ASSOCIATION_ROWS\n32 \n33 \n34 def get_vtk_type(typ):\n35     \"\"\"Look up the VTK type for a given numpy data type.\n36 \n37     Corrects for string type mapping issues.\n38 \n39     Parameters\n40     ----------\n41     typ : numpy.dtype\n42         Numpy data type.\n43 \n44     Returns\n45     -------\n46     int\n47         Integer type id specified in ``vtkType.h``\n48 \n49     \"\"\"\n50     typ = _vtk.get_vtk_array_type(typ)\n51     # This handles a silly string type bug\n52     if typ == 3:\n53         return 13\n54     return typ\n55 \n56 \n57 def vtk_bit_array_to_char(vtkarr_bint):\n58     \"\"\"Cast vtk bit array to a char array.\n59 \n60     Parameters\n61     ----------\n62     vtkarr_bint : vtk.vtkBitArray\n63         VTK binary array.\n64 \n65     Returns\n66     -------\n67     vtk.vtkCharArray\n68         VTK char array.\n69 \n70     Notes\n71     -----\n72     This performs a copy.\n73 \n74     \"\"\"\n75     vtkarr = _vtk.vtkCharArray()\n76     vtkarr.DeepCopy(vtkarr_bint)\n77     return vtkarr\n78 \n79 \n80 def vtk_id_list_to_array(vtk_id_list):\n81     \"\"\"Convert a vtkIdList to a NumPy array.\n82 \n83     Parameters\n84     ----------\n85     vtk_id_list : vtk.vtkIdList\n86         VTK ID list.\n87 \n88     Returns\n89     -------\n90     numpy.ndarray\n91         Array of IDs.\n92 \n93     \"\"\"\n94     return np.array([vtk_id_list.GetId(i) for i in range(vtk_id_list.GetNumberOfIds())])\n95 \n96 \n97 def convert_string_array(arr, name=None):\n98     \"\"\"Convert a numpy array of strings to a vtkStringArray or vice versa.\n99 \n100     Parameters\n101     ----------\n102     arr : numpy.ndarray\n103         Numpy string array to convert.\n104 \n105     name : str, optional\n106         Name to set the vtkStringArray to.\n107 \n108     Returns\n109     -------\n110     vtkStringArray\n111         VTK string array.\n112 \n113     Notes\n114     -----\n115     Note that this is terribly inefficient. If you have ideas on how\n116     to make this faster, please consider opening a pull request.\n117 \n118     \"\"\"\n119     if isinstance(arr, np.ndarray):\n120         # VTK default fonts only support ASCII. See https://gitlab.kitware.com/vtk/vtk/-/issues/16904\n121         if np.issubdtype(arr.dtype, np.str_) and not ''.join(arr).isascii():  # avoids segfault\n122             raise ValueError(\n123                 'String array contains non-ASCII characters that are not supported by VTK.'\n124             )\n125         vtkarr = _vtk.vtkStringArray()\n126         ########### OPTIMIZE ###########\n127         for val in arr:\n128             vtkarr.InsertNextValue(val)\n129         ################################\n130         if isinstance(name, str):\n131             vtkarr.SetName(name)\n132         return vtkarr\n133     # Otherwise it is a vtk array and needs to be converted back to numpy\n134     ############### OPTIMIZE ###############\n135     nvalues = arr.GetNumberOfValues()\n136     return np.array([arr.GetValue(i) for i in range(nvalues)], dtype='|U')\n137     ########################################\n138 \n139 \n140 def convert_array(arr, name=None, deep=False, array_type=None):\n141     \"\"\"Convert a NumPy array to a vtkDataArray or vice versa.\n142 \n143     Parameters\n144     ----------\n145     arr : np.ndarray | vtkDataArray\n146         A numpy array or vtkDataArry to convert.\n147     name : str, optional\n148         The name of the data array for VTK.\n149     deep : bool, default: False\n150         If input is numpy array then deep copy values.\n151     array_type : int, optional\n152         VTK array type ID as specified in specified in ``vtkType.h``.\n153 \n154     Returns\n155     -------\n156     vtkDataArray or numpy.ndarray\n157         The converted array.  If input is a :class:`numpy.ndarray` then\n158         returns ``vtkDataArray`` or is input is ``vtkDataArray`` then\n159         returns NumPy ``ndarray``.\n160 \n161     \"\"\"\n162     if arr is None:\n163         return\n164     if isinstance(arr, (list, tuple)):\n165         arr = np.array(arr)\n166     if isinstance(arr, np.ndarray):\n167         if arr.dtype == np.dtype('O'):\n168             arr = arr.astype('|S')\n169         arr = np.ascontiguousarray(arr)\n170         if arr.dtype.type in (np.str_, np.bytes_):\n171             # This handles strings\n172             vtk_data = convert_string_array(arr)\n173         else:\n174             # This will handle numerical data\n175             arr = np.ascontiguousarray(arr)\n176             vtk_data = _vtk.numpy_to_vtk(num_array=arr, deep=deep, array_type=array_type)\n177         if isinstance(name, str):\n178             vtk_data.SetName(name)\n179         return vtk_data\n180     # Otherwise input must be a vtkDataArray\n181     if not isinstance(arr, (_vtk.vtkDataArray, _vtk.vtkBitArray, _vtk.vtkStringArray)):\n182         raise TypeError(f'Invalid input array type ({type(arr)}).')\n183     # Handle booleans\n184     if isinstance(arr, _vtk.vtkBitArray):\n185         arr = vtk_bit_array_to_char(arr)\n186     # Handle string arrays\n187     if isinstance(arr, _vtk.vtkStringArray):\n188         return convert_string_array(arr)\n189     # Convert from vtkDataArry to NumPy\n190     return _vtk.vtk_to_numpy(arr)\n191 \n192 \n193 def is_pyvista_dataset(obj):\n194     \"\"\"Return ``True`` if the object is a PyVista wrapped dataset.\n195 \n196     Parameters\n197     ----------\n198     obj : Any\n199         Any object to test.\n200 \n201     Returns\n202     -------\n203     bool\n204         ``True`` when the object is a :class:`pyvista.DataSet`.\n205 \n206     \"\"\"\n207     return isinstance(obj, (pyvista.DataSet, pyvista.MultiBlock))\n208 \n209 \n210 def _assoc_array(obj, name, association='point'):\n211     \"\"\"Return a point, cell, or field array from a pyvista.DataSet or VTK object.\n212 \n213     If the array or index doesn't exist, return nothing. This matches VTK's\n214     behavior when using ``GetAbstractArray`` with an invalid key or index.\n215 \n216     \"\"\"\n217     vtk_attr = f'Get{association.title()}Data'\n218     python_attr = f'{association.lower()}_data'\n219 \n220     if isinstance(obj, pyvista.DataSet):\n221         try:\n222             return getattr(obj, python_attr).get_array(name)\n223         except KeyError:  # pragma: no cover\n224             return None\n225     abstract_array = getattr(obj, vtk_attr)().GetAbstractArray(name)\n226     if abstract_array is not None:\n227         return pyvista.pyvista_ndarray(abstract_array)\n228     return None\n229 \n230 \n231 def point_array(obj, name):\n232     \"\"\"Return point array of a pyvista or vtk object.\n233 \n234     Parameters\n235     ----------\n236     obj : pyvista.DataSet | vtk.vtkDataSet\n237         PyVista or VTK dataset.\n238 \n239     name : str | int\n240         Name or index of the array.\n241 \n242     Returns\n243     -------\n244     pyvista.pyvista_ndarray or None\n245         Wrapped array if the index or name is valid. Otherwise, ``None``.\n246 \n247     \"\"\"\n248     return _assoc_array(obj, name, 'point')\n249 \n250 \n251 def field_array(obj, name):\n252     \"\"\"Return field data of a pyvista or vtk object.\n253 \n254     Parameters\n255     ----------\n256     obj : pyvista.DataSet or vtk.vtkDataSet\n257         PyVista or VTK dataset.\n258 \n259     name : str | int\n260         Name or index of the array.\n261 \n262     Returns\n263     -------\n264     pyvista.pyvista_ndarray or None\n265         Wrapped array if the index or name is valid. Otherwise, ``None``.\n266 \n267     \"\"\"\n268     return _assoc_array(obj, name, 'field')\n269 \n270 \n271 def cell_array(obj, name):\n272     \"\"\"Return cell array of a pyvista or vtk object.\n273 \n274     Parameters\n275     ----------\n276     obj : pyvista.DataSet or vtk.vtkDataSet\n277         PyVista or VTK dataset.\n278 \n279     name : str | int\n280         Name or index of the array.\n281 \n282     Returns\n283     -------\n284     pyvista.pyvista_ndarray or None\n285         Wrapped array if the index or name is valid. Otherwise, ``None``.\n286 \n287     \"\"\"\n288     return _assoc_array(obj, name, 'cell')\n289 \n290 \n291 def row_array(obj, name):\n292     \"\"\"Return row array of a vtk object.\n293 \n294     Parameters\n295     ----------\n296     obj : vtk.vtkDataSet\n297         PyVista or VTK dataset.\n298 \n299     name : str\n300         Name of the array.\n301 \n302     Returns\n303     -------\n304     numpy.ndarray\n305         Wrapped array.\n306 \n307     \"\"\"\n308     vtkarr = obj.GetRowData().GetAbstractArray(name)\n309     return convert_array(vtkarr)\n310 \n311 \n312 def parse_field_choice(field):\n313     \"\"\"Return a field association object for a given field type string.\n314 \n315     Parameters\n316     ----------\n317     field : str, FieldAssociation\n318         Name of the field (e.g, ``'cell'``, ``'field'``, ``'point'``,\n319         ``'row'``).\n320 \n321     Returns\n322     -------\n323     pyvista.FieldAssociation\n324         Field association.\n325 \n326     \"\"\"\n327     if isinstance(field, str):\n328         field = field.strip().lower()\n329         if field in ['cell', 'c', 'cells']:\n330             field = FieldAssociation.CELL\n331         elif field in ['point', 'p', 'points']:\n332             field = FieldAssociation.POINT\n333         elif field in ['field', 'f', 'fields']:\n334             field = FieldAssociation.NONE\n335         elif field in ['row', 'r']:\n336             field = FieldAssociation.ROW\n337         else:\n338             raise ValueError(f'Data field ({field}) not supported.')\n339     elif isinstance(field, FieldAssociation):\n340         pass\n341     else:\n342         raise TypeError(f'Data field ({field}) not supported.')\n343     return field\n344 \n345 \n346 def get_array(mesh, name, preference='cell', err=False) -> Optional[np.ndarray]:\n347     \"\"\"Search point, cell and field data for an array.\n348 \n349     Parameters\n350     ----------\n351     mesh : pyvista.DataSet\n352         Dataset to get the array from.\n353 \n354     name : str\n355         The name of the array to get the range.\n356 \n357     preference : str, default: \"cell\"\n358         When scalars is specified, this is the preferred array type to\n359         search for in the dataset.  Must be either ``'point'``,\n360         ``'cell'``, or ``'field'``.\n361 \n362     err : bool, default: False\n363         Whether to throw an error if array is not present.\n364 \n365     Returns\n366     -------\n367     pyvista.pyvista_ndarray or ``None``\n368         Requested array.  Return ``None`` if there is no array\n369         matching the ``name`` and ``err=False``.\n370 \n371     \"\"\"\n372     if isinstance(mesh, _vtk.vtkTable):\n373         arr = row_array(mesh, name)\n374         if arr is None and err:\n375             raise KeyError(f'Data array ({name}) not present in this dataset.')\n376         return arr\n377 \n378     if not isinstance(preference, str):\n379         raise TypeError('`preference` must be a string')\n380     if preference not in ['cell', 'point', 'field']:\n381         raise ValueError(\n382             f'`preference` must be either \"cell\", \"point\", \"field\" for a '\n383             f'{type(mesh)}, not \"{preference}\".'\n384         )\n385 \n386     parr = point_array(mesh, name)\n387     carr = cell_array(mesh, name)\n388     farr = field_array(mesh, name)\n389     preference = parse_field_choice(preference)\n390     if sum([array is not None for array in (parr, carr, farr)]) > 1:\n391         if preference == FieldAssociation.CELL:\n392             return carr\n393         elif preference == FieldAssociation.POINT:\n394             return parr\n395         else:  # must be field\n396             return farr\n397 \n398     if parr is not None:\n399         return parr\n400     elif carr is not None:\n401         return carr\n402     elif farr is not None:\n403         return farr\n404     elif err:\n405         raise KeyError(f'Data array ({name}) not present in this dataset.')\n406     return None\n407 \n408 \n409 def get_array_association(mesh, name, preference='cell', err=False) -> FieldAssociation:\n410     \"\"\"Return the array association.\n411 \n412     Parameters\n413     ----------\n414     mesh : Dataset\n415         Dataset to get the array association from.\n416 \n417     name : str\n418         The name of the array.\n419 \n420     preference : str, default: \"cell\"\n421         When scalars is specified, this is the preferred array type to\n422         search for in the dataset.  Must be either ``'point'``,\n423         ``'cell'``, or ``'field'``.\n424 \n425     err : bool, default: False\n426         Boolean to control whether to throw an error if array is not\n427         present.\n428 \n429     Returns\n430     -------\n431     pyvista.utilities.helpers.FieldAssociation\n432         Association of the array. If array is not present and ``err`` is\n433         ``False``, ``FieldAssociation.NONE`` is returned.\n434 \n435     \"\"\"\n436     if isinstance(mesh, _vtk.vtkTable):\n437         arr = row_array(mesh, name)\n438         if arr is None and err:\n439             raise KeyError(f'Data array ({name}) not present in this dataset.')\n440         return FieldAssociation.ROW\n441 \n442     # with multiple arrays, return the array preference if possible\n443     parr = point_array(mesh, name)\n444     carr = cell_array(mesh, name)\n445     farr = field_array(mesh, name)\n446     arrays = [parr, carr, farr]\n447     preferences = [FieldAssociation.POINT, FieldAssociation.CELL, FieldAssociation.NONE]\n448     preference = parse_field_choice(preference)\n449     if preference not in preferences:\n450         raise ValueError(f'Data field ({preference}) not supported.')\n451 \n452     matches = [pref for pref, array in zip(preferences, arrays) if array is not None]\n453     # optionally raise if no match\n454     if not matches:\n455         if err:\n456             raise KeyError(f'Data array ({name}) not present in this dataset.')\n457         return FieldAssociation.NONE\n458     # use preference if it applies\n459     if preference in matches:\n460         return preference\n461     # otherwise return first in order of point -> cell -> field\n462     return matches[0]\n463 \n464 \n465 def vtk_points(points, deep=True, force_float=False):\n466     \"\"\"Convert numpy array or array-like to a ``vtkPoints`` object.\n467 \n468     Parameters\n469     ----------\n470     points : numpy.ndarray or sequence\n471         Points to convert.  Should be 1 or 2 dimensional.  Accepts a\n472         single point or several points.\n473 \n474     deep : bool, default: True\n475         Perform a deep copy of the array.  Only applicable if\n476         ``points`` is a :class:`numpy.ndarray`.\n477 \n478     force_float : bool, default: False\n479         Casts the datatype to ``float32`` if points datatype is\n480         non-float.  Set this to ``False`` to allow non-float types,\n481         though this may lead to truncation of intermediate floats\n482         when transforming datasets.\n483 \n484     Returns\n485     -------\n486     vtk.vtkPoints\n487         The vtkPoints object.\n488 \n489     Examples\n490     --------\n491     >>> import pyvista\n492     >>> import numpy as np\n493     >>> points = np.random.random((10, 3))\n494     >>> vpoints = pyvista.vtk_points(points)\n495     >>> vpoints  # doctest:+SKIP\n496     (vtkmodules.vtkCommonCore.vtkPoints)0x7f0c2e26af40\n497 \n498     \"\"\"\n499     points = np.asanyarray(points)\n500 \n501     # verify is numeric\n502     if not np.issubdtype(points.dtype, np.number):\n503         raise TypeError('Points must be a numeric type')\n504 \n505     if force_float:\n506         if not np.issubdtype(points.dtype, np.floating):\n507             warnings.warn(\n508                 'Points is not a float type. This can cause issues when '\n509                 'transforming or applying filters. Casting to '\n510                 '``np.float32``. Disable this by passing '\n511                 '``force_float=False``.'\n512             )\n513             points = points.astype(np.float32)\n514 \n515     # check dimensionality\n516     if points.ndim == 1:\n517         points = points.reshape(-1, 3)\n518     elif points.ndim > 2:\n519         raise ValueError(f'Dimension of ``points`` should be 1 or 2, not {points.ndim}')\n520 \n521     # verify shape\n522     if points.shape[1] != 3:\n523         raise ValueError(\n524             'Points array must contain three values per point. '\n525             f'Shape is {points.shape} and should be (X, 3)'\n526         )\n527 \n528     # use the underlying vtk data if present to avoid memory leaks\n529     if not deep and isinstance(points, pyvista.pyvista_ndarray):\n530         if points.VTKObject is not None:\n531             vtk_object = points.VTKObject\n532 \n533             # we can only use the underlying data if `points` is not a slice of\n534             # the VTK data object\n535             if vtk_object.GetSize() == points.size:\n536                 vtkpts = _vtk.vtkPoints()\n537                 vtkpts.SetData(points.VTKObject)\n538                 return vtkpts\n539             else:\n540                 deep = True\n541 \n542     # points must be contiguous\n543     points = np.require(points, requirements=['C'])\n544     vtkpts = _vtk.vtkPoints()\n545     vtk_arr = _vtk.numpy_to_vtk(points, deep=deep)\n546     vtkpts.SetData(vtk_arr)\n547 \n548     return vtkpts\n549 \n550 \n551 def line_segments_from_points(points):\n552     \"\"\"Generate non-connected line segments from points.\n553 \n554     Assumes points are ordered as line segments and an even number of\n555     points.\n556 \n557     Parameters\n558     ----------\n559     points : array_like[float]\n560         Points representing line segments. An even number must be\n561         given as every two vertices represent a single line\n562         segment. For example, two line segments would be represented\n563         as ``np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n564 \n565     Returns\n566     -------\n567     pyvista.PolyData\n568         PolyData with lines and cells.\n569 \n570     Examples\n571     --------\n572     This example plots two line segments at right angles to each other.\n573 \n574     >>> import pyvista\n575     >>> import numpy as np\n576     >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 0, 0], [1, 1, 0]])\n577     >>> lines = pyvista.line_segments_from_points(points)\n578     >>> lines.plot()\n579 \n580     \"\"\"\n581     if len(points) % 2 != 0:\n582         raise ValueError(\"An even number of points must be given to define each segment.\")\n583     # Assuming ordered points, create array defining line order\n584     n_points = len(points)\n585     n_lines = n_points // 2\n586     lines = np.c_[\n587         (\n588             2 * np.ones(n_lines, np.int_),\n589             np.arange(0, n_points - 1, step=2),\n590             np.arange(1, n_points + 1, step=2),\n591         )\n592     ]\n593     poly = pyvista.PolyData()\n594     poly.points = points\n595     poly.lines = lines\n596     return poly\n597 \n598 \n599 def lines_from_points(points, close=False):\n600     \"\"\"Make a connected line set given an array of points.\n601 \n602     Parameters\n603     ----------\n604     points : array_like[float]\n605         Points representing the vertices of the connected\n606         segments. For example, two line segments would be represented\n607         as ``np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])``.\n608 \n609     close : bool, default: False\n610         If ``True``, close the line segments into a loop.\n611 \n612     Returns\n613     -------\n614     pyvista.PolyData\n615         PolyData with lines and cells.\n616 \n617     Examples\n618     --------\n619     >>> import numpy as np\n620     >>> import pyvista\n621     >>> points = np.array([[0, 0, 0], [1, 0, 0], [1, 1, 0]])\n622     >>> poly = pyvista.lines_from_points(points)\n623     >>> poly.plot(line_width=5)\n624 \n625     \"\"\"\n626     poly = pyvista.PolyData()\n627     poly.points = points\n628     cells = np.full((len(points) - 1, 3), 2, dtype=np.int_)\n629     cells[:, 1] = np.arange(0, len(points) - 1, dtype=np.int_)\n630     cells[:, 2] = np.arange(1, len(points), dtype=np.int_)\n631     if close:\n632         cells = np.append(cells, [[2, len(points) - 1, 0]], axis=0)\n633     poly.lines = cells\n634     return poly\n635 \n636 \n637 def make_tri_mesh(points, faces):\n638     \"\"\"Construct a ``pyvista.PolyData`` mesh using points and faces arrays.\n639 \n640     Construct a mesh from an Nx3 array of points and an Mx3 array of\n641     triangle indices, resulting in a mesh with N vertices and M\n642     triangles.  This function does not require the standard VTK\n643     \"padding\" column and simplifies mesh creation.\n644 \n645     Parameters\n646     ----------\n647     points : np.ndarray\n648         Array of points with shape ``(N, 3)`` storing the vertices of the\n649         triangle mesh.\n650 \n651     faces : np.ndarray\n652         Array of indices with shape ``(M, 3)`` containing the triangle\n653         indices.\n654 \n655     Returns\n656     -------\n657     pyvista.PolyData\n658         PolyData instance containing the triangle mesh.\n659 \n660     Examples\n661     --------\n662     This example discretizes the unit square into a triangle mesh with\n663     nine vertices and eight faces.\n664 \n665     >>> import numpy as np\n666     >>> import pyvista\n667     >>> points = np.array(\n668     ...     [\n669     ...         [0, 0, 0],\n670     ...         [0.5, 0, 0],\n671     ...         [1, 0, 0],\n672     ...         [0, 0.5, 0],\n673     ...         [0.5, 0.5, 0],\n674     ...         [1, 0.5, 0],\n675     ...         [0, 1, 0],\n676     ...         [0.5, 1, 0],\n677     ...         [1, 1, 0],\n678     ...     ]\n679     ... )\n680     >>> faces = np.array(\n681     ...     [\n682     ...         [0, 1, 4],\n683     ...         [4, 7, 6],\n684     ...         [2, 5, 4],\n685     ...         [4, 5, 8],\n686     ...         [0, 4, 3],\n687     ...         [3, 4, 6],\n688     ...         [1, 2, 4],\n689     ...         [4, 8, 7],\n690     ...     ]\n691     ... )\n692     >>> tri_mesh = pyvista.make_tri_mesh(points, faces)\n693     >>> tri_mesh.plot(show_edges=True, line_width=5)\n694 \n695     \"\"\"\n696     if points.shape[1] != 3:\n697         raise ValueError(\"Points array should have shape (N, 3).\")\n698     if faces.ndim != 2 or faces.shape[1] != 3:\n699         raise ValueError(\"Face array should have shape (M, 3).\")\n700     cells = np.empty((faces.shape[0], 4), dtype=faces.dtype)\n701     cells[:, 0] = 3\n702     cells[:, 1:] = faces\n703     return pyvista.PolyData(points, cells)\n704 \n705 \n706 def vector_poly_data(orig, vec):\n707     \"\"\"Create a pyvista.PolyData object composed of vectors.\n708 \n709     Parameters\n710     ----------\n711     orig : array_like[float]\n712         Array of vector origins.\n713 \n714     vec : array_like[float]\n715         Array of vectors.\n716 \n717     Returns\n718     -------\n719     pyvista.PolyData\n720         Mesh containing the ``orig`` points along with the\n721         ``'vectors'`` and ``'mag'`` point arrays representing the\n722         vectors and magnitude of the vectors at each point.\n723 \n724     Examples\n725     --------\n726     Create basic vector field.  This is a point cloud where each point\n727     has a vector and magnitude attached to it.\n728 \n729     >>> import pyvista\n730     >>> import numpy as np\n731     >>> x, y = np.meshgrid(np.linspace(-5, 5, 10), np.linspace(-5, 5, 10))\n732     >>> points = np.vstack((x.ravel(), y.ravel(), np.zeros(x.size))).T\n733     >>> u = x / np.sqrt(x**2 + y**2)\n734     >>> v = y / np.sqrt(x**2 + y**2)\n735     >>> vectors = np.vstack(\n736     ...     (u.ravel() ** 3, v.ravel() ** 3, np.zeros(u.size))\n737     ... ).T\n738     >>> pdata = pyvista.vector_poly_data(points, vectors)\n739     >>> pdata.point_data.keys()\n740     ['vectors', 'mag']\n741 \n742     Convert these to arrows and plot it.\n743 \n744     >>> pdata.glyph(orient='vectors', scale='mag').plot()\n745 \n746     \"\"\"\n747     # shape, dimension checking\n748     if not isinstance(orig, np.ndarray):\n749         orig = np.asarray(orig)\n750 \n751     if not isinstance(vec, np.ndarray):\n752         vec = np.asarray(vec)\n753 \n754     if orig.ndim != 2:\n755         orig = orig.reshape((-1, 3))\n756     elif orig.shape[1] != 3:\n757         raise ValueError('orig array must be 3D')\n758 \n759     if vec.ndim != 2:\n760         vec = vec.reshape((-1, 3))\n761     elif vec.shape[1] != 3:\n762         raise ValueError('vec array must be 3D')\n763 \n764     # Create vtk points and cells objects\n765     vpts = _vtk.vtkPoints()\n766     vpts.SetData(_vtk.numpy_to_vtk(np.ascontiguousarray(orig), deep=True))\n767 \n768     npts = orig.shape[0]\n769     cells = np.empty((npts, 2), dtype=pyvista.ID_TYPE)\n770     cells[:, 0] = 1\n771     cells[:, 1] = np.arange(npts, dtype=pyvista.ID_TYPE)\n772     vcells = pyvista.utilities.cells.CellArray(cells, npts)\n773 \n774     # Create vtkPolyData object\n775     pdata = _vtk.vtkPolyData()\n776     pdata.SetPoints(vpts)\n777     pdata.SetVerts(vcells)\n778 \n779     # Add vectors to polydata\n780     name = 'vectors'\n781     vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(vec), deep=True)\n782     vtkfloat.SetName(name)\n783     pdata.GetPointData().AddArray(vtkfloat)\n784     pdata.GetPointData().SetActiveVectors(name)\n785 \n786     # Add magnitude of vectors to polydata\n787     name = 'mag'\n788     scalars = (vec * vec).sum(1) ** 0.5\n789     vtkfloat = _vtk.numpy_to_vtk(np.ascontiguousarray(scalars), deep=True)\n790     vtkfloat.SetName(name)\n791     pdata.GetPointData().AddArray(vtkfloat)\n792     pdata.GetPointData().SetActiveScalars(name)\n793 \n794     return pyvista.PolyData(pdata)\n795 \n796 \n797 def trans_from_matrix(matrix):  # pragma: no cover\n798     \"\"\"Convert a vtk matrix to a numpy.ndarray.\n799 \n800     DEPRECATED: Please use ``array_from_vtkmatrix``.\n801 \n802     \"\"\"\n803     # import needs to happen here to prevent a circular import\n804     from pyvista.core.errors import DeprecationError\n805 \n806     raise DeprecationError('DEPRECATED: Please use ``array_from_vtkmatrix``.')\n807 \n808 \n809 def array_from_vtkmatrix(matrix):\n810     \"\"\"Convert a vtk matrix to an array.\n811 \n812     Parameters\n813     ----------\n814     matrix : vtk.vtkMatrix3x3 | vtk.vtkMatrix4x4\n815         The vtk matrix to be converted to a ``numpy.ndarray``.\n816         Returned ndarray has shape (3, 3) or (4, 4) as appropriate.\n817 \n818     Returns\n819     -------\n820     numpy.ndarray\n821         Numpy array containing the data from ``matrix``.\n822 \n823     \"\"\"\n824     if isinstance(matrix, _vtk.vtkMatrix3x3):\n825         shape = (3, 3)\n826     elif isinstance(matrix, _vtk.vtkMatrix4x4):\n827         shape = (4, 4)\n828     else:\n829         raise TypeError(\n830             'Expected vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4 input,'\n831             f' got {type(matrix).__name__} instead.'\n832         )\n833     array = np.zeros(shape)\n834     for i in range(shape[0]):\n835         for j in range(shape[1]):\n836             array[i, j] = matrix.GetElement(i, j)\n837     return array\n838 \n839 \n840 def vtkmatrix_from_array(array):\n841     \"\"\"Convert a ``numpy.ndarray`` or array-like to a vtk matrix.\n842 \n843     Parameters\n844     ----------\n845     array : array_like[float]\n846         The array or array-like to be converted to a vtk matrix.\n847         Shape (3, 3) gets converted to a ``vtk.vtkMatrix3x3``, shape (4, 4)\n848         gets converted to a ``vtk.vtkMatrix4x4``. No other shapes are valid.\n849 \n850     Returns\n851     -------\n852     vtk.vtkMatrix3x3 or vtk.vtkMatrix4x4\n853         VTK matrix.\n854 \n855     \"\"\"\n856     array = np.asarray(array)\n857     if array.shape == (3, 3):\n858         matrix = _vtk.vtkMatrix3x3()\n859     elif array.shape == (4, 4):\n860         matrix = _vtk.vtkMatrix4x4()\n861     else:\n862         raise ValueError(f'Invalid shape {array.shape}, must be (3, 3) or (4, 4).')\n863     m, n = array.shape\n864     for i in range(m):\n865         for j in range(n):\n866             matrix.SetElement(i, j, array[i, j])\n867     return matrix\n868 \n869 \n870 def is_meshio_mesh(obj):\n871     \"\"\"Test if passed object is instance of ``meshio.Mesh``.\n872 \n873     Parameters\n874     ----------\n875     obj\n876         Any object.\n877 \n878     Returns\n879     -------\n880     bool\n881         ``True`` if ``obj`` is an ``meshio.Mesh``.\n882 \n883     \"\"\"\n884     try:\n885         import meshio\n886 \n887         return isinstance(obj, meshio.Mesh)\n888     except ImportError:\n889         return False\n890 \n891 \n892 def wrap(dataset):\n893     \"\"\"Wrap any given VTK data object to its appropriate PyVista data object.\n894 \n895     Other formats that are supported include:\n896 \n897     * 2D :class:`numpy.ndarray` of XYZ vertices\n898     * 3D :class:`numpy.ndarray` representing a volume. Values will be scalars.\n899     * 3D :class:`trimesh.Trimesh` mesh.\n900     * 3D :class:`meshio.Mesh` mesh.\n901 \n902     .. versionchanged:: 0.38.0\n903         If the passed object is already a wrapped PyVista object, then\n904         this is no-op and will return that object directly. In previous\n905         versions of PyVista, this would perform a shallow copy.\n906 \n907     Parameters\n908     ----------\n909     dataset : :class:`numpy.ndarray` | :class:`trimesh.Trimesh` | vtk.DataSet\n910         Dataset to wrap.\n911 \n912     Returns\n913     -------\n914     pyvista.DataSet\n915         The PyVista wrapped dataset.\n916 \n917     Examples\n918     --------\n919     Wrap a numpy array representing a random point cloud.\n920 \n921     >>> import numpy as np\n922     >>> import pyvista\n923     >>> points = np.random.random((10, 3))\n924     >>> cloud = pyvista.wrap(points)\n925     >>> cloud  # doctest:+SKIP\n926     PolyData (0x7fc52db83d70)\n927       N Cells:  10\n928       N Points: 10\n929       X Bounds: 1.123e-01, 7.457e-01\n930       Y Bounds: 1.009e-01, 9.877e-01\n931       Z Bounds: 2.346e-03, 9.640e-01\n932       N Arrays: 0\n933 \n934     Wrap a Trimesh object.\n935 \n936     >>> import trimesh\n937     >>> import pyvista\n938     >>> points = [[0, 0, 0], [0, 0, 1], [0, 1, 0]]\n939     >>> faces = [[0, 1, 2]]\n940     >>> tmesh = trimesh.Trimesh(points, faces=faces, process=False)\n941     >>> mesh = pyvista.wrap(tmesh)\n942     >>> mesh  # doctest:+SKIP\n943     PolyData (0x7fc55ff27ad0)\n944       N Cells:  1\n945       N Points: 3\n946       X Bounds: 0.000e+00, 0.000e+00\n947       Y Bounds: 0.000e+00, 1.000e+00\n948       Z Bounds: 0.000e+00, 1.000e+00\n949       N Arrays: 0\n950 \n951     Wrap a VTK object.\n952 \n953     >>> import pyvista\n954     >>> import vtk\n955     >>> points = vtk.vtkPoints()\n956     >>> p = [1.0, 2.0, 3.0]\n957     >>> vertices = vtk.vtkCellArray()\n958     >>> pid = points.InsertNextPoint(p)\n959     >>> _ = vertices.InsertNextCell(1)\n960     >>> _ = vertices.InsertCellPoint(pid)\n961     >>> point = vtk.vtkPolyData()\n962     >>> _ = point.SetPoints(points)\n963     >>> _ = point.SetVerts(vertices)\n964     >>> mesh = pyvista.wrap(point)\n965     >>> mesh  # doctest:+SKIP\n966     PolyData (0x7fc55ff27ad0)\n967       N Cells:  1\n968       N Points: 3\n969       X Bounds: 0.000e+00, 0.000e+00\n970       Y Bounds: 0.000e+00, 1.000e+00\n971       Z Bounds: 0.000e+00, 1.000e+00\n972       N Arrays: 0\n973 \n974     \"\"\"\n975     # Return if None\n976     if dataset is None:\n977         return\n978 \n979     if isinstance(dataset, tuple(pyvista._wrappers.values())):\n980         # Return object if it is already wrapped\n981         return dataset\n982 \n983     # Check if dataset is a numpy array.  We do this first since\n984     # pyvista_ndarray contains a VTK type that we don't want to\n985     # directly wrap.\n986     if isinstance(dataset, (np.ndarray, pyvista.pyvista_ndarray)):\n987         if dataset.ndim == 1 and dataset.shape[0] == 3:\n988             return pyvista.PolyData(dataset)\n989         if dataset.ndim > 1 and dataset.ndim < 3 and dataset.shape[1] == 3:\n990             return pyvista.PolyData(dataset)\n991         elif dataset.ndim == 3:\n992             mesh = pyvista.UniformGrid(dimensions=dataset.shape)\n993             if isinstance(dataset, pyvista.pyvista_ndarray):\n994                 # this gets rid of pesky VTK reference since we're raveling this\n995                 dataset = np.array(dataset, copy=False)\n996             mesh['values'] = dataset.ravel(order='F')\n997             mesh.active_scalars_name = 'values'\n998             return mesh\n999         else:\n1000             raise NotImplementedError('NumPy array could not be wrapped pyvista.')\n1001 \n1002     # wrap VTK arrays as pyvista_ndarray\n1003     if isinstance(dataset, _vtk.vtkDataArray):\n1004         return pyvista.pyvista_ndarray(dataset)\n1005 \n1006     # Check if a dataset is a VTK type\n1007     if hasattr(dataset, 'GetClassName'):\n1008         key = dataset.GetClassName()\n1009         try:\n1010             return pyvista._wrappers[key](dataset)\n1011         except KeyError:\n1012             raise TypeError(f'VTK data type ({key}) is not currently supported by pyvista.')\n1013         return\n1014 \n1015     # wrap meshio\n1016     if is_meshio_mesh(dataset):\n1017         return from_meshio(dataset)\n1018 \n1019     # wrap trimesh\n1020     if dataset.__class__.__name__ == 'Trimesh':\n1021         # trimesh doesn't pad faces\n1022         n_face = dataset.faces.shape[0]\n1023         faces = np.empty((n_face, 4), dataset.faces.dtype)\n1024         faces[:, 1:] = dataset.faces\n1025         faces[:, 0] = 3\n1026         polydata = pyvista.PolyData(np.asarray(dataset.vertices), faces)\n1027         # If the Trimesh object has uv, pass them to the PolyData\n1028         if hasattr(dataset.visual, 'uv'):\n1029             polydata.active_t_coords = np.asarray(dataset.visual.uv)\n1030         return polydata\n1031 \n1032     # otherwise, flag tell the user we can't wrap this object\n1033     raise NotImplementedError(f'Unable to wrap ({type(dataset)}) into a pyvista type.')\n1034 \n1035 \n1036 def image_to_texture(image):\n1037     \"\"\"Convert ``vtkImageData`` (:class:`pyvista.UniformGrid`) to a ``vtkTexture``.\n1038 \n1039     Parameters\n1040     ----------\n1041     image : pyvista.UniformGrid | vtkImageData\n1042         Image to convert.\n1043 \n1044     Returns\n1045     -------\n1046     vtkTexture\n1047         VTK texture.\n1048 \n1049     \"\"\"\n1050     return pyvista.Texture(image)\n1051 \n1052 \n1053 def numpy_to_texture(image):\n1054     \"\"\"Convert a NumPy image array to a vtk.vtkTexture.\n1055 \n1056     Parameters\n1057     ----------\n1058     image : numpy.ndarray\n1059         Numpy image array. Texture datatype expected to be ``np.uint8``.\n1060 \n1061     Returns\n1062     -------\n1063     pyvista.Texture\n1064         PyVista texture.\n1065 \n1066     Examples\n1067     --------\n1068     Create an all white texture.\n1069 \n1070     >>> import pyvista as pv\n1071     >>> import numpy as np\n1072     >>> tex_arr = np.ones((1024, 1024, 3), dtype=np.uint8) * 255\n1073     >>> tex = pv.numpy_to_texture(tex_arr)\n1074 \n1075     \"\"\"\n1076     if image.dtype != np.uint8:\n1077         image = image.astype(np.uint8)\n1078         warnings.warn(\n1079             'Expected `image` dtype to be ``np.uint8``. `image` has been copied '\n1080             'and converted to np.uint8.',\n1081             UserWarning,\n1082         )\n1083 \n1084     return pyvista.Texture(image)\n1085 \n1086 \n1087 def is_inside_bounds(point, bounds):\n1088     \"\"\"Check if a point is inside a set of bounds.\n1089 \n1090     This is implemented through recursion so that this is N-dimensional.\n1091 \n1092     Parameters\n1093     ----------\n1094     point : sequence[float]\n1095         Three item cartesian point (i.e. ``[x, y, z]``).\n1096 \n1097     bounds : sequence[float]\n1098         Six item bounds in the form of ``(xMin, xMax, yMin, yMax, zMin, zMax)``.\n1099 \n1100     Returns\n1101     -------\n1102     bool\n1103         ``True`` when ``point`` is inside ``bounds``.\n1104 \n1105     \"\"\"\n1106     if isinstance(point, (int, float)):\n1107         point = [point]\n1108     if isinstance(point, (np.ndarray, collections.abc.Sequence)) and not isinstance(\n1109         point, collections.deque\n1110     ):\n1111         if len(bounds) < 2 * len(point) or len(bounds) % 2 != 0:\n1112             raise ValueError('Bounds mismatch point dimensionality')\n1113         point = collections.deque(point)\n1114         bounds = collections.deque(bounds)\n1115         return is_inside_bounds(point, bounds)\n1116     if not isinstance(point, collections.deque):\n1117         raise TypeError(f'Unknown input data type ({type(point)}).')\n1118     if len(point) < 1:\n1119         return True\n1120     p = point.popleft()\n1121     lower, upper = bounds.popleft(), bounds.popleft()\n1122     if lower <= p <= upper:\n1123         return is_inside_bounds(point, bounds)\n1124     return False\n1125 \n1126 \n1127 def fit_plane_to_points(points, return_meta=False):\n1128     \"\"\"Fit a plane to a set of points using the SVD algorithm.\n1129 \n1130     Parameters\n1131     ----------\n1132     points : array_like[float]\n1133         Size ``[N x 3]`` sequence of points to fit a plane through.\n1134 \n1135     return_meta : bool, default: False\n1136         If ``True``, also returns the center and normal used to\n1137         generate the plane.\n1138 \n1139     Returns\n1140     -------\n1141     pyvista.PolyData\n1142         Plane mesh.\n1143 \n1144     numpy.ndarray\n1145         Plane center if ``return_meta=True``.\n1146 \n1147     numpy.ndarray\n1148         Plane normal if ``return_meta=True``.\n1149 \n1150     Examples\n1151     --------\n1152     Fit a plane to a random point cloud.\n1153 \n1154     >>> import pyvista\n1155     >>> import numpy as np\n1156     >>> cloud = np.random.random((10, 3))\n1157     >>> cloud[:, 2] *= 0.1\n1158     >>> plane, center, normal = pyvista.fit_plane_to_points(\n1159     ...     cloud, return_meta=True\n1160     ... )\n1161 \n1162     Plot the fitted plane.\n1163 \n1164     >>> pl = pyvista.Plotter()\n1165     >>> _ = pl.add_mesh(\n1166     ...     plane, color='tan', style='wireframe', line_width=4\n1167     ... )\n1168     >>> _ = pl.add_points(\n1169     ...     cloud,\n1170     ...     render_points_as_spheres=True,\n1171     ...     color='r',\n1172     ...     point_size=30,\n1173     ... )\n1174     >>> pl.show()\n1175 \n1176     \"\"\"\n1177     data = np.array(points)\n1178     center = data.mean(axis=0)\n1179     result = np.linalg.svd(data - center)\n1180     normal = np.cross(result[2][0], result[2][1])\n1181     plane = pyvista.Plane(center=center, direction=normal)\n1182     if return_meta:\n1183         return plane, center, normal\n1184     return plane\n1185 \n1186 \n1187 def raise_not_matching(scalars, dataset):\n1188     \"\"\"Raise exception about inconsistencies.\n1189 \n1190     Parameters\n1191     ----------\n1192     scalars : numpy.ndarray\n1193         Array of scalars.\n1194 \n1195     dataset : pyvista.DataSet\n1196         Dataset to check against.\n1197 \n1198     Raises\n1199     ------\n1200     ValueError\n1201         Raises a ValueError if the size of scalars does not the dataset.\n1202     \"\"\"\n1203     if isinstance(dataset, _vtk.vtkTable):\n1204         raise ValueError(\n1205             f'Number of scalars ({scalars.shape[0]}) must match number of rows ({dataset.n_rows}).'\n1206         )\n1207     raise ValueError(\n1208         f'Number of scalars ({scalars.shape[0]}) '\n1209         f'must match either the number of points ({dataset.n_points}) '\n1210         f'or the number of cells ({dataset.n_cells}).'\n1211     )\n1212 \n1213 \n1214 def generate_plane(normal, origin):\n1215     \"\"\"Return a _vtk.vtkPlane.\n1216 \n1217     Parameters\n1218     ----------\n1219     normal : sequence[float]\n1220         Three item sequence representing the normal of the plane.\n1221 \n1222     origin : sequence[float]\n1223         Three item sequence representing the origin of the plane.\n1224 \n1225     Returns\n1226     -------\n1227     vtk.vtkPlane\n1228         VTK plane.\n1229 \n1230     \"\"\"\n1231     plane = _vtk.vtkPlane()\n1232     # NORMAL MUST HAVE MAGNITUDE OF 1\n1233     normal = normal / np.linalg.norm(normal)\n1234     plane.SetNormal(normal)\n1235     plane.SetOrigin(origin)\n1236     return plane\n1237 \n1238 \n1239 def try_callback(func, *args):\n1240     \"\"\"Wrap a given callback in a try statement.\n1241 \n1242     Parameters\n1243     ----------\n1244     func : callable\n1245         Callable object.\n1246 \n1247     *args\n1248         Any arguments.\n1249 \n1250     \"\"\"\n1251     try:\n1252         func(*args)\n1253     except Exception:\n1254         etype, exc, tb = sys.exc_info()\n1255         stack = traceback.extract_tb(tb)[1:]\n1256         formatted_exception = 'Encountered issue in callback (most recent call last):\\n' + ''.join(\n1257             traceback.format_list(stack) + traceback.format_exception_only(etype, exc)\n1258         ).rstrip('\\n')\n1259         warnings.warn(formatted_exception)\n1260 \n1261 \n1262 def check_depth_peeling(number_of_peels=100, occlusion_ratio=0.0):\n1263     \"\"\"Check if depth peeling is available.\n1264 \n1265     Attempts to use depth peeling to see if it is available for the\n1266     current environment. Returns ``True`` if depth peeling is\n1267     available and has been successfully leveraged, otherwise\n1268     ``False``.\n1269 \n1270     Parameters\n1271     ----------\n1272     number_of_peels : int, default: 100\n1273         Maximum number of depth peels.\n1274 \n1275     occlusion_ratio : float, default: 0.0\n1276         Occlusion ratio.\n1277 \n1278     Returns\n1279     -------\n1280     bool\n1281         ``True`` when system supports depth peeling with the specified\n1282         settings.\n1283 \n1284     \"\"\"\n1285     # Try Depth Peeling with a basic scene\n1286     source = _vtk.vtkSphereSource()\n1287     mapper = _vtk.vtkPolyDataMapper()\n1288     mapper.SetInputConnection(source.GetOutputPort())\n1289     actor = _vtk.vtkActor()\n1290     actor.SetMapper(mapper)\n1291     # requires opacity < 1\n1292     actor.GetProperty().SetOpacity(0.5)\n1293     renderer = _vtk.vtkRenderer()\n1294     renderWindow = _vtk.vtkRenderWindow()\n1295     renderWindow.AddRenderer(renderer)\n1296     renderWindow.SetOffScreenRendering(True)\n1297     renderWindow.SetAlphaBitPlanes(True)\n1298     renderWindow.SetMultiSamples(0)\n1299     renderer.AddActor(actor)\n1300     renderer.SetUseDepthPeeling(True)\n1301     renderer.SetMaximumNumberOfPeels(number_of_peels)\n1302     renderer.SetOcclusionRatio(occlusion_ratio)\n1303     renderWindow.Render()\n1304     return renderer.GetLastRenderingUsedDepthPeeling() == 1\n1305 \n1306 \n1307 def threaded(fn):\n1308     \"\"\"Call a function using a thread.\n1309 \n1310     Parameters\n1311     ----------\n1312     fn : callable\n1313         Callable object.\n1314 \n1315     Returns\n1316     -------\n1317     function\n1318         Wrapped function.\n1319 \n1320     \"\"\"\n1321 \n1322     def wrapper(*args, **kwargs):\n1323         thread = Thread(target=fn, args=args, kwargs=kwargs)\n1324         thread.start()\n1325         return thread\n1326 \n1327     return wrapper\n1328 \n1329 \n1330 class conditional_decorator:\n1331     \"\"\"Conditional decorator for methods.\n1332 \n1333     Parameters\n1334     ----------\n1335     dec\n1336         Decorator\n1337     condition\n1338         Condition to match.\n1339 \n1340     \"\"\"\n1341 \n1342     def __init__(self, dec, condition):\n1343         \"\"\"Initialize.\"\"\"\n1344         self.decorator = dec\n1345         self.condition = condition\n1346 \n1347     def __call__(self, func):\n1348         \"\"\"Call the decorated function if condition is matched.\"\"\"\n1349         if not self.condition:\n1350             # Return the function unchanged, not decorated.\n1351             return func\n1352         return self.decorator(func)\n1353 \n1354 \n1355 class ProgressMonitor:\n1356     \"\"\"A standard class for monitoring the progress of a VTK algorithm.\n1357 \n1358     This must be use in a ``with`` context and it will block keyboard\n1359     interrupts from happening until the exit event as interrupts will crash\n1360     the kernel if the VTK algorithm is still executing.\n1361 \n1362     Parameters\n1363     ----------\n1364     algorithm\n1365         VTK algorithm or filter.\n1366 \n1367     message : str, default: \"\"\n1368         Message to display in the progress bar.\n1369 \n1370     scaling : float, optional\n1371         Unused keyword argument.\n1372 \n1373     \"\"\"\n1374 \n1375     def __init__(self, algorithm, message=\"\", scaling=None):\n1376         \"\"\"Initialize observer.\"\"\"\n1377         try:\n1378             from tqdm import tqdm  # noqa\n1379         except ImportError:\n1380             raise ImportError(\"Please install `tqdm` to monitor algorithms.\")\n1381         self.event_type = _vtk.vtkCommand.ProgressEvent\n1382         self.progress = 0.0\n1383         self._last_progress = self.progress\n1384         self.algorithm = algorithm\n1385         self.message = message\n1386         self._interrupt_signal_received = False\n1387         self._old_progress = 0\n1388         self._old_handler = None\n1389         self._progress_bar = None\n1390 \n1391     def handler(self, sig, frame):\n1392         \"\"\"Pass signal to custom interrupt handler.\"\"\"\n1393         self._interrupt_signal_received = (sig, frame)\n1394         logging.debug('SIGINT received. Delaying KeyboardInterrupt until VTK algorithm finishes.')\n1395 \n1396     def __call__(self, obj, event, *args):\n1397         \"\"\"Call progress update callback.\n1398 \n1399         On an event occurrence, this function executes.\n1400         \"\"\"\n1401         if self._interrupt_signal_received:\n1402             obj.AbortExecuteOn()\n1403         else:\n1404             progress = obj.GetProgress()\n1405             step = progress - self._old_progress\n1406             self._progress_bar.update(step)\n1407             self._old_progress = progress\n1408 \n1409     def __enter__(self):\n1410         \"\"\"Enter event for ``with`` context.\"\"\"\n1411         from tqdm import tqdm\n1412 \n1413         # check if in main thread\n1414         if threading.current_thread().__class__.__name__ == '_MainThread':\n1415             self._old_handler = signal.signal(signal.SIGINT, self.handler)\n1416         self._progress_bar = tqdm(\n1417             total=1, leave=True, bar_format='{l_bar}{bar}[{elapsed}<{remaining}]'\n1418         )\n1419         self._progress_bar.set_description(self.message)\n1420         self.algorithm.AddObserver(self.event_type, self)\n1421         return self._progress_bar\n1422 \n1423     def __exit__(self, type, value, traceback):\n1424         \"\"\"Exit event for ``with`` context.\"\"\"\n1425         self._progress_bar.total = 1\n1426         self._progress_bar.refresh()\n1427         self._progress_bar.close()\n1428         self.algorithm.RemoveObservers(self.event_type)\n1429         if threading.current_thread().__class__.__name__ == '_MainThread':\n1430             signal.signal(signal.SIGINT, self._old_handler)\n1431 \n1432 \n1433 def abstract_class(cls_):\n1434     \"\"\"Decorate a class, overriding __new__.\n1435 \n1436     Preventing a class from being instantiated similar to abc.ABCMeta\n1437     but does not require an abstract method.\n1438     \"\"\"\n1439 \n1440     def __new__(cls, *args, **kwargs):\n1441         if cls is cls_:\n1442             raise TypeError(f'{cls.__name__} is an abstract class and may not be instantiated.')\n1443         return object.__new__(cls)\n1444 \n1445     cls_.__new__ = __new__\n1446     return cls_\n1447 \n1448 \n1449 def axis_rotation(points, angle, inplace=False, deg=True, axis='z'):\n1450     \"\"\"Rotate points by angle about an axis.\n1451 \n1452     Parameters\n1453     ----------\n1454     points : numpy.ndarray\n1455         Array of points with shape ``(N, 3)``.\n1456 \n1457     angle : float\n1458         Rotation angle.\n1459 \n1460     inplace : bool, default: False\n1461         Updates points in-place while returning nothing.\n1462 \n1463     deg : bool, default: True\n1464         If ``True``, the angle is interpreted as degrees instead of\n1465         radians.\n1466 \n1467     axis : str, default: \"z\"\n1468         Name of axis to rotate about. Valid options are ``'x'``, ``'y'``,\n1469         and ``'z'``.\n1470 \n1471     Returns\n1472     -------\n1473     numpy.ndarray\n1474         Rotated points.\n1475 \n1476     Examples\n1477     --------\n1478     Rotate a set of points by 90 degrees about the x-axis in-place.\n1479 \n1480     >>> import numpy as np\n1481     >>> import pyvista\n1482     >>> from pyvista import examples\n1483     >>> points = examples.load_airplane().points\n1484     >>> points_orig = points.copy()\n1485     >>> pyvista.axis_rotation(points, 90, axis='x', deg=True, inplace=True)\n1486     >>> assert np.all(np.isclose(points[:, 0], points_orig[:, 0]))\n1487     >>> assert np.all(np.isclose(points[:, 1], -points_orig[:, 2]))\n1488     >>> assert np.all(np.isclose(points[:, 2], points_orig[:, 1]))\n1489     \"\"\"\n1490     axis = axis.lower()\n1491     axis_to_vec = {'x': (1, 0, 0), 'y': (0, 1, 0), 'z': (0, 0, 1)}\n1492 \n1493     if axis not in axis_to_vec:\n1494         raise ValueError('Invalid axis. Must be either \"x\", \"y\", or \"z\"')\n1495 \n1496     rot_mat = transformations.axis_angle_rotation(axis_to_vec[axis], angle, deg=deg)\n1497     return transformations.apply_transformation_to_points(rot_mat, points, inplace=inplace)\n1498 \n1499 \n1500 def cubemap(path='', prefix='', ext='.jpg'):\n1501     \"\"\"Construct a cubemap from 6 images from a directory.\n1502 \n1503     Each of the 6 images must be in the following format:\n1504 \n1505     - <prefix>negx<ext>\n1506     - <prefix>negy<ext>\n1507     - <prefix>negz<ext>\n1508     - <prefix>posx<ext>\n1509     - <prefix>posy<ext>\n1510     - <prefix>posz<ext>\n1511 \n1512     Prefix may be empty, and extension will default to ``'.jpg'``\n1513 \n1514     For example, if you have 6 images with the skybox2 prefix:\n1515 \n1516     - ``'skybox2-negx.jpg'``\n1517     - ``'skybox2-negy.jpg'``\n1518     - ``'skybox2-negz.jpg'``\n1519     - ``'skybox2-posx.jpg'``\n1520     - ``'skybox2-posy.jpg'``\n1521     - ``'skybox2-posz.jpg'``\n1522 \n1523     Parameters\n1524     ----------\n1525     path : str, default: \"\"\n1526         Directory containing the cubemap images.\n1527 \n1528     prefix : str, default: \"\"\n1529         Prefix to the filename.\n1530 \n1531     ext : str, default: \".jpg\"\n1532         The filename extension.  For example ``'.jpg'``.\n1533 \n1534     Returns\n1535     -------\n1536     pyvista.Texture\n1537         Texture with cubemap.\n1538 \n1539     Notes\n1540     -----\n1541     Cubemap will appear flipped relative to the XY plane between VTK v9.1 and\n1542     VTK v9.2.\n1543 \n1544     Examples\n1545     --------\n1546     Load a skybox given a directory, prefix, and file extension.\n1547 \n1548     >>> import pyvista\n1549     >>> skybox = pyvista.cubemap(\n1550     ...     'my_directory', 'skybox', '.jpeg'\n1551     ... )  # doctest:+SKIP\n1552 \n1553     \"\"\"\n1554     sets = ['posx', 'negx', 'posy', 'negy', 'posz', 'negz']\n1555     image_paths = [os.path.join(path, f'{prefix}{suffix}{ext}') for suffix in sets]\n1556     return _cubemap_from_paths(image_paths)\n1557 \n1558 \n1559 def cubemap_from_filenames(image_paths):\n1560     \"\"\"Construct a cubemap from 6 images.\n1561 \n1562     Images must be in the following order:\n1563 \n1564     - Positive X\n1565     - Negative X\n1566     - Positive Y\n1567     - Negative Y\n1568     - Positive Z\n1569     - Negative Z\n1570 \n1571     Parameters\n1572     ----------\n1573     image_paths : sequence[str]\n1574         Paths of the individual cubemap images.\n1575 \n1576     Returns\n1577     -------\n1578     pyvista.Texture\n1579         Texture with cubemap.\n1580 \n1581     Examples\n1582     --------\n1583     Load a skybox given a list of image paths.\n1584 \n1585     >>> image_paths = [\n1586     ...     '/home/user/_px.jpg',\n1587     ...     '/home/user/_nx.jpg',\n1588     ...     '/home/user/_py.jpg',\n1589     ...     '/home/user/_ny.jpg',\n1590     ...     '/home/user/_pz.jpg',\n1591     ...     '/home/user/_nz.jpg',\n1592     ... ]\n1593     >>> skybox = pyvista.cubemap(image_paths=image_paths)  # doctest:+SKIP\n1594 \n1595     \"\"\"\n1596     if len(image_paths) != 6:\n1597         raise ValueError(\"image_paths must contain 6 paths\")\n1598 \n1599     return _cubemap_from_paths(image_paths)\n1600 \n1601 \n1602 def _cubemap_from_paths(image_paths):\n1603     \"\"\"Construct a cubemap from image paths.\"\"\"\n1604     for image_path in image_paths:\n1605         if not os.path.isfile(image_path):\n1606             file_str = '\\n'.join(image_paths)\n1607             raise FileNotFoundError(\n1608                 f'Unable to locate {image_path}\\n'\n1609                 'Expected to find the following files:\\n'\n1610                 f'{file_str}'\n1611             )\n1612 \n1613     texture = pyvista.Texture()\n1614     texture.SetMipmap(True)\n1615     texture.SetInterpolate(True)\n1616     texture.cube_map = True  # Must be set prior to setting images\n1617 \n1618     # add each image to the cubemap\n1619     for i, fn in enumerate(image_paths):\n1620         image = pyvista.read(fn)\n1621         flip = _vtk.vtkImageFlip()\n1622         flip.SetInputDataObject(image)\n1623         flip.SetFilteredAxis(1)  # flip y axis\n1624         flip.Update()\n1625         texture.SetInputDataObject(i, flip.GetOutput())\n1626 \n1627     return texture\n1628 \n1629 \n1630 def set_default_active_vectors(mesh: 'pyvista.DataSet') -> None:\n1631     \"\"\"Set a default vectors array on mesh, if not already set.\n1632 \n1633     If an active vector already exists, no changes are made.\n1634 \n1635     If an active vectors does not exist, it checks for possibly cell\n1636     or point arrays with shape ``(n, 3)``.  If only one exists, then\n1637     it is set as the active vectors.  Otherwise, an error is raised.\n1638 \n1639     Parameters\n1640     ----------\n1641     mesh : pyvista.DataSet\n1642         Dataset to set default active vectors.\n1643 \n1644     Raises\n1645     ------\n1646     MissingDataError\n1647         If no vector-like arrays exist.\n1648 \n1649     AmbiguousDataError\n1650         If more than one vector-like arrays exist.\n1651 \n1652     \"\"\"\n1653     if mesh.active_vectors_name is not None:\n1654         return\n1655 \n1656     point_data = mesh.point_data\n1657     cell_data = mesh.cell_data\n1658 \n1659     possible_vectors_point = [\n1660         name for name, value in point_data.items() if value.ndim == 2 and value.shape[1] == 3\n1661     ]\n1662     possible_vectors_cell = [\n1663         name for name, value in cell_data.items() if value.ndim == 2 and value.shape[1] == 3\n1664     ]\n1665 \n1666     possible_vectors = possible_vectors_point + possible_vectors_cell\n1667     n_possible_vectors = len(possible_vectors)\n1668 \n1669     if n_possible_vectors == 1:\n1670         if len(possible_vectors_point) == 1:\n1671             preference = 'point'\n1672         else:\n1673             preference = 'cell'\n1674         mesh.set_active_vectors(possible_vectors[0], preference=preference)\n1675     elif n_possible_vectors < 1:\n1676         raise MissingDataError(\"No vector-like data available.\")\n1677     elif n_possible_vectors > 1:\n1678         raise AmbiguousDataError(\n1679             \"Multiple vector-like data available\\n\"\n1680             f\"cell data: {possible_vectors_cell}.\\n\"\n1681             f\"point data: {possible_vectors_point}.\\n\"\n1682             \"Set one as active using DataSet.set_active_vectors(name, preference=type)\"\n1683         )\n1684 \n1685 \n1686 def set_default_active_scalars(mesh: 'pyvista.DataSet') -> None:\n1687     \"\"\"Set a default scalars array on mesh, if not already set.\n1688 \n1689     If an active scalars already exists, no changes are made.\n1690 \n1691     If an active scalars does not exist, it checks for point or cell\n1692     arrays.  If only one exists, then it is set as the active scalars.\n1693     Otherwise, an error is raised.\n1694 \n1695     Parameters\n1696     ----------\n1697     mesh : pyvista.DataSet\n1698         Dataset to set default active scalars.\n1699 \n1700     Raises\n1701     ------\n1702     MissingDataError\n1703         If no arrays exist.\n1704 \n1705     AmbiguousDataError\n1706         If more than one array exists.\n1707 \n1708     \"\"\"\n1709     if mesh.active_scalars_name is not None:\n1710         return\n1711 \n1712     point_data = mesh.point_data\n1713     cell_data = mesh.cell_data\n1714 \n1715     possible_scalars_point = point_data.keys()\n1716     possible_scalars_cell = cell_data.keys()\n1717 \n1718     possible_scalars = possible_scalars_point + possible_scalars_cell\n1719     n_possible_scalars = len(possible_scalars)\n1720 \n1721     if n_possible_scalars == 1:\n1722         if len(possible_scalars_point) == 1:\n1723             preference = 'point'\n1724         else:\n1725             preference = 'cell'\n1726         mesh.set_active_scalars(possible_scalars[0], preference=preference)\n1727     elif n_possible_scalars < 1:\n1728         raise MissingDataError(\"No data available.\")\n1729     elif n_possible_scalars > 1:\n1730         raise AmbiguousDataError(\n1731             \"Multiple data available\\n\"\n1732             f\"cell data: {possible_scalars_cell}.\\n\"\n1733             f\"point data: {possible_scalars_point}.\\n\"\n1734             \"Set one as active using DataSet.set_active_scalars(name, preference=type)\"\n1735         )\n1736 \n[end of pyvista/utilities/helpers.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pyvista/core/filters/rectilinear_grid.py\n+++ b/pyvista/core/filters/rectilinear_grid.py\n@@ -16,7 +16,7 @@\n \n import numpy as np\n from pyvista import _vtk, abstract_class\n-from pyvista.core.filters import _get_output, _update_alg\n+from pyvista.core.filters import _get_output, _update_alg, check_valid_vector\n \n \n @abstract_class\n@@ -82,7 +82,7 @@\n \n         Examples\n         --------\n-        Divide a rectangular grid into tetrahedrons. Each cell contains by\n+        Divide a rectangular grid into tetrahedrons. Each cell contains by\n         default 5 tetrahedrons.\n \n         First, create and plot the grid.\n@@ -106,6 +106,12 @@\n         >>> grid = pv.RectilinearGrid(xrng, yrng, zrng)\n         >>> grid.plot()\n \n+        Now, let's ensure that the input sequences are valid.\n+        >>> check_valid_vector(xrng, 'xrng')\n+        >>> check_valid_vector(yrng, 'yrng')\n+        >>> check_valid_vector(zrng, 'zrng')\n+\n         Now, generate the tetrahedra plot in the exploded view of the cell.\n \n         >>> tet_grid = grid.to_tetrahedra()\n```", "model_patch": "--- a/pyvista/core/filters/rectilinear_grid.py\n+++ b/pyvista/core/filters/rectilinear_grid.py\n@@ -16,7 +16,7 @@\n \n import numpy as np\n from pyvista import _vtk, abstract_class\n-from pyvista.core.filters import _get_output, _update_alg\n+from pyvista.core.filters import _get_output, _update_alg, check_valid_vector\n \n \n @abstract_class\n@@ -82,7 +82,7 @@\n \n         Examples\n         --------\n-        Divide a rectangular grid into tetrahedrons. Each cell contains by\n+        Divide a rectangular grid into tetrahedrons. Each cell contains by\n         default 5 tetrahedrons.\n \n         First, create and plot the grid.\n@@ -106,6 +106,12 @@\n         >>> grid = pv.RectilinearGrid(xrng, yrng, zrng)\n         >>> grid.plot()\n \n+        Now, let's ensure that the input sequences are valid.\n+        >>> check_valid_vector(xrng, 'xrng')\n+        >>> check_valid_vector(yrng, 'yrng')\n+        >>> check_valid_vector(zrng, 'zrng')\n+\n         Now, generate the tetrahedra plot in the exploded view of the cell.\n \n         >>> tet_grid = grid.to_tetrahedra()\n"}
{"instance_id": "pydicom__pydicom-1255", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nMypy errors\n**Describe the bug**\r\nSeveral of the type hints are problematic and result in mypy errors.\r\n\r\nOne example:\r\n\r\n```none\r\ncat << EOF > /tmp/test.py\r\nfrom pydicom import Dataset, dcmread\r\n\r\ndataset = Dataset()\r\ndataset.Rows = 10\r\ndataset.Columns = 20\r\ndataset.NumberOfFrames = \"5\"\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\n\r\nfilename = '/tmp/test.dcm'\r\ndataset.save_as(str(filename))\r\n\r\ndataset = dcmread(filename)\r\n\r\nassert int(dataset.NumberOfFrames) == 5\r\nEOF\r\n```\r\n\r\n```none\r\nmypy /tmp/test.py\r\n/tmp/test.py:15: error: No overload variant of \"int\" matches argument type \"object\"\r\n/tmp/test.py:15: note: Possible overload variant:\r\n/tmp/test.py:15: note:     def int(self, x: Union[str, bytes, SupportsInt, _SupportsIndex] = ...) -> int\r\n/tmp/test.py:15: note:     <1 more non-matching overload not shown>\r\nFound 1 error in 1 file (checked 1 source file)\r\n```\r\n\r\n**Expected behavior**\r\nMypy should not report any errors.\r\n\r\n**Steps To Reproduce**\r\nSee above\r\n\r\n**Your environment**\r\n```none\r\npython -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.6-x86_64-i386-64bit\r\nPython       | 3.8.6 (default, Oct  8 2020, 14:06:32)  [Clang 12.0.0 (clang-1200.0.32.2)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.19.3\r\nPIL          | 8.0.1\r\n```\nImportError: cannot import name 'NoReturn'\n**Describe the bug**\r\nthrow following excetion when import pydicom package:\r\n```\r\nxxx/python3.6/site-packages/pydicom/filebase.py in <module>\r\n5 from struct import unpack, pack\r\n      6 from types import TracebackType\r\n----> 7 from typing import (\r\n      8     Tuple, Optional, NoReturn, BinaryIO, Callable, Type, Union, cast, TextIO,\r\n      9     TYPE_CHECKING, Any\r\n\r\nImportError: cannot import name 'NoReturn'\r\n```\r\n\r\n**Expected behavior**\r\nimort pydicom sucessfully\r\n\r\n**Steps To Reproduce**\r\nHow to reproduce the issue. Please include a minimum working code sample, the\r\ntraceback (if any) and the anonymized DICOM dataset (if relevant).\r\n\r\n**Your environment**\r\npython:3.6.0\r\npydicom:2.1\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)\n2 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)\n3 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)\n4 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)\n5 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3891702.svg)](https://doi.org/10.5281/zenodo.3891702)\n6 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n7 \n8 # *pydicom*\n9 \n10 *pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files. It lets you read, modify and write DICOM data in an easy \"pythonic\" way.\n11 \n12 As a pure Python package, *pydicom* can run anywhere Python runs without any other requirements, although if you're working with *Pixel Data* then we recommend you also install [NumPy](http://www.numpy.org).\n13 \n14 If you're looking for a Python library for DICOM networking then you might be interested in another of our projects: [pynetdicom](https://github.com/pydicom/pynetdicom).\n15 \n16 ## Installation\n17 \n18 Using [pip](https://pip.pypa.io/en/stable/):\n19 ```\n20 pip install pydicom\n21 ```\n22 Using [conda](https://docs.conda.io/en/latest/):\n23 ```\n24 conda install -c conda-forge pydicom\n25 ```\n26 \n27 For more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).\n28 \n29 \n30 ## Documentation\n31 \n32 The *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/old/pydicom_user_guide.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.\n33 \n34 ## *Pixel Data*\n35 \n36 Compressed and uncompressed *Pixel Data* is always available to\n37 be read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):\n38 ```python\n39 >>> from pydicom import dcmread\n40 >>> from pydicom.data import get_testdata_file\n41 >>> path = get_testdata_file(\"CT_small.dcm\")\n42 >>> ds = dcmread(path)\n43 >>> type(ds.PixelData)\n44 <class 'bytes'>\n45 >>> len(ds.PixelData)\n46 32768\n47 >>> ds.PixelData[:2]\n48 b'\\xaf\\x00'\n49 \n50 ```\n51 \n52 If [NumPy](http://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:\n53 \n54 ```python\n55 >>> arr = ds.pixel_array\n56 >>> arr.shape\n57 (128, 128)\n58 >>> arr\n59 array([[175, 180, 166, ..., 203, 207, 216],\n60        [186, 183, 157, ..., 181, 190, 239],\n61        [184, 180, 171, ..., 152, 164, 235],\n62        ...,\n63        [906, 910, 923, ..., 922, 929, 927],\n64        [914, 954, 938, ..., 942, 925, 905],\n65        [959, 955, 916, ..., 911, 904, 909]], dtype=int16)\n66 ```\n67 ### Compressed *Pixel Data*\n68 #### JPEG, JPEG-LS and JPEG 2000\n69 Converting JPEG compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/dev/old/image_data_handlers.html#guide-compressed).\n70 \n71 Compressing data into one of the JPEG formats is not currently supported.\n72 \n73 #### RLE\n74 RLE encoded *Pixel Data* only requires NumPy, and compression and decompression are both supported.\n75 \n76 ## Examples\n77 More [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.\n78 \n79 **Change a patient's ID**\n80 ```python\n81 from pydicom import dcmread\n82 \n83 ds = dcmread(\"/path/to/file.dcm\")\n84 # Edit the (0010,0020) 'Patient ID' element\n85 ds.PatientID = \"12345678\"\n86 ds.save_as(\"/path/to/file_updated.dcm\")\n87 ```\n88 \n89 **Display the Pixel Data**\n90 \n91 With [NumPy](http://www.numpy.org) and [matplotlib](https://matplotlib.org/)\n92 ```python\n93 import matplotlib.pyplot as plt\n94 from pydicom import dcmread\n95 from pydicom.data import get_testdata_file\n96 \n97 # The path to a pydicom test dataset\n98 path = get_testdata_file(\"CT_small.dcm\")\n99 ds = dcmread(path)\n100 # `arr` is a numpy.ndarray\n101 arr = ds.pixel_array\n102 \n103 plt.imshow(arr, cmap=\"gray\")\n104 plt.show()\n105 ```\n106 \n107 ## Contributing\n108 \n109 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).\n110 \n111 To contribute an example or extension of *pydicom* that doesn't belong with the core software, see our contribution repository:\n112 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).\n113 \n[end of README.md]\n[start of pydicom/pixel_data_handlers/util.py]\n1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n2 \"\"\"Utility functions used in the pixel data handlers.\"\"\"\n3 \n4 from struct import unpack\n5 from sys import byteorder\n6 from typing import Dict, Optional, Union, List, Tuple, TYPE_CHECKING, cast\n7 import warnings\n8 \n9 try:\n10     import numpy as np\n11     HAVE_NP = True\n12 except ImportError:\n13     HAVE_NP = False\n14 \n15 from pydicom.data import get_palette_files\n16 from pydicom.uid import UID\n17 \n18 if TYPE_CHECKING:\n19     from pydicom.dataset import Dataset\n20 \n21 \n22 def apply_color_lut(\n23     arr: \"np.ndarray\",\n24     ds: Optional[\"Dataset\"] = None,\n25     palette: Optional[Union[str, UID]] = None\n26 ) -> \"np.ndarray\":\n27     \"\"\"Apply a color palette lookup table to `arr`.\n28 \n29     .. versionadded:: 1.4\n30 \n31     If (0028,1201-1203) *Palette Color Lookup Table Data* are missing\n32     then (0028,1221-1223) *Segmented Palette Color Lookup Table Data* must be\n33     present and vice versa. The presence of (0028,1204) *Alpha Palette Color\n34     Lookup Table Data* or (0028,1224) *Alpha Segmented Palette Color Lookup\n35     Table Data* is optional.\n36 \n37     Use of this function with the :dcm:`Enhanced Palette Color Lookup Table\n38     Module<part03/sect_C.7.6.23.html>` or :dcm:`Supplemental Palette Color LUT\n39     Module<part03/sect_C.7.6.19.html>` is not currently supported.\n40 \n41     Parameters\n42     ----------\n43     arr : numpy.ndarray\n44         The pixel data to apply the color palette to.\n45     ds : dataset.Dataset, optional\n46         Required if `palette` is not supplied. A\n47         :class:`~pydicom.dataset.Dataset` containing a suitable\n48         :dcm:`Image Pixel<part03/sect_C.7.6.3.html>` or\n49         :dcm:`Palette Color Lookup Table<part03/sect_C.7.9.html>` Module.\n50     palette : str or uid.UID, optional\n51         Required if `ds` is not supplied. The name of one of the\n52         :dcm:`well-known<part06/chapter_B.html>` color palettes defined by the\n53         DICOM Standard. One of: ``'HOT_IRON'``, ``'PET'``,\n54         ``'HOT_METAL_BLUE'``, ``'PET_20_STEP'``, ``'SPRING'``, ``'SUMMER'``,\n55         ``'FALL'``, ``'WINTER'`` or the corresponding well-known (0008,0018)\n56         *SOP Instance UID*.\n57 \n58     Returns\n59     -------\n60     numpy.ndarray\n61         The RGB or RGBA pixel data as an array of ``np.uint8`` or ``np.uint16``\n62         values, depending on the 3rd value of (0028,1201) *Red Palette Color\n63         Lookup Table Descriptor*.\n64 \n65     References\n66     ----------\n67 \n68     * :dcm:`Image Pixel Module<part03/sect_C.7.6.3.html>`\n69     * :dcm:`Supplemental Palette Color LUT Module<part03/sect_C.7.6.19.html>`\n70     * :dcm:`Enhanced Palette Color LUT Module<part03/sect_C.7.6.23.html>`\n71     * :dcm:`Palette Colour LUT Module<part03/sect_C.7.9.html>`\n72     * :dcm:`Supplemental Palette Color LUTs\n73       <part03/sect_C.8.16.2.html#sect_C.8.16.2.1.1.1>`\n74     \"\"\"\n75     # Note: input value (IV) is the stored pixel value in `arr`\n76     # LUTs[IV] -> [R, G, B] values at the IV pixel location in `arr`\n77     if not ds and not palette:\n78         raise ValueError(\"Either 'ds' or 'palette' is required\")\n79 \n80     if palette:\n81         # Well-known palettes are all 8-bits per entry\n82         datasets = {\n83             '1.2.840.10008.1.5.1': 'hotiron.dcm',\n84             '1.2.840.10008.1.5.2': 'pet.dcm',\n85             '1.2.840.10008.1.5.3': 'hotmetalblue.dcm',\n86             '1.2.840.10008.1.5.4': 'pet20step.dcm',\n87             '1.2.840.10008.1.5.5': 'spring.dcm',\n88             '1.2.840.10008.1.5.6': 'summer.dcm',\n89             '1.2.840.10008.1.5.7': 'fall.dcm',\n90             '1.2.840.10008.1.5.8': 'winter.dcm',\n91         }\n92         if not UID(palette).is_valid:\n93             try:\n94                 uids = {\n95                     'HOT_IRON': '1.2.840.10008.1.5.1',\n96                     'PET': '1.2.840.10008.1.5.2',\n97                     'HOT_METAL_BLUE': '1.2.840.10008.1.5.3',\n98                     'PET_20_STEP': '1.2.840.10008.1.5.4',\n99                     'SPRING': '1.2.840.10008.1.5.5',\n100                     'SUMMER': '1.2.840.10008.1.5.6',\n101                     'FALL': '1.2.840.10008.1.5.8',\n102                     'WINTER': '1.2.840.10008.1.5.7',\n103                 }\n104                 palette = uids[palette]\n105             except KeyError:\n106                 raise ValueError(\"Unknown palette '{}'\".format(palette))\n107 \n108         try:\n109             from pydicom import dcmread\n110             fname = datasets[palette]\n111             ds = dcmread(get_palette_files(fname)[0])\n112         except KeyError:\n113             raise ValueError(\"Unknown palette '{}'\".format(palette))\n114 \n115     # C.8.16.2.1.1.1: Supplemental Palette Color LUT\n116     # TODO: Requires greyscale visualisation pipeline\n117     if getattr(ds, 'PixelPresentation', None) in ['MIXED', 'COLOR']:\n118         raise ValueError(\n119             \"Use of this function with the Supplemental Palette Color Lookup \"\n120             \"Table Module is not currently supported\"\n121         )\n122 \n123     if 'RedPaletteColorLookupTableDescriptor' not in ds:\n124         raise ValueError(\"No suitable Palette Color Lookup Table Module found\")\n125 \n126     # All channels are supposed to be identical\n127     lut_desc = ds.RedPaletteColorLookupTableDescriptor\n128     # A value of 0 = 2^16 entries\n129     nr_entries = lut_desc[0] or 2**16\n130 \n131     # May be negative if Pixel Representation is 1\n132     first_map = lut_desc[1]\n133     # Actual bit depth may be larger (8 bit entries in 16 bits allocated)\n134     nominal_depth = lut_desc[2]\n135     dtype = np.dtype('uint{:.0f}'.format(nominal_depth))\n136 \n137     luts = []\n138     if 'RedPaletteColorLookupTableData' in ds:\n139         # LUT Data is described by PS3.3, C.7.6.3.1.6\n140         r_lut = ds.RedPaletteColorLookupTableData\n141         g_lut = ds.GreenPaletteColorLookupTableData\n142         b_lut = ds.BluePaletteColorLookupTableData\n143         a_lut = getattr(ds, 'AlphaPaletteColorLookupTableData', None)\n144 \n145         actual_depth = len(r_lut) / nr_entries * 8\n146         dtype = np.dtype('uint{:.0f}'.format(actual_depth))\n147 \n148         for lut in [ii for ii in [r_lut, g_lut, b_lut, a_lut] if ii]:\n149             luts.append(np.frombuffer(lut, dtype=dtype))\n150     elif 'SegmentedRedPaletteColorLookupTableData' in ds:\n151         # Segmented LUT Data is described by PS3.3, C.7.9.2\n152         r_lut = ds.SegmentedRedPaletteColorLookupTableData\n153         g_lut = ds.SegmentedGreenPaletteColorLookupTableData\n154         b_lut = ds.SegmentedBluePaletteColorLookupTableData\n155         a_lut = getattr(ds, 'SegmentedAlphaPaletteColorLookupTableData', None)\n156 \n157         endianness = '<' if ds.is_little_endian else '>'\n158         byte_depth = nominal_depth // 8\n159         fmt = 'B' if byte_depth == 1 else 'H'\n160         actual_depth = nominal_depth\n161 \n162         for seg in [ii for ii in [r_lut, g_lut, b_lut, a_lut] if ii]:\n163             len_seg = len(seg) // byte_depth\n164             s_fmt = endianness + str(len_seg) + fmt\n165             lut = _expand_segmented_lut(unpack(s_fmt, seg), s_fmt)\n166             luts.append(np.asarray(lut, dtype=dtype))\n167     else:\n168         raise ValueError(\"No suitable Palette Color Lookup Table Module found\")\n169 \n170     if actual_depth not in [8, 16]:\n171         raise ValueError(\n172             \"The bit depth of the LUT data '{:.1f}' is invalid (only 8 or 16 \"\n173             \"bits per entry allowed)\".format(actual_depth)\n174         )\n175 \n176     lut_lengths = [len(ii) for ii in luts]\n177     if not all(ii == lut_lengths[0] for ii in lut_lengths[1:]):\n178         raise ValueError(\"LUT data must be the same length\")\n179 \n180     # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n181     clipped_iv = np.zeros(arr.shape, dtype=dtype)\n182     # IVs >= `first_map` are mapped by the Palette Color LUTs\n183     # `first_map` may be negative, positive or 0\n184     mapped_pixels = arr >= first_map\n185     clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n186     # IVs > number of entries get set to last entry\n187     np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n188 \n189     # Output array may be RGB or RGBA\n190     out = np.empty(list(arr.shape) + [len(luts)], dtype=dtype)\n191     for ii, lut in enumerate(luts):\n192         out[..., ii] = lut[clipped_iv]\n193 \n194     return out\n195 \n196 \n197 def apply_modality_lut(arr: \"np.ndarray\", ds: \"Dataset\") -> \"np.ndarray\":\n198     \"\"\"Apply a modality lookup table or rescale operation to `arr`.\n199 \n200     .. versionadded:: 1.4\n201 \n202     Parameters\n203     ----------\n204     arr : numpy.ndarray\n205         The :class:`~numpy.ndarray` to apply the modality LUT or rescale\n206         operation to.\n207     ds : dataset.Dataset\n208         A dataset containing a :dcm:`Modality LUT Module\n209         <part03/sect_C.11.html#sect_C.11.1>`.\n210 \n211     Returns\n212     -------\n213     numpy.ndarray\n214         An array with applied modality LUT or rescale operation. If\n215         (0028,3000) *Modality LUT Sequence* is present then returns an array\n216         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n217         (0028,3002) *LUT Descriptor*. If (0028,1052) *Rescale Intercept* and\n218         (0028,1053) *Rescale Slope* are present then returns an array of\n219         ``np.float64``. If neither are present then `arr` will be returned\n220         unchanged.\n221 \n222     Notes\n223     -----\n224     When *Rescale Slope* and *Rescale Intercept* are used, the output range\n225     is from (min. pixel value * Rescale Slope + Rescale Intercept) to\n226     (max. pixel value * Rescale Slope + Rescale Intercept), where min. and\n227     max. pixel value are determined from (0028,0101) *Bits Stored* and\n228     (0028,0103) *Pixel Representation*.\n229 \n230     References\n231     ----------\n232     * DICOM Standard, Part 3, :dcm:`Annex C.11.1\n233       <part03/sect_C.11.html#sect_C.11.1>`\n234     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n235       <part04/sect_N.2.html#sect_N.2.1.1>`\n236     \"\"\"\n237     if 'ModalityLUTSequence' in ds:\n238         item = ds.ModalityLUTSequence[0]\n239         nr_entries = item.LUTDescriptor[0] or 2**16\n240         first_map = item.LUTDescriptor[1]\n241         nominal_depth = item.LUTDescriptor[2]\n242 \n243         dtype = 'uint{}'.format(nominal_depth)\n244 \n245         # Ambiguous VR, US or OW\n246         if item['LUTData'].VR == 'OW':\n247             endianness = '<' if ds.is_little_endian else '>'\n248             unpack_fmt = '{}{}H'.format(endianness, nr_entries)\n249             lut_data = unpack(unpack_fmt, item.LUTData)\n250         else:\n251             lut_data = item.LUTData\n252         lut_data = np.asarray(lut_data, dtype=dtype)\n253 \n254         # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n255         clipped_iv = np.zeros(arr.shape, dtype=arr.dtype)\n256         # IVs >= `first_map` are mapped by the Modality LUT\n257         # `first_map` may be negative, positive or 0\n258         mapped_pixels = arr >= first_map\n259         clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n260         # IVs > number of entries get set to last entry\n261         np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n262 \n263         return lut_data[clipped_iv]\n264     elif 'RescaleSlope' in ds and 'RescaleIntercept' in ds:\n265         arr = arr.astype(np.float64) * ds.RescaleSlope\n266         arr += ds.RescaleIntercept\n267 \n268     return arr\n269 \n270 \n271 def apply_voi_lut(\n272     arr: \"np.ndarray\",\n273     ds: \"Dataset\",\n274     index: int = 0,\n275     prefer_lut: bool = True\n276 ) -> \"np.ndarray\":\n277     \"\"\"Apply a VOI lookup table or windowing operation to `arr`.\n278 \n279     .. versionadded:: 1.4\n280 \n281     .. versionchanged:: 2.1\n282 \n283         Added the `prefer_lut` keyword parameter\n284 \n285     Parameters\n286     ----------\n287     arr : numpy.ndarray\n288         The :class:`~numpy.ndarray` to apply the VOI LUT or windowing operation\n289         to.\n290     ds : dataset.Dataset\n291         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n292         If (0028,3010) *VOI LUT Sequence* is present then returns an array\n293         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n294         (0028,3002) *LUT Descriptor*. If (0028,1050) *Window Center* and\n295         (0028,1051) *Window Width* are present then returns an array of\n296         ``np.float64``. If neither are present then `arr` will be returned\n297         unchanged.\n298     index : int, optional\n299         When the VOI LUT Module contains multiple alternative views, this is\n300         the index of the view to return (default ``0``).\n301     prefer_lut : bool\n302         When the VOI LUT Module contains both *Window Width*/*Window Center*\n303         and *VOI LUT Sequence*, if ``True`` (default) then apply the VOI LUT,\n304         otherwise apply the windowing operation.\n305 \n306     Returns\n307     -------\n308     numpy.ndarray\n309         An array with applied VOI LUT or windowing operation.\n310 \n311     Notes\n312     -----\n313     When the dataset requires a modality LUT or rescale operation as part of\n314     the Modality LUT module then that must be applied before any windowing\n315     operation.\n316 \n317     See Also\n318     --------\n319     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n320     :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n321     :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n322 \n323     References\n324     ----------\n325     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n326       <part03/sect_C.11.html#sect_C.11.2>`\n327     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n328       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n329     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n330       <part04/sect_N.2.html#sect_N.2.1.1>`\n331     \"\"\"\n332     valid_voi = False\n333     if 'VOILUTSequence' in ds:\n334         valid_voi = None not in [\n335             ds.VOILUTSequence[0].get('LUTDescriptor', None),\n336             ds.VOILUTSequence[0].get('LUTData', None)\n337         ]\n338     valid_windowing = None not in [\n339         ds.get('WindowCenter', None),\n340         ds.get('WindowWidth', None)\n341     ]\n342 \n343     if valid_voi and valid_windowing:\n344         if prefer_lut:\n345             return apply_voi(arr, ds, index)\n346 \n347         return apply_windowing(arr, ds, index)\n348 \n349     if valid_voi:\n350         return apply_voi(arr, ds, index)\n351 \n352     if valid_windowing:\n353         return apply_windowing(arr, ds, index)\n354 \n355     return arr\n356 \n357 \n358 def apply_voi(\n359     arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n360 ) -> \"np.ndarray\":\n361     \"\"\"Apply a VOI lookup table to `arr`.\n362 \n363     .. versionadded:: 2.1\n364 \n365     Parameters\n366     ----------\n367     arr : numpy.ndarray\n368         The :class:`~numpy.ndarray` to apply the VOI LUT to.\n369     ds : dataset.Dataset\n370         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n371         If (0028,3010) *VOI LUT Sequence* is present then returns an array\n372         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n373         (0028,3002) *LUT Descriptor*, otherwise `arr` will be returned\n374         unchanged.\n375     index : int, optional\n376         When the VOI LUT Module contains multiple alternative views, this is\n377         the index of the view to return (default ``0``).\n378 \n379     Returns\n380     -------\n381     numpy.ndarray\n382         An array with applied VOI LUT.\n383 \n384     See Also\n385     --------\n386     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n387     :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n388 \n389     References\n390     ----------\n391     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n392       <part03/sect_C.11.html#sect_C.11.2>`\n393     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n394       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n395     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n396       <part04/sect_N.2.html#sect_N.2.1.1>`\n397     \"\"\"\n398     if \"VOILUTSequence\" not in ds:\n399         return arr\n400 \n401     if not np.issubdtype(arr.dtype, np.integer):\n402         warnings.warn(\n403             \"Applying a VOI LUT on a float input array may give \"\n404             \"incorrect results\"\n405         )\n406 \n407     # VOI LUT Sequence contains one or more items\n408     item = ds.VOILUTSequence[index]\n409     nr_entries = item.LUTDescriptor[0] or 2**16\n410     first_map = item.LUTDescriptor[1]\n411 \n412     # PS3.3 C.8.11.3.1.5: may be 8, 10-16\n413     nominal_depth = item.LUTDescriptor[2]\n414     if nominal_depth in list(range(10, 17)):\n415         dtype = 'uint16'\n416     elif nominal_depth == 8:\n417         dtype = 'uint8'\n418     else:\n419         raise NotImplementedError(\n420             f\"'{nominal_depth}' bits per LUT entry is not supported\"\n421         )\n422 \n423     # Ambiguous VR, US or OW\n424     if item['LUTData'].VR == 'OW':\n425         endianness = '<' if ds.is_little_endian else '>'\n426         unpack_fmt = f'{endianness}{nr_entries}H'\n427         lut_data = unpack(unpack_fmt, item.LUTData)\n428     else:\n429         lut_data = item.LUTData\n430     lut_data = np.asarray(lut_data, dtype=dtype)\n431 \n432     # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n433     clipped_iv = np.zeros(arr.shape, dtype=dtype)\n434     # IVs >= `first_map` are mapped by the VOI LUT\n435     # `first_map` may be negative, positive or 0\n436     mapped_pixels = arr >= first_map\n437     clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n438     # IVs > number of entries get set to last entry\n439     np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n440 \n441     return lut_data[clipped_iv]\n442 \n443 \n444 def apply_windowing(\n445     arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n446 ) -> \"np.ndarray\":\n447     \"\"\"Apply a windowing operation to `arr`.\n448 \n449     .. versionadded:: 2.1\n450 \n451     Parameters\n452     ----------\n453     arr : numpy.ndarray\n454         The :class:`~numpy.ndarray` to apply the windowing operation to.\n455     ds : dataset.Dataset\n456         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n457         If (0028,1050) *Window Center* and (0028,1051) *Window Width* are\n458         present then returns an array of ``np.float64``, otherwise `arr` will\n459         be returned unchanged.\n460     index : int, optional\n461         When the VOI LUT Module contains multiple alternative views, this is\n462         the index of the view to return (default ``0``).\n463 \n464     Returns\n465     -------\n466     numpy.ndarray\n467         An array with applied windowing operation.\n468 \n469     Notes\n470     -----\n471     When the dataset requires a modality LUT or rescale operation as part of\n472     the Modality LUT module then that must be applied before any windowing\n473     operation.\n474 \n475     See Also\n476     --------\n477     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n478     :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n479 \n480     References\n481     ----------\n482     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n483       <part03/sect_C.11.html#sect_C.11.2>`\n484     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n485       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n486     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n487       <part04/sect_N.2.html#sect_N.2.1.1>`\n488     \"\"\"\n489     if \"WindowWidth\" not in ds and \"WindowCenter\" not in ds:\n490         return arr\n491 \n492     if ds.PhotometricInterpretation not in ['MONOCHROME1', 'MONOCHROME2']:\n493         raise ValueError(\n494             \"When performing a windowing operation only 'MONOCHROME1' and \"\n495             \"'MONOCHROME2' are allowed for (0028,0004) Photometric \"\n496             \"Interpretation\"\n497         )\n498 \n499     # May be LINEAR (default), LINEAR_EXACT, SIGMOID or not present, VM 1\n500     voi_func = cast(str, getattr(ds, 'VOILUTFunction', 'LINEAR')).upper()\n501     # VR DS, VM 1-n\n502     elem = ds['WindowCenter']\n503     center = elem.value[index] if elem.VM > 1 else elem.value\n504     elem = ds['WindowWidth']\n505     width = elem.value[index] if elem.VM > 1 else elem.value\n506 \n507     # The output range depends on whether or not a modality LUT or rescale\n508     #   operation has been applied\n509     if 'ModalityLUTSequence' in ds:\n510         # Unsigned - see PS3.3 C.11.1.1.1\n511         y_min = 0\n512         bit_depth = ds.ModalityLUTSequence[0].LUTDescriptor[2]\n513         y_max = 2**bit_depth - 1\n514     elif ds.PixelRepresentation == 0:\n515         # Unsigned\n516         y_min = 0\n517         y_max = 2**ds.BitsStored - 1\n518     else:\n519         # Signed\n520         y_min = -2**(ds.BitsStored - 1)\n521         y_max = 2**(ds.BitsStored - 1) - 1\n522 \n523     slope = ds.get('RescaleSlope', None)\n524     intercept = ds.get('RescaleIntercept', None)\n525     if slope is not None and intercept is not None:\n526         # Otherwise its the actual data range\n527         y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\n528         y_max = y_max * ds.RescaleSlope + ds.RescaleIntercept\n529 \n530     y_range = y_max - y_min\n531     arr = arr.astype('float64')\n532 \n533     if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n534         # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n535         if voi_func == 'LINEAR':\n536             if width < 1:\n537                 raise ValueError(\n538                     \"The (0028,1051) Window Width must be greater than or \"\n539                     \"equal to 1 for a 'LINEAR' windowing operation\"\n540                 )\n541             center -= 0.5\n542             width -= 1\n543         elif width <= 0:\n544             raise ValueError(\n545                 \"The (0028,1051) Window Width must be greater than 0 \"\n546                 \"for a 'LINEAR_EXACT' windowing operation\"\n547             )\n548 \n549         below = arr <= (center - width / 2)\n550         above = arr > (center + width / 2)\n551         between = np.logical_and(~below, ~above)\n552 \n553         arr[below] = y_min\n554         arr[above] = y_max\n555         if between.any():\n556             arr[between] = (\n557                 ((arr[between] - center) / width + 0.5) * y_range + y_min\n558             )\n559     elif voi_func == 'SIGMOID':\n560         # PS3.3 C.11.2.1.3.1\n561         if width <= 0:\n562             raise ValueError(\n563                 \"The (0028,1051) Window Width must be greater than 0 \"\n564                 \"for a 'SIGMOID' windowing operation\"\n565             )\n566 \n567         arr = y_range / (1 + np.exp(-4 * (arr - center) / width)) + y_min\n568     else:\n569         raise ValueError(\n570             f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\"\n571         )\n572 \n573     return arr\n574 \n575 \n576 def convert_color_space(\n577     arr: \"np.ndarray\", current: str, desired: str\n578 ) -> \"np.ndarray\":\n579     \"\"\"Convert the image(s) in `arr` from one color space to another.\n580 \n581     .. versionchanged:: 1.4\n582 \n583         Added support for ``YBR_FULL_422``\n584 \n585     Parameters\n586     ----------\n587     arr : numpy.ndarray\n588         The image(s) as a :class:`numpy.ndarray` with\n589         :attr:`~numpy.ndarray.shape` (frames, rows, columns, 3)\n590         or (rows, columns, 3).\n591     current : str\n592         The current color space, should be a valid value for (0028,0004)\n593         *Photometric Interpretation*. One of ``'RGB'``, ``'YBR_FULL'``,\n594         ``'YBR_FULL_422'``.\n595     desired : str\n596         The desired color space, should be a valid value for (0028,0004)\n597         *Photometric Interpretation*. One of ``'RGB'``, ``'YBR_FULL'``,\n598         ``'YBR_FULL_422'``.\n599 \n600     Returns\n601     -------\n602     numpy.ndarray\n603         The image(s) converted to the desired color space.\n604 \n605     References\n606     ----------\n607 \n608     * DICOM Standard, Part 3,\n609       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n610     * ISO/IEC 10918-5:2012 (`ITU T.871\n611       <https://www.ijg.org/files/T-REC-T.871-201105-I!!PDF-E.pdf>`_),\n612       Section 7\n613     \"\"\"\n614     def _no_change(arr):\n615         return arr\n616 \n617     _converters = {\n618         'YBR_FULL_422': {\n619             'YBR_FULL_422': _no_change,\n620             'YBR_FULL': _no_change,\n621             'RGB': _convert_YBR_FULL_to_RGB,\n622         },\n623         'YBR_FULL': {\n624             'YBR_FULL': _no_change,\n625             'YBR_FULL_422': _no_change,\n626             'RGB': _convert_YBR_FULL_to_RGB,\n627         },\n628         'RGB': {\n629             'RGB': _no_change,\n630             'YBR_FULL': _convert_RGB_to_YBR_FULL,\n631             'YBR_FULL_422': _convert_RGB_to_YBR_FULL,\n632         }\n633     }\n634     try:\n635         converter = _converters[current][desired]\n636     except KeyError:\n637         raise NotImplementedError(\n638             \"Conversion from {0} to {1} is not supported.\"\n639             .format(current, desired)\n640         )\n641 \n642     return converter(arr)\n643 \n644 \n645 def _convert_RGB_to_YBR_FULL(arr: \"np.ndarray\") -> \"np.ndarray\":\n646     \"\"\"Return an ndarray converted from RGB to YBR_FULL color space.\n647 \n648     Parameters\n649     ----------\n650     arr : numpy.ndarray\n651         An ndarray of an 8-bit per channel images in RGB color space.\n652 \n653     Returns\n654     -------\n655     numpy.ndarray\n656         The array in YBR_FULL color space.\n657 \n658     References\n659     ----------\n660 \n661     * DICOM Standard, Part 3,\n662       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n663     * ISO/IEC 10918-5:2012 (`ITU T.871\n664       <https://www.ijg.org/files/T-REC-T.871-201105-I!!PDF-E.pdf>`_),\n665       Section 7\n666     \"\"\"\n667     orig_dtype = arr.dtype\n668 \n669     rgb_to_ybr = np.asarray(\n670         [[+0.299, -0.299 / 1.772, +0.701 / 1.402],\n671          [+0.587, -0.587 / 1.772, -0.587 / 1.402],\n672          [+0.114, +0.886 / 1.772, -0.114 / 1.402]],\n673         dtype=np.float\n674     )\n675 \n676     arr = np.dot(arr, rgb_to_ybr)\n677     arr += [0.5, 128.5, 128.5]\n678     # Round(x) -> floor of (arr + 0.5) : 0.5 added in previous step\n679     arr = np.floor(arr)\n680     # Max(0, arr) -> 0 if 0 >= arr, arr otherwise\n681     # Min(arr, 255) -> arr if arr <= 255, 255 otherwise\n682     arr = np.clip(arr, 0, 255)\n683 \n684     return arr.astype(orig_dtype)\n685 \n686 \n687 def _convert_YBR_FULL_to_RGB(arr: \"np.ndarray\") -> \"np.ndarray\":\n688     \"\"\"Return an ndarray converted from YBR_FULL to RGB color space.\n689 \n690     Parameters\n691     ----------\n692     arr : numpy.ndarray\n693         An ndarray of an 8-bit per channel images in YBR_FULL color space.\n694 \n695     Returns\n696     -------\n697     numpy.ndarray\n698         The array in RGB color space.\n699 \n700     References\n701     ----------\n702 \n703     * DICOM Standard, Part 3,\n704       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n705     * ISO/IEC 10918-5:2012, Section 7\n706     \"\"\"\n707     orig_dtype = arr.dtype\n708 \n709     ybr_to_rgb = np.asarray(\n710         [[1.000, 1.000, 1.000],\n711          [0.000, -0.114 * 1.772 / 0.587, 1.772],\n712          [1.402, -0.299 * 1.402 / 0.587, 0.000]],\n713         dtype=np.float\n714     )\n715 \n716     arr = arr.astype(np.float)\n717     arr -= [0, 128, 128]\n718     arr = np.dot(arr, ybr_to_rgb)\n719 \n720     # Round(x) -> floor of (arr + 0.5)\n721     arr = np.floor(arr + 0.5)\n722     # Max(0, arr) -> 0 if 0 >= arr, arr otherwise\n723     # Min(arr, 255) -> arr if arr <= 255, 255 otherwise\n724     arr = np.clip(arr, 0, 255)\n725 \n726     return arr.astype(orig_dtype)\n727 \n728 \n729 def dtype_corrected_for_endianness(\n730     is_little_endian: bool, numpy_dtype: \"np.dtype\"\n731 ) -> \"np.dtype\":\n732     \"\"\"Return a :class:`numpy.dtype` corrected for system and :class:`Dataset`\n733     endianness.\n734 \n735     Parameters\n736     ----------\n737     is_little_endian : bool\n738         The endianess of the affected :class:`~pydicom.dataset.Dataset`.\n739     numpy_dtype : numpy.dtype\n740         The numpy data type used for the *Pixel Data* without considering\n741         endianess.\n742 \n743     Raises\n744     ------\n745     ValueError\n746         If `is_little_endian` is ``None``, e.g. not initialized.\n747 \n748     Returns\n749     -------\n750     numpy.dtype\n751         The numpy data type used for the *Pixel Data* without considering\n752         endianess.\n753     \"\"\"\n754     if is_little_endian is None:\n755         raise ValueError(\"Dataset attribute 'is_little_endian' \"\n756                          \"has to be set before writing the dataset\")\n757 \n758     if is_little_endian != (byteorder == 'little'):\n759         return numpy_dtype.newbyteorder('S')\n760 \n761     return numpy_dtype\n762 \n763 \n764 def _expand_segmented_lut(\n765     data: Tuple[int, ...],\n766     fmt: str,\n767     nr_segments: Optional[int] = None,\n768     last_value: Optional[int] = None\n769 ) -> List[int]:\n770     \"\"\"Return a list containing the expanded lookup table data.\n771 \n772     Parameters\n773     ----------\n774     data : tuple of int\n775         The decoded segmented palette lookup table data. May be padded by a\n776         trailing null.\n777     fmt : str\n778         The format of the data, should contain `'B'` for 8-bit, `'H'` for\n779         16-bit, `'<'` for little endian and `'>'` for big endian.\n780     nr_segments : int, optional\n781         Expand at most `nr_segments` from the data. Should be used when\n782         the opcode is ``2`` (indirect). If used then `last_value` should also\n783         be used.\n784     last_value : int, optional\n785         The previous value in the expanded lookup table. Should be used when\n786         the opcode is ``2`` (indirect). If used then `nr_segments` should also\n787         be used.\n788 \n789     Returns\n790     -------\n791     list of int\n792         The reconstructed lookup table data.\n793 \n794     References\n795     ----------\n796 \n797     * DICOM Standard, Part 3, Annex C.7.9\n798     \"\"\"\n799     # Indirect segment byte offset is dependent on endianness for 8-bit\n800     # Little endian: e.g. 0x0302 0x0100, big endian, e.g. 0x0203 0x0001\n801     indirect_ii = [3, 2, 1, 0] if '<' in fmt else [2, 3, 0, 1]\n802 \n803     lut: List[int] = []\n804     offset = 0\n805     segments_read = 0\n806     # Use `offset + 1` to account for possible trailing null\n807     #   can do this because all segment types are longer than 2\n808     while offset + 1 < len(data):\n809         opcode = data[offset]\n810         length = data[offset + 1]\n811         offset += 2\n812 \n813         if opcode == 0:\n814             # C.7.9.2.1: Discrete segment\n815             lut.extend(data[offset:offset + length])\n816             offset += length\n817         elif opcode == 1:\n818             # C.7.9.2.2: Linear segment\n819             if lut:\n820                 y0 = lut[-1]\n821             elif last_value:\n822                 # Indirect segment with linear segment at 0th offset\n823                 y0 = last_value\n824             else:\n825                 raise ValueError(\n826                     \"Error expanding a segmented palette color lookup table: \"\n827                     \"the first segment cannot be a linear segment\"\n828                 )\n829 \n830             y1 = data[offset]\n831             offset += 1\n832 \n833             if y0 == y1:\n834                 lut.extend([y1] * length)\n835             else:\n836                 step = (y1 - y0) / length\n837                 vals = np.around(np.arange(y0 + step, y1 + step, step))\n838                 lut.extend([int(vv) for vv in vals])\n839         elif opcode == 2:\n840             # C.7.9.2.3: Indirect segment\n841             if not lut:\n842                 raise ValueError(\n843                     \"Error expanding a segmented palette color lookup table: \"\n844                     \"the first segment cannot be an indirect segment\"\n845                 )\n846 \n847             if 'B' in fmt:\n848                 # 8-bit segment entries\n849                 ii = [data[offset + vv] for vv in indirect_ii]\n850                 byte_offset = (ii[0] << 8 | ii[1]) << 16 | (ii[2] << 8 | ii[3])\n851                 offset += 4\n852             else:\n853                 # 16-bit segment entries\n854                 byte_offset = data[offset + 1] << 16 | data[offset]\n855                 offset += 2\n856 \n857             lut.extend(\n858                 _expand_segmented_lut(data[byte_offset:], fmt, length, lut[-1])\n859             )\n860         else:\n861             raise ValueError(\n862                 \"Error expanding a segmented palette lookup table: \"\n863                 \"unknown segment type '{}'\".format(opcode)\n864             )\n865 \n866         segments_read += 1\n867         if segments_read == nr_segments:\n868             return lut\n869 \n870     return lut\n871 \n872 \n873 def get_expected_length(ds: \"Dataset\", unit: str = 'bytes') -> int:\n874     \"\"\"Return the expected length (in terms of bytes or pixels) of the *Pixel\n875     Data*.\n876 \n877     +------------------------------------------------+-------------+\n878     | Element                                        | Required or |\n879     +-------------+---------------------------+------+ optional    |\n880     | Tag         | Keyword                   | Type |             |\n881     +=============+===========================+======+=============+\n882     | (0028,0002) | SamplesPerPixel           | 1    | Required    |\n883     +-------------+---------------------------+------+-------------+\n884     | (0028,0004) | PhotometricInterpretation | 1    | Required    |\n885     +-------------+---------------------------+------+-------------+\n886     | (0028,0008) | NumberOfFrames            | 1C   | Optional    |\n887     +-------------+---------------------------+------+-------------+\n888     | (0028,0010) | Rows                      | 1    | Required    |\n889     +-------------+---------------------------+------+-------------+\n890     | (0028,0011) | Columns                   | 1    | Required    |\n891     +-------------+---------------------------+------+-------------+\n892     | (0028,0100) | BitsAllocated             | 1    | Required    |\n893     +-------------+---------------------------+------+-------------+\n894 \n895     .. versionchanged:: 1.4\n896 \n897         Added support for a *Photometric Interpretation* of  ``YBR_FULL_422``\n898 \n899     Parameters\n900     ----------\n901     ds : Dataset\n902         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n903         and *Pixel Data*.\n904     unit : str, optional\n905         If ``'bytes'`` then returns the expected length of the *Pixel Data* in\n906         whole bytes and NOT including an odd length trailing NULL padding\n907         byte. If ``'pixels'`` then returns the expected length of the *Pixel\n908         Data* in terms of the total number of pixels (default ``'bytes'``).\n909 \n910     Returns\n911     -------\n912     int\n913         The expected length of the *Pixel Data* in either whole bytes or\n914         pixels, excluding the NULL trailing padding byte for odd length data.\n915     \"\"\"\n916     length: int = ds.Rows * ds.Columns * ds.SamplesPerPixel\n917     length *= get_nr_frames(ds)\n918 \n919     if unit == 'pixels':\n920         return length\n921 \n922     # Correct for the number of bytes per pixel\n923     bits_allocated = cast(int, ds.BitsAllocated)\n924     if bits_allocated == 1:\n925         # Determine the nearest whole number of bytes needed to contain\n926         #   1-bit pixel data. e.g. 10 x 10 1-bit pixels is 100 bits, which\n927         #   are packed into 12.5 -> 13 bytes\n928         length = length // 8 + (length % 8 > 0)\n929     else:\n930         length *= bits_allocated // 8\n931 \n932     # DICOM Standard, Part 4, Annex C.7.6.3.1.2\n933     if ds.PhotometricInterpretation == 'YBR_FULL_422':\n934         length = length // 3 * 2\n935 \n936     return length\n937 \n938 \n939 def get_image_pixel_ids(ds: \"Dataset\") -> Dict[str, int]:\n940     \"\"\"Return a dict of the pixel data affecting element's :func:`id` values.\n941 \n942     .. versionadded:: 1.4\n943 \n944     +------------------------------------------------+\n945     | Element                                        |\n946     +-------------+---------------------------+------+\n947     | Tag         | Keyword                   | Type |\n948     +=============+===========================+======+\n949     | (0028,0002) | SamplesPerPixel           | 1    |\n950     +-------------+---------------------------+------+\n951     | (0028,0004) | PhotometricInterpretation | 1    |\n952     +-------------+---------------------------+------+\n953     | (0028,0006) | PlanarConfiguration       | 1C   |\n954     +-------------+---------------------------+------+\n955     | (0028,0008) | NumberOfFrames            | 1C   |\n956     +-------------+---------------------------+------+\n957     | (0028,0010) | Rows                      | 1    |\n958     +-------------+---------------------------+------+\n959     | (0028,0011) | Columns                   | 1    |\n960     +-------------+---------------------------+------+\n961     | (0028,0100) | BitsAllocated             | 1    |\n962     +-------------+---------------------------+------+\n963     | (0028,0101) | BitsStored                | 1    |\n964     +-------------+---------------------------+------+\n965     | (0028,0103) | PixelRepresentation       | 1    |\n966     +-------------+---------------------------+------+\n967     | (7FE0,0008) | FloatPixelData            | 1C   |\n968     +-------------+---------------------------+------+\n969     | (7FE0,0009) | DoubleFloatPixelData      | 1C   |\n970     +-------------+---------------------------+------+\n971     | (7FE0,0010) | PixelData                 | 1C   |\n972     +-------------+---------------------------+------+\n973 \n974     Parameters\n975     ----------\n976     ds : Dataset\n977         The :class:`~pydicom.dataset.Dataset` containing the pixel data.\n978 \n979     Returns\n980     -------\n981     dict\n982         A dict containing the :func:`id` values for the elements that affect\n983         the pixel data.\n984 \n985     \"\"\"\n986     keywords = [\n987         'SamplesPerPixel', 'PhotometricInterpretation', 'PlanarConfiguration',\n988         'NumberOfFrames', 'Rows', 'Columns', 'BitsAllocated', 'BitsStored',\n989         'PixelRepresentation', 'FloatPixelData', 'DoubleFloatPixelData',\n990         'PixelData'\n991     ]\n992 \n993     return {kw: id(getattr(ds, kw, None)) for kw in keywords}\n994 \n995 \n996 def get_j2k_parameters(codestream: bytes) -> Dict[str, object]:\n997     \"\"\"Return a dict containing JPEG 2000 component parameters.\n998 \n999     .. versionadded:: 2.1\n1000 \n1001     Parameters\n1002     ----------\n1003     codestream : bytes\n1004         The JPEG 2000 (ISO/IEC 15444-1) codestream to be parsed.\n1005 \n1006     Returns\n1007     -------\n1008     dict\n1009         A dict containing parameters for the first component sample in the\n1010         JPEG 2000 `codestream`, or an empty dict if unable to parse the data.\n1011         Available parameters are ``{\"precision\": int, \"is_signed\": bool}``.\n1012     \"\"\"\n1013     try:\n1014         # First 2 bytes must be the SOC marker - if not then wrong format\n1015         if codestream[0:2] != b'\\xff\\x4f':\n1016             return {}\n1017 \n1018         # SIZ is required to be the second marker - Figure A-3 in 15444-1\n1019         if codestream[2:4] != b'\\xff\\x51':\n1020             return {}\n1021 \n1022         # See 15444-1 A.5.1 for format of the SIZ box and contents\n1023         ssiz = codestream[42]\n1024         if ssiz & 0x80:\n1025             return {\"precision\": (ssiz & 0x7F) + 1, \"is_signed\": True}\n1026 \n1027         return {\"precision\": ssiz + 1, \"is_signed\": False}\n1028     except (IndexError, TypeError):\n1029         pass\n1030 \n1031     return {}\n1032 \n1033 \n1034 def get_nr_frames(ds: \"Dataset\") -> int:\n1035     \"\"\"Return NumberOfFrames or 1 if NumberOfFrames is None.\n1036 \n1037     Parameters\n1038     ----------\n1039     ds : dataset.Dataset\n1040         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n1041         corresponding to the data in `arr`.\n1042 \n1043     Returns\n1044     -------\n1045     int\n1046         An integer for the NumberOfFrames or 1 if NumberOfFrames is None\n1047     \"\"\"\n1048     nr_frames: Optional[int] = getattr(ds, 'NumberOfFrames', 1)\n1049     # 'NumberOfFrames' may exist in the DICOM file but have value equal to None\n1050     if nr_frames is None:\n1051         warnings.warn(\"A value of None for (0028,0008) 'Number of Frames' is \"\n1052                       \"non-conformant. It's recommended that this value be \"\n1053                       \"changed to 1\")\n1054         nr_frames = 1\n1055 \n1056     return nr_frames\n1057 \n1058 \n1059 def pixel_dtype(ds: \"Dataset\", as_float: bool = False) -> \"np.dtype\":\n1060     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n1061 \n1062     Suitable for use with IODs containing the Image Pixel module (with\n1063     ``as_float=False``) and the Floating Point Image Pixel and Double Floating\n1064     Point Image Pixel modules (with ``as_float=True``).\n1065 \n1066     +------------------------------------------+------------------+\n1067     | Element                                  | Supported        |\n1068     +-------------+---------------------+------+ values           |\n1069     | Tag         | Keyword             | Type |                  |\n1070     +=============+=====================+======+==================+\n1071     | (0028,0101) | BitsAllocated       | 1    | 1, 8, 16, 32, 64 |\n1072     +-------------+---------------------+------+------------------+\n1073     | (0028,0103) | PixelRepresentation | 1    | 0, 1             |\n1074     +-------------+---------------------+------+------------------+\n1075 \n1076     .. versionchanged:: 1.4\n1077 \n1078         Added `as_float` keyword parameter and support for float dtypes.\n1079 \n1080 \n1081     Parameters\n1082     ----------\n1083     ds : Dataset\n1084         The :class:`~pydicom.dataset.Dataset` containing the pixel data you\n1085         wish to get the data type for.\n1086     as_float : bool, optional\n1087         If ``True`` then return a float dtype, otherwise return an integer\n1088         dtype (default ``False``). Float dtypes are only supported when\n1089         (0028,0101) *Bits Allocated* is 32 or 64.\n1090 \n1091     Returns\n1092     -------\n1093     numpy.dtype\n1094         A :class:`numpy.dtype` suitable for containing the pixel data.\n1095 \n1096     Raises\n1097     ------\n1098     NotImplementedError\n1099         If the pixel data is of a type that isn't supported by either numpy\n1100         or *pydicom*.\n1101     \"\"\"\n1102     if not HAVE_NP:\n1103         raise ImportError(\"Numpy is required to determine the dtype.\")\n1104 \n1105     if ds.is_little_endian is None:\n1106         ds.is_little_endian = ds.file_meta.TransferSyntaxUID.is_little_endian\n1107 \n1108     if not as_float:\n1109         # (0028,0103) Pixel Representation, US, 1\n1110         #   Data representation of the pixel samples\n1111         #   0x0000 - unsigned int\n1112         #   0x0001 - 2's complement (signed int)\n1113         pixel_repr = ds.PixelRepresentation\n1114         if pixel_repr == 0:\n1115             dtype_str = 'uint'\n1116         elif pixel_repr == 1:\n1117             dtype_str = 'int'\n1118         else:\n1119             raise ValueError(\n1120                 \"Unable to determine the data type to use to contain the \"\n1121                 \"Pixel Data as a value of '{}' for '(0028,0103) Pixel \"\n1122                 \"Representation' is invalid\".format(pixel_repr)\n1123             )\n1124     else:\n1125         dtype_str = 'float'\n1126 \n1127     # (0028,0100) Bits Allocated, US, 1\n1128     #   The number of bits allocated for each pixel sample\n1129     #   PS3.5 8.1.1: Bits Allocated shall either be 1 or a multiple of 8\n1130     #   For bit packed data we use uint8\n1131     bits_allocated = ds.BitsAllocated\n1132     if bits_allocated == 1:\n1133         dtype_str = 'uint8'\n1134     elif bits_allocated > 0 and bits_allocated % 8 == 0:\n1135         dtype_str += str(bits_allocated)\n1136     else:\n1137         raise ValueError(\n1138             \"Unable to determine the data type to use to contain the \"\n1139             \"Pixel Data as a value of '{}' for '(0028,0100) Bits \"\n1140             \"Allocated' is invalid\".format(bits_allocated)\n1141         )\n1142 \n1143     # Check to see if the dtype is valid for numpy\n1144     try:\n1145         dtype = np.dtype(dtype_str)\n1146     except TypeError:\n1147         raise NotImplementedError(\n1148             \"The data type '{}' needed to contain the Pixel Data is not \"\n1149             \"supported by numpy\".format(dtype_str)\n1150         )\n1151 \n1152     # Correct for endianness of the system vs endianness of the dataset\n1153     if ds.is_little_endian != (byteorder == 'little'):\n1154         # 'S' swap from current to opposite\n1155         dtype = dtype.newbyteorder('S')\n1156 \n1157     return dtype\n1158 \n1159 \n1160 def reshape_pixel_array(ds: \"Dataset\", arr: \"np.ndarray\") -> \"np.ndarray\":\n1161     \"\"\"Return a reshaped :class:`numpy.ndarray` `arr`.\n1162 \n1163     +------------------------------------------+-----------+----------+\n1164     | Element                                  | Supported |          |\n1165     +-------------+---------------------+------+ values    |          |\n1166     | Tag         | Keyword             | Type |           |          |\n1167     +=============+=====================+======+===========+==========+\n1168     | (0028,0002) | SamplesPerPixel     | 1    | N > 0     | Required |\n1169     +-------------+---------------------+------+-----------+----------+\n1170     | (0028,0006) | PlanarConfiguration | 1C   | 0, 1      | Optional |\n1171     +-------------+---------------------+------+-----------+----------+\n1172     | (0028,0008) | NumberOfFrames      | 1C   | N > 0     | Optional |\n1173     +-------------+---------------------+------+-----------+----------+\n1174     | (0028,0010) | Rows                | 1    | N > 0     | Required |\n1175     +-------------+---------------------+------+-----------+----------+\n1176     | (0028,0011) | Columns             | 1    | N > 0     | Required |\n1177     +-------------+---------------------+------+-----------+----------+\n1178 \n1179     (0028,0008) *Number of Frames* is required when *Pixel Data* contains\n1180     more than 1 frame. (0028,0006) *Planar Configuration* is required when\n1181     (0028,0002) *Samples per Pixel* is greater than 1. For certain\n1182     compressed transfer syntaxes it is always taken to be either 0 or 1 as\n1183     shown in the table below.\n1184 \n1185     +---------------------------------------------+-----------------------+\n1186     | Transfer Syntax                             | Planar Configuration  |\n1187     +------------------------+--------------------+                       |\n1188     | UID                    | Name               |                       |\n1189     +========================+====================+=======================+\n1190     | 1.2.840.10008.1.2.4.50 | JPEG Baseline      | 0                     |\n1191     +------------------------+--------------------+-----------------------+\n1192     | 1.2.840.10008.1.2.4.57 | JPEG Lossless,     | 0                     |\n1193     |                        | Non-hierarchical   |                       |\n1194     +------------------------+--------------------+-----------------------+\n1195     | 1.2.840.10008.1.2.4.70 | JPEG Lossless,     | 0                     |\n1196     |                        | Non-hierarchical,  |                       |\n1197     |                        | SV1                |                       |\n1198     +------------------------+--------------------+-----------------------+\n1199     | 1.2.840.10008.1.2.4.80 | JPEG-LS Lossless   | 0                     |\n1200     +------------------------+--------------------+-----------------------+\n1201     | 1.2.840.10008.1.2.4.81 | JPEG-LS Lossy      | 0                     |\n1202     +------------------------+--------------------+-----------------------+\n1203     | 1.2.840.10008.1.2.4.90 | JPEG 2000 Lossless | 0                     |\n1204     +------------------------+--------------------+-----------------------+\n1205     | 1.2.840.10008.1.2.4.91 | JPEG 2000 Lossy    | 0                     |\n1206     +------------------------+--------------------+-----------------------+\n1207     | 1.2.840.10008.1.2.5    | RLE Lossless       | 1                     |\n1208     +------------------------+--------------------+-----------------------+\n1209 \n1210     .. versionchanged:: 2.1\n1211 \n1212         JPEG-LS transfer syntaxes changed to *Planar Configuration* of 0\n1213 \n1214     Parameters\n1215     ----------\n1216     ds : dataset.Dataset\n1217         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n1218         corresponding to the data in `arr`.\n1219     arr : numpy.ndarray\n1220         The 1D array containing the pixel data.\n1221 \n1222     Returns\n1223     -------\n1224     numpy.ndarray\n1225         A reshaped array containing the pixel data. The shape of the array\n1226         depends on the contents of the dataset:\n1227 \n1228         * For single frame, single sample data (rows, columns)\n1229         * For single frame, multi-sample data (rows, columns, planes)\n1230         * For multi-frame, single sample data (frames, rows, columns)\n1231         * For multi-frame, multi-sample data (frames, rows, columns, planes)\n1232 \n1233     References\n1234     ----------\n1235 \n1236     * DICOM Standard, Part 3,\n1237       :dcm:`Annex C.7.6.3.1<part03/sect_C.7.6.3.html#sect_C.7.6.3.1>`\n1238     * DICOM Standard, Part 5, :dcm:`Section 8.2<part05/sect_8.2.html>`\n1239     \"\"\"\n1240     if not HAVE_NP:\n1241         raise ImportError(\"Numpy is required to reshape the pixel array.\")\n1242 \n1243     nr_frames = get_nr_frames(ds)\n1244     nr_samples = ds.SamplesPerPixel\n1245 \n1246     if nr_frames < 1:\n1247         raise ValueError(\n1248             \"Unable to reshape the pixel array as a value of {} for \"\n1249             \"(0028,0008) 'Number of Frames' is invalid.\"\n1250             .format(nr_frames)\n1251         )\n1252 \n1253     if nr_samples < 1:\n1254         raise ValueError(\n1255             \"Unable to reshape the pixel array as a value of {} for \"\n1256             \"(0028,0002) 'Samples per Pixel' is invalid.\"\n1257             .format(nr_samples)\n1258         )\n1259 \n1260     # Valid values for Planar Configuration are dependent on transfer syntax\n1261     if nr_samples > 1:\n1262         transfer_syntax = ds.file_meta.TransferSyntaxUID\n1263         if transfer_syntax in ['1.2.840.10008.1.2.4.50',\n1264                                '1.2.840.10008.1.2.4.57',\n1265                                '1.2.840.10008.1.2.4.70',\n1266                                '1.2.840.10008.1.2.4.80',\n1267                                '1.2.840.10008.1.2.4.81',\n1268                                '1.2.840.10008.1.2.4.90',\n1269                                '1.2.840.10008.1.2.4.91']:\n1270             planar_configuration = 0\n1271         elif transfer_syntax in ['1.2.840.10008.1.2.5']:\n1272             planar_configuration = 1\n1273         else:\n1274             planar_configuration = ds.PlanarConfiguration\n1275 \n1276         if planar_configuration not in [0, 1]:\n1277             raise ValueError(\n1278                 \"Unable to reshape the pixel array as a value of {} for \"\n1279                 \"(0028,0006) 'Planar Configuration' is invalid.\"\n1280                 .format(planar_configuration)\n1281             )\n1282 \n1283     if nr_frames > 1:\n1284         # Multi-frame\n1285         if nr_samples == 1:\n1286             # Single plane\n1287             arr = arr.reshape(nr_frames, ds.Rows, ds.Columns)\n1288         else:\n1289             # Multiple planes, usually 3\n1290             if planar_configuration == 0:\n1291                 arr = arr.reshape(nr_frames, ds.Rows, ds.Columns, nr_samples)\n1292             else:\n1293                 arr = arr.reshape(nr_frames, nr_samples, ds.Rows, ds.Columns)\n1294                 arr = arr.transpose(0, 2, 3, 1)\n1295     else:\n1296         # Single frame\n1297         if nr_samples == 1:\n1298             # Single plane\n1299             arr = arr.reshape(ds.Rows, ds.Columns)\n1300         else:\n1301             # Multiple planes, usually 3\n1302             if planar_configuration == 0:\n1303                 arr = arr.reshape(ds.Rows, ds.Columns, nr_samples)\n1304             else:\n1305                 arr = arr.reshape(nr_samples, ds.Rows, ds.Columns)\n1306                 arr = arr.transpose(1, 2, 0)\n1307 \n1308     return arr\n1309 \n[end of pydicom/pixel_data_handlers/util.py]\n[start of pydicom/valuerep.py]\n1 # Copyright 2008-2020 pydicom authors. See LICENSE file for details.\n2 \"\"\"Special classes for DICOM value representations (VR)\"\"\"\n3 \n4 import datetime\n5 from decimal import Decimal\n6 import platform\n7 import re\n8 import sys\n9 from typing import (\n10     TypeVar, Type, Tuple, Optional, List, Dict, Union, Any, Generator, AnyStr,\n11     Callable, Iterator, overload\n12 )\n13 from typing import Sequence as SequenceType\n14 import warnings\n15 \n16 # don't import datetime_conversion directly\n17 from pydicom import config\n18 from pydicom.multival import MultiValue\n19 from pydicom.uid import UID\n20 \n21 \n22 # Types\n23 _T = TypeVar('_T')\n24 _DA = TypeVar(\"_DA\", bound=\"DA\")\n25 _DT = TypeVar(\"_DT\", bound=\"DT\")\n26 _TM = TypeVar(\"_TM\", bound=\"TM\")\n27 _IS = TypeVar(\"_IS\", bound=\"IS\")\n28 _DSfloat = TypeVar(\"_DSfloat\", bound=\"DSfloat\")\n29 _DSdecimal = TypeVar(\"_DSdecimal\", bound=\"DSdecimal\")\n30 _PersonName = TypeVar(\"_PersonName\", bound=\"PersonName\")\n31 \n32 # can't import from charset or get circular import\n33 default_encoding = \"iso8859\"\n34 \n35 # For reading/writing data elements,\n36 # these ones have longer explicit VR format\n37 # Taken from PS3.5 Section 7.1.2\n38 extra_length_VRs = ('OB', 'OD', 'OF', 'OL', 'OW', 'SQ', 'UC', 'UN', 'UR', 'UT')\n39 \n40 # VRs that can be affected by character repertoire\n41 # in (0008,0005) Specific Character Set\n42 # See PS-3.5 (2011), section 6.1.2 Graphic Characters\n43 # and PN, but it is handled separately.\n44 text_VRs: Tuple[str, ...] = ('SH', 'LO', 'ST', 'LT', 'UC', 'UT')\n45 \n46 # Delimiters for text strings and person name that reset the encoding.\n47 # See PS3.5, Section 6.1.2.5.3\n48 # Note: We use character codes for Python 3\n49 # because those are the types yielded if iterating over a byte string.\n50 \n51 # Characters/Character codes for text VR delimiters: LF, CR, TAB, FF\n52 TEXT_VR_DELIMS = {0x0d, 0x0a, 0x09, 0x0c}\n53 \n54 # Character/Character code for PN delimiter: name part separator '^'\n55 # (the component separator '=' is handled separately)\n56 PN_DELIMS = {0xe5}\n57 \n58 \n59 class _DateTimeBase:\n60     \"\"\"Base class for DT, DA and TM element sub-classes.\"\"\"\n61     # Add pickling support for the mutable additions\n62     def __getstate__(self) -> Dict[str, Any]:\n63         return self.__dict__.copy()\n64 \n65     def __setstate__(self, state: Dict[str, Any]) -> None:\n66         self.__dict__.update(state)\n67 \n68     def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n69         return super().__reduce_ex__(protocol) + (self.__getstate__(),)\n70 \n71     def __str__(self) -> str:\n72         if hasattr(self, 'original_string'):\n73             return self.original_string\n74 \n75         return super().__str__()\n76 \n77     def __repr__(self) -> str:\n78         return f'\"{str(self)}\"'\n79 \n80 \n81 class DA(_DateTimeBase, datetime.date):\n82     \"\"\"Store value for an element with VR **DA** as :class:`datetime.date`.\n83 \n84     Note that the :class:`datetime.date` base class is immutable.\n85     \"\"\"\n86     def __new__(\n87         cls: Type[_DA], val: Union[None, str, _DA, datetime.date]\n88     ) -> Optional[_DA]:\n89         \"\"\"Create an instance of DA object.\n90 \n91         Raise an exception if the string cannot be parsed or the argument\n92         is otherwise incompatible.\n93 \n94         Parameters\n95         ----------\n96         val : str\n97             A string conformant to the DA definition in the DICOM Standard,\n98             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n99         \"\"\"\n100         if val is None:\n101             return None\n102 \n103         if isinstance(val, str):\n104             if val.strip() == '':\n105                 return None  # empty date\n106 \n107             if len(val) == 8:\n108                 year = int(val[0:4])\n109                 month = int(val[4:6])\n110                 day = int(val[6:8])\n111                 return super().__new__(cls, year, month, day)\n112 \n113             if len(val) == 10 and val[4] == '.' and val[7] == '.':\n114                 # ACR-NEMA Standard 300, predecessor to DICOM\n115                 # for compatibility with a few old pydicom example files\n116                 year = int(val[0:4])\n117                 month = int(val[5:7])\n118                 day = int(val[8:10])\n119                 return super().__new__(cls, year, month, day)\n120 \n121         if isinstance(val, datetime.date):\n122             return super().__new__(cls, val.year, val.month, val.day)\n123 \n124         try:\n125             return super().__new__(cls, val)\n126         except Exception as exc:\n127             raise ValueError(\n128                 f\"Unable to convert '{val}' to 'DA' object\"\n129             ) from exc\n130 \n131     def __init__(self, val: Union[str, _DA, datetime.date]) -> None:\n132         \"\"\"Create a new **DA** element value.\"\"\"\n133         if isinstance(val, str):\n134             self.original_string = val\n135         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n136             self.original_string = val.original_string\n137 \n138 \n139 class DT(_DateTimeBase, datetime.datetime):\n140     \"\"\"Store value for an element with VR **DT** as :class:`datetime.datetime`.\n141 \n142     Note that the :class:`datetime.datetime` base class is immutable.\n143     \"\"\"\n144     _regex_dt = re.compile(r\"((\\d{4,14})(\\.(\\d{1,6}))?)([+-]\\d{4})?\")\n145 \n146     @staticmethod\n147     def _utc_offset(value: str) -> datetime.timezone:\n148         \"\"\"Return the UTC Offset suffix as a :class:`datetime.timezone`.\n149 \n150         Parameters\n151         ----------\n152         value : str\n153             The value of the UTC offset suffix, such as ``'-1000'`` or\n154             ``'+0245'``.\n155 \n156         Returns\n157         -------\n158         datetime.timezone\n159         \"\"\"\n160         # Format is &ZZXX, & = '+' or '-', ZZ is hours, XX is minutes\n161         hour = int(value[1:3]) * 60  # Convert hours to minutes\n162         minute = int(value[3:5])  # In minutes\n163         offset = (hour + minute) * 60  # Convert minutes to seconds\n164         offset = -offset if value[0] == '-' else offset\n165 \n166         return datetime.timezone(\n167             datetime.timedelta(seconds=offset),\n168             name=value\n169         )\n170 \n171     def __new__(\n172         cls: Type[_DT], val: Union[None, str, _DT, datetime.datetime]\n173     ) -> Optional[_DT]:\n174         \"\"\"Create an instance of DT object.\n175 \n176         Raise an exception if the string cannot be parsed or the argument\n177         is otherwise incompatible.\n178 \n179         Parameters\n180         ----------\n181         val : str\n182             A string conformant to the DT definition in the DICOM Standard,\n183             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n184         \"\"\"\n185         if val is None:\n186             return None\n187 \n188         if isinstance(val, str):\n189             if val.strip() == '':\n190                 return None\n191 \n192             match = cls._regex_dt.match(val)\n193             if not match or len(val) > 26:\n194                 raise ValueError(\n195                     f\"Unable to convert non-conformant value '{val}' to 'DT' \"\n196                     \"object\"\n197                 )\n198 \n199             dt_match = match.group(2)\n200             args = [\n201                 int(dt_match[0:4]),  # year\n202                 1 if len(dt_match) < 6 else int(dt_match[4:6]),  # month\n203                 1 if len(dt_match) < 8 else int(dt_match[6:8]),  # day\n204                 0 if len(dt_match) < 10 else int(dt_match[8:10]),  # hour\n205                 0 if len(dt_match) < 12 else int(dt_match[10:12]),  # minute\n206                 0 if len(dt_match) < 14 else int(dt_match[12:14]),  # second\n207             ]\n208             # microsecond\n209             if len(dt_match) >= 14 and match.group(4):\n210                 args.append(int(match.group(4).rstrip().ljust(6, '0')))\n211             else:\n212                 args.append(0)\n213 \n214             # Timezone offset\n215             tz_match = match.group(5)\n216             args.append(cls._utc_offset(tz_match) if tz_match else None)\n217 \n218             if args[5] == 60:\n219                 warnings.warn(\n220                     \"'datetime.datetime' doesn't allow a value of '60' for \"\n221                     \"the seconds component, changing to '59'\"\n222                 )\n223                 args[5] = 59\n224 \n225             return super().__new__(cls, *args)\n226 \n227         if isinstance(val, datetime.datetime):\n228             return super().__new__(\n229                 cls, *val.timetuple()[:6], val.microsecond, val.tzinfo\n230             )\n231 \n232         try:\n233             return super().__new__(cls, val)\n234         except Exception as exc:\n235             raise ValueError(\n236                 f\"Unable to convert '{val}' to 'DT' object\"\n237             ) from exc\n238 \n239     def __init__(self, val: Union[str, _DT, datetime.datetime]) -> None:\n240         if isinstance(val, str):\n241             self.original_string = val\n242         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n243             self.original_string = val.original_string\n244 \n245 \n246 class TM(_DateTimeBase, datetime.time):\n247     \"\"\"Store value for an element with VR **TM** as :class:`datetime.time`.\n248 \n249     Note that the :class:`datetime.time` base class is immutable.\n250     \"\"\"\n251     _RE_TIME = re.compile(\n252         r\"(?P<h>^([01][0-9]|2[0-3]))\"\n253         r\"((?P<m>([0-5][0-9]))?\"\n254         r\"(?(5)(?P<s>([0-5][0-9]|60))?)\"\n255         r\"(?(7)(\\.(?P<ms>([0-9]{1,6})?))?))$\"\n256     )\n257 \n258     def __new__(\n259         cls: Type[_TM], val: Union[None, str, _TM, datetime.time]\n260     ) -> Optional[_TM]:\n261         \"\"\"Create an instance of TM object from a string.\n262 \n263         Raise an exception if the string cannot be parsed or the argument\n264         is otherwise incompatible.\n265 \n266         Parameters\n267         ----------\n268         val : str\n269             A string conformant to the TM definition in the DICOM Standard,\n270             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n271         \"\"\"\n272         if val is None:\n273             return None\n274 \n275         if isinstance(val, str):\n276             if val.strip() == '':\n277                 return None  # empty time\n278 \n279             match = cls._RE_TIME.match(val)\n280             if not match:\n281                 raise ValueError(\n282                     f\"Unable to convert non-conformant value '{val}' to 'TM' \"\n283                     \"object\"\n284                 )\n285 \n286             hour = int(match.group('h'))\n287             minute = 0 if match.group('m') is None else int(match.group('m'))\n288             second = 0 if match.group('s') is None else int(match.group('s'))\n289 \n290             if second == 60:\n291                 warnings.warn(\n292                     \"'datetime.time' doesn't allow a value of '60' for the \"\n293                     \"seconds component, changing to '59'\"\n294                 )\n295                 second = 59\n296 \n297             microsecond = 0\n298             if match.group('ms'):\n299                 microsecond = int(match.group('ms').rstrip().ljust(6, '0'))\n300 \n301             return super().__new__(cls, hour, minute, second, microsecond)\n302 \n303         if isinstance(val, datetime.time):\n304             return super().__new__(\n305                 cls, val.hour, val.minute, val.second, val.microsecond\n306             )\n307 \n308         try:\n309             return super().__new__(cls, val)\n310         except Exception as exc:\n311             raise ValueError(\n312                 f\"Unable to convert '{val}' to 'TM' object\"\n313             ) from exc\n314 \n315     def __init__(self, val: Union[str, _TM, datetime.time]) -> None:\n316         if isinstance(val, str):\n317             self.original_string = val\n318         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n319             self.original_string = val.original_string\n320 \n321     if platform.python_implementation() == \"PyPy\":\n322         # Workaround for CPython/PyPy bug in time.__reduce_ex__()\n323         #   caused by returning (time, ...) rather than (self.__class__, ...)\n324         def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n325             return (\n326                 self.__class__,\n327                 super()._getstate(protocol),\n328                 self.__getstate__()\n329             )\n330 \n331 \n332 class DSfloat(float):\n333     \"\"\"Store value for an element with VR **DS** as :class:`float`.\n334 \n335     If constructed from an empty string, return the empty string,\n336     not an instance of this class.\n337 \n338     \"\"\"\n339     def __init__(\n340         self, val: Union[str, int, float, Decimal]\n341     ) -> None:\n342         \"\"\"Store the original string if one given, for exact write-out of same\n343         value later.\n344         \"\"\"\n345         # ... also if user changes a data element value, then will get\n346         # a different object, because float is immutable.\n347         has_attribute = hasattr(val, 'original_string')\n348         if isinstance(val, str):\n349             self.original_string = val\n350         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:\n351             self.original_string = val.original_string\n352 \n353     def __str__(self) -> str:\n354         if hasattr(self, 'original_string'):\n355             return self.original_string\n356 \n357         # Issue #937 (Python 3.8 compatibility)\n358         return repr(self)[1:-1]\n359 \n360     def __repr__(self) -> str:\n361         return f'\"{super().__repr__()}\"'\n362 \n363 \n364 class DSdecimal(Decimal):\n365     \"\"\"Store value for an element with VR **DS** as :class:`decimal.Decimal`.\n366 \n367     Notes\n368     -----\n369     If constructed from an empty string, returns the empty string, not an\n370     instance of this class.\n371     \"\"\"\n372     def __new__(\n373         cls: Type[_DSdecimal],\n374         val: Union[str, int, float, Decimal]\n375     ) -> Optional[_DSdecimal]:\n376         \"\"\"Create an instance of DS object, or return a blank string if one is\n377         passed in, e.g. from a type 2 DICOM blank value.\n378 \n379         Parameters\n380         ----------\n381         val : str or numeric\n382             A string or a number type which can be converted to a decimal.\n383         \"\"\"\n384         if isinstance(val, float) and not config.allow_DS_float:\n385             raise TypeError(\n386                 \"'DS' cannot be instantiated with a float value unless \"\n387                 \"'config.allow_DS_float' is set to True. You should convert \"\n388                 \"the value to a string with the desired number of digits, \"\n389                 \"or use 'Decimal.quantize()' and pass a 'Decimal' instance.\"\n390             )\n391 \n392         if isinstance(val, str):\n393             val = val.strip()\n394             if val == '':\n395                 return None\n396 \n397         val = super().__new__(cls, val)\n398         if len(str(val)) > 16 and config.enforce_valid_values:\n399             raise OverflowError(\n400                 \"Values for elements with a VR of 'DS' values must be <= 16 \"\n401                 \"characters long. Use a smaller string, set \"\n402                 \"'config.enforce_valid_values' to False to override the \"\n403                 \"length check, or use 'Decimal.quantize()' and initialize \"\n404                 \"with a 'Decimal' instance.\"\n405             )\n406 \n407         return val\n408 \n409     def __init__(\n410         self, val: Union[str, int, float, Decimal]\n411     ) -> None:\n412         \"\"\"Store the original string if one given, for exact write-out of same\n413         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would\n414         write ``'123'``, but :class:`DS` will use the original.\n415         \"\"\"\n416         # ... also if user changes a data element value, then will get\n417         # a different Decimal, as Decimal is immutable.\n418         has_str = hasattr(val, 'original_string')\n419         if isinstance(val, str):\n420             self.original_string = val\n421         elif isinstance(val, (DSfloat, DSdecimal)) and has_str:\n422             self.original_string = val.original_string\n423 \n424     def __str__(self) -> str:\n425         has_str = hasattr(self, 'original_string')\n426         if has_str and len(self.original_string) <= 16:\n427             return self.original_string\n428 \n429         return super().__str__()\n430 \n431     def __repr__(self) -> str:\n432         return f'\"{str(self)}\"'\n433 \n434 \n435 # CHOOSE TYPE OF DS\n436 if config.use_DS_decimal:\n437     DSclass = DSdecimal\n438 else:\n439     DSclass = DSfloat\n440 \n441 \n442 def DS(\n443     val: Union[None, str, int, float, Decimal]\n444 ) -> Union[None, str, DSfloat, DSdecimal]:\n445     \"\"\"Factory function for creating DS class instances.\n446 \n447     Checks for blank string; if so, returns that, else calls :class:`DSfloat`\n448     or :class:`DSdecimal` to create the class instance. This avoids overriding\n449     ``DSfloat.__new__()`` (which carries a time penalty for large arrays of\n450     DS).\n451 \n452     Similarly the string clean and check can be avoided and :class:`DSfloat`\n453     called directly if a string has already been processed.\n454     \"\"\"\n455     if isinstance(val, str):\n456         val = val.strip()\n457 \n458     if val == '' or val is None:\n459         return val\n460 \n461     return DSclass(val)\n462 \n463 \n464 class IS(int):\n465     \"\"\"Store value for an element with VR **IS** as :class:`int`.\n466 \n467     Stores original integer string for exact rewriting of the string\n468     originally read or stored.\n469     \"\"\"\n470 \n471     def __new__(\n472         cls: Type[_IS], val: Union[None, str, int, float, Decimal]\n473     ) -> Optional[_IS]:\n474         \"\"\"Create instance if new integer string\"\"\"\n475         if val is None:\n476             return val\n477 \n478         if isinstance(val, str) and val.strip() == '':\n479             return None\n480 \n481         try:\n482             newval: _IS = super().__new__(cls, val)\n483         except ValueError:\n484             # accept float strings when no integer loss, e.g. \"1.0\"\n485             newval: _IS = super().__new__(cls, float(val))\n486 \n487         # check if a float or Decimal passed in, then could have lost info,\n488         # and will raise error. E.g. IS(Decimal('1')) is ok, but not IS(1.23)\n489         #   IS('1.23') will raise ValueError\n490         if isinstance(val, (float, Decimal, str)) and newval != float(val):\n491             raise TypeError(\"Could not convert value to integer without loss\")\n492 \n493         # Checks in case underlying int is >32 bits, DICOM does not allow this\n494         if not -2**31 <= newval < 2**31 and config.enforce_valid_values:\n495             raise OverflowError(\n496                 \"Elements with a VR of IS must have a value between -2**31 \"\n497                 \"and (2**31 - 1). Set 'config.enforce_valid_values' to False \"\n498                 \"to override the value check\"\n499             )\n500 \n501         return newval\n502 \n503     def __init__(self, val: Union[str, int, float, Decimal]) -> None:\n504         # If a string passed, then store it\n505         if isinstance(val, str):\n506             self.original_string = val\n507         elif isinstance(val, IS) and hasattr(val, 'original_string'):\n508             self.original_string = val.original_string\n509 \n510     def __str__(self) -> str:\n511         if hasattr(self, 'original_string'):\n512             return self.original_string\n513 \n514         # Issue #937 (Python 3.8 compatibility)\n515         return repr(self)[1:-1]\n516 \n517     def __repr__(self) -> str:\n518         return f'\"{super().__repr__()}\"'\n519 \n520 \n521 def MultiString(\n522     val: str,\n523     valtype: Optional[Union[Type[_T], Callable[[object], _T]]] = None\n524 ) -> Union[_T, SequenceType[_T]]:\n525     \"\"\"Split a bytestring by delimiters if there are any\n526 \n527     Parameters\n528     ----------\n529     val : str\n530         The string to split up.\n531     valtype : type or callable, optional\n532         Default :class:`str`, but can be e.g. :class:`~pydicom.uid.UID` to\n533         overwrite to a specific type.\n534 \n535     Returns\n536     -------\n537     valtype or MultiValue of valtype\n538         The split value as `valtype` or a :class:`list` of `valtype`.\n539     \"\"\"\n540     valtype = str if valtype is None else valtype\n541     # Remove trailing blank used to pad to even length\n542     # 2005.05.25: also check for trailing 0, error made\n543     # in PET files we are converting\n544     while val and val.endswith((' ', '\\x00')):\n545         val = val[:-1]\n546 \n547     splitup = val.split(\"\\\\\")\n548     if len(splitup) == 1:\n549         val = splitup[0]\n550         return valtype(val) if val else val\n551 \n552     return MultiValue(valtype, splitup)\n553 \n554 \n555 def _verify_encodings(encodings):\n556     \"\"\"Checks the encoding to ensure proper format\"\"\"\n557     if encodings is not None:\n558         if not isinstance(encodings, (list, tuple)):\n559             return encodings,\n560         return tuple(encodings)\n561     return encodings\n562 \n563 \n564 def _decode_personname(components, encodings):\n565     \"\"\"Return a list of decoded person name components.\n566 \n567     Parameters\n568     ----------\n569     components : list of byte string\n570         The list of the up to three encoded person name components\n571     encodings : list of str\n572         The Python encodings uses to decode `components`.\n573 \n574     Returns\n575     -------\n576     text type\n577         The unicode string representing the person name.\n578         If the decoding of some component parts is not possible using the\n579         given encodings, they are decoded with the first encoding using\n580         replacement characters for bytes that cannot be decoded.\n581     \"\"\"\n582     from pydicom.charset import decode_bytes\n583 \n584     if isinstance(components[0], str):\n585         comps = components\n586     else:\n587         comps = [\n588             decode_bytes(comp, encodings, PN_DELIMS) for comp in components\n589         ]\n590     # Remove empty elements from the end to avoid trailing '='\n591     while len(comps) and not comps[-1]:\n592         comps.pop()\n593     return tuple(comps)\n594 \n595 \n596 def _encode_personname(components, encodings):\n597     \"\"\"Encode a list of text string person name components.\n598 \n599     Parameters\n600     ----------\n601     components : list of text type\n602         The list of the up to three unicode person name components\n603     encodings : list of str\n604         The Python encodings uses to encode `components`.\n605 \n606     Returns\n607     -------\n608     byte string\n609         The byte string that can be written as a PN DICOM tag value.\n610         If the encoding of some component parts is not possible using the\n611         given encodings, they are encoded with the first encoding using\n612         replacement bytes for characters that cannot be encoded.\n613     \"\"\"\n614     from pydicom.charset import encode_string\n615 \n616     encoded_comps = []\n617     for comp in components:\n618         groups = [encode_string(group, encodings)\n619                   for group in comp.split('^')]\n620         encoded_comps.append(b'^'.join(groups))\n621 \n622     # Remove empty elements from the end\n623     while len(encoded_comps) and not encoded_comps[-1]:\n624         encoded_comps.pop()\n625     return b'='.join(encoded_comps)\n626 \n627 \n628 class PersonName:\n629     \"\"\"Representation of the value for an element with VR **PN**.\"\"\"\n630     def __new__(\n631         cls: Type[_PersonName], *args, **kwargs\n632     ) -> Optional[_PersonName]:\n633         # Handle None value by returning None instead of a PersonName object\n634         if len(args) and args[0] is None:\n635             return None\n636 \n637         return super().__new__(cls)\n638 \n639     def __init__(\n640         self,\n641         val: Union[bytes, str, \"PersonName\"],\n642         encodings: Optional[List[str]] = None,\n643         original_string: Optional[str] = None\n644     ) -> None:\n645         \"\"\"Create a new ``PersonName``.\n646 \n647         Parameters\n648         ----------\n649         val: str, bytes, PersonName\n650             The value to use for the **PN** element.\n651         encodings: list of str, optional\n652             A list of the encodings used for the value.\n653         original_string: str, optional\n654             When creating a ``PersonName`` using a decoded string, this is the\n655             original encoded value.\n656         \"\"\"\n657         self.original_string: Union[None, str, bytes] = None\n658         self._components = None\n659 \n660         if isinstance(val, PersonName):\n661             encodings = val.encodings\n662             self.original_string = val.original_string\n663             self._components = tuple(str(val).split('='))\n664         elif isinstance(val, bytes):\n665             # this is the raw byte string - decode it on demand\n666             self.original_string = val\n667             self._components = None\n668         else:\n669             # handle None `val` as empty string\n670             val = val or ''\n671 \n672             # this is the decoded string - save the original string if\n673             # available for easier writing back\n674             self.original_string = original_string\n675             components = val.split('=')\n676             # Remove empty elements from the end to avoid trailing '='\n677             while len(components) and not components[-1]:\n678                 components.pop()\n679             self._components = tuple(components)\n680 \n681             # if the encoding is not given, leave it as undefined (None)\n682         self.encodings: List[str] = _verify_encodings(encodings)\n683         self._dict = {}\n684 \n685     def _create_dict(self) -> Dict[str, str]:\n686         \"\"\"Creates a dictionary of person name group and component names.\n687 \n688         Used exclusively for `formatted` for backwards compatibility.\n689         \"\"\"\n690         parts = [\n691             'family_name', 'given_name', 'middle_name', 'name_prefix',\n692             'name_suffix', 'ideographic', 'phonetic'\n693         ]\n694         return {c: getattr(self, c, '') for c in parts}\n695 \n696     @property\n697     def components(self) -> List[str]:\n698         \"\"\"Returns up to three decoded person name components.\n699 \n700         .. versionadded:: 1.2\n701 \n702         The returned components represent the alphabetic, ideographic and\n703         phonetic representations as a list of unicode strings.\n704         \"\"\"\n705         if self._components is None:\n706             groups = self.original_string.split(b'=')\n707             encodings = self.encodings or [default_encoding]\n708             self._components = _decode_personname(groups, encodings)\n709 \n710         return self._components\n711 \n712     def _name_part(self, i) -> str:\n713         \"\"\"Return the `i`th part of the name.\"\"\"\n714         try:\n715             return self.components[0].split('^')[i]\n716         except IndexError:\n717             return ''\n718 \n719     @property\n720     def family_name(self) -> str:\n721         \"\"\"Return the first (family name) group of the alphabetic person name\n722         representation as a unicode string\n723 \n724         .. versionadded:: 1.2\n725         \"\"\"\n726         return self._name_part(0)\n727 \n728     @property\n729     def given_name(self) -> str:\n730         \"\"\"Return the second (given name) group of the alphabetic person name\n731         representation as a unicode string\n732 \n733         .. versionadded:: 1.2\n734         \"\"\"\n735         return self._name_part(1)\n736 \n737     @property\n738     def middle_name(self) -> str:\n739         \"\"\"Return the third (middle name) group of the alphabetic person name\n740         representation as a unicode string\n741 \n742         .. versionadded:: 1.2\n743         \"\"\"\n744         return self._name_part(2)\n745 \n746     @property\n747     def name_prefix(self) -> str:\n748         \"\"\"Return the fourth (name prefix) group of the alphabetic person name\n749         representation as a unicode string\n750 \n751         .. versionadded:: 1.2\n752         \"\"\"\n753         return self._name_part(3)\n754 \n755     @property\n756     def name_suffix(self) -> str:\n757         \"\"\"Return the fifth (name suffix) group of the alphabetic person name\n758         representation as a unicode string\n759 \n760         .. versionadded:: 1.2\n761         \"\"\"\n762         return self._name_part(4)\n763 \n764     @property\n765     def ideographic(self) -> str:\n766         \"\"\"Return the second (ideographic) person name component as a\n767         unicode string\n768 \n769         .. versionadded:: 1.2\n770         \"\"\"\n771         try:\n772             return self.components[1]\n773         except IndexError:\n774             return ''\n775 \n776     @property\n777     def phonetic(self) -> str:\n778         \"\"\"Return the third (phonetic) person name component as a\n779         unicode string\n780 \n781         .. versionadded:: 1.2\n782         \"\"\"\n783         try:\n784             return self.components[2]\n785         except IndexError:\n786             return ''\n787 \n788     def __eq__(self, other: object) -> bool:\n789         \"\"\"Return ``True`` if `other` equals the current name.\"\"\"\n790         return str(self) == other\n791 \n792     def __ne__(self, other: object) -> bool:\n793         \"\"\"Return ``True`` if `other` doesn't equal the current name.\"\"\"\n794         return not self == other\n795 \n796     def __str__(self) -> str:\n797         \"\"\"Return a string representation of the name.\"\"\"\n798         return '='.join(self.components).__str__()\n799 \n800     def __iter__(self) -> Generator[str, None, None]:\n801         \"\"\"Iterate through the name.\"\"\"\n802         yield from self.__str__()\n803 \n804     def __len__(self) -> int:\n805         \"\"\"Return the length of the person name.\"\"\"\n806         return len(self.__str__())\n807 \n808     def __contains__(self, x: str) -> bool:\n809         \"\"\"Return ``True`` if `x` is in the name.\"\"\"\n810         return x in self.__str__()\n811 \n812     def __repr__(self) -> str:\n813         \"\"\"Return a representation of the name.\"\"\"\n814         return '='.join(self.components).__repr__()\n815 \n816     def __hash__(self) -> int:\n817         \"\"\"Return a hash of the name.\"\"\"\n818         return hash(self.components)\n819 \n820     def decode(self, encodings: Optional[List[str]] = None) -> \"PersonName\":\n821         \"\"\"Return the patient name decoded by the given `encodings`.\n822 \n823         Parameters\n824         ----------\n825         encodings : list of str, optional\n826             The list of encodings used for decoding the byte string. If not\n827             given, the initial encodings set in the object are used.\n828 \n829         Returns\n830         -------\n831         valuerep.PersonName\n832             A person name object that will return the decoded string with\n833             the given encodings on demand. If the encodings are not given,\n834             the current object is returned.\n835         \"\"\"\n836         # in the common case (encoding did not change) we decode on demand\n837         if encodings is None or encodings == self.encodings:\n838             return self\n839         # the encoding was unknown or incorrect - create a new\n840         # PersonName object with the changed encoding\n841         encodings = _verify_encodings(encodings)\n842         if self.original_string is None:\n843             # if the original encoding was not set, we set it now\n844             self.original_string = _encode_personname(\n845                 self.components, self.encodings or [default_encoding])\n846         return PersonName(self.original_string, encodings)\n847 \n848     def encode(\n849         self, encodings: Optional[List[str]] = None\n850     ) -> bytes:\n851         \"\"\"Return the patient name decoded by the given `encodings`.\n852 \n853         Parameters\n854         ----------\n855         encodings : list of str, optional\n856             The list of encodings used for encoding the unicode string. If\n857             not given, the initial encodings set in the object are used.\n858 \n859         Returns\n860         -------\n861         bytes\n862             The person name encoded with the given encodings as a byte string.\n863             If no encoding is given, the original byte string is returned, if\n864             available, otherwise each group of the patient name is encoded\n865             with the first matching of the given encodings.\n866         \"\"\"\n867         encodings = _verify_encodings(encodings) or self.encodings\n868 \n869         # if the encoding is not the original encoding, we have to return\n870         # a re-encoded string (without updating the original string)\n871         if encodings != self.encodings and self.encodings is not None:\n872             return _encode_personname(self.components, encodings)\n873         if self.original_string is None:\n874             # if the original encoding was not set, we set it now\n875             self.original_string = _encode_personname(\n876                 self.components, encodings or [default_encoding])\n877         return self.original_string\n878 \n879     def family_comma_given(self) -> str:\n880         \"\"\"Return the name as \"Family, Given\".\"\"\"\n881         return self.formatted('%(family_name)s, %(given_name)s')\n882 \n883     def formatted(self, format_str: str) -> str:\n884         \"\"\"Return the name as a :class:`str` formatted using `format_str`.\"\"\"\n885         return format_str % self._create_dict()\n886 \n887     def __bool__(self) -> bool:\n888         \"\"\"Return ``True`` if the name is not empty.\"\"\"\n889         if self.original_string is None:\n890             return (\n891                 bool(self._components)\n892                 and (len(self._components) > 1 or bool(self._components[0]))\n893             )\n894 \n895         return bool(self.original_string)\n896 \n897 \n898 # Alias old class names for backwards compat in user code\n899 def __getattr__(name):\n900     if name == \"PersonNameUnicode\":\n901         warnings.warn(\n902             \"'PersonNameUnicode' is deprecated and will be removed in \"\n903             \"pydicom v3.0, use 'PersonName' instead\",\n904             DeprecationWarning\n905         )\n906         return globals()['PersonName']\n907 \n908     raise AttributeError(f\"module {__name__} has no attribute {name}\")\n909 \n910 \n911 if sys.version_info[:2] < (3, 7):\n912     PersonNameUnicode = PersonName\n913 \n[end of pydicom/valuerep.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pydicom/valuerep.py\n+++ b/pydicom/valuerep.py\n@@ -15,10 +15,12 @@\n from typing import (\n     TypeVar, Type, Tuple, Optional, List, Dict, Union, Any, Generator, AnyStr,\n     Callable, Iterator, overload\n )\n+from typing import NoReturn\n from typing import Sequence as SequenceType\n import warnings\n \n # don't import datetime_conversion directly\n from pydicom import config\n from pydicom.multival import MultiValue\n from pydicom.uid import UID\n \n@@ -22,6 +24\n # Types\n _T = TypeVar('_T')\n _DA = TypeVar(\"_DA\", bound=\"DA\")\n _DT = TypeVar(\"_DT\", bound=\"DT\")\n _TM = TypeVar(\"_TM\", bound=\"TM\")\n _IS = TypeVar(\"_IS\", bound=\"IS\")\n _DSfloat = TypeVar(\"_DSfloat\", bound=\"DSfloat\")\n _DSdecimal = TypeVar(\"_DSdecimal\", bound=\"DSdecimal\")\n _PersonName = TypeVar(\"_PersonName\", bound=\"PersonName\")\n \n+_NoReturn = TypeVar('_NoReturn', bound=NoReturn)\n \n # can't import from charset or get circular import\n default_encoding = \"iso8859\"\n \n@@ -56,7 +60\n     def __getstate__(self) -> Dict[str, Any]:\n         return self.__dict__.copy()\n \n     def __setstate__(self, state: Dict[str, Any]) -> None:\n         self.__dict__.update(state)\n \n     def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n         return super().__reduce_ex__(protocol) + (self.__getstate__(),)\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         return super().__str__()\n \n     def __repr__(self) -> str:\n         return f'\"{str(self)}\"'\n \n@@ -81,6 +88\n     def __new__(\n         cls: Type[_DA], val: Union[None, str, _DA, datetime.date]\n     ) -> Optional[_DA]:\n         \"\"\"Create an instance of DA object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the DA definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty date\n \n             if len(val) == 8:\n                 year = int(val[0:4])\n                 month = int(val[4:6])\n                 day = int(val[6:8])\n                 return super().__new__(cls, year, month, day)\n \n             if len(val) == 10 and val[4] == '.' and val[7] == '.':\n                 # ACR-NEMA Standard 300, predecessor to DICOM\n                 # for compatibility with a few old pydicom example files\n                 year = int(val[0:4])\n                 month = int(val[5:7])\n                 day = int(val[8:10])\n                 return super().__new__(cls, year, month, day)\n \n         if isinstance(val, datetime.date):\n             return super().__new__(cls, val.year, val.month, val.day)\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DA' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _DA, datetime.date]) -> None:\n         \"\"\"Create a new **DA** element value.\"\"\"\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class DT(_DateTimeBase, datetime.datetime):\n     \"\"\"Store value for an element with VR **DT** as :class:`datetime.datetime`.\n \n     Note that the :class:`datetime.datetime` base class is immutable.\n     \"\"\"\n     _regex_dt = re.compile(r\"((\\d{4,14})(\\.(\\d{1,6}))?)([+-]\\d{4})?\")\n \n     @staticmethod\n     def _utc_offset(value: str) -> datetime.timezone:\n         \"\"\"Return the UTC Offset suffix as a :class:`datetime.timezone`.\n \n         Parameters\n         ----------\n         value : str\n             The value of the UTC offset suffix, such as ``'-1000'`` or\n             ``'+0245'``.\n \n         Returns\n         -------\n         datetime.timezone\n         \"\"\"\n         # Format is &ZZXX, & = '+' or '-', ZZ is hours, XX is minutes\n         hour = int(value[1:3]) * 60  # Convert hours to minutes\n         minute = int(value[3:5])  # In minutes\n         offset = (hour + minute) * 60  # Convert minutes to seconds\n         offset = -offset if value[0] == '-' else offset\n \n         return datetime.timezone(\n             datetime.timedelta(seconds=offset),\n             name=value\n         )\n \n     def __new__(\n         cls: Type[_DT], val: Union[None, str, _DT, datetime.datetime]\n     ) -> Optional[_DT]:\n         \"\"\"Create an instance of DT object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the DT definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None\n \n             match = cls._regex_dt.match(val)\n             if not match or len(val) > 26:\n                 raise ValueError(\n                     f\"Unable to convert non-conformant value '{val}' to 'DT' \"\n                     \"object\"\n                 )\n \n             dt_match = match.group(2)\n             args = [\n                 int(dt_match[0:4]),  # year\n                 1 if len(dt_match) < 6 else int(dt_match[4:6]),  # month\n                 1 if len(dt_match) < 8 else int(dt_match[6:8]),  # day\n                 0 if len(dt_match) < 10 else int(dt_match[8:10]),  # hour\n                 0 if len(dt_match) < 12 else int(dt_match[10:12]),  # minute\n                 0 if len(dt_match) < 14 else int(dt_match[12:14]),  # second\n             ]\n             # microsecond\n             if len(dt_match) >= 14 and match.group(4):\n                 args.append(int(match.group(4).rstrip().ljust(6, '0')))\n             else:\n                 args.append(0)\n \n             # Timezone offset\n             tz_match = match.group(5)\n             args.append(cls._utc_offset(tz_match) if tz_match else None)\n \n             if args[5] == 60:\n                 warnings.warn(\n                     \"'datetime.datetime' doesn't allow a value of '60' for \"\n                     \"the seconds component, changing to '59'\"\n                 )\n                 args[5] = 59\n \n             return super().__new__(cls, *args)\n \n         if isinstance(val, datetime.datetime):\n             return super().__new__(\n                 cls, *val.timetuple()[:6], val.microsecond, val.tzinfo\n             )\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DT' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _DT, datetime.datetime]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class TM(_DateTimeBase, datetime.time):\n     \"\"\"Store value for an element with VR **TM** as :class:`datetime.time`.\n \n     Note that the :class:`datetime.time` base class is immutable.\n     \"\"\"\n     _RE_TIME = re.compile(\n         r\"(?P<h>^([01][0-9]|2[0-3]))\"\n         r\"((?P<m>([0-5][0-9]))?\"\n         r\"(?(5)(?P<s>([0-5][0-9]|60))?)\"\n         r\"(?(7)(\\.(?P<ms>([0-9]{1,6})?))?))$\"\n     )\n \n     def __new__(\n         cls: Type[_TM], val: Union[None, str, _TM, datetime.time]\n     ) -> Optional[_TM]:\n         \"\"\"Create an instance of TM object from a string.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the TM definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty time\n \n             match = cls._RE_TIME.match(val)\n             if not match:\n                 raise ValueError(\n                     f\"Unable to convert non-conformant value '{val}' to 'TM' \"\n                     \"object\"\n                 )\n \n             hour = int(match.group('h'))\n             minute = 0 if match.group('m') is None else int(match.group('m'))\n             second = 0 if match.group('s') is None else int(match.group('s'))\n \n             if second == 60:\n                 warnings.warn(\n                     \"'datetime.time' doesn't allow a value of '60' for the \"\n                     \"seconds component, changing to '59'\"\n                 )\n                 second = 59\n \n             microsecond = 0\n             if match.group('ms'):\n                 microsecond = int(match.group('ms').rstrip().ljust(6, '0'))\n \n             return super().__new__(cls, hour, minute, second, microsecond)\n \n         if isinstance(val, datetime.time):\n             return super().__new__(\n                 cls, val.hour, val.minute, val.second, val.microsecond\n             )\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'TM' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _TM, datetime.time]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n     if platform.python_implementation() == \"PyPy\":\n         # Workaround for CPython/PyPy bug in time.__reduce_ex__()\n         #   caused by returning (time, ...) rather than (self.__class__, ...)\n         def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n             return (\n                 self.__class__,\n                 super()._getstate(protocol),\n                 self.__getstate__()\n             )\n \n class DSfloat(float):\n     \"\"\"Store value for an element with VR **DS** as :class:`float`.\n \n     If constructed from an empty string, return the empty string,\n     not an instance of this class.\n \n     \"\"\"\n     def __init__(\n         self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later.\n         \"\"\"\n         # ... also if user changes a data element value, then will get\n         # a different object, because float is immutable.\n         has_attribute = hasattr(val, 'original_string')\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n         return f'\"{super().__repr__()}\"'\n \n class DSdecimal(Decimal):\n     \"\"\"Store value for an element with VR **DS** as :class:`decimal.Decimal`.\n \n     Notes\n     -----\n     If constructed from an empty string, returns the empty string, not an\n     instance of this class.\n     \"\"\"\n     def __new__(\n         cls: Type[_DSdecimal],\n         val: Union[str, int, float, Decimal]\n     ) -> Optional[_DSdecimal]:\n         \"\"\"Create an instance of DS object, or return a blank string if one is\n         passed in, e.g. from a type 2 DICOM blank value.\n \n         Parameters\n         ----------\n         val : str or numeric\n             A string or a number type which can be converted to a decimal.\n         \"\"\"\n         if isinstance(val, float) and not config.allow_DS_float:\n             raise TypeError(\n                 \"'DS' cannot be instantiated with a float value unless \"\n                 \"'config.allow_DS_float' is set to True. You should convert \"\n                 \"the value to a string with the desired number of digits, \"\n                 \"or use 'Decimal.quantize()' and pass a 'Decimal' instance.\"\n             )\n \n         if isinstance(val, str):\n             val = val.strip()\n             if val == '':\n                 return None\n \n         val = super().__new__(cls, val)\n         if len(str(val)) > 16 and config.enforce_valid_values:\n             raise OverflowError(\n                 \"Values for elements with a VR of 'DS' values must be <= 16 \"\n                 \"characters long. Use a smaller string, set \"\n                 \"'config.enforce_valid_values' to False to override the \"\n                 \"length check, or use 'Decimal.quantize()' and initialize \"\n                 \"with a 'Decimal' instance.\"\n             )\n \n         return val\n \n     def __init__(\n         self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would\n         write ``'123'``, but :class:`DS` will use the original.\n         \"\"\"\n         # ... also if user changes a data element value, then will get\n         # a different Decimal, as Decimal is immutable.\n         has_str = hasattr(val, 'original_string')\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_str:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         has_str = hasattr(self, 'original_string')\n         if has_str and len(self.original_string) <= 16:\n             return self.original_string\n \n         return super().__str__()\n \n     def __repr__(self) -> str:\n         return f'\"{str(self)}\"'\n \n # CHOOSE TYPE OF DS\n if config.use_DS_decimal:\n     DSclass = DSdecimal\n else:\n     DSclass = DSfloat\n \n def DS(\n     val: Union[None, str, int, float, Decimal]\n ) -> Union[None, str, DSfloat, DSdecimal]:\n     \"\"\"Factory function for creating DS class instances.\n \n     Checks for blank string; if so, returns that, else calls :class:`DSfloat`\n     or :class:`DSdecimal` to create the class instance. This avoids overriding\n     ``DSfloat.__new__()`` (which carries a time penalty for large arrays of\n     DS).\n \n     Similarly the string clean and check can be avoided and :class:`DSfloat`\n     called directly if a string has already been processed.\n     \"\"\"\n     if isinstance(val, str):\n         val = val.strip()\n \n     if val == '' or val is None:\n         return val\n \n     return DSclass(val)\n \n class IS(int):\n     \"\"\"Store value for an element with VR **IS** as :class:`int`.\n \n     Stores original integer string for exact rewriting of the string\n     originally read or stored.\n     \"\"\"\n \n     def __new__(\n         cls: Type[_IS], val: Union[None, str, int, float, Decimal]\n     ) -> Optional[_IS]:\n         \"\"\"Create instance if new integer string\"\"\"\n         if val is None:\n             return val\n \n         if isinstance(val, str) and val.strip() == '':\n             return None\n \n         try:\n             newval: _IS = super().__new__(cls, val)\n         except ValueError:\n             # accept float strings when no integer loss, e.g. \"1.0\"\n             newval: _IS = super().__new__(cls, float(val))\n \n         # check if a float or Decimal passed in, then could have lost info,\n         # and will raise error. E.g. IS(Decimal('1')) is ok, but not IS(1.23)\n         #   IS('1.23') will raise ValueError\n         if isinstance(val, (float, Decimal, str)) and newval != float(val):\n             raise TypeError(\"Could not convert value to integer without loss\")\n \n         # Checks in case underlying int is >32 bits, DICOM does not allow this\n         if not -2**31 <= newval < 2**31 and config.enforce_valid_values:\n             raise OverflowError(\n                 \"Elements with a VR of IS must have a value between -2**31 \"\n                 \"and (2**31 - 1). Set 'config.enforce_valid_values' to False \"\n                 \"to override the value check\"\n             )\n \n         return newval\n \n     def __init__(self, val: Union[str, int, float, Decimal]) -> None:\n         # If a string passed, then store it\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, IS) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n         return f'\"{super().__repr__()}\"'\n \n def MultiString(\n     val: str,\n     valtype: Optional[Union[Type[_T], Callable[[object], _T]]] = None\n ) -> Union[_T, SequenceType[_T]]:\n     \"\"\"Split a bytestring by delimiters if there are any\n \n     Parameters\n     ----------\n     val : str\n         The string to split up.\n     valtype : type or callable, optional\n         Default :class:`str`, but can be e.g. :class:`~pydicom.uid.UID` to\n         overwrite to a specific type.\n \n     Returns\n     -------\n     valtype or MultiValue of valtype\n         The split value as `valtype` or a :class:`list` of `valtype`.\n     \"\"\"\n     valtype = str if valtype is None else valtype\n     # Remove trailing blank used to pad to even length\n     # 2005.05.25: also check for trailing 0, error made\n     # in PET files we are converting\n     while val and val.endswith((' ', '\\x00')):\n         val = val[:-1]\n \n     splitup = val.split(\"\\\\\")\n     if len(splitup) == 1:\n         val = splitup[0]\n         return valtype(val) if val else val\n \n     return MultiValue(valtype, splitup)\n \n def _verify_encodings(encodings):\n     \"\"\"Checks the encoding to ensure proper format\"\"\"\n     if encodings is not None:\n         if not isinstance(encodings, (list, tuple)):\n             return encodings,\n         return tuple(encodings)\n     return encodings\n \n def _decode_personname(components, encodings):\n     \"\"\"Return a list of decoded person name components.\n \n     Parameters\n     ----------\n     components : list of byte string\n         The list of the up to three encoded person name components\n     encodings : list of str\n         The Python encodings uses to decode `components`.\n \n     Returns\n     -------\n     text type\n         The unicode string representing the person name.\n         If the decoding of some component parts is not possible using the\n         given encodings, they are decoded with the first encoding using\n         replacement characters for bytes that cannot be decoded.\n     \"\"\"\n     from pydicom.charset import decode_bytes\n \n     if isinstance(components[0], str):\n         comps = components\n     else:\n         comps = [\n             decode_bytes(comp, encodings, PN_DELIMS) for comp in components\n         ]\n     # Remove empty elements from the end to avoid trailing '='\n     while len(comps) and not comps[-1]:\n         comps.pop()\n     return tuple(comps)\n \n def _encode_personname(components, encodings):\n     \"\"\"Encode a list of text string person name components.\n \n     Parameters\n     ----------\n     components : list of text type\n         The list of text string representing the person name components.\n     encodings : list of str\n         The Python encodings uses to encode `components`.\n \n     Returns\n     -------\n     byte string\n         The byte string that can be written as a PN DICOM tag value.\n         If the encoding of some component parts is not possible using the\n         given encodings, they are encoded with the first encoding using\n         replacement bytes for characters that cannot be encoded.\n     \"\"\"\n     from pydicom.charset import encode_string\n \n     encoded_comps = []\n     for comp in components:\n         groups = [encode_string(group, encodings)\n                   for group in comp.split('^')]\n         encoded_comps.append(b'^'.join(groups))\n \n     # Remove empty elements from the end\n     while len(encoded_comps) and not encoded_comps[-1]:\n         encoded_comps.pop()\n     return b'='.join(encoded_comps)\n \n class PersonName:\n     \"\"\"Representation of the value for an element with VR **PN**.\"\"\"\n     def __new__(\n         cls: Type[_PersonName], *args, **kwargs\n     ) -> Optional[_PersonName]:\n         # Handle None value by returning None instead of a PersonName object\n         if len(args) and args[0] is None:\n             return None\n \n         return super().__new__(cls)\n \n     def __init__(\n         self,\n         val: Union[bytes, str, \"PersonName\"],\n         encodings: Optional[List[str]] = None,\n         original_string: Optional[str] = None\n     ) -> None:\n         \"\"\"Create a new ``PersonName``.\n \n         Parameters\n         ----------\n         val: str, bytes, PersonName\n             The value to use for the **PN** element.\n         encodings: list of str, optional\n             A list of the encodings used for the value.\n         original_string: str, optional\n             When creating a ``PersonName`` using a decoded string, this is the\n             original encoded value.\n         \"\"\"\n         self.original_string: Union[None, str, bytes] = None\n         self._components = None\n \n         if isinstance(val, PersonName):\n             encodings = val.encodings\n             self.original_string = val.original_string\n             self._components = tuple(str(val).split('='))\n         elif isinstance(val, bytes):\n             # this is the raw byte string - decode it on demand\n             self.original_string = val\n             self._components = None\n         else:\n             # handle None `val` as empty string\n             val = val or ''\n \n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n             components = val.split('=')\n             # Remove empty elements from the end to avoid trailing '='\n             while len(components) and not components[-1]:\n                 components.pop()\n             self._components = tuple(components)\n \n             # if the encoding is not given, leave it as undefined (None)\n         self.encodings: List[str] = _verify_encodings(encodings)\n         self._dict = {}\n \n     def _create_dict(self) -> Dict[str, str]:\n         \"\"\"Creates a dictionary of person name group and component names.\n \n         Used exclusively for `formatted` for backwards compatibility.\n         \"\"\"\n         parts = [\n             'family_name', 'given_name', 'middle_name', 'name_prefix',\n             'name_suffix', 'ideographic', 'phonetic'\n         ]\n         return {c: getattr(self, c, '') for c in parts}\n \n     @property\n     def components(self) -> List[str]:\n         \"\"\"Returns up to three decoded person name components.\n \n         .. versionadded:: 1.2\n \n         The returned components represent the alphabetic, ideographic and\n         phonetic representations as a list of unicode strings.\n         \"\"\"\n         if self._components is None:\n             groups = self.original_string.split(b'=')\n             encodings = self.encodings or [default_encoding]\n             self._components = _decode_personname(groups, encodings)\n \n         return self._components\n \n     def _name_part(self, i) -> str:\n         \"\"\"Return the `i`th part of the name.\"\"\"\n         try:\n             return self.components[0].split('^')[i]\n         except IndexError:\n             return ''\n \n     @property\n     def family_name(self) -> str:\n         \"\"\"Return the first (family name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(0)\n \n     @property\n     def given_name(self) -> str:\n         \"\"\"Return the second (given name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(1)\n \n     @property\n     def middle_name(self) -> str:\n         \"\"\"Return the third (middle name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(2)\n \n     @property\n     def name_prefix(self) -> str:\n         \"\"\"Return the fourth (name prefix) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(3)\n \n     @property\n     def name_suffix(self) -> str:\n         \"\"\"Return the fifth (name suffix) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(4)\n \n     @property\n     def ideographic(self) -> str:\n         \"\"\"Return the second (ideographic) person name component as a\n         unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         try:\n             return self.components[1]\n         except IndexError:\n             return ''\n \n     @property\n     def phonetic(self) -> str:\n         \"\"\"Return the third (phonetic) person name component as a\n         unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         try:\n             return self.components[2]\n         except IndexError:\n             return ''\n \n     def __eq__(self, other: object) -> bool:\n         \"\"\"Return ``True`` if `other` equals the current name.\"\"\"\n         return str(self) == other\n \n     def __ne__(self, other: object) -> bool:\n         \"\"\"Return ``True`` if `other` doesn't equal the current name.\"\"\"\n         return not self == other\n \n     def __str__(self) -> str:\n         \"\"\"Return a string representation of the name.\"\"\"\n         return '='.join(self.components).__str__()\n \n     def __iter__(self) -> Generator[str, None, None]:\n         \"\"\"Iterate through the name.\"\"\"\n         yield from self.__str__()\n \n     def __len__(self) -> int:\n         \"\"\"Return the length of the person name.\"\"\"\n         return len(self.__str__())\n \n     def __contains__(self, x: str) -> bool:\n         \"\"\"Return ``True`` if `x` is in the name.\"\"\"\n         return x in self.__str__()\n \n     def __repr__(self) -> str:\n         \"\"\"Return a representation of the name.\"\"\"\n         return '='.join(self.components).__repr__()\n \n     def __hash__(self) -> int:\n         \"\"\"Return a hash of the name.\"\"\"\n         return hash(self.components)\n \n     def decode(self, encodings: Optional[List[str]] = None) -> \"PersonName\":\n         \"\"\"Return the patient name decoded by the given `encodings`.\n \n         Parameters\n         ----------\n         encodings : list of str, optional\n             The list of encodings used for decoding the byte string. If not\n             given, the initial encodings set in the object are used.\n \n         Returns\n         -------\n         valuerep.PersonName\n             A person name object that will return the decoded string with\n             the given encodings on demand. If the encodings are not given,\n             the current object is returned.\n         \"\"\"\n         # in the common case (encoding did not change) we decode on demand\n         if encodings is None or encodings == self.encodings:\n             return self\n         # the encoding was unknown or incorrect - create a new\n         # PersonName object with the changed encoding\n         encodings = _verify_encodings(encodings)\n         if self.original_string is None:\n             # if the original encoding was not set, we set it now\n             self.original_string = _encode_personname(\n                 self.components, self.encodings or [default_encoding])\n         return PersonName(self.original_string, encodings)\n \n     def encode(\n         self, encodings: Optional[List[str]] = None\n     ) -> bytes:\n         \"\"\"Return the patient name decoded by the given `encodings`.\n \n         Parameters\n         ----------\n         encodings : list of str, optional\n             The list of encodings used for encoding the unicode string. If\n             not given, the initial encodings set in the object are used.\n \n         Returns\n         -------\n         bytes\n             The person name encoded with the given encodings as a byte string.\n             If no encoding is given, the original byte string is returned, if\n             available, otherwise each group of the patient name is encoded\n             with the first matching of the given encodings.\n         \"\"\"\n         encodings = _verify_encodings(encodings) or self.encodings\n \n         # if the encoding is not the original encoding, we have to return\n         # a re-encoded string (without updating the original string)\n         if encodings != self.encodings and self.encodings is not None:\n             return _encode_personname(self.components, encodings)\n         if self.original_string is None:\n             # if the original encoding was not set, we set it now\n             self.original_string = _encode_personname(\n                 self.components, encodings or [default_encoding])\n         return self.original_string\n \n     def family_comma_given(self) -> str:\n         \"\"\"Return the name as \"Family, Given\".\"\"\"\n         return self.formatted('%(family_name)s, %(given_name)s')\n \n     def formatted(self, format_str: str) -> str:\n         \"\"\"Return the name as a :class:`str` formatted using `format_str`.\"\"\"\n         return format_str % self._create_dict()\n \n     def __bool__(self) -> bool:\n         \"\"\"Return ``True`` if the name is not empty.\"\"\"\n         if self.original_string is None:\n             return (\n                 bool(self._components)\n                 and (len(self._components) > 1 or bool(self._components[0]))\n             )\n \n         return bool(self.original_string)\n \n # Alias old class names for backwards compat in user code\n def __getattr__(name):\n     if name == \"PersonNameUnicode\":\n         warnings.warn(\n             \"'PersonNameUnicode' is deprecated and will be removed in \"\n             \"pydicom v3.0, use 'PersonName' instead\",\n             DeprecationWarning\n         )\n         return globals()['PersonName']\n \n     raise AttributeError(f\"module {__name__} has no attribute {name}\")\n \n if sys.version_info[:2] < (3, 7):\n     PersonNameUnicode = PersonName\n```", "model_patch": "--- a/pydicom/valuerep.py\n+++ b/pydicom/valuerep.py\n@@ -15,10 +15,12 @@\n from typing import (\n     TypeVar, Type, Tuple, Optional, List, Dict, Union, Any, Generator, AnyStr,\n     Callable, Iterator, overload\n )\n+from typing import NoReturn\n from typing import Sequence as SequenceType\n import warnings\n \n # don't import datetime_conversion directly\n from pydicom import config\n from pydicom.multival import MultiValue\n from pydicom.uid import UID\n \n@@ -22,6 +24\n # Types\n _T = TypeVar('_T')\n _DA = TypeVar(\"_DA\", bound=\"DA\")\n _DT = TypeVar(\"_DT\", bound=\"DT\")\n _TM = TypeVar(\"_TM\", bound=\"TM\")\n _IS = TypeVar(\"_IS\", bound=\"IS\")\n _DSfloat = TypeVar(\"_DSfloat\", bound=\"DSfloat\")\n _DSdecimal = TypeVar(\"_DSdecimal\", bound=\"DSdecimal\")\n _PersonName = TypeVar(\"_PersonName\", bound=\"PersonName\")\n \n+_NoReturn = TypeVar('_NoReturn', bound=NoReturn)\n \n # can't import from charset or get circular import\n default_encoding = \"iso8859\"\n \n@@ -56,7 +60\n     def __getstate__(self) -> Dict[str, Any]:\n         return self.__dict__.copy()\n \n     def __setstate__(self, state: Dict[str, Any]) -> None:\n         self.__dict__.update(state)\n \n     def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n         return super().__reduce_ex__(protocol) + (self.__getstate__(),)\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         return super().__str__()\n \n     def __repr__(self) -> str:\n         return f'\"{str(self)}\"'\n \n@@ -81,6 +88\n     def __new__(\n         cls: Type[_DA], val: Union[None, str, _DA, datetime.date]\n     ) -> Optional[_DA]:\n         \"\"\"Create an instance of DA object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the DA definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty date\n \n             if len(val) == 8:\n                 year = int(val[0:4])\n                 month = int(val[4:6])\n                 day = int(val[6:8])\n                 return super().__new__(cls, year, month, day)\n \n             if len(val) == 10 and val[4] == '.' and val[7] == '.':\n                 # ACR-NEMA Standard 300, predecessor to DICOM\n                 # for compatibility with a few old pydicom example files\n                 year = int(val[0:4])\n                 month = int(val[5:7])\n                 day = int(val[8:10])\n                 return super().__new__(cls, year, month, day)\n \n         if isinstance(val, datetime.date):\n             return super().__new__(cls, val.year, val.month, val.day)\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DA' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _DA, datetime.date]) -> None:\n         \"\"\"Create a new **DA** element value.\"\"\"\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DA) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class DT(_DateTimeBase, datetime.datetime):\n     \"\"\"Store value for an element with VR **DT** as :class:`datetime.datetime`.\n \n     Note that the :class:`datetime.datetime` base class is immutable.\n     \"\"\"\n     _regex_dt = re.compile(r\"((\\d{4,14})(\\.(\\d{1,6}))?)([+-]\\d{4})?\")\n \n     @staticmethod\n     def _utc_offset(value: str) -> datetime.timezone:\n         \"\"\"Return the UTC Offset suffix as a :class:`datetime.timezone`.\n \n         Parameters\n         ----------\n         value : str\n             The value of the UTC offset suffix, such as ``'-1000'`` or\n             ``'+0245'``.\n \n         Returns\n         -------\n         datetime.timezone\n         \"\"\"\n         # Format is &ZZXX, & = '+' or '-', ZZ is hours, XX is minutes\n         hour = int(value[1:3]) * 60  # Convert hours to minutes\n         minute = int(value[3:5])  # In minutes\n         offset = (hour + minute) * 60  # Convert minutes to seconds\n         offset = -offset if value[0] == '-' else offset\n \n         return datetime.timezone(\n             datetime.timedelta(seconds=offset),\n             name=value\n         )\n \n     def __new__(\n         cls: Type[_DT], val: Union[None, str, _DT, datetime.datetime]\n     ) -> Optional[_DT]:\n         \"\"\"Create an instance of DT object.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the DT definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None\n \n             match = cls._regex_dt.match(val)\n             if not match or len(val) > 26:\n                 raise ValueError(\n                     f\"Unable to convert non-conformant value '{val}' to 'DT' \"\n                     \"object\"\n                 )\n \n             dt_match = match.group(2)\n             args = [\n                 int(dt_match[0:4]),  # year\n                 1 if len(dt_match) < 6 else int(dt_match[4:6]),  # month\n                 1 if len(dt_match) < 8 else int(dt_match[6:8]),  # day\n                 0 if len(dt_match) < 10 else int(dt_match[8:10]),  # hour\n                 0 if len(dt_match) < 12 else int(dt_match[10:12]),  # minute\n                 0 if len(dt_match) < 14 else int(dt_match[12:14]),  # second\n             ]\n             # microsecond\n             if len(dt_match) >= 14 and match.group(4):\n                 args.append(int(match.group(4).rstrip().ljust(6, '0')))\n             else:\n                 args.append(0)\n \n             # Timezone offset\n             tz_match = match.group(5)\n             args.append(cls._utc_offset(tz_match) if tz_match else None)\n \n             if args[5] == 60:\n                 warnings.warn(\n                     \"'datetime.datetime' doesn't allow a value of '60' for \"\n                     \"the seconds component, changing to '59'\"\n                 )\n                 args[5] = 59\n \n             return super().__new__(cls, *args)\n \n         if isinstance(val, datetime.datetime):\n             return super().__new__(\n                 cls, *val.timetuple()[:6], val.microsecond, val.tzinfo\n             )\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'DT' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _DT, datetime.datetime]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, DT) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n \n class TM(_DateTimeBase, datetime.time):\n     \"\"\"Store value for an element with VR **TM** as :class:`datetime.time`.\n \n     Note that the :class:`datetime.time` base class is immutable.\n     \"\"\"\n     _RE_TIME = re.compile(\n         r\"(?P<h>^([01][0-9]|2[0-3]))\"\n         r\"((?P<m>([0-5][0-9]))?\"\n         r\"(?(5)(?P<s>([0-5][0-9]|60))?)\"\n         r\"(?(7)(\\.(?P<ms>([0-9]{1,6})?))?))$\"\n     )\n \n     def __new__(\n         cls: Type[_TM], val: Union[None, str, _TM, datetime.time]\n     ) -> Optional[_TM]:\n         \"\"\"Create an instance of TM object from a string.\n \n         Raise an exception if the string cannot be parsed or the argument\n         is otherwise incompatible.\n \n         Parameters\n         ----------\n         val : str\n             A string conformant to the TM definition in the DICOM Standard,\n             Part 5, :dcm:`Table 6.2-1<part05/sect_6.2.html#table_6.2-1>`.\n         \"\"\"\n         if val is None:\n             return None\n \n         if isinstance(val, str):\n             if val.strip() == '':\n                 return None  # empty time\n \n             match = cls._RE_TIME.match(val)\n             if not match:\n                 raise ValueError(\n                     f\"Unable to convert non-conformant value '{val}' to 'TM' \"\n                     \"object\"\n                 )\n \n             hour = int(match.group('h'))\n             minute = 0 if match.group('m') is None else int(match.group('m'))\n             second = 0 if match.group('s') is None else int(match.group('s'))\n \n             if second == 60:\n                 warnings.warn(\n                     \"'datetime.time' doesn't allow a value of '60' for the \"\n                     \"seconds component, changing to '59'\"\n                 )\n                 second = 59\n \n             microsecond = 0\n             if match.group('ms'):\n                 microsecond = int(match.group('ms').rstrip().ljust(6, '0'))\n \n             return super().__new__(cls, hour, minute, second, microsecond)\n \n         if isinstance(val, datetime.time):\n             return super().__new__(\n                 cls, val.hour, val.minute, val.second, val.microsecond\n             )\n \n         try:\n             return super().__new__(cls, val)\n         except Exception as exc:\n             raise ValueError(\n                 f\"Unable to convert '{val}' to 'TM' object\"\n             ) from exc\n \n     def __init__(self, val: Union[str, _TM, datetime.time]) -> None:\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, TM) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n     if platform.python_implementation() == \"PyPy\":\n         # Workaround for CPython/PyPy bug in time.__reduce_ex__()\n         #   caused by returning (time, ...) rather than (self.__class__, ...)\n         def __reduce_ex__(self, protocol: int) -> Union[str, Tuple[Any, ...]]:\n             return (\n                 self.__class__,\n                 super()._getstate(protocol),\n                 self.__getstate__()\n             )\n \n class DSfloat(float):\n     \"\"\"Store value for an element with VR **DS** as :class:`float`.\n \n     If constructed from an empty string, return the empty string,\n     not an instance of this class.\n \n     \"\"\"\n     def __init__(\n         self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later.\n         \"\"\"\n         # ... also if user changes a data element value, then will get\n         # a different object, because float is immutable.\n         has_attribute = hasattr(val, 'original_string')\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_attribute:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n         return f'\"{super().__repr__()}\"'\n \n class DSdecimal(Decimal):\n     \"\"\"Store value for an element with VR **DS** as :class:`decimal.Decimal`.\n \n     Notes\n     -----\n     If constructed from an empty string, returns the empty string, not an\n     instance of this class.\n     \"\"\"\n     def __new__(\n         cls: Type[_DSdecimal],\n         val: Union[str, int, float, Decimal]\n     ) -> Optional[_DSdecimal]:\n         \"\"\"Create an instance of DS object, or return a blank string if one is\n         passed in, e.g. from a type 2 DICOM blank value.\n \n         Parameters\n         ----------\n         val : str or numeric\n             A string or a number type which can be converted to a decimal.\n         \"\"\"\n         if isinstance(val, float) and not config.allow_DS_float:\n             raise TypeError(\n                 \"'DS' cannot be instantiated with a float value unless \"\n                 \"'config.allow_DS_float' is set to True. You should convert \"\n                 \"the value to a string with the desired number of digits, \"\n                 \"or use 'Decimal.quantize()' and pass a 'Decimal' instance.\"\n             )\n \n         if isinstance(val, str):\n             val = val.strip()\n             if val == '':\n                 return None\n \n         val = super().__new__(cls, val)\n         if len(str(val)) > 16 and config.enforce_valid_values:\n             raise OverflowError(\n                 \"Values for elements with a VR of 'DS' values must be <= 16 \"\n                 \"characters long. Use a smaller string, set \"\n                 \"'config.enforce_valid_values' to False to override the \"\n                 \"length check, or use 'Decimal.quantize()' and initialize \"\n                 \"with a 'Decimal' instance.\"\n             )\n \n         return val\n \n     def __init__(\n         self, val: Union[str, int, float, Decimal]\n     ) -> None:\n         \"\"\"Store the original string if one given, for exact write-out of same\n         value later. E.g. if set ``'1.23e2'``, :class:`~decimal.Decimal` would\n         write ``'123'``, but :class:`DS` will use the original.\n         \"\"\"\n         # ... also if user changes a data element value, then will get\n         # a different Decimal, as Decimal is immutable.\n         has_str = hasattr(val, 'original_string')\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, (DSfloat, DSdecimal)) and has_str:\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         has_str = hasattr(self, 'original_string')\n         if has_str and len(self.original_string) <= 16:\n             return self.original_string\n \n         return super().__str__()\n \n     def __repr__(self) -> str:\n         return f'\"{str(self)}\"'\n \n # CHOOSE TYPE OF DS\n if config.use_DS_decimal:\n     DSclass = DSdecimal\n else:\n     DSclass = DSfloat\n \n def DS(\n     val: Union[None, str, int, float, Decimal]\n ) -> Union[None, str, DSfloat, DSdecimal]:\n     \"\"\"Factory function for creating DS class instances.\n \n     Checks for blank string; if so, returns that, else calls :class:`DSfloat`\n     or :class:`DSdecimal` to create the class instance. This avoids overriding\n     ``DSfloat.__new__()`` (which carries a time penalty for large arrays of\n     DS).\n \n     Similarly the string clean and check can be avoided and :class:`DSfloat`\n     called directly if a string has already been processed.\n     \"\"\"\n     if isinstance(val, str):\n         val = val.strip()\n \n     if val == '' or val is None:\n         return val\n \n     return DSclass(val)\n \n class IS(int):\n     \"\"\"Store value for an element with VR **IS** as :class:`int`.\n \n     Stores original integer string for exact rewriting of the string\n     originally read or stored.\n     \"\"\"\n \n     def __new__(\n         cls: Type[_IS], val: Union[None, str, int, float, Decimal]\n     ) -> Optional[_IS]:\n         \"\"\"Create instance if new integer string\"\"\"\n         if val is None:\n             return val\n \n         if isinstance(val, str) and val.strip() == '':\n             return None\n \n         try:\n             newval: _IS = super().__new__(cls, val)\n         except ValueError:\n             # accept float strings when no integer loss, e.g. \"1.0\"\n             newval: _IS = super().__new__(cls, float(val))\n \n         # check if a float or Decimal passed in, then could have lost info,\n         # and will raise error. E.g. IS(Decimal('1')) is ok, but not IS(1.23)\n         #   IS('1.23') will raise ValueError\n         if isinstance(val, (float, Decimal, str)) and newval != float(val):\n             raise TypeError(\"Could not convert value to integer without loss\")\n \n         # Checks in case underlying int is >32 bits, DICOM does not allow this\n         if not -2**31 <= newval < 2**31 and config.enforce_valid_values:\n             raise OverflowError(\n                 \"Elements with a VR of IS must have a value between -2**31 \"\n                 \"and (2**31 - 1). Set 'config.enforce_valid_values' to False \"\n                 \"to override the value check\"\n             )\n \n         return newval\n \n     def __init__(self, val: Union[str, int, float, Decimal]) -> None:\n         # If a string passed, then store it\n         if isinstance(val, str):\n             self.original_string = val\n         elif isinstance(val, IS) and hasattr(val, 'original_string'):\n             self.original_string = val.original_string\n \n     def __str__(self) -> str:\n         if hasattr(self, 'original_string'):\n             return self.original_string\n \n         # Issue #937 (Python 3.8 compatibility)\n         return repr(self)[1:-1]\n \n     def __repr__(self) -> str:\n         return f'\"{super().__repr__()}\"'\n \n def MultiString(\n     val: str,\n     valtype: Optional[Union[Type[_T], Callable[[object], _T]]] = None\n ) -> Union[_T, SequenceType[_T]]:\n     \"\"\"Split a bytestring by delimiters if there are any\n \n     Parameters\n     ----------\n     val : str\n         The string to split up.\n     valtype : type or callable, optional\n         Default :class:`str`, but can be e.g. :class:`~pydicom.uid.UID` to\n         overwrite to a specific type.\n \n     Returns\n     -------\n     valtype or MultiValue of valtype\n         The split value as `valtype` or a :class:`list` of `valtype`.\n     \"\"\"\n     valtype = str if valtype is None else valtype\n     # Remove trailing blank used to pad to even length\n     # 2005.05.25: also check for trailing 0, error made\n     # in PET files we are converting\n     while val and val.endswith((' ', '\\x00')):\n         val = val[:-1]\n \n     splitup = val.split(\"\\\\\")\n     if len(splitup) == 1:\n         val = splitup[0]\n         return valtype(val) if val else val\n \n     return MultiValue(valtype, splitup)\n \n def _verify_encodings(encodings):\n     \"\"\"Checks the encoding to ensure proper format\"\"\"\n     if encodings is not None:\n         if not isinstance(encodings, (list, tuple)):\n             return encodings,\n         return tuple(encodings)\n     return encodings\n \n def _decode_personname(components, encodings):\n     \"\"\"Return a list of decoded person name components.\n \n     Parameters\n     ----------\n     components : list of byte string\n         The list of the up to three encoded person name components\n     encodings : list of str\n         The Python encodings uses to decode `components`.\n \n     Returns\n     -------\n     text type\n         The unicode string representing the person name.\n         If the decoding of some component parts is not possible using the\n         given encodings, they are decoded with the first encoding using\n         replacement characters for bytes that cannot be decoded.\n     \"\"\"\n     from pydicom.charset import decode_bytes\n \n     if isinstance(components[0], str):\n         comps = components\n     else:\n         comps = [\n             decode_bytes(comp, encodings, PN_DELIMS) for comp in components\n         ]\n     # Remove empty elements from the end to avoid trailing '='\n     while len(comps) and not comps[-1]:\n         comps.pop()\n     return tuple(comps)\n \n def _encode_personname(components, encodings):\n     \"\"\"Encode a list of text string person name components.\n \n     Parameters\n     ----------\n     components : list of text type\n         The list of text string representing the person name components.\n     encodings : list of str\n         The Python encodings uses to encode `components`.\n \n     Returns\n     -------\n     byte string\n         The byte string that can be written as a PN DICOM tag value.\n         If the encoding of some component parts is not possible using the\n         given encodings, they are encoded with the first encoding using\n         replacement bytes for characters that cannot be encoded.\n     \"\"\"\n     from pydicom.charset import encode_string\n \n     encoded_comps = []\n     for comp in components:\n         groups = [encode_string(group, encodings)\n                   for group in comp.split('^')]\n         encoded_comps.append(b'^'.join(groups))\n \n     # Remove empty elements from the end\n     while len(encoded_comps) and not encoded_comps[-1]:\n         encoded_comps.pop()\n     return b'='.join(encoded_comps)\n \n class PersonName:\n     \"\"\"Representation of the value for an element with VR **PN**.\"\"\"\n     def __new__(\n         cls: Type[_PersonName], *args, **kwargs\n     ) -> Optional[_PersonName]:\n         # Handle None value by returning None instead of a PersonName object\n         if len(args) and args[0] is None:\n             return None\n \n         return super().__new__(cls)\n \n     def __init__(\n         self,\n         val: Union[bytes, str, \"PersonName\"],\n         encodings: Optional[List[str]] = None,\n         original_string: Optional[str] = None\n     ) -> None:\n         \"\"\"Create a new ``PersonName``.\n \n         Parameters\n         ----------\n         val: str, bytes, PersonName\n             The value to use for the **PN** element.\n         encodings: list of str, optional\n             A list of the encodings used for the value.\n         original_string: str, optional\n             When creating a ``PersonName`` using a decoded string, this is the\n             original encoded value.\n         \"\"\"\n         self.original_string: Union[None, str, bytes] = None\n         self._components = None\n \n         if isinstance(val, PersonName):\n             encodings = val.encodings\n             self.original_string = val.original_string\n             self._components = tuple(str(val).split('='))\n         elif isinstance(val, bytes):\n             # this is the raw byte string - decode it on demand\n             self.original_string = val\n             self._components = None\n         else:\n             # handle None `val` as empty string\n             val = val or ''\n \n             # this is the decoded string - save the original string if\n             # available for easier writing back\n             self.original_string = original_string\n             components = val.split('=')\n             # Remove empty elements from the end to avoid trailing '='\n             while len(components) and not components[-1]:\n                 components.pop()\n             self._components = tuple(components)\n \n             # if the encoding is not given, leave it as undefined (None)\n         self.encodings: List[str] = _verify_encodings(encodings)\n         self._dict = {}\n \n     def _create_dict(self) -> Dict[str, str]:\n         \"\"\"Creates a dictionary of person name group and component names.\n \n         Used exclusively for `formatted` for backwards compatibility.\n         \"\"\"\n         parts = [\n             'family_name', 'given_name', 'middle_name', 'name_prefix',\n             'name_suffix', 'ideographic', 'phonetic'\n         ]\n         return {c: getattr(self, c, '') for c in parts}\n \n     @property\n     def components(self) -> List[str]:\n         \"\"\"Returns up to three decoded person name components.\n \n         .. versionadded:: 1.2\n \n         The returned components represent the alphabetic, ideographic and\n         phonetic representations as a list of unicode strings.\n         \"\"\"\n         if self._components is None:\n             groups = self.original_string.split(b'=')\n             encodings = self.encodings or [default_encoding]\n             self._components = _decode_personname(groups, encodings)\n \n         return self._components\n \n     def _name_part(self, i) -> str:\n         \"\"\"Return the `i`th part of the name.\"\"\"\n         try:\n             return self.components[0].split('^')[i]\n         except IndexError:\n             return ''\n \n     @property\n     def family_name(self) -> str:\n         \"\"\"Return the first (family name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(0)\n \n     @property\n     def given_name(self) -> str:\n         \"\"\"Return the second (given name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(1)\n \n     @property\n     def middle_name(self) -> str:\n         \"\"\"Return the third (middle name) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(2)\n \n     @property\n     def name_prefix(self) -> str:\n         \"\"\"Return the fourth (name prefix) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(3)\n \n     @property\n     def name_suffix(self) -> str:\n         \"\"\"Return the fifth (name suffix) group of the alphabetic person name\n         representation as a unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         return self._name_part(4)\n \n     @property\n     def ideographic(self) -> str:\n         \"\"\"Return the second (ideographic) person name component as a\n         unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         try:\n             return self.components[1]\n         except IndexError:\n             return ''\n \n     @property\n     def phonetic(self) -> str:\n         \"\"\"Return the third (phonetic) person name component as a\n         unicode string\n \n         .. versionadded:: 1.2\n         \"\"\"\n         try:\n             return self.components[2]\n         except IndexError:\n             return ''\n \n     def __eq__(self, other: object) -> bool:\n         \"\"\"Return ``True`` if `other` equals the current name.\"\"\"\n         return str(self) == other\n \n     def __ne__(self, other: object) -> bool:\n         \"\"\"Return ``True`` if `other` doesn't equal the current name.\"\"\"\n         return not self == other\n \n     def __str__(self) -> str:\n         \"\"\"Return a string representation of the name.\"\"\"\n         return '='.join(self.components).__str__()\n \n     def __iter__(self) -> Generator[str, None, None]:\n         \"\"\"Iterate through the name.\"\"\"\n         yield from self.__str__()\n \n     def __len__(self) -> int:\n         \"\"\"Return the length of the person name.\"\"\"\n         return len(self.__str__())\n \n     def __contains__(self, x: str) -> bool:\n         \"\"\"Return ``True`` if `x` is in the name.\"\"\"\n         return x in self.__str__()\n \n     def __repr__(self) -> str:\n         \"\"\"Return a representation of the name.\"\"\"\n         return '='.join(self.components).__repr__()\n \n     def __hash__(self) -> int:\n         \"\"\"Return a hash of the name.\"\"\"\n         return hash(self.components)\n \n     def decode(self, encodings: Optional[List[str]] = None) -> \"PersonName\":\n         \"\"\"Return the patient name decoded by the given `encodings`.\n \n         Parameters\n         ----------\n         encodings : list of str, optional\n             The list of encodings used for decoding the byte string. If not\n             given, the initial encodings set in the object are used.\n \n         Returns\n         -------\n         valuerep.PersonName\n             A person name object that will return the decoded string with\n             the given encodings on demand. If the encodings are not given,\n             the current object is returned.\n         \"\"\"\n         # in the common case (encoding did not change) we decode on demand\n         if encodings is None or encodings == self.encodings:\n             return self\n         # the encoding was unknown or incorrect - create a new\n         # PersonName object with the changed encoding\n         encodings = _verify_encodings(encodings)\n         if self.original_string is None:\n             # if the original encoding was not set, we set it now\n             self.original_string = _encode_personname(\n                 self.components, self.encodings or [default_encoding])\n         return PersonName(self.original_string, encodings)\n \n     def encode(\n         self, encodings: Optional[List[str]] = None\n     ) -> bytes:\n         \"\"\"Return the patient name decoded by the given `encodings`.\n \n         Parameters\n         ----------\n         encodings : list of str, optional\n             The list of encodings used for encoding the unicode string. If\n             not given, the initial encodings set in the object are used.\n \n         Returns\n         -------\n         bytes\n             The person name encoded with the given encodings as a byte string.\n             If no encoding is given, the original byte string is returned, if\n             available, otherwise each group of the patient name is encoded\n             with the first matching of the given encodings.\n         \"\"\"\n         encodings = _verify_encodings(encodings) or self.encodings\n \n         # if the encoding is not the original encoding, we have to return\n         # a re-encoded string (without updating the original string)\n         if encodings != self.encodings and self.encodings is not None:\n             return _encode_personname(self.components, encodings)\n         if self.original_string is None:\n             # if the original encoding was not set, we set it now\n             self.original_string = _encode_personname(\n                 self.components, encodings or [default_encoding])\n         return self.original_string\n \n     def family_comma_given(self) -> str:\n         \"\"\"Return the name as \"Family, Given\".\"\"\"\n         return self.formatted('%(family_name)s, %(given_name)s')\n \n     def formatted(self, format_str: str) -> str:\n         \"\"\"Return the name as a :class:`str` formatted using `format_str`.\"\"\"\n         return format_str % self._create_dict()\n \n     def __bool__(self) -> bool:\n         \"\"\"Return ``True`` if the name is not empty.\"\"\"\n         if self.original_string is None:\n             return (\n                 bool(self._components)\n                 and (len(self._components) > 1 or bool(self._components[0]))\n             )\n \n         return bool(self.original_string)\n \n # Alias old class names for backwards compat in user code\n def __getattr__(name):\n     if name == \"PersonNameUnicode\":\n         warnings.warn(\n             \"'PersonNameUnicode' is deprecated and will be removed in \"\n             \"pydicom v3.0, use 'PersonName' instead\",\n             DeprecationWarning\n         )\n         return globals()['PersonName']\n \n     raise AttributeError(f\"module {__name__} has no attribute {name}\")\n \n if sys.version_info[:2] < (3, 7):\n     PersonNameUnicode = PersonName\n"}
{"instance_id": "pydicom__pydicom-1416", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nSegmented LUTs are incorrectly expanded\n**Describe the bug**\r\n`pydicom.pixel_data_handlers.util._expand_segmented_lut()` expands segmented LUTs to an incorrect length.\r\n\r\n**Expected behavior**\r\nA correct length LUT to be produced.\r\n\r\n**Steps To Reproduce**\r\nInitialize the following variables.\r\n```\r\nimport numpy as np\r\nlength = 48\r\ny0 = 163\r\ny1 = 255\r\n```\r\n\r\nRun the following two lines from [`pydicom.pixel_data_handlers.util._expand_segmented_lut()`](https://github.com/pydicom/pydicom/blob/699c9f0a8e190d463dd828822106250523d38154/pydicom/pixel_data_handlers/util.py#L875\r\n)\r\n```\r\nstep = (y1 - y0) / length\r\nvals = np.around(np.arange(y0 + step, y1 + step, step))\r\n```\r\n\r\nConfirm that variable `vals` if of incorrect length\r\n```\r\nprint(len(vals) == length)\r\n> False\r\n```\r\n\r\nAlternatively, the code below produces similarly false results\r\n\r\n```\r\nfrom pydicom.pixel_data_handlers.util import _expand_segmented_lut \r\nlut = _expand_segmented_lut(([0, 1, 163, 1, 48, 255]), \"B\")\r\nprint(len(lut) == (1+48))\r\n> False\r\n```\r\n\r\n`np.arange` [explicitly states](https://numpy.org/doc/stable/reference/generated/numpy.arange.html) that it's \"results will often not be consistent\" when using \"non-integer step\", which is a very possible scenario in this function. The following alternative code does function correctly:\r\n\r\n```\r\nvals = np.around(np.linspace(y0 + step, y1, length))\r\n```\r\n\r\n**Your environment**\r\n```bash\r\n$ python -m pydicom.env_info\r\nmodule       | version\r\n------       | -------\r\nplatform     | Darwin-20.5.0-x86_64-i386-64bit\r\nPython       | 3.7.10 (default, Feb 26 2021, 10:16:00)  [Clang 10.0.0 ]\r\npydicom      | 2.1.2\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | 1.20.3\r\nPIL          | 8.2.0\r\n```\n\n</issue>\n<code>\n[start of README.md]\n1 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)\n2 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)\n3 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)\n4 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)\n5 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4197955.svg)](https://doi.org/10.5281/zenodo.4197955)\n6 [![Gitter](https://badges.gitter.im/pydicom/Lobby.svg)](https://gitter.im/pydicom/Lobby?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n7 \n8 # *pydicom*\n9 \n10 *pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files. It lets you read, modify and write DICOM data in an easy \"pythonic\" way.\n11 \n12 As a pure Python package, *pydicom* can run anywhere Python runs without any other requirements, although if you're working with *Pixel Data* then we recommend you also install [NumPy](http://www.numpy.org).\n13 \n14 If you're looking for a Python library for DICOM networking then you might be interested in another of our projects: [pynetdicom](https://github.com/pydicom/pynetdicom).\n15 \n16 ## Installation\n17 \n18 Using [pip](https://pip.pypa.io/en/stable/):\n19 ```\n20 pip install pydicom\n21 ```\n22 Using [conda](https://docs.conda.io/en/latest/):\n23 ```\n24 conda install -c conda-forge pydicom\n25 ```\n26 \n27 For more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).\n28 \n29 \n30 ## Documentation\n31 \n32 The *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/old/pydicom_user_guide.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.\n33 \n34 ## *Pixel Data*\n35 \n36 Compressed and uncompressed *Pixel Data* is always available to\n37 be read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):\n38 ```python\n39 >>> from pydicom import dcmread\n40 >>> from pydicom.data import get_testdata_file\n41 >>> path = get_testdata_file(\"CT_small.dcm\")\n42 >>> ds = dcmread(path)\n43 >>> type(ds.PixelData)\n44 <class 'bytes'>\n45 >>> len(ds.PixelData)\n46 32768\n47 >>> ds.PixelData[:2]\n48 b'\\xaf\\x00'\n49 \n50 ```\n51 \n52 If [NumPy](http://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:\n53 \n54 ```python\n55 >>> arr = ds.pixel_array\n56 >>> arr.shape\n57 (128, 128)\n58 >>> arr\n59 array([[175, 180, 166, ..., 203, 207, 216],\n60        [186, 183, 157, ..., 181, 190, 239],\n61        [184, 180, 171, ..., 152, 164, 235],\n62        ...,\n63        [906, 910, 923, ..., 922, 929, 927],\n64        [914, 954, 938, ..., 942, 925, 905],\n65        [959, 955, 916, ..., 911, 904, 909]], dtype=int16)\n66 ```\n67 ### Compressed *Pixel Data*\n68 #### JPEG, JPEG-LS and JPEG 2000\n69 Converting JPEG compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/stable/old/image_data_handlers.html#guide-compressed).\n70 \n71 Compressing data into one of the JPEG formats is not currently supported.\n72 \n73 #### RLE\n74 Encoding and decoding RLE *Pixel Data* only requires NumPy, however it can\n75 be quite slow. You may want to consider [installing one or more additional\n76 Python libraries](https://pydicom.github.io/pydicom/stable/old/image_data_compression.html) to speed up the process.\n77 \n78 ## Examples\n79 More [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.\n80 \n81 **Change a patient's ID**\n82 ```python\n83 from pydicom import dcmread\n84 \n85 ds = dcmread(\"/path/to/file.dcm\")\n86 # Edit the (0010,0020) 'Patient ID' element\n87 ds.PatientID = \"12345678\"\n88 ds.save_as(\"/path/to/file_updated.dcm\")\n89 ```\n90 \n91 **Display the Pixel Data**\n92 \n93 With [NumPy](http://www.numpy.org) and [matplotlib](https://matplotlib.org/)\n94 ```python\n95 import matplotlib.pyplot as plt\n96 from pydicom import dcmread\n97 from pydicom.data import get_testdata_file\n98 \n99 # The path to a pydicom test dataset\n100 path = get_testdata_file(\"CT_small.dcm\")\n101 ds = dcmread(path)\n102 # `arr` is a numpy.ndarray\n103 arr = ds.pixel_array\n104 \n105 plt.imshow(arr, cmap=\"gray\")\n106 plt.show()\n107 ```\n108 \n109 ## Contributing\n110 \n111 To contribute to *pydicom*, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).\n112 \n113 To contribute an example or extension of *pydicom* that doesn't belong with the core software, see our contribution repository:\n114 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).\n115 \n[end of README.md]\n[start of pydicom/pixel_data_handlers/numpy_handler.py]\n1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n2 \"\"\"Use the `numpy <https://numpy.org/>`_ package to convert supported pixel\n3 data to a :class:`numpy.ndarray`.\n4 \n5 **Supported transfer syntaxes**\n6 \n7 * 1.2.840.10008.1.2 : Implicit VR Little Endian\n8 * 1.2.840.10008.1.2.1 : Explicit VR Little Endian\n9 * 1.2.840.10008.1.2.1.99 : Deflated Explicit VR Little Endian\n10 * 1.2.840.10008.1.2.2 : Explicit VR Big Endian\n11 \n12 **Supported data**\n13 \n14 The numpy handler supports the conversion of data in the (7FE0,0008) *Float\n15 Pixel Data*, (7FE0,0009) *Double Float Pixel Data* and (7FE0,0010)\n16 *Pixel Data* elements to a :class:`~numpy.ndarray` provided the\n17 related :dcm:`Image Pixel<part03/sect_C.7.6.3.html>`, :dcm:`Floating Point\n18 Image Pixel<part03/sect_C.7.6.24.html>` or  :dcm:`Double Floating Point Image\n19 Pixel<part03/sect_C.7.6.25.html>` module elements have values given in the\n20 table below.\n21 \n22 +------------------------------------------------+---------------+----------+\n23 | Element                                        | Supported     |          |\n24 +-------------+---------------------------+------+ values        |          |\n25 | Tag         | Keyword                   | Type |               |          |\n26 +=============+===========================+======+===============+==========+\n27 | (0028,0002) | SamplesPerPixel           | 1    | N             | Required |\n28 +-------------+---------------------------+------+---------------+----------+\n29 | (0028,0004) | PhotometricInterpretation | 1    | MONOCHROME1,  | Required |\n30 |             |                           |      | MONOCHROME2,  |          |\n31 |             |                           |      | RGB,          |          |\n32 |             |                           |      | YBR_FULL,     |          |\n33 |             |                           |      | YBR_FULL_422  |          |\n34 +-------------+---------------------------+------+---------------+----------+\n35 | (0028,0006) | PlanarConfiguration       | 1C   | 0, 1          | Optional |\n36 +-------------+---------------------------+------+---------------+----------+\n37 | (0028,0008) | NumberOfFrames            | 1C   | N             | Optional |\n38 +-------------+---------------------------+------+---------------+----------+\n39 | (0028,0010) | Rows                      | 1    | N             | Required |\n40 +-------------+---------------------------+------+---------------+----------+\n41 | (0028,0011) | Columns                   | 1    | N             | Required |\n42 +-------------+---------------------------+------+---------------+----------+\n43 | (0028,0100) | BitsAllocated             | 1    | 1, 8, 16, 32, | Required |\n44 |             |                           |      | 64            |          |\n45 +-------------+---------------------------+------+---------------+----------+\n46 | (0028,0103) | PixelRepresentation       | 1    | 0, 1          | Required |\n47 +-------------+---------------------------+------+---------------+----------+\n48 \n49 \"\"\"\n50 \n51 try:\n52     import numpy as np  # type: ignore[import]\n53     HAVE_NP = True\n54 except ImportError:\n55     HAVE_NP = False\n56 \n57 import warnings\n58 \n59 from pydicom.pixel_data_handlers.util import pixel_dtype, get_expected_length\n60 import pydicom.uid\n61 \n62 HANDLER_NAME = 'Numpy'\n63 \n64 DEPENDENCIES = {\n65     'numpy': ('http://www.numpy.org/', 'NumPy'),\n66 }\n67 \n68 SUPPORTED_TRANSFER_SYNTAXES = [\n69     pydicom.uid.ExplicitVRLittleEndian,\n70     pydicom.uid.ImplicitVRLittleEndian,\n71     pydicom.uid.DeflatedExplicitVRLittleEndian,\n72     pydicom.uid.ExplicitVRBigEndian,\n73 ]\n74 \n75 \n76 def is_available():\n77     \"\"\"Return ``True`` if the handler has its dependencies met.\"\"\"\n78     return HAVE_NP\n79 \n80 \n81 def supports_transfer_syntax(transfer_syntax):\n82     \"\"\"Return ``True`` if the handler supports the `transfer_syntax`.\n83 \n84     Parameters\n85     ----------\n86     transfer_syntax : uid.UID\n87         The Transfer Syntax UID of the *Pixel Data* that is to be used with\n88         the handler.\n89     \"\"\"\n90     return transfer_syntax in SUPPORTED_TRANSFER_SYNTAXES\n91 \n92 \n93 def needs_to_convert_to_RGB(ds):\n94     \"\"\"Return ``True`` if the *Pixel Data* should to be converted from YCbCr to\n95     RGB.\n96 \n97     This affects JPEG transfer syntaxes.\n98     \"\"\"\n99     return False\n100 \n101 \n102 def should_change_PhotometricInterpretation_to_RGB(ds):\n103     \"\"\"Return ``True`` if the *Photometric Interpretation* should be changed\n104     to RGB.\n105 \n106     This affects JPEG transfer syntaxes.\n107     \"\"\"\n108     return False\n109 \n110 \n111 def pack_bits(arr: \"np.ndarray\", pad: bool = True) -> bytes:\n112     \"\"\"Pack a binary :class:`numpy.ndarray` for use with *Pixel Data*.\n113 \n114     .. versionadded:: 1.2\n115 \n116     Should be used in conjunction with (0028,0100) *Bits Allocated* = 1.\n117 \n118     .. versionchanged:: 2.1\n119 \n120         Added the `pad` keyword parameter and changed to allow `arr` to be\n121         2 or 3D.\n122 \n123     Parameters\n124     ----------\n125     arr : numpy.ndarray\n126         The :class:`numpy.ndarray` containing 1-bit data as ints. `arr` must\n127         only contain integer values of 0 and 1 and must have an 'uint'  or\n128         'int' :class:`numpy.dtype`. For the sake of efficiency it's recommended\n129         that the length of `arr` be a multiple of 8 (i.e. that any empty\n130         bit-padding to round out the byte has already been added). The input\n131         `arr` should either be shaped as (rows, columns) or (frames, rows,\n132         columns) or the equivalent 1D array used to ensure that the packed\n133         data is in the correct order.\n134     pad : bool, optional\n135         If ``True`` (default) then add a null byte to the end of the packed\n136         data to ensure even length, otherwise no padding will be added.\n137 \n138     Returns\n139     -------\n140     bytes\n141         The bit packed data.\n142 \n143     Raises\n144     ------\n145     ValueError\n146         If `arr` contains anything other than 0 or 1.\n147 \n148     References\n149     ----------\n150     DICOM Standard, Part 5,\n151     :dcm:`Section 8.1.1<part05/chapter_8.html#sect_8.1.1>` and\n152     :dcm:`Annex D<part05/chapter_D.html>`\n153     \"\"\"\n154     if arr.shape == (0,):\n155         return bytes()\n156 \n157     # Test array\n158     if not np.array_equal(arr, arr.astype(bool)):\n159         raise ValueError(\n160             \"Only binary arrays (containing ones or zeroes) can be packed.\"\n161         )\n162 \n163     if len(arr.shape) > 1:\n164         arr = arr.ravel()\n165 \n166     # The array length must be a multiple of 8, pad the end\n167     if arr.shape[0] % 8:\n168         arr = np.append(arr, np.zeros(8 - arr.shape[0] % 8))\n169 \n170     # Reshape so each row is 8 bits\n171     arr = np.reshape(arr, (-1, 8))\n172     arr = np.fliplr(arr)\n173     arr = np.packbits(arr.astype('uint8'))\n174 \n175     packed: bytes = arr.tobytes()\n176     if pad:\n177         return packed + b'\\x00' if len(packed) % 2 else packed\n178 \n179     return packed\n180 \n181 \n182 def unpack_bits(bytestream):\n183     \"\"\"Unpack bit packed *Pixel Data* or *Overlay Data* into a\n184     :class:`numpy.ndarray`.\n185 \n186     Suitable for use when (0028,0011) *Bits Allocated* or (60xx,0100) *Overlay\n187     Bits Allocated* is 1.\n188 \n189     Parameters\n190     ----------\n191     bytestream : bytes\n192         The bit packed pixel data.\n193 \n194     Returns\n195     -------\n196     numpy.ndarray\n197         The unpacked *Pixel Data* as a 1D array.\n198 \n199     References\n200     ----------\n201     DICOM Standard, Part 5,\n202     :dcm:`Section 8.1.1<part05/chapter_8.html#sect_8.1.1>` and\n203     :dcm:`Annex D<part05/chapter_D.html>`\n204     \"\"\"\n205     # Thanks to @sbrodehl (#643)\n206     # e.g. b'\\xC0\\x09' -> [192, 9]\n207     arr = np.frombuffer(bytestream, dtype='uint8')\n208     # -> [1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n209     arr = np.unpackbits(arr)\n210     # -> [[1 1 0 0 0 0 0 0],\n211     #     [0 0 0 0 1 0 0 1]]\n212     arr = np.reshape(arr, (-1, 8))\n213     # -> [[0 0 0 0 0 0 1 1],\n214     #     [1 0 0 1 0 0 0 0]]\n215     arr = np.fliplr(arr)\n216     # -> [0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0]\n217     arr = np.ravel(arr)\n218 \n219     return arr\n220 \n221 \n222 def get_pixeldata(ds, read_only=False):\n223     \"\"\"Return a :class:`numpy.ndarray` of the pixel data.\n224 \n225     .. versionchanged:: 1.4\n226 \n227         * Added support for uncompressed pixel data with a *Photometric\n228           Interpretation* of ``YBR_FULL_422``.\n229         * Added support for *Float Pixel Data* and *Double Float Pixel Data*\n230 \n231 \n232     Parameters\n233     ----------\n234     ds : Dataset\n235         The :class:`Dataset` containing an Image Pixel, Floating Point Image\n236         Pixel or Double Floating Point Image Pixel module and the\n237         *Pixel Data*, *Float Pixel Data* or *Double Float Pixel Data* to be\n238         converted. If (0028,0004) *Photometric Interpretation* is\n239         `'YBR_FULL_422'` then the pixel data will be\n240         resampled to 3 channel data as per Part 3, :dcm:`Annex C.7.6.3.1.2\n241         <part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>` of the DICOM Standard.\n242     read_only : bool, optional\n243         If ``False`` (default) then returns a writeable array that no longer\n244         uses the original memory. If ``True`` and the value of (0028,0100)\n245         *Bits Allocated* > 1 then returns a read-only array that uses the\n246         original memory buffer of the pixel data. If *Bits Allocated* = 1 then\n247         always returns a writeable array.\n248 \n249     Returns\n250     -------\n251     np.ndarray\n252         The contents of (7FE0,0010) *Pixel Data*, (7FE0,0008) *Float Pixel\n253         Data* or (7FE0,0009) *Double Float Pixel Data* as a 1D array.\n254 \n255     Raises\n256     ------\n257     AttributeError\n258         If `ds` is missing a required element.\n259     NotImplementedError\n260         If `ds` contains pixel data in an unsupported format.\n261     ValueError\n262         If the actual length of the pixel data doesn't match the expected\n263         length.\n264     \"\"\"\n265     transfer_syntax = ds.file_meta.TransferSyntaxUID\n266     # The check of transfer syntax must be first\n267     if transfer_syntax not in SUPPORTED_TRANSFER_SYNTAXES:\n268         raise NotImplementedError(\n269             \"Unable to convert the pixel data as the transfer syntax \"\n270             \"is not supported by the numpy pixel data handler.\"\n271         )\n272 \n273     # Check required elements\n274     keywords = ['PixelData', 'FloatPixelData', 'DoubleFloatPixelData']\n275     px_keyword = [kw for kw in keywords if kw in ds]\n276     if len(px_keyword) != 1:\n277         raise AttributeError(\n278             \"Unable to convert the pixel data: one of Pixel Data, Float \"\n279             \"Pixel Data or Double Float Pixel Data must be present in \"\n280             \"the dataset\"\n281         )\n282 \n283     required_elements = [\n284         'BitsAllocated', 'Rows', 'Columns', 'PixelRepresentation',\n285         'SamplesPerPixel', 'PhotometricInterpretation'\n286     ]\n287     missing = [elem for elem in required_elements if elem not in ds]\n288     if missing:\n289         raise AttributeError(\n290             \"Unable to convert the pixel data as the following required \"\n291             \"elements are missing from the dataset: \" + \", \".join(missing)\n292         )\n293 \n294     # May be Pixel Data, Float Pixel Data or Double Float Pixel Data\n295     pixel_data = getattr(ds, px_keyword[0])\n296 \n297     # Calculate the expected length of the pixel data (in bytes)\n298     #   Note: this does NOT include the trailing null byte for odd length data\n299     expected_len = get_expected_length(ds)\n300 \n301     # Check that the actual length of the pixel data is as expected\n302     actual_length = len(pixel_data)\n303 \n304     # Correct for the trailing NULL byte padding for odd length data\n305     padded_expected_len = expected_len + expected_len % 2\n306     if actual_length < padded_expected_len:\n307         if actual_length == expected_len:\n308             warnings.warn(\n309                 \"The odd length pixel data is missing a trailing padding byte\"\n310             )\n311         else:\n312             raise ValueError(\n313                 \"The length of the pixel data in the dataset ({} bytes) \"\n314                 \"doesn't match the expected length ({} bytes). \"\n315                 \"The dataset may be corrupted or there may be an issue \"\n316                 \"with the pixel data handler.\"\n317                 .format(actual_length, padded_expected_len)\n318             )\n319     elif actual_length > padded_expected_len:\n320         # PS 3.5, Section 8.1.1\n321         msg = (\n322             \"The length of the pixel data in the dataset ({} bytes) indicates \"\n323             \"it contains excess padding. {} bytes will be removed from the \"\n324             \"end of the data\"\n325             .format(actual_length, actual_length - expected_len)\n326         )\n327         # PS 3.3, Annex C.7.6.3\n328         if ds.PhotometricInterpretation == 'YBR_FULL_422':\n329             # Check to ensure we do have subsampled YBR 422 data\n330             ybr_full_length = expected_len / 2 * 3 + expected_len / 2 * 3 % 2\n331             # >= as may also include excess padding\n332             if actual_length >= ybr_full_length:\n333                 msg = (\n334                     \"The Photometric Interpretation of the dataset is \"\n335                     \"YBR_FULL_422, however the length of the pixel data \"\n336                     \"({} bytes) is a third larger than expected ({} bytes) \"\n337                     \"which indicates that this may be incorrect. You may \"\n338                     \"need to change the Photometric Interpretation to \"\n339                     \"the correct value.\".format(actual_length, expected_len)\n340                 )\n341         warnings.warn(msg)\n342 \n343     # Unpack the pixel data into a 1D ndarray\n344     if ds.BitsAllocated == 1:\n345         # Skip any trailing padding bits\n346         nr_pixels = get_expected_length(ds, unit='pixels')\n347         arr = unpack_bits(pixel_data)[:nr_pixels]\n348     else:\n349         # Skip the trailing padding byte(s) if present\n350         dtype = pixel_dtype(ds, as_float=('Float' in px_keyword[0]))\n351         arr = np.frombuffer(pixel_data[:expected_len], dtype=dtype)\n352         if ds.PhotometricInterpretation == 'YBR_FULL_422':\n353             # PS3.3 C.7.6.3.1.2: YBR_FULL_422 data needs to be resampled\n354             # Y1 Y2 B1 R1 -> Y1 B1 R1 Y2 B1 R1\n355             out = np.zeros(expected_len // 2 * 3, dtype=dtype)\n356             out[::6] = arr[::4]  # Y1\n357             out[3::6] = arr[1::4]  # Y2\n358             out[1::6], out[4::6] = arr[2::4], arr[2::4]  # B\n359             out[2::6], out[5::6] = arr[3::4], arr[3::4]  # R\n360             arr = out\n361 \n362     if should_change_PhotometricInterpretation_to_RGB(ds):\n363         ds.PhotometricInterpretation = \"RGB\"\n364 \n365     if not read_only and ds.BitsAllocated > 1:\n366         return arr.copy()\n367 \n368     return arr\n369 \n[end of pydicom/pixel_data_handlers/numpy_handler.py]\n[start of pydicom/pixel_data_handlers/rle_handler.py]\n1 # Copyright 2008-2021 pydicom authors. See LICENSE file for details.\n2 \"\"\"Use the `numpy <https://numpy.org/>`_ package to convert RLE lossless *Pixel\n3 Data* to a :class:`numpy.ndarray`.\n4 \n5 **Supported transfer syntaxes**\n6 \n7 * 1.2.840.10008.1.2.5 : RLE Lossless\n8 \n9 **Supported data**\n10 \n11 The RLE handler supports the conversion of data in the (7FE0,0010)\n12 *Pixel Data* element to a numpy ndarray provided the related\n13 :dcm:`Image Pixel<part03/sect_C.7.6.3.html>` module elements have values given\n14 in the table below.\n15 \n16 +------------------------------------------------+--------------+----------+\n17 | Element                                        | Supported    |          |\n18 +-------------+---------------------------+------+ values       |          |\n19 | Tag         | Keyword                   | Type |              |          |\n20 +=============+===========================+======+==============+==========+\n21 | (0028,0002) | SamplesPerPixel           | 1    | N            | Required |\n22 +-------------+---------------------------+------+--------------+----------+\n23 | (0028,0006) | PlanarConfiguration       | 1C   | 1            | Optional |\n24 +-------------+---------------------------+------+--------------+----------+\n25 | (0028,0008) | NumberOfFrames            | 1C   | N            | Optional |\n26 +-------------+---------------------------+------+--------------+----------+\n27 | (0028,0010) | Rows                      | 1    | N            | Required |\n28 +-------------+---------------------------+------+--------------+----------+\n29 | (0028,0011) | Columns                   | 1    | N            | Required |\n30 +-------------+---------------------------+------+--------------+----------+\n31 | (0028,0100) | BitsAllocated             | 1    | 8, 16, 32    | Required |\n32 +-------------+---------------------------+------+--------------+----------+\n33 | (0028,0103) | PixelRepresentation       | 1    | 0, 1         | Required |\n34 +-------------+---------------------------+------+--------------+----------+\n35 \n36 \"\"\"\n37 \n38 from struct import unpack\n39 import sys\n40 from typing import List, TYPE_CHECKING, cast\n41 \n42 try:\n43     import numpy as np  # type: ignore[import]\n44     HAVE_RLE = True\n45 except ImportError:\n46     HAVE_RLE = False\n47 \n48 from pydicom.encaps import decode_data_sequence, defragment_data\n49 from pydicom.pixel_data_handlers.util import pixel_dtype\n50 from pydicom.encoders.native import _encode_frame\n51 import pydicom.uid\n52 \n53 if TYPE_CHECKING:  # pragma: no cover\n54     import numpy\n55     from pydicom.dataset import Dataset, FileMetaDataset\n56 \n57 \n58 HANDLER_NAME = 'RLE Lossless'\n59 DEPENDENCIES = {'numpy': ('http://www.numpy.org/', 'NumPy')}\n60 SUPPORTED_TRANSFER_SYNTAXES = [pydicom.uid.RLELossless]\n61 \n62 \n63 def is_available() -> bool:\n64     \"\"\"Return ``True`` if the handler has its dependencies met.\"\"\"\n65     return HAVE_RLE\n66 \n67 \n68 def supports_transfer_syntax(transfer_syntax: str) -> bool:\n69     \"\"\"Return ``True`` if the handler supports the `transfer_syntax`.\n70 \n71     Parameters\n72     ----------\n73     transfer_syntax : uid.UID\n74         The Transfer Syntax UID of the *Pixel Data* that is to be used with\n75         the handler.\n76     \"\"\"\n77     return transfer_syntax in SUPPORTED_TRANSFER_SYNTAXES\n78 \n79 \n80 def needs_to_convert_to_RGB(ds: \"Dataset\") -> bool:\n81     \"\"\"Return ``True`` if the *Pixel Data* should to be converted from YCbCr to\n82     RGB.\n83 \n84     This affects JPEG transfer syntaxes.\n85     \"\"\"\n86     return False\n87 \n88 \n89 def should_change_PhotometricInterpretation_to_RGB(ds: \"Dataset\") -> bool:\n90     \"\"\"Return ``True`` if the *Photometric Interpretation* should be changed\n91     to RGB.\n92 \n93     This affects JPEG transfer syntaxes.\n94     \"\"\"\n95     return False\n96 \n97 \n98 def get_pixeldata(ds: \"Dataset\", rle_segment_order: str = '>') -> \"np.ndarray\":\n99     \"\"\"Return an :class:`numpy.ndarray` of the *Pixel Data*.\n100 \n101     Parameters\n102     ----------\n103     ds : dataset.Dataset\n104         The :class:`Dataset` containing an Image Pixel module and the RLE\n105         encoded *Pixel Data* to be converted.\n106     rle_segment_order : str\n107         The order of segments used by the RLE decoder when dealing with *Bits\n108         Allocated* > 8. Each RLE segment contains 8-bits of the pixel data,\n109         and segments are supposed to be ordered from MSB to LSB. A value of\n110         ``'>'`` means interpret the segments as being in big endian order\n111         (default) while a value of ``'<'`` means interpret the segments as\n112         being in little endian order which may be possible if the encoded data\n113         is non-conformant.\n114 \n115     Returns\n116     -------\n117     numpy.ndarray\n118         The decoded contents of (7FE0,0010) *Pixel Data* as a 1D array.\n119 \n120     Raises\n121     ------\n122     AttributeError\n123         If `ds` is missing a required element.\n124     NotImplementedError\n125         If `ds` contains pixel data in an unsupported format.\n126     ValueError\n127         If the actual length of the pixel data doesn't match the expected\n128         length.\n129     \"\"\"\n130     transfer_syntax = ds.file_meta.TransferSyntaxUID\n131     # The check of transfer syntax must be first\n132     if transfer_syntax not in SUPPORTED_TRANSFER_SYNTAXES:\n133         raise NotImplementedError(\n134             \"Unable to convert the pixel data as the transfer syntax \"\n135             \"is not supported by the RLE pixel data handler.\"\n136         )\n137 \n138     # Check required elements\n139     required_elements = ['PixelData', 'BitsAllocated', 'Rows', 'Columns',\n140                          'PixelRepresentation', 'SamplesPerPixel']\n141     missing = [elem for elem in required_elements if elem not in ds]\n142     if missing:\n143         raise AttributeError(\n144             \"Unable to convert the pixel data as the following required \"\n145             \"elements are missing from the dataset: \" + \", \".join(missing)\n146         )\n147 \n148     nr_bits = cast(int, ds.BitsAllocated)\n149     nr_samples = cast(int, ds.SamplesPerPixel)\n150     nr_frames = cast(int, getattr(ds, 'NumberOfFrames', 1) or 1)\n151     rows = cast(int, ds.Rows)\n152     cols = cast(int, ds.Columns)\n153 \n154     # Decompress each frame of the pixel data\n155     pixel_data = bytearray()\n156     if nr_frames > 1:\n157         for rle_frame in decode_data_sequence(ds.PixelData):\n158             frame = _rle_decode_frame(\n159                 rle_frame, rows, cols, nr_samples, nr_bits, rle_segment_order\n160             )\n161             pixel_data.extend(frame)\n162     else:\n163         frame = _rle_decode_frame(\n164             defragment_data(ds.PixelData),\n165             rows,\n166             cols,\n167             nr_samples,\n168             nr_bits,\n169             rle_segment_order\n170         )\n171         pixel_data.extend(frame)\n172 \n173     arr = np.frombuffer(pixel_data, pixel_dtype(ds))\n174 \n175     if should_change_PhotometricInterpretation_to_RGB(ds):\n176         ds.PhotometricInterpretation = \"RGB\"\n177 \n178     return arr\n179 \n180 \n181 def _parse_rle_header(header: bytes) -> List[int]:\n182     \"\"\"Return a list of byte offsets for the segments in RLE data.\n183 \n184     **RLE Header Format**\n185 \n186     The RLE Header contains the number of segments for the image and the\n187     starting offset of each segment. Each of these numbers is represented as\n188     an unsigned long stored in little-endian. The RLE Header is 16 long words\n189     in length (i.e. 64 bytes) which allows it to describe a compressed image\n190     with up to 15 segments. All unused segment offsets shall be set to zero.\n191 \n192     As an example, the table below describes an RLE Header with 3 segments as\n193     would typically be used with 8-bit RGB or YCbCr data (with 1 segment per\n194     channel).\n195 \n196     +--------------+---------------------------------+------------+\n197     | Byte  offset | Description                     | Value      |\n198     +==============+=================================+============+\n199     | 0            | Number of segments              | 3          |\n200     +--------------+---------------------------------+------------+\n201     | 4            | Offset of segment 1, N bytes    | 64         |\n202     +--------------+---------------------------------+------------+\n203     | 8            | Offset of segment 2, M bytes    | 64 + N     |\n204     +--------------+---------------------------------+------------+\n205     | 12           | Offset of segment 3             | 64 + N + M |\n206     +--------------+---------------------------------+------------+\n207     | 16           | Offset of segment 4 (not used)  | 0          |\n208     +--------------+---------------------------------+------------+\n209     | ...          | ...                             | 0          |\n210     +--------------+---------------------------------+------------+\n211     | 60           | Offset of segment 15 (not used) | 0          |\n212     +--------------+---------------------------------+------------+\n213 \n214     Parameters\n215     ----------\n216     header : bytes\n217         The RLE header data (i.e. the first 64 bytes of an RLE frame).\n218 \n219     Returns\n220     -------\n221     list of int\n222         The byte offsets for each segment in the RLE data.\n223 \n224     Raises\n225     ------\n226     ValueError\n227         If there are more than 15 segments or if the header is not 64 bytes\n228         long.\n229 \n230     References\n231     ----------\n232     DICOM Standard, Part 5, :dcm:`Annex G<part05/chapter_G.html>`\n233     \"\"\"\n234     if len(header) != 64:\n235         raise ValueError('The RLE header can only be 64 bytes long')\n236 \n237     nr_segments = unpack('<L', header[:4])[0]\n238     if nr_segments > 15:\n239         raise ValueError(\n240             \"The RLE header specifies an invalid number of segments ({})\"\n241             .format(nr_segments)\n242         )\n243 \n244     offsets = unpack('<{}L'.format(nr_segments),\n245                      header[4:4 * (nr_segments + 1)])\n246 \n247     return list(offsets)\n248 \n249 \n250 def _rle_decode_frame(\n251     data: bytes,\n252     rows: int,\n253     columns: int,\n254     nr_samples: int,\n255     nr_bits: int,\n256     segment_order: str = '>'\n257 ) -> bytearray:\n258     \"\"\"Decodes a single frame of RLE encoded data.\n259 \n260     Each frame may contain up to 15 segments of encoded data.\n261 \n262     Parameters\n263     ----------\n264     data : bytes\n265         The RLE frame data\n266     rows : int\n267         The number of output rows\n268     columns : int\n269         The number of output columns\n270     nr_samples : int\n271         Number of samples per pixel (e.g. 3 for RGB data).\n272     nr_bits : int\n273         Number of bits per sample - must be a multiple of 8\n274     segment_order : str\n275         The segment order of the `data`, '>' for big endian (default),\n276         '<' for little endian (non-conformant).\n277 \n278     Returns\n279     -------\n280     bytearray\n281         The frame's decoded data in little endian and planar configuration 1\n282         byte ordering (i.e. for RGB data this is all red pixels then all\n283         green then all blue, with the bytes for each pixel ordered from\n284         MSB to LSB when reading left to right).\n285     \"\"\"\n286     if nr_bits % 8:\n287         raise NotImplementedError(\n288             \"Unable to decode RLE encoded pixel data with a (0028,0100) \"\n289             f\"'Bits Allocated' value of {nr_bits}\"\n290         )\n291 \n292     # Parse the RLE Header\n293     offsets = _parse_rle_header(data[:64])\n294     nr_segments = len(offsets)\n295 \n296     # Check that the actual number of segments is as expected\n297     bytes_per_sample = nr_bits // 8\n298     if nr_segments != nr_samples * bytes_per_sample:\n299         raise ValueError(\n300             \"The number of RLE segments in the pixel data doesn't match the \"\n301             f\"expected amount ({nr_segments} vs. \"\n302             f\"{nr_samples * bytes_per_sample} segments)\"\n303         )\n304 \n305     # Ensure the last segment gets decoded\n306     offsets.append(len(data))\n307 \n308     # Preallocate with null bytes\n309     decoded = bytearray(rows * columns * nr_samples * bytes_per_sample)\n310 \n311     # Example:\n312     # RLE encoded data is ordered like this (for 16-bit, 3 sample):\n313     #  Segment: 0     | 1     | 2     | 3     | 4     | 5\n314     #           R MSB | R LSB | G MSB | G LSB | B MSB | B LSB\n315     #  A segment contains only the MSB or LSB parts of all the sample pixels\n316 \n317     # To minimise the amount of array manipulation later, and to make things\n318     # faster we interleave each segment in a manner consistent with a planar\n319     # configuration of 1 (and use little endian byte ordering):\n320     #    All red samples             | All green samples           | All blue\n321     #    Pxl 1   Pxl 2   ... Pxl N   | Pxl 1   Pxl 2   ... Pxl N   | ...\n322     #    LSB MSB LSB MSB ... LSB MSB | LSB MSB LSB MSB ... LSB MSB | ...\n323 \n324     # `stride` is the total number of bytes of each sample plane\n325     stride = bytes_per_sample * rows * columns\n326     for sample_number in range(nr_samples):\n327         le_gen = range(bytes_per_sample)\n328         byte_offsets = le_gen if segment_order == '<' else reversed(le_gen)\n329         for byte_offset in byte_offsets:\n330             # Decode the segment\n331             ii = sample_number * bytes_per_sample + byte_offset\n332             # ii is 1, 0, 3, 2, 5, 4 for the example above\n333             # This is where the segment order correction occurs\n334             segment = _rle_decode_segment(data[offsets[ii]:offsets[ii + 1]])\n335             # Check that the number of decoded pixels is correct\n336             if len(segment) != rows * columns:\n337                 raise ValueError(\n338                     \"The amount of decoded RLE segment data doesn't match the \"\n339                     f\"expected amount ({len(segment)} vs. \"\n340                     f\"{rows * columns} bytes)\"\n341                 )\n342 \n343             if segment_order == '>':\n344                 byte_offset = bytes_per_sample - byte_offset - 1\n345 \n346             # For 100 pixel/plane, 32-bit, 3 sample data, `start` will be\n347             #   0, 1, 2, 3, 400, 401, 402, 403, 800, 801, 802, 803\n348             start = byte_offset + (sample_number * stride)\n349             decoded[start:start + stride:bytes_per_sample] = segment\n350 \n351     return decoded\n352 \n353 \n354 def _rle_decode_segment(data: bytes) -> bytearray:\n355     \"\"\"Return a single segment of decoded RLE data as bytearray.\n356 \n357     Parameters\n358     ----------\n359     data : bytes\n360         The segment data to be decoded.\n361 \n362     Returns\n363     -------\n364     bytearray\n365         The decoded segment.\n366     \"\"\"\n367     data = bytearray(data)\n368     result = bytearray()\n369     pos = 0\n370     result_extend = result.extend\n371 \n372     try:\n373         while True:\n374             # header_byte is N + 1\n375             header_byte = data[pos] + 1\n376             pos += 1\n377             if header_byte > 129:\n378                 # Extend by copying the next byte (-N + 1) times\n379                 # however since using uint8 instead of int8 this will be\n380                 # (256 - N + 1) times\n381                 result_extend(data[pos:pos + 1] * (258 - header_byte))\n382                 pos += 1\n383             elif header_byte < 129:\n384                 # Extend by literally copying the next (N + 1) bytes\n385                 result_extend(data[pos:pos + header_byte])\n386                 pos += header_byte\n387 \n388     except IndexError:\n389         pass\n390 \n391     return result\n392 \n393 \n394 # Old function kept for backwards compatibility\n395 def rle_encode_frame(arr: \"numpy.ndarray\") -> bytes:\n396     \"\"\"Return an :class:`numpy.ndarray` image frame as RLE encoded\n397     :class:`bytearray`.\n398 \n399     .. versionadded:: 1.3\n400 \n401     .. deprecated:: 2.2\n402 \n403         Use :meth:`~pydicom.dataset.Dataset.compress` instead\n404 \n405     Parameters\n406     ----------\n407     arr : numpy.ndarray\n408         A 2D (if *Samples Per Pixel* = 1) or 3D (if *Samples Per Pixel* = 3)\n409         ndarray containing a single frame of the image to be RLE encoded.\n410 \n411     Returns\n412     -------\n413     bytes\n414         An RLE encoded frame, including the RLE header, following the format\n415         specified by the DICOM Standard, Part 5,\n416         :dcm:`Annex G<part05/chapter_G.html>`.\n417     \"\"\"\n418     shape = arr.shape\n419     if len(shape) > 3:\n420         # Note: only raises if multi-sample pixel data with multiple frames\n421         raise ValueError(\n422             \"Unable to encode multiple frames at once, please encode one \"\n423             \"frame at a time\"\n424         )\n425 \n426     # Check the expected number of segments\n427     nr_segments = arr.dtype.itemsize\n428     if len(shape) == 3:\n429         # Number of samples * bytes per sample\n430         nr_segments *= shape[-1]\n431 \n432     if nr_segments > 15:\n433         raise ValueError(\n434             \"Unable to encode as the DICOM standard only allows \"\n435             \"a maximum of 15 segments in RLE encoded data\"\n436         )\n437 \n438     dtype = arr.dtype\n439     kwargs = {\n440         'bits_allocated': arr.dtype.itemsize * 8,\n441         'rows': shape[0],\n442         'columns': shape[1],\n443         'samples_per_pixel': 3 if len(shape) == 3 else 1,\n444         'byteorder': '<',\n445     }\n446 \n447     sys_endianness = '<' if sys.byteorder == 'little' else '>'\n448     byteorder = dtype.byteorder\n449     byteorder = sys_endianness if byteorder == '=' else byteorder\n450     if byteorder == '>':\n451         arr = arr.astype(dtype.newbyteorder('<'))\n452 \n453     return _encode_frame(arr.tobytes(), **kwargs)\n454 \n[end of pydicom/pixel_data_handlers/rle_handler.py]\n[start of pydicom/pixel_data_handlers/util.py]\n1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n2 \"\"\"Utility functions used in the pixel data handlers.\"\"\"\n3 \n4 from struct import unpack\n5 from sys import byteorder\n6 from typing import (\n7     Dict, Optional, Union, List, Tuple, TYPE_CHECKING, cast, Iterable\n8 )\n9 import warnings\n10 \n11 try:\n12     import numpy as np  # type: ignore[import]\n13     HAVE_NP = True\n14 except ImportError:\n15     HAVE_NP = False\n16 \n17 from pydicom.data import get_palette_files\n18 from pydicom.uid import UID\n19 \n20 if TYPE_CHECKING:  # pragma: no cover\n21     from pydicom.dataset import Dataset, FileMetaDataset, FileDataset\n22 \n23 \n24 def apply_color_lut(\n25     arr: \"np.ndarray\",\n26     ds: Optional[\"Dataset\"] = None,\n27     palette: Optional[Union[str, UID]] = None\n28 ) -> \"np.ndarray\":\n29     \"\"\"Apply a color palette lookup table to `arr`.\n30 \n31     .. versionadded:: 1.4\n32 \n33     If (0028,1201-1203) *Palette Color Lookup Table Data* are missing\n34     then (0028,1221-1223) *Segmented Palette Color Lookup Table Data* must be\n35     present and vice versa. The presence of (0028,1204) *Alpha Palette Color\n36     Lookup Table Data* or (0028,1224) *Alpha Segmented Palette Color Lookup\n37     Table Data* is optional.\n38 \n39     Use of this function with the :dcm:`Enhanced Palette Color Lookup Table\n40     Module<part03/sect_C.7.6.23.html>` or :dcm:`Supplemental Palette Color LUT\n41     Module<part03/sect_C.7.6.19.html>` is not currently supported.\n42 \n43     Parameters\n44     ----------\n45     arr : numpy.ndarray\n46         The pixel data to apply the color palette to.\n47     ds : dataset.Dataset, optional\n48         Required if `palette` is not supplied. A\n49         :class:`~pydicom.dataset.Dataset` containing a suitable\n50         :dcm:`Image Pixel<part03/sect_C.7.6.3.html>` or\n51         :dcm:`Palette Color Lookup Table<part03/sect_C.7.9.html>` Module.\n52     palette : str or uid.UID, optional\n53         Required if `ds` is not supplied. The name of one of the\n54         :dcm:`well-known<part06/chapter_B.html>` color palettes defined by the\n55         DICOM Standard. One of: ``'HOT_IRON'``, ``'PET'``,\n56         ``'HOT_METAL_BLUE'``, ``'PET_20_STEP'``, ``'SPRING'``, ``'SUMMER'``,\n57         ``'FALL'``, ``'WINTER'`` or the corresponding well-known (0008,0018)\n58         *SOP Instance UID*.\n59 \n60     Returns\n61     -------\n62     numpy.ndarray\n63         The RGB or RGBA pixel data as an array of ``np.uint8`` or ``np.uint16``\n64         values, depending on the 3rd value of (0028,1201) *Red Palette Color\n65         Lookup Table Descriptor*.\n66 \n67     References\n68     ----------\n69 \n70     * :dcm:`Image Pixel Module<part03/sect_C.7.6.3.html>`\n71     * :dcm:`Supplemental Palette Color LUT Module<part03/sect_C.7.6.19.html>`\n72     * :dcm:`Enhanced Palette Color LUT Module<part03/sect_C.7.6.23.html>`\n73     * :dcm:`Palette Colour LUT Module<part03/sect_C.7.9.html>`\n74     * :dcm:`Supplemental Palette Color LUTs\n75       <part03/sect_C.8.16.2.html#sect_C.8.16.2.1.1.1>`\n76     \"\"\"\n77     # Note: input value (IV) is the stored pixel value in `arr`\n78     # LUTs[IV] -> [R, G, B] values at the IV pixel location in `arr`\n79     if not ds and not palette:\n80         raise ValueError(\"Either 'ds' or 'palette' is required\")\n81 \n82     if palette:\n83         # Well-known palettes are all 8-bits per entry\n84         datasets = {\n85             '1.2.840.10008.1.5.1': 'hotiron.dcm',\n86             '1.2.840.10008.1.5.2': 'pet.dcm',\n87             '1.2.840.10008.1.5.3': 'hotmetalblue.dcm',\n88             '1.2.840.10008.1.5.4': 'pet20step.dcm',\n89             '1.2.840.10008.1.5.5': 'spring.dcm',\n90             '1.2.840.10008.1.5.6': 'summer.dcm',\n91             '1.2.840.10008.1.5.7': 'fall.dcm',\n92             '1.2.840.10008.1.5.8': 'winter.dcm',\n93         }\n94         if not UID(palette).is_valid:\n95             try:\n96                 uids = {\n97                     'HOT_IRON': '1.2.840.10008.1.5.1',\n98                     'PET': '1.2.840.10008.1.5.2',\n99                     'HOT_METAL_BLUE': '1.2.840.10008.1.5.3',\n100                     'PET_20_STEP': '1.2.840.10008.1.5.4',\n101                     'SPRING': '1.2.840.10008.1.5.5',\n102                     'SUMMER': '1.2.840.10008.1.5.6',\n103                     'FALL': '1.2.840.10008.1.5.8',\n104                     'WINTER': '1.2.840.10008.1.5.7',\n105                 }\n106                 palette = uids[palette]\n107             except KeyError:\n108                 raise ValueError(\"Unknown palette '{}'\".format(palette))\n109 \n110         try:\n111             from pydicom import dcmread\n112             fname = datasets[palette]\n113             ds = dcmread(get_palette_files(fname)[0])\n114         except KeyError:\n115             raise ValueError(\"Unknown palette '{}'\".format(palette))\n116 \n117     ds = cast(\"Dataset\", ds)\n118 \n119     # C.8.16.2.1.1.1: Supplemental Palette Color LUT\n120     # TODO: Requires greyscale visualisation pipeline\n121     if getattr(ds, 'PixelPresentation', None) in ['MIXED', 'COLOR']:\n122         raise ValueError(\n123             \"Use of this function with the Supplemental Palette Color Lookup \"\n124             \"Table Module is not currently supported\"\n125         )\n126 \n127     if 'RedPaletteColorLookupTableDescriptor' not in ds:\n128         raise ValueError(\"No suitable Palette Color Lookup Table Module found\")\n129 \n130     # All channels are supposed to be identical\n131     lut_desc = cast(List[int], ds.RedPaletteColorLookupTableDescriptor)\n132     # A value of 0 = 2^16 entries\n133     nr_entries = lut_desc[0] or 2**16\n134 \n135     # May be negative if Pixel Representation is 1\n136     first_map = lut_desc[1]\n137     # Actual bit depth may be larger (8 bit entries in 16 bits allocated)\n138     nominal_depth = lut_desc[2]\n139     dtype = np.dtype('uint{:.0f}'.format(nominal_depth))\n140 \n141     luts = []\n142     if 'RedPaletteColorLookupTableData' in ds:\n143         # LUT Data is described by PS3.3, C.7.6.3.1.6\n144         r_lut = cast(bytes, ds.RedPaletteColorLookupTableData)\n145         g_lut = cast(bytes, ds.GreenPaletteColorLookupTableData)\n146         b_lut = cast(bytes, ds.BluePaletteColorLookupTableData)\n147         a_lut = cast(\n148             Optional[bytes],\n149             getattr(ds, 'AlphaPaletteColorLookupTableData', None)\n150         )\n151 \n152         actual_depth = len(r_lut) / nr_entries * 8\n153         dtype = np.dtype('uint{:.0f}'.format(actual_depth))\n154 \n155         for lut_bytes in [ii for ii in [r_lut, g_lut, b_lut, a_lut] if ii]:\n156             luts.append(np.frombuffer(lut_bytes, dtype=dtype))\n157     elif 'SegmentedRedPaletteColorLookupTableData' in ds:\n158         # Segmented LUT Data is described by PS3.3, C.7.9.2\n159         r_lut = cast(bytes, ds.SegmentedRedPaletteColorLookupTableData)\n160         g_lut = cast(bytes, ds.SegmentedGreenPaletteColorLookupTableData)\n161         b_lut = cast(bytes, ds.SegmentedBluePaletteColorLookupTableData)\n162         a_lut = cast(\n163             Optional[bytes],\n164             getattr(ds, 'SegmentedAlphaPaletteColorLookupTableData', None)\n165         )\n166 \n167         endianness = '<' if ds.is_little_endian else '>'\n168         byte_depth = nominal_depth // 8\n169         fmt = 'B' if byte_depth == 1 else 'H'\n170         actual_depth = nominal_depth\n171 \n172         for seg in [ii for ii in [r_lut, g_lut, b_lut, a_lut] if ii]:\n173             len_seg = len(seg) // byte_depth\n174             s_fmt = endianness + str(len_seg) + fmt\n175             lut_ints = _expand_segmented_lut(unpack(s_fmt, seg), s_fmt)\n176             luts.append(np.asarray(lut_ints, dtype=dtype))\n177     else:\n178         raise ValueError(\"No suitable Palette Color Lookup Table Module found\")\n179 \n180     if actual_depth not in [8, 16]:\n181         raise ValueError(\n182             f\"The bit depth of the LUT data '{actual_depth:.1f}' \"\n183             \"is invalid (only 8 or 16 bits per entry allowed)\"\n184         )\n185 \n186     lut_lengths = [len(ii) for ii in luts]\n187     if not all(ii == lut_lengths[0] for ii in lut_lengths[1:]):\n188         raise ValueError(\"LUT data must be the same length\")\n189 \n190     # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n191     clipped_iv = np.zeros(arr.shape, dtype=dtype)\n192     # IVs >= `first_map` are mapped by the Palette Color LUTs\n193     # `first_map` may be negative, positive or 0\n194     mapped_pixels = arr >= first_map\n195     clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n196     # IVs > number of entries get set to last entry\n197     np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n198 \n199     # Output array may be RGB or RGBA\n200     out = np.empty(list(arr.shape) + [len(luts)], dtype=dtype)\n201     for ii, lut in enumerate(luts):\n202         out[..., ii] = lut[clipped_iv]\n203 \n204     return out\n205 \n206 \n207 def apply_modality_lut(arr: \"np.ndarray\", ds: \"Dataset\") -> \"np.ndarray\":\n208     \"\"\"Apply a modality lookup table or rescale operation to `arr`.\n209 \n210     .. versionadded:: 1.4\n211 \n212     Parameters\n213     ----------\n214     arr : numpy.ndarray\n215         The :class:`~numpy.ndarray` to apply the modality LUT or rescale\n216         operation to.\n217     ds : dataset.Dataset\n218         A dataset containing a :dcm:`Modality LUT Module\n219         <part03/sect_C.11.html#sect_C.11.1>`.\n220 \n221     Returns\n222     -------\n223     numpy.ndarray\n224         An array with applied modality LUT or rescale operation. If\n225         (0028,3000) *Modality LUT Sequence* is present then returns an array\n226         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n227         (0028,3002) *LUT Descriptor*. If (0028,1052) *Rescale Intercept* and\n228         (0028,1053) *Rescale Slope* are present then returns an array of\n229         ``np.float64``. If neither are present then `arr` will be returned\n230         unchanged.\n231 \n232     Notes\n233     -----\n234     When *Rescale Slope* and *Rescale Intercept* are used, the output range\n235     is from (min. pixel value * Rescale Slope + Rescale Intercept) to\n236     (max. pixel value * Rescale Slope + Rescale Intercept), where min. and\n237     max. pixel value are determined from (0028,0101) *Bits Stored* and\n238     (0028,0103) *Pixel Representation*.\n239 \n240     References\n241     ----------\n242     * DICOM Standard, Part 3, :dcm:`Annex C.11.1\n243       <part03/sect_C.11.html#sect_C.11.1>`\n244     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n245       <part04/sect_N.2.html#sect_N.2.1.1>`\n246     \"\"\"\n247     if 'ModalityLUTSequence' in ds:\n248         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\n249         nr_entries = cast(List[int], item.LUTDescriptor)[0] or 2**16\n250         first_map = cast(List[int], item.LUTDescriptor)[1]\n251         nominal_depth = cast(List[int], item.LUTDescriptor)[2]\n252 \n253         dtype = 'uint{}'.format(nominal_depth)\n254 \n255         # Ambiguous VR, US or OW\n256         unc_data: Iterable[int]\n257         if item['LUTData'].VR == 'OW':\n258             endianness = '<' if ds.is_little_endian else '>'\n259             unpack_fmt = '{}{}H'.format(endianness, nr_entries)\n260             unc_data = unpack(unpack_fmt, cast(bytes, item.LUTData))\n261         else:\n262             unc_data = cast(List[int], item.LUTData)\n263 \n264         lut_data: \"np.ndarray\" = np.asarray(unc_data, dtype=dtype)\n265 \n266         # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n267         clipped_iv = np.zeros(arr.shape, dtype=arr.dtype)\n268         # IVs >= `first_map` are mapped by the Modality LUT\n269         # `first_map` may be negative, positive or 0\n270         mapped_pixels = arr >= first_map\n271         clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n272         # IVs > number of entries get set to last entry\n273         np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n274 \n275         return lut_data[clipped_iv]\n276     elif 'RescaleSlope' in ds and 'RescaleIntercept' in ds:\n277         arr = arr.astype(np.float64) * cast(float, ds.RescaleSlope)\n278         arr += cast(float, ds.RescaleIntercept)\n279 \n280     return arr\n281 \n282 \n283 def apply_voi_lut(\n284     arr: \"np.ndarray\",\n285     ds: \"Dataset\",\n286     index: int = 0,\n287     prefer_lut: bool = True\n288 ) -> \"np.ndarray\":\n289     \"\"\"Apply a VOI lookup table or windowing operation to `arr`.\n290 \n291     .. versionadded:: 1.4\n292 \n293     .. versionchanged:: 2.1\n294 \n295         Added the `prefer_lut` keyword parameter\n296 \n297     Parameters\n298     ----------\n299     arr : numpy.ndarray\n300         The :class:`~numpy.ndarray` to apply the VOI LUT or windowing operation\n301         to.\n302     ds : dataset.Dataset\n303         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n304         If (0028,3010) *VOI LUT Sequence* is present then returns an array\n305         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n306         (0028,3002) *LUT Descriptor*. If (0028,1050) *Window Center* and\n307         (0028,1051) *Window Width* are present then returns an array of\n308         ``np.float64``. If neither are present then `arr` will be returned\n309         unchanged.\n310     index : int, optional\n311         When the VOI LUT Module contains multiple alternative views, this is\n312         the index of the view to return (default ``0``).\n313     prefer_lut : bool\n314         When the VOI LUT Module contains both *Window Width*/*Window Center*\n315         and *VOI LUT Sequence*, if ``True`` (default) then apply the VOI LUT,\n316         otherwise apply the windowing operation.\n317 \n318     Returns\n319     -------\n320     numpy.ndarray\n321         An array with applied VOI LUT or windowing operation.\n322 \n323     Notes\n324     -----\n325     When the dataset requires a modality LUT or rescale operation as part of\n326     the Modality LUT module then that must be applied before any windowing\n327     operation.\n328 \n329     See Also\n330     --------\n331     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n332     :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n333     :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n334 \n335     References\n336     ----------\n337     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n338       <part03/sect_C.11.html#sect_C.11.2>`\n339     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n340       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n341     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n342       <part04/sect_N.2.html#sect_N.2.1.1>`\n343     \"\"\"\n344     valid_voi = False\n345     if 'VOILUTSequence' in ds:\n346         ds.VOILUTSequence = cast(List[\"Dataset\"], ds.VOILUTSequence)\n347         valid_voi = None not in [\n348             ds.VOILUTSequence[0].get('LUTDescriptor', None),\n349             ds.VOILUTSequence[0].get('LUTData', None)\n350         ]\n351     valid_windowing = None not in [\n352         ds.get('WindowCenter', None),\n353         ds.get('WindowWidth', None)\n354     ]\n355 \n356     if valid_voi and valid_windowing:\n357         if prefer_lut:\n358             return apply_voi(arr, ds, index)\n359 \n360         return apply_windowing(arr, ds, index)\n361 \n362     if valid_voi:\n363         return apply_voi(arr, ds, index)\n364 \n365     if valid_windowing:\n366         return apply_windowing(arr, ds, index)\n367 \n368     return arr\n369 \n370 \n371 def apply_voi(\n372     arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n373 ) -> \"np.ndarray\":\n374     \"\"\"Apply a VOI lookup table to `arr`.\n375 \n376     .. versionadded:: 2.1\n377 \n378     Parameters\n379     ----------\n380     arr : numpy.ndarray\n381         The :class:`~numpy.ndarray` to apply the VOI LUT to.\n382     ds : dataset.Dataset\n383         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n384         If (0028,3010) *VOI LUT Sequence* is present then returns an array\n385         of ``np.uint8`` or ``np.uint16``, depending on the 3rd value of\n386         (0028,3002) *LUT Descriptor*, otherwise `arr` will be returned\n387         unchanged.\n388     index : int, optional\n389         When the VOI LUT Module contains multiple alternative views, this is\n390         the index of the view to return (default ``0``).\n391 \n392     Returns\n393     -------\n394     numpy.ndarray\n395         An array with applied VOI LUT.\n396 \n397     See Also\n398     --------\n399     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n400     :func:`~pydicom.pixel_data_handlers.util.apply_windowing`\n401 \n402     References\n403     ----------\n404     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n405       <part03/sect_C.11.html#sect_C.11.2>`\n406     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n407       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n408     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n409       <part04/sect_N.2.html#sect_N.2.1.1>`\n410     \"\"\"\n411     if \"VOILUTSequence\" not in ds:\n412         return arr\n413 \n414     if not np.issubdtype(arr.dtype, np.integer):\n415         warnings.warn(\n416             \"Applying a VOI LUT on a float input array may give \"\n417             \"incorrect results\"\n418         )\n419 \n420     # VOI LUT Sequence contains one or more items\n421     item = cast(List[\"Dataset\"], ds.VOILUTSequence)[index]\n422     lut_descriptor = cast(List[int], item.LUTDescriptor)\n423     nr_entries = lut_descriptor[0] or 2**16\n424     first_map = lut_descriptor[1]\n425 \n426     # PS3.3 C.8.11.3.1.5: may be 8, 10-16\n427     nominal_depth = lut_descriptor[2]\n428     if nominal_depth in list(range(10, 17)):\n429         dtype = 'uint16'\n430     elif nominal_depth == 8:\n431         dtype = 'uint8'\n432     else:\n433         raise NotImplementedError(\n434             f\"'{nominal_depth}' bits per LUT entry is not supported\"\n435         )\n436 \n437     # Ambiguous VR, US or OW\n438     unc_data: Iterable[int]\n439     if item['LUTData'].VR == 'OW':\n440         endianness = '<' if ds.is_little_endian else '>'\n441         unpack_fmt = f'{endianness}{nr_entries}H'\n442         unc_data = unpack(unpack_fmt, cast(bytes, item.LUTData))\n443     else:\n444         unc_data = cast(List[int], item.LUTData)\n445 \n446     lut_data: \"np.ndarray\" = np.asarray(unc_data, dtype=dtype)\n447 \n448     # IVs < `first_map` get set to first LUT entry (i.e. index 0)\n449     clipped_iv = np.zeros(arr.shape, dtype=dtype)\n450     # IVs >= `first_map` are mapped by the VOI LUT\n451     # `first_map` may be negative, positive or 0\n452     mapped_pixels = arr >= first_map\n453     clipped_iv[mapped_pixels] = arr[mapped_pixels] - first_map\n454     # IVs > number of entries get set to last entry\n455     np.clip(clipped_iv, 0, nr_entries - 1, out=clipped_iv)\n456 \n457     return lut_data[clipped_iv]\n458 \n459 \n460 def apply_windowing(\n461     arr: \"np.ndarray\", ds: \"Dataset\", index: int = 0\n462 ) -> \"np.ndarray\":\n463     \"\"\"Apply a windowing operation to `arr`.\n464 \n465     .. versionadded:: 2.1\n466 \n467     Parameters\n468     ----------\n469     arr : numpy.ndarray\n470         The :class:`~numpy.ndarray` to apply the windowing operation to.\n471     ds : dataset.Dataset\n472         A dataset containing a :dcm:`VOI LUT Module<part03/sect_C.11.2.html>`.\n473         If (0028,1050) *Window Center* and (0028,1051) *Window Width* are\n474         present then returns an array of ``np.float64``, otherwise `arr` will\n475         be returned unchanged.\n476     index : int, optional\n477         When the VOI LUT Module contains multiple alternative views, this is\n478         the index of the view to return (default ``0``).\n479 \n480     Returns\n481     -------\n482     numpy.ndarray\n483         An array with applied windowing operation.\n484 \n485     Notes\n486     -----\n487     When the dataset requires a modality LUT or rescale operation as part of\n488     the Modality LUT module then that must be applied before any windowing\n489     operation.\n490 \n491     See Also\n492     --------\n493     :func:`~pydicom.pixel_data_handlers.util.apply_modality_lut`\n494     :func:`~pydicom.pixel_data_handlers.util.apply_voi`\n495 \n496     References\n497     ----------\n498     * DICOM Standard, Part 3, :dcm:`Annex C.11.2\n499       <part03/sect_C.11.html#sect_C.11.2>`\n500     * DICOM Standard, Part 3, :dcm:`Annex C.8.11.3.1.5\n501       <part03/sect_C.8.11.3.html#sect_C.8.11.3.1.5>`\n502     * DICOM Standard, Part 4, :dcm:`Annex N.2.1.1\n503       <part04/sect_N.2.html#sect_N.2.1.1>`\n504     \"\"\"\n505     if \"WindowWidth\" not in ds and \"WindowCenter\" not in ds:\n506         return arr\n507 \n508     if ds.PhotometricInterpretation not in ['MONOCHROME1', 'MONOCHROME2']:\n509         raise ValueError(\n510             \"When performing a windowing operation only 'MONOCHROME1' and \"\n511             \"'MONOCHROME2' are allowed for (0028,0004) Photometric \"\n512             \"Interpretation\"\n513         )\n514 \n515     # May be LINEAR (default), LINEAR_EXACT, SIGMOID or not present, VM 1\n516     voi_func = cast(str, getattr(ds, 'VOILUTFunction', 'LINEAR')).upper()\n517     # VR DS, VM 1-n\n518     elem = ds['WindowCenter']\n519     center = (\n520         cast(List[float], elem.value)[index] if elem.VM > 1 else elem.value\n521     )\n522     center = cast(float, center)\n523     elem = ds['WindowWidth']\n524     width = cast(List[float], elem.value)[index] if elem.VM > 1 else elem.value\n525     width = cast(float, width)\n526 \n527     # The output range depends on whether or not a modality LUT or rescale\n528     #   operation has been applied\n529     ds.BitsStored = cast(int, ds.BitsStored)\n530     y_min: float\n531     y_max: float\n532     if 'ModalityLUTSequence' in ds:\n533         # Unsigned - see PS3.3 C.11.1.1.1\n534         y_min = 0\n535         item = cast(List[\"Dataset\"], ds.ModalityLUTSequence)[0]\n536         bit_depth = cast(List[int], item.LUTDescriptor)[2]\n537         y_max = 2**bit_depth - 1\n538     elif ds.PixelRepresentation == 0:\n539         # Unsigned\n540         y_min = 0\n541         y_max = 2**ds.BitsStored - 1\n542     else:\n543         # Signed\n544         y_min = -2**(ds.BitsStored - 1)\n545         y_max = 2**(ds.BitsStored - 1) - 1\n546 \n547     slope = ds.get('RescaleSlope', None)\n548     intercept = ds.get('RescaleIntercept', None)\n549     if slope is not None and intercept is not None:\n550         ds.RescaleSlope = cast(float, ds.RescaleSlope)\n551         ds.RescaleIntercept = cast(float, ds.RescaleIntercept)\n552         # Otherwise its the actual data range\n553         y_min = y_min * ds.RescaleSlope + ds.RescaleIntercept\n554         y_max = y_max * ds.RescaleSlope + ds.RescaleIntercept\n555 \n556     y_range = y_max - y_min\n557     arr = arr.astype('float64')\n558 \n559     if voi_func in ['LINEAR', 'LINEAR_EXACT']:\n560         # PS3.3 C.11.2.1.2.1 and C.11.2.1.3.2\n561         if voi_func == 'LINEAR':\n562             if width < 1:\n563                 raise ValueError(\n564                     \"The (0028,1051) Window Width must be greater than or \"\n565                     \"equal to 1 for a 'LINEAR' windowing operation\"\n566                 )\n567             center -= 0.5\n568             width -= 1\n569         elif width <= 0:\n570             raise ValueError(\n571                 \"The (0028,1051) Window Width must be greater than 0 \"\n572                 \"for a 'LINEAR_EXACT' windowing operation\"\n573             )\n574 \n575         below = arr <= (center - width / 2)\n576         above = arr > (center + width / 2)\n577         between = np.logical_and(~below, ~above)\n578 \n579         arr[below] = y_min\n580         arr[above] = y_max\n581         if between.any():\n582             arr[between] = (\n583                 ((arr[between] - center) / width + 0.5) * y_range + y_min\n584             )\n585     elif voi_func == 'SIGMOID':\n586         # PS3.3 C.11.2.1.3.1\n587         if width <= 0:\n588             raise ValueError(\n589                 \"The (0028,1051) Window Width must be greater than 0 \"\n590                 \"for a 'SIGMOID' windowing operation\"\n591             )\n592 \n593         arr = y_range / (1 + np.exp(-4 * (arr - center) / width)) + y_min\n594     else:\n595         raise ValueError(\n596             f\"Unsupported (0028,1056) VOI LUT Function value '{voi_func}'\"\n597         )\n598 \n599     return arr\n600 \n601 \n602 def convert_color_space(\n603     arr: \"np.ndarray\", current: str, desired: str, per_frame: bool = False\n604 ) -> \"np.ndarray\":\n605     \"\"\"Convert the image(s) in `arr` from one color space to another.\n606 \n607     .. versionchanged:: 1.4\n608 \n609         Added support for ``YBR_FULL_422``\n610 \n611     .. versionchanged:: 2.2\n612 \n613         Added `per_frame` keyword parameter.\n614 \n615     Parameters\n616     ----------\n617     arr : numpy.ndarray\n618         The image(s) as a :class:`numpy.ndarray` with\n619         :attr:`~numpy.ndarray.shape` (frames, rows, columns, 3)\n620         or (rows, columns, 3).\n621     current : str\n622         The current color space, should be a valid value for (0028,0004)\n623         *Photometric Interpretation*. One of ``'RGB'``, ``'YBR_FULL'``,\n624         ``'YBR_FULL_422'``.\n625     desired : str\n626         The desired color space, should be a valid value for (0028,0004)\n627         *Photometric Interpretation*. One of ``'RGB'``, ``'YBR_FULL'``,\n628         ``'YBR_FULL_422'``.\n629     per_frame : bool, optional\n630         If ``True`` and the input array contains multiple frames then process\n631         each frame individually to reduce memory usage. Default ``False``.\n632 \n633     Returns\n634     -------\n635     numpy.ndarray\n636         The image(s) converted to the desired color space.\n637 \n638     References\n639     ----------\n640 \n641     * DICOM Standard, Part 3,\n642       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n643     * ISO/IEC 10918-5:2012 (`ITU T.871\n644       <https://www.ijg.org/files/T-REC-T.871-201105-I!!PDF-E.pdf>`_),\n645       Section 7\n646     \"\"\"\n647     def _no_change(arr):\n648         return arr\n649 \n650     _converters = {\n651         'YBR_FULL_422': {\n652             'YBR_FULL_422': _no_change,\n653             'YBR_FULL': _no_change,\n654             'RGB': _convert_YBR_FULL_to_RGB,\n655         },\n656         'YBR_FULL': {\n657             'YBR_FULL': _no_change,\n658             'YBR_FULL_422': _no_change,\n659             'RGB': _convert_YBR_FULL_to_RGB,\n660         },\n661         'RGB': {\n662             'RGB': _no_change,\n663             'YBR_FULL': _convert_RGB_to_YBR_FULL,\n664             'YBR_FULL_422': _convert_RGB_to_YBR_FULL,\n665         }\n666     }\n667     try:\n668         converter = _converters[current][desired]\n669     except KeyError:\n670         raise NotImplementedError(\n671             f\"Conversion from {current} to {desired} is not supported.\"\n672         )\n673 \n674     if len(arr.shape) == 4 and per_frame:\n675         for idx, frame in enumerate(arr):\n676             arr[idx] = converter(frame)\n677 \n678         return arr\n679 \n680     return converter(arr)\n681 \n682 \n683 def _convert_RGB_to_YBR_FULL(arr: \"np.ndarray\") -> \"np.ndarray\":\n684     \"\"\"Return an ndarray converted from RGB to YBR_FULL color space.\n685 \n686     Parameters\n687     ----------\n688     arr : numpy.ndarray\n689         An ndarray of an 8-bit per channel images in RGB color space.\n690 \n691     Returns\n692     -------\n693     numpy.ndarray\n694         The array in YBR_FULL color space.\n695 \n696     References\n697     ----------\n698 \n699     * DICOM Standard, Part 3,\n700       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n701     * ISO/IEC 10918-5:2012 (`ITU T.871\n702       <https://www.ijg.org/files/T-REC-T.871-201105-I!!PDF-E.pdf>`_),\n703       Section 7\n704     \"\"\"\n705     orig_dtype = arr.dtype\n706 \n707     rgb_to_ybr = np.asarray(\n708         [[+0.299, -0.299 / 1.772, +0.701 / 1.402],\n709          [+0.587, -0.587 / 1.772, -0.587 / 1.402],\n710          [+0.114, +0.886 / 1.772, -0.114 / 1.402]],\n711         dtype=np.float32\n712     )\n713 \n714     arr = np.matmul(arr, rgb_to_ybr, dtype=np.float32)\n715     arr += [0.5, 128.5, 128.5]\n716     # Round(x) -> floor of (arr + 0.5) : 0.5 added in previous step\n717     np.floor(arr, out=arr)\n718     # Max(0, arr) -> 0 if 0 >= arr, arr otherwise\n719     # Min(arr, 255) -> arr if arr <= 255, 255 otherwise\n720     np.clip(arr, 0, 255, out=arr)\n721 \n722     return arr.astype(orig_dtype)\n723 \n724 \n725 def _convert_YBR_FULL_to_RGB(arr: \"np.ndarray\") -> \"np.ndarray\":\n726     \"\"\"Return an ndarray converted from YBR_FULL to RGB color space.\n727 \n728     Parameters\n729     ----------\n730     arr : numpy.ndarray\n731         An ndarray of an 8-bit per channel images in YBR_FULL color space.\n732 \n733     Returns\n734     -------\n735     numpy.ndarray\n736         The array in RGB color space.\n737 \n738     References\n739     ----------\n740 \n741     * DICOM Standard, Part 3,\n742       :dcm:`Annex C.7.6.3.1.2<part03/sect_C.7.6.3.html#sect_C.7.6.3.1.2>`\n743     * ISO/IEC 10918-5:2012, Section 7\n744     \"\"\"\n745     orig_dtype = arr.dtype\n746 \n747     ybr_to_rgb = np.asarray(\n748         [[1.000, 1.000, 1.000],\n749          [0.000, -0.114 * 1.772 / 0.587, 1.772],\n750          [1.402, -0.299 * 1.402 / 0.587, 0.000]],\n751         dtype=np.float32\n752     )\n753 \n754     arr = arr.astype(np.float32)\n755     arr -= [0, 128, 128]\n756 \n757     # Round(x) -> floor of (arr + 0.5)\n758     np.matmul(arr, ybr_to_rgb, out=arr)\n759     arr += 0.5\n760     np.floor(arr, out=arr)\n761     # Max(0, arr) -> 0 if 0 >= arr, arr otherwise\n762     # Min(arr, 255) -> arr if arr <= 255, 255 otherwise\n763     np.clip(arr, 0, 255, out=arr)\n764 \n765     return arr.astype(orig_dtype)\n766 \n767 \n768 def dtype_corrected_for_endianness(\n769     is_little_endian: bool, numpy_dtype: \"np.dtype\"\n770 ) -> \"np.dtype\":\n771     \"\"\"Return a :class:`numpy.dtype` corrected for system and :class:`Dataset`\n772     endianness.\n773 \n774     Parameters\n775     ----------\n776     is_little_endian : bool\n777         The endianess of the affected :class:`~pydicom.dataset.Dataset`.\n778     numpy_dtype : numpy.dtype\n779         The numpy data type used for the *Pixel Data* without considering\n780         endianess.\n781 \n782     Raises\n783     ------\n784     ValueError\n785         If `is_little_endian` is ``None``, e.g. not initialized.\n786 \n787     Returns\n788     -------\n789     numpy.dtype\n790         The numpy data type used for the *Pixel Data* without considering\n791         endianess.\n792     \"\"\"\n793     if is_little_endian is None:\n794         raise ValueError(\"Dataset attribute 'is_little_endian' \"\n795                          \"has to be set before writing the dataset\")\n796 \n797     if is_little_endian != (byteorder == 'little'):\n798         return numpy_dtype.newbyteorder('S')\n799 \n800     return numpy_dtype\n801 \n802 \n803 def _expand_segmented_lut(\n804     data: Tuple[int, ...],\n805     fmt: str,\n806     nr_segments: Optional[int] = None,\n807     last_value: Optional[int] = None\n808 ) -> List[int]:\n809     \"\"\"Return a list containing the expanded lookup table data.\n810 \n811     Parameters\n812     ----------\n813     data : tuple of int\n814         The decoded segmented palette lookup table data. May be padded by a\n815         trailing null.\n816     fmt : str\n817         The format of the data, should contain `'B'` for 8-bit, `'H'` for\n818         16-bit, `'<'` for little endian and `'>'` for big endian.\n819     nr_segments : int, optional\n820         Expand at most `nr_segments` from the data. Should be used when\n821         the opcode is ``2`` (indirect). If used then `last_value` should also\n822         be used.\n823     last_value : int, optional\n824         The previous value in the expanded lookup table. Should be used when\n825         the opcode is ``2`` (indirect). If used then `nr_segments` should also\n826         be used.\n827 \n828     Returns\n829     -------\n830     list of int\n831         The reconstructed lookup table data.\n832 \n833     References\n834     ----------\n835 \n836     * DICOM Standard, Part 3, Annex C.7.9\n837     \"\"\"\n838     # Indirect segment byte offset is dependent on endianness for 8-bit\n839     # Little endian: e.g. 0x0302 0x0100, big endian, e.g. 0x0203 0x0001\n840     indirect_ii = [3, 2, 1, 0] if '<' in fmt else [2, 3, 0, 1]\n841 \n842     lut: List[int] = []\n843     offset = 0\n844     segments_read = 0\n845     # Use `offset + 1` to account for possible trailing null\n846     #   can do this because all segment types are longer than 2\n847     while offset + 1 < len(data):\n848         opcode = data[offset]\n849         length = data[offset + 1]\n850         offset += 2\n851 \n852         if opcode == 0:\n853             # C.7.9.2.1: Discrete segment\n854             lut.extend(data[offset:offset + length])\n855             offset += length\n856         elif opcode == 1:\n857             # C.7.9.2.2: Linear segment\n858             if lut:\n859                 y0 = lut[-1]\n860             elif last_value:\n861                 # Indirect segment with linear segment at 0th offset\n862                 y0 = last_value\n863             else:\n864                 raise ValueError(\n865                     \"Error expanding a segmented palette color lookup table: \"\n866                     \"the first segment cannot be a linear segment\"\n867                 )\n868 \n869             y1 = data[offset]\n870             offset += 1\n871 \n872             if y0 == y1:\n873                 lut.extend([y1] * length)\n874             else:\n875                 step = (y1 - y0) / length\n876                 vals = np.around(np.arange(y0 + step, y1 + step, step))\n877                 lut.extend([int(vv) for vv in vals])\n878         elif opcode == 2:\n879             # C.7.9.2.3: Indirect segment\n880             if not lut:\n881                 raise ValueError(\n882                     \"Error expanding a segmented palette color lookup table: \"\n883                     \"the first segment cannot be an indirect segment\"\n884                 )\n885 \n886             if 'B' in fmt:\n887                 # 8-bit segment entries\n888                 ii = [data[offset + vv] for vv in indirect_ii]\n889                 byte_offset = (ii[0] << 8 | ii[1]) << 16 | (ii[2] << 8 | ii[3])\n890                 offset += 4\n891             else:\n892                 # 16-bit segment entries\n893                 byte_offset = data[offset + 1] << 16 | data[offset]\n894                 offset += 2\n895 \n896             lut.extend(\n897                 _expand_segmented_lut(data[byte_offset:], fmt, length, lut[-1])\n898             )\n899         else:\n900             raise ValueError(\n901                 \"Error expanding a segmented palette lookup table: \"\n902                 f\"unknown segment type '{opcode}'\"\n903             )\n904 \n905         segments_read += 1\n906         if segments_read == nr_segments:\n907             return lut\n908 \n909     return lut\n910 \n911 \n912 def get_expected_length(ds: \"Dataset\", unit: str = 'bytes') -> int:\n913     \"\"\"Return the expected length (in terms of bytes or pixels) of the *Pixel\n914     Data*.\n915 \n916     +------------------------------------------------+-------------+\n917     | Element                                        | Required or |\n918     +-------------+---------------------------+------+ optional    |\n919     | Tag         | Keyword                   | Type |             |\n920     +=============+===========================+======+=============+\n921     | (0028,0002) | SamplesPerPixel           | 1    | Required    |\n922     +-------------+---------------------------+------+-------------+\n923     | (0028,0004) | PhotometricInterpretation | 1    | Required    |\n924     +-------------+---------------------------+------+-------------+\n925     | (0028,0008) | NumberOfFrames            | 1C   | Optional    |\n926     +-------------+---------------------------+------+-------------+\n927     | (0028,0010) | Rows                      | 1    | Required    |\n928     +-------------+---------------------------+------+-------------+\n929     | (0028,0011) | Columns                   | 1    | Required    |\n930     +-------------+---------------------------+------+-------------+\n931     | (0028,0100) | BitsAllocated             | 1    | Required    |\n932     +-------------+---------------------------+------+-------------+\n933 \n934     .. versionchanged:: 1.4\n935 \n936         Added support for a *Photometric Interpretation* of  ``YBR_FULL_422``\n937 \n938     Parameters\n939     ----------\n940     ds : Dataset\n941         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n942         and *Pixel Data*.\n943     unit : str, optional\n944         If ``'bytes'`` then returns the expected length of the *Pixel Data* in\n945         whole bytes and NOT including an odd length trailing NULL padding\n946         byte. If ``'pixels'`` then returns the expected length of the *Pixel\n947         Data* in terms of the total number of pixels (default ``'bytes'``).\n948 \n949     Returns\n950     -------\n951     int\n952         The expected length of the *Pixel Data* in either whole bytes or\n953         pixels, excluding the NULL trailing padding byte for odd length data.\n954     \"\"\"\n955     rows = cast(int, ds.Rows)\n956     columns = cast(int, ds.Columns)\n957     samples_per_pixel = cast(int, ds.SamplesPerPixel)\n958     bits_allocated = cast(int, ds.BitsAllocated)\n959 \n960     length = rows * columns * samples_per_pixel\n961     length *= get_nr_frames(ds)\n962 \n963     if unit == 'pixels':\n964         return length\n965 \n966     # Correct for the number of bytes per pixel\n967     if bits_allocated == 1:\n968         # Determine the nearest whole number of bytes needed to contain\n969         #   1-bit pixel data. e.g. 10 x 10 1-bit pixels is 100 bits, which\n970         #   are packed into 12.5 -> 13 bytes\n971         length = length // 8 + (length % 8 > 0)\n972     else:\n973         length *= bits_allocated // 8\n974 \n975     # DICOM Standard, Part 4, Annex C.7.6.3.1.2\n976     if ds.PhotometricInterpretation == 'YBR_FULL_422':\n977         length = length // 3 * 2\n978 \n979     return length\n980 \n981 \n982 def get_image_pixel_ids(ds: \"Dataset\") -> Dict[str, int]:\n983     \"\"\"Return a dict of the pixel data affecting element's :func:`id` values.\n984 \n985     .. versionadded:: 1.4\n986 \n987     +------------------------------------------------+\n988     | Element                                        |\n989     +-------------+---------------------------+------+\n990     | Tag         | Keyword                   | Type |\n991     +=============+===========================+======+\n992     | (0028,0002) | SamplesPerPixel           | 1    |\n993     +-------------+---------------------------+------+\n994     | (0028,0004) | PhotometricInterpretation | 1    |\n995     +-------------+---------------------------+------+\n996     | (0028,0006) | PlanarConfiguration       | 1C   |\n997     +-------------+---------------------------+------+\n998     | (0028,0008) | NumberOfFrames            | 1C   |\n999     +-------------+---------------------------+------+\n1000     | (0028,0010) | Rows                      | 1    |\n1001     +-------------+---------------------------+------+\n1002     | (0028,0011) | Columns                   | 1    |\n1003     +-------------+---------------------------+------+\n1004     | (0028,0100) | BitsAllocated             | 1    |\n1005     +-------------+---------------------------+------+\n1006     | (0028,0101) | BitsStored                | 1    |\n1007     +-------------+---------------------------+------+\n1008     | (0028,0103) | PixelRepresentation       | 1    |\n1009     +-------------+---------------------------+------+\n1010     | (7FE0,0008) | FloatPixelData            | 1C   |\n1011     +-------------+---------------------------+------+\n1012     | (7FE0,0009) | DoubleFloatPixelData      | 1C   |\n1013     +-------------+---------------------------+------+\n1014     | (7FE0,0010) | PixelData                 | 1C   |\n1015     +-------------+---------------------------+------+\n1016 \n1017     Parameters\n1018     ----------\n1019     ds : Dataset\n1020         The :class:`~pydicom.dataset.Dataset` containing the pixel data.\n1021 \n1022     Returns\n1023     -------\n1024     dict\n1025         A dict containing the :func:`id` values for the elements that affect\n1026         the pixel data.\n1027 \n1028     \"\"\"\n1029     keywords = [\n1030         'SamplesPerPixel', 'PhotometricInterpretation', 'PlanarConfiguration',\n1031         'NumberOfFrames', 'Rows', 'Columns', 'BitsAllocated', 'BitsStored',\n1032         'PixelRepresentation', 'FloatPixelData', 'DoubleFloatPixelData',\n1033         'PixelData'\n1034     ]\n1035 \n1036     return {kw: id(getattr(ds, kw, None)) for kw in keywords}\n1037 \n1038 \n1039 def get_j2k_parameters(codestream: bytes) -> Dict[str, object]:\n1040     \"\"\"Return a dict containing JPEG 2000 component parameters.\n1041 \n1042     .. versionadded:: 2.1\n1043 \n1044     Parameters\n1045     ----------\n1046     codestream : bytes\n1047         The JPEG 2000 (ISO/IEC 15444-1) codestream to be parsed.\n1048 \n1049     Returns\n1050     -------\n1051     dict\n1052         A dict containing parameters for the first component sample in the\n1053         JPEG 2000 `codestream`, or an empty dict if unable to parse the data.\n1054         Available parameters are ``{\"precision\": int, \"is_signed\": bool}``.\n1055     \"\"\"\n1056     try:\n1057         # First 2 bytes must be the SOC marker - if not then wrong format\n1058         if codestream[0:2] != b'\\xff\\x4f':\n1059             return {}\n1060 \n1061         # SIZ is required to be the second marker - Figure A-3 in 15444-1\n1062         if codestream[2:4] != b'\\xff\\x51':\n1063             return {}\n1064 \n1065         # See 15444-1 A.5.1 for format of the SIZ box and contents\n1066         ssiz = codestream[42]\n1067         if ssiz & 0x80:\n1068             return {\"precision\": (ssiz & 0x7F) + 1, \"is_signed\": True}\n1069 \n1070         return {\"precision\": ssiz + 1, \"is_signed\": False}\n1071     except (IndexError, TypeError):\n1072         pass\n1073 \n1074     return {}\n1075 \n1076 \n1077 def get_nr_frames(ds: \"Dataset\") -> int:\n1078     \"\"\"Return NumberOfFrames or 1 if NumberOfFrames is None.\n1079 \n1080     Parameters\n1081     ----------\n1082     ds : dataset.Dataset\n1083         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n1084         corresponding to the data in `arr`.\n1085 \n1086     Returns\n1087     -------\n1088     int\n1089         An integer for the NumberOfFrames or 1 if NumberOfFrames is None\n1090     \"\"\"\n1091     nr_frames: Optional[int] = getattr(ds, 'NumberOfFrames', 1)\n1092     # 'NumberOfFrames' may exist in the DICOM file but have value equal to None\n1093     if nr_frames is None:\n1094         warnings.warn(\"A value of None for (0028,0008) 'Number of Frames' is \"\n1095                       \"non-conformant. It's recommended that this value be \"\n1096                       \"changed to 1\")\n1097         nr_frames = 1\n1098 \n1099     return nr_frames\n1100 \n1101 \n1102 def pixel_dtype(ds: \"Dataset\", as_float: bool = False) -> \"np.dtype\":\n1103     \"\"\"Return a :class:`numpy.dtype` for the pixel data in `ds`.\n1104 \n1105     Suitable for use with IODs containing the Image Pixel module (with\n1106     ``as_float=False``) and the Floating Point Image Pixel and Double Floating\n1107     Point Image Pixel modules (with ``as_float=True``).\n1108 \n1109     +------------------------------------------+------------------+\n1110     | Element                                  | Supported        |\n1111     +-------------+---------------------+------+ values           |\n1112     | Tag         | Keyword             | Type |                  |\n1113     +=============+=====================+======+==================+\n1114     | (0028,0101) | BitsAllocated       | 1    | 1, 8, 16, 32, 64 |\n1115     +-------------+---------------------+------+------------------+\n1116     | (0028,0103) | PixelRepresentation | 1    | 0, 1             |\n1117     +-------------+---------------------+------+------------------+\n1118 \n1119     .. versionchanged:: 1.4\n1120 \n1121         Added `as_float` keyword parameter and support for float dtypes.\n1122 \n1123 \n1124     Parameters\n1125     ----------\n1126     ds : Dataset\n1127         The :class:`~pydicom.dataset.Dataset` containing the pixel data you\n1128         wish to get the data type for.\n1129     as_float : bool, optional\n1130         If ``True`` then return a float dtype, otherwise return an integer\n1131         dtype (default ``False``). Float dtypes are only supported when\n1132         (0028,0101) *Bits Allocated* is 32 or 64.\n1133 \n1134     Returns\n1135     -------\n1136     numpy.dtype\n1137         A :class:`numpy.dtype` suitable for containing the pixel data.\n1138 \n1139     Raises\n1140     ------\n1141     NotImplementedError\n1142         If the pixel data is of a type that isn't supported by either numpy\n1143         or *pydicom*.\n1144     \"\"\"\n1145     if not HAVE_NP:\n1146         raise ImportError(\"Numpy is required to determine the dtype.\")\n1147 \n1148     if ds.is_little_endian is None:\n1149         ds.is_little_endian = ds.file_meta.TransferSyntaxUID.is_little_endian\n1150 \n1151     if not as_float:\n1152         # (0028,0103) Pixel Representation, US, 1\n1153         #   Data representation of the pixel samples\n1154         #   0x0000 - unsigned int\n1155         #   0x0001 - 2's complement (signed int)\n1156         pixel_repr = cast(int, ds.PixelRepresentation)\n1157         if pixel_repr == 0:\n1158             dtype_str = 'uint'\n1159         elif pixel_repr == 1:\n1160             dtype_str = 'int'\n1161         else:\n1162             raise ValueError(\n1163                 \"Unable to determine the data type to use to contain the \"\n1164                 f\"Pixel Data as a value of '{pixel_repr}' for '(0028,0103) \"\n1165                 \"Pixel Representation' is invalid\"\n1166             )\n1167     else:\n1168         dtype_str = 'float'\n1169 \n1170     # (0028,0100) Bits Allocated, US, 1\n1171     #   The number of bits allocated for each pixel sample\n1172     #   PS3.5 8.1.1: Bits Allocated shall either be 1 or a multiple of 8\n1173     #   For bit packed data we use uint8\n1174     bits_allocated = cast(int, ds.BitsAllocated)\n1175     if bits_allocated == 1:\n1176         dtype_str = 'uint8'\n1177     elif bits_allocated > 0 and bits_allocated % 8 == 0:\n1178         dtype_str += str(bits_allocated)\n1179     else:\n1180         raise ValueError(\n1181             \"Unable to determine the data type to use to contain the \"\n1182             f\"Pixel Data as a value of '{bits_allocated}' for '(0028,0100) \"\n1183             \"Bits Allocated' is invalid\"\n1184         )\n1185 \n1186     # Check to see if the dtype is valid for numpy\n1187     try:\n1188         dtype = np.dtype(dtype_str)\n1189     except TypeError:\n1190         raise NotImplementedError(\n1191             f\"The data type '{dtype_str}' needed to contain the Pixel Data \"\n1192             \"is not supported by numpy\"\n1193         )\n1194 \n1195     # Correct for endianness of the system vs endianness of the dataset\n1196     if ds.is_little_endian != (byteorder == 'little'):\n1197         # 'S' swap from current to opposite\n1198         dtype = dtype.newbyteorder('S')\n1199 \n1200     return dtype\n1201 \n1202 \n1203 def reshape_pixel_array(ds: \"Dataset\", arr: \"np.ndarray\") -> \"np.ndarray\":\n1204     \"\"\"Return a reshaped :class:`numpy.ndarray` `arr`.\n1205 \n1206     +------------------------------------------+-----------+----------+\n1207     | Element                                  | Supported |          |\n1208     +-------------+---------------------+------+ values    |          |\n1209     | Tag         | Keyword             | Type |           |          |\n1210     +=============+=====================+======+===========+==========+\n1211     | (0028,0002) | SamplesPerPixel     | 1    | N > 0     | Required |\n1212     +-------------+---------------------+------+-----------+----------+\n1213     | (0028,0006) | PlanarConfiguration | 1C   | 0, 1      | Optional |\n1214     +-------------+---------------------+------+-----------+----------+\n1215     | (0028,0008) | NumberOfFrames      | 1C   | N > 0     | Optional |\n1216     +-------------+---------------------+------+-----------+----------+\n1217     | (0028,0010) | Rows                | 1    | N > 0     | Required |\n1218     +-------------+---------------------+------+-----------+----------+\n1219     | (0028,0011) | Columns             | 1    | N > 0     | Required |\n1220     +-------------+---------------------+------+-----------+----------+\n1221 \n1222     (0028,0008) *Number of Frames* is required when *Pixel Data* contains\n1223     more than 1 frame. (0028,0006) *Planar Configuration* is required when\n1224     (0028,0002) *Samples per Pixel* is greater than 1. For certain\n1225     compressed transfer syntaxes it is always taken to be either 0 or 1 as\n1226     shown in the table below.\n1227 \n1228     +---------------------------------------------+-----------------------+\n1229     | Transfer Syntax                             | Planar Configuration  |\n1230     +------------------------+--------------------+                       |\n1231     | UID                    | Name               |                       |\n1232     +========================+====================+=======================+\n1233     | 1.2.840.10008.1.2.4.50 | JPEG Baseline      | 0                     |\n1234     +------------------------+--------------------+-----------------------+\n1235     | 1.2.840.10008.1.2.4.57 | JPEG Lossless,     | 0                     |\n1236     |                        | Non-hierarchical   |                       |\n1237     +------------------------+--------------------+-----------------------+\n1238     | 1.2.840.10008.1.2.4.70 | JPEG Lossless,     | 0                     |\n1239     |                        | Non-hierarchical,  |                       |\n1240     |                        | SV1                |                       |\n1241     +------------------------+--------------------+-----------------------+\n1242     | 1.2.840.10008.1.2.4.80 | JPEG-LS Lossless   | 0                     |\n1243     +------------------------+--------------------+-----------------------+\n1244     | 1.2.840.10008.1.2.4.81 | JPEG-LS Lossy      | 0                     |\n1245     +------------------------+--------------------+-----------------------+\n1246     | 1.2.840.10008.1.2.4.90 | JPEG 2000 Lossless | 0                     |\n1247     +------------------------+--------------------+-----------------------+\n1248     | 1.2.840.10008.1.2.4.91 | JPEG 2000 Lossy    | 0                     |\n1249     +------------------------+--------------------+-----------------------+\n1250     | 1.2.840.10008.1.2.5    | RLE Lossless       | 1                     |\n1251     +------------------------+--------------------+-----------------------+\n1252 \n1253     .. versionchanged:: 2.1\n1254 \n1255         JPEG-LS transfer syntaxes changed to *Planar Configuration* of 0\n1256 \n1257     Parameters\n1258     ----------\n1259     ds : dataset.Dataset\n1260         The :class:`~pydicom.dataset.Dataset` containing the Image Pixel module\n1261         corresponding to the data in `arr`.\n1262     arr : numpy.ndarray\n1263         The 1D array containing the pixel data.\n1264 \n1265     Returns\n1266     -------\n1267     numpy.ndarray\n1268         A reshaped array containing the pixel data. The shape of the array\n1269         depends on the contents of the dataset:\n1270 \n1271         * For single frame, single sample data (rows, columns)\n1272         * For single frame, multi-sample data (rows, columns, planes)\n1273         * For multi-frame, single sample data (frames, rows, columns)\n1274         * For multi-frame, multi-sample data (frames, rows, columns, planes)\n1275 \n1276     References\n1277     ----------\n1278 \n1279     * DICOM Standard, Part 3,\n1280       :dcm:`Annex C.7.6.3.1<part03/sect_C.7.6.3.html#sect_C.7.6.3.1>`\n1281     * DICOM Standard, Part 5, :dcm:`Section 8.2<part05/sect_8.2.html>`\n1282     \"\"\"\n1283     if not HAVE_NP:\n1284         raise ImportError(\"Numpy is required to reshape the pixel array.\")\n1285 \n1286     nr_frames = get_nr_frames(ds)\n1287     nr_samples = cast(int, ds.SamplesPerPixel)\n1288 \n1289     if nr_frames < 1:\n1290         raise ValueError(\n1291             f\"Unable to reshape the pixel array as a value of {nr_frames} for \"\n1292             \"(0028,0008) 'Number of Frames' is invalid.\"\n1293         )\n1294 \n1295     if nr_samples < 1:\n1296         raise ValueError(\n1297             f\"Unable to reshape the pixel array as a value of {nr_samples} \"\n1298             \"for (0028,0002) 'Samples per Pixel' is invalid.\"\n1299         )\n1300 \n1301     # Valid values for Planar Configuration are dependent on transfer syntax\n1302     if nr_samples > 1:\n1303         transfer_syntax = ds.file_meta.TransferSyntaxUID\n1304         if transfer_syntax in ['1.2.840.10008.1.2.4.50',\n1305                                '1.2.840.10008.1.2.4.57',\n1306                                '1.2.840.10008.1.2.4.70',\n1307                                '1.2.840.10008.1.2.4.80',\n1308                                '1.2.840.10008.1.2.4.81',\n1309                                '1.2.840.10008.1.2.4.90',\n1310                                '1.2.840.10008.1.2.4.91']:\n1311             planar_configuration = 0\n1312         elif transfer_syntax in ['1.2.840.10008.1.2.5']:\n1313             planar_configuration = 1\n1314         else:\n1315             planar_configuration = ds.PlanarConfiguration\n1316 \n1317         if planar_configuration not in [0, 1]:\n1318             raise ValueError(\n1319                 \"Unable to reshape the pixel array as a value of \"\n1320                 f\"{planar_configuration} for (0028,0006) 'Planar \"\n1321                 \"Configuration' is invalid.\"\n1322             )\n1323 \n1324     rows = cast(int, ds.Rows)\n1325     columns = cast(int, ds.Columns)\n1326     if nr_frames > 1:\n1327         # Multi-frame\n1328         if nr_samples == 1:\n1329             # Single plane\n1330             arr = arr.reshape(nr_frames, rows, columns)\n1331         else:\n1332             # Multiple planes, usually 3\n1333             if planar_configuration == 0:\n1334                 arr = arr.reshape(nr_frames, rows, columns, nr_samples)\n1335             else:\n1336                 arr = arr.reshape(nr_frames, nr_samples, rows, columns)\n1337                 arr = arr.transpose(0, 2, 3, 1)\n1338     else:\n1339         # Single frame\n1340         if nr_samples == 1:\n1341             # Single plane\n1342             arr = arr.reshape(rows, columns)\n1343         else:\n1344             # Multiple planes, usually 3\n1345             if planar_configuration == 0:\n1346                 arr = arr.reshape(rows, columns, nr_samples)\n1347             else:\n1348                 arr = arr.reshape(nr_samples, rows, columns)\n1349                 arr = arr.transpose(1, 2, 0)\n1350 \n1351     return arr\n1352 \n[end of pydicom/pixel_data_handlers/util.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pydicom/pixel_data_handlers/util.py\n+++ b/pydicom/pixel_data_handlers/util.py\n@@ -876,7 +876,7 @@\n             step = (y1 - y0) / length\n-            vals = np.around(np.arange(y0 + step, y1 + step, step))\n+            vals = np.around(np.linspace(y0 + step, y1, length))\n             lut.extend([int(vv) for vv in vals])\n```", "model_patch": "--- a/pydicom/pixel_data_handlers/util.py\n+++ b/pydicom/pixel_data_handlers/util.py\n@@ -876,7 +876,7 @@\n             step = (y1 - y0) / length\n-            vals = np.around(np.arange(y0 + step, y1 + step, step))\n+            vals = np.around(np.linspace(y0 + step, y1, length))\n             lut.extend([int(vv) for vv in vals])\n"}
{"instance_id": "pydicom__pydicom-903", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nHandling of DS too long to be encoded in explicit encoding\n<!-- Instructions For Filing a Bug: https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md#filing-bugs -->\r\n\r\n#### Description\r\nThis is probably not a bug, but I'm not sure about the wanted behavior.\r\nAn RTPlan dataset encoded as Little Endian Implicit contains multiple values in the DS tag DHV Data (3004,0058) with an overall length not fitting into 2 bytes. Trying to write this as explicit Little Endian fails with an exception (`\"ushort format requires 0 &lt;= number &lt;= (0x7fff * 2 + 1)\"`) which is raised by the `pack` call in `write_leUS` while trying to write the length.\r\n\r\nThe standard says for this case in PS3.5, Table 6.2-1 (for VR DS):\r\n```\r\nNote\r\nData Elements with multiple values using this VR may not be properly encoded if Explicit-VR transfer syntax is used and the VL of this attribute exceeds 65534 bytes.\r\n```\r\nSo, as I understand it, this is valid DICOM, that cannot be converted to explicit encoding without data loss.\r\nThe question is how to handle this. What comes to mind:\r\n- truncate the value and log a warning\r\n- raise a meaningful exception\r\n- adapt the behavior depending on some config setting\r\n\r\nAny thoughts?\r\n\r\n<!-- Example: Attribute Error thrown when printing (0x0010, 0x0020) patient Id> 0-->\r\n\r\n#### Steps/Code to Reproduce\r\n<!--\r\nExample:\r\n```py\r\nfrom io import BytesIO\r\nfrom pydicom import dcmread\r\n\r\nbytestream = b'\\x02\\x00\\x02\\x00\\x55\\x49\\x16\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31' \\\r\n             b'\\x30\\x30\\x30\\x38\\x2e\\x35\\x2e\\x31\\x2e\\x31\\x2e\\x39\\x00\\x02\\x00\\x10\\x00' \\\r\n             b'\\x55\\x49\\x12\\x00\\x31\\x2e\\x32\\x2e\\x38\\x34\\x30\\x2e\\x31\\x30\\x30\\x30\\x38' \\\r\n             b'\\x2e\\x31\\x2e\\x32\\x00\\x20\\x20\\x10\\x00\\x02\\x00\\x00\\x00\\x01\\x00\\x20\\x20' \\\r\n             b'\\x20\\x00\\x06\\x00\\x00\\x00\\x4e\\x4f\\x52\\x4d\\x41\\x4c'\r\n\r\nfp = BytesIO(bytestream)\r\nds = dcmread(fp, force=True)\r\n\r\nprint(ds.PatientID)\r\n```\r\nIf the code is too long, feel free to put it in a public gist and link\r\nit in the issue: https://gist.github.com\r\n\r\nWhen possible use pydicom testing examples to reproduce the errors. Otherwise, provide\r\nan anonymous version of the data in order to replicate the errors.\r\n-->\r\n\r\n#### Expected Results\r\n<!-- Please paste or describe the expected results.\r\nExample: No error is thrown and the name of the patient is printed.-->\r\n\r\n#### Actual Results\r\n<!-- Please paste or specifically describe the actual output or traceback.\r\n(Use %xmode to deactivate ipython's trace beautifier)\r\nExample: ```AttributeError: 'FileDataset' object has no attribute 'PatientID'```\r\n-->\r\n\r\n#### Versions\r\n<!--\r\nPlease run the following snippet and paste the output below.\r\nimport platform; print(platform.platform())\r\nimport sys; print(\"Python\", sys.version)\r\nimport pydicom; print(\"pydicom\", pydicom.__version__)\r\n-->\r\n\r\n\r\n<!-- Thanks for contributing! -->\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 pydicom\n2 =======\n3 \n4 [![Build Status](https://travis-ci.org/pydicom/pydicom.svg?branch=master)](https://travis-ci.org/pydicom/pydicom)\n5 [![AppVeyor](https://ci.appveyor.com/api/projects/status/1vjtkr82lumnd3i7?svg=true)](https://ci.appveyor.com/project/glemaitre/pydicom)\n6 [![CircleCI](https://circleci.com/gh/pydicom/pydicom/tree/master.svg?style=shield)](https://circleci.com/gh/pydicom/pydicom/tree/master)\n7 [![codecov](https://codecov.io/gh/pydicom/pydicom/branch/master/graph/badge.svg)](https://codecov.io/gh/pydicom/pydicom)\n8 [![Python version](https://img.shields.io/pypi/pyversions/pydicom.svg)](https://img.shields.io/pypi/pyversions/pydicom.svg)\n9 [![PyPI version](https://badge.fury.io/py/pydicom.svg)](https://badge.fury.io/py/pydicom)\n10 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3333768.svg)](https://doi.org/10.5281/zenodo.3333768)\n11 \n12 pydicom is a pure python package for working with [DICOM](http://medical.nema.org/) files.\n13 It was made for inspecting and modifying DICOM data in an easy \"pythonic\" way.\n14 The modifications can be written again to a new file.\n15 \n16 As a pure python package, pydicom can run anywhere python runs without any other requirements,\n17 although [NumPy](http://www.numpy.org) is needed if manipulating pixel data.\n18 \n19 pydicom is not a DICOM server, and is not primarily about viewing images.\n20 It is designed to let you\n21 manipulate data elements in DICOM files with python code.\n22 \n23 Limitations -- for files with _compressed_ pixel data, pydicom can decompress\n24 it (with additional libraries installed) and allow you to manipulate the data,\n25 but can only store changed pixel data as uncompressed. Files can always be\n26 read and saved (including compressed pixel data that has not been modified),\n27 but once decompressed, modified pixel data cannot be compressed again.\n28 \n29 Documentation\n30 -------------\n31 \n32 pydicom documentation is available on GitHub Pages both for the [development\n33  (master) version](https://pydicom.github.io/pydicom/dev) and for the\n34 [released version](https://pydicom.github.io/pydicom/stable). The\n35 documentation for [the previous 0.9.9 version](https://pydicom.github.io/pydicom/0.9/)\n36 is still there for reference.\n37 \n38 See [Getting Started](https://pydicom.github.io/pydicom/stable/getting_started.html)\n39 for installation and basic information, and the\n40 [User Guide](https://pydicom.github.io/pydicom/stable/pydicom_user_guide.html)\n41 for an overview of how to use the pydicom library.\n42 To contribute to pydicom, read our [contribution guide](https://github.com/pydicom/pydicom/blob/master/CONTRIBUTING.md).\n43 To contribute an example or extension of pydicom that does not belong with\n44 the core software, see our contribution repository,\n45 [contrib-pydicom](https://www.github.com/pydicom/contrib-pydicom).\n46 \n[end of README.md]\n[start of pydicom/dataset.py]\n1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n2 \"\"\"Define the Dataset and FileDataset classes.\n3 \n4 The Dataset class represents the DICOM Dataset while the FileDataset class\n5 adds extra functionality to Dataset when data is read from or written to file.\n6 \n7 Overview of DICOM object model\n8 ------------------------------\n9 Dataset (dict subclass)\n10   Contains DataElement instances, each of which has a tag, VR, VM and value.\n11     The DataElement value can be:\n12         * A single value, such as a number, string, etc. (i.e. VM = 1)\n13         * A list of numbers, strings, etc. (i.e. VM > 1)\n14         * A Sequence (list subclass), where each item is a Dataset which\n15             contains its own DataElements, and so on in a recursive manner.\n16 \"\"\"\n17 \n18 import base64\n19 import inspect  # for __dir__\n20 import io\n21 import json\n22 import os\n23 import os.path\n24 from bisect import bisect_left\n25 from itertools import takewhile\n26 \n27 import pydicom  # for dcmwrite\n28 import pydicom.charset\n29 import pydicom.config\n30 from pydicom import compat, datadict\n31 from pydicom._version import __version_info__\n32 from pydicom.charset import default_encoding, convert_encodings\n33 from pydicom.config import logger\n34 from pydicom.datadict import dictionary_VR\n35 from pydicom.datadict import (tag_for_keyword, keyword_for_tag,\n36                               repeater_has_keyword)\n37 from pydicom.dataelem import DataElement, DataElement_from_raw, RawDataElement\n38 from pydicom.multival import MultiValue\n39 from pydicom.pixel_data_handlers.util import (convert_color_space,\n40                                               reshape_pixel_array)\n41 from pydicom.tag import Tag, BaseTag, tag_in_exception\n42 from pydicom.uid import (ExplicitVRLittleEndian, ImplicitVRLittleEndian,\n43                          ExplicitVRBigEndian, PYDICOM_IMPLEMENTATION_UID)\n44 \n45 \n46 if compat.in_py2:\n47     from pkgutil import find_loader as have_package\n48     from pydicom.valuerep import PersonNameUnicode\n49 else:\n50     from importlib.util import find_spec as have_package\n51 \n52 have_numpy = True\n53 try:\n54     import numpy\n55 except ImportError:\n56     have_numpy = False\n57 \n58 \n59 class PropertyError(Exception):\n60     \"\"\"For AttributeErrors caught in a property, so do not go to __getattr__\"\"\"\n61     #  http://docs.python.org/release/3.1.3/tutorial/errors.html#tut-userexceptions\n62     pass\n63 \n64 \n65 class PrivateBlock(object):\n66     \"\"\"Helper class for a private block in the dataset.\n67     (See PS3.5, Section 7.8.1 - Private Data Element Tags)\n68 \n69     Attributes\n70     ----------\n71     group : 32 bit int\n72         The private group where the private block is located.\n73     private_creator : str\n74         The private creator string related to the block.\n75     dataset : Dataset\n76         The parent dataset.\n77     block_start : 32 bit int\n78         The start element of the private block.\n79         Note that the 2 low order hex digits of the element are always 0.\n80     \"\"\"\n81 \n82     def __init__(self, key, dataset, private_creator_element):\n83         \"\"\"Initializes an object corresponding to a private tag block.\n84 \n85         Parameters\n86         ----------\n87         key : tuple (int, str)\n88             The private group and private creator. The group must be an odd\n89             number.\n90         dataset : Dataset\n91             The parent dataset.\n92         private_creator_element : 32 bit int\n93             The element of the private creator tag.\n94         \"\"\"\n95         self.group = key[0]\n96         self.private_creator = key[1]\n97         self.dataset = dataset\n98         self.block_start = private_creator_element << 8\n99 \n100     def get_tag(self, element_offset):\n101         \"\"\"Return the private tag ID for the given element offset.\n102 \n103         Parameters\n104         ----------\n105         element_offset : 16 bit int\n106             The lower 16 bit (e.g. 2 hex numbers) of the element tag.\n107 \n108         Returns\n109         -------\n110             The tag ID defined by the private block location and the\n111             given element offset.\n112 \n113         Raises\n114         ------\n115         ValueError\n116             If `element_offset` is too large.\n117         \"\"\"\n118         if element_offset > 0xff:\n119             raise ValueError('Element offset must be less than 256')\n120         return Tag(self.group, self.block_start + element_offset)\n121 \n122     def __contains__(self, element_offset):\n123         \"\"\"Return True if the tag with given element offset is contained in\n124         the parent dataset.\"\"\"\n125         return self.get_tag(element_offset) in self.dataset\n126 \n127     def __getitem__(self, element_offset):\n128         \"\"\"Return the data element in the parent dataset for the given element\n129         offset.\n130 \n131         Parameters\n132         ----------\n133         element_offset : 16 bit int\n134             The lower 16 bit (e.g. 2 hex numbers) of the element tag.\n135 \n136         Returns\n137         -------\n138             The data element of the tag in the parent dataset defined by the\n139             private block location and the given element offset.\n140 \n141         Raises\n142         ------\n143         ValueError\n144             If `element_offset` is too large.\n145         KeyError\n146             If no data element exists at that offset.\n147         \"\"\"\n148         return self.dataset.__getitem__(self.get_tag(element_offset))\n149 \n150     def __delitem__(self, element_offset):\n151         \"\"\"Delete the tag with the given element offset from the dataset.\n152 \n153         Parameters\n154         ----------\n155         element_offset : 16 bit int\n156             The lower 16 bit (e.g. 2 hex numbers) of the element tag\n157             to be deleted.\n158 \n159         Raises\n160         ------\n161         ValueError\n162             If `element_offset` is too large.\n163         KeyError\n164             If no data element exists at that offset.\n165         \"\"\"\n166         del self.dataset[self.get_tag(element_offset)]\n167 \n168     def add_new(self, element_offset, VR, value):\n169         \"\"\"Adds the private tag with the given VR and value to the\n170          parent dataset at the tag ID defined by the private block\n171          and the given element offset.\n172 \n173         Parameters\n174         ----------\n175         element_offset : 16 bit int\n176             The lower 16 bit (e.g. 2 hex numbers) of the element tag\n177             to be added.\n178         VR : str\n179             The 2 character DICOM value representation.\n180         value\n181             The value of the data element.\n182             See `pydicom.Dataset.add_new` for a description.\n183         \"\"\"\n184         self.dataset.add_new(self.get_tag(element_offset), VR, value)\n185 \n186 \n187 def _dict_equal(a, b, exclude=None):\n188     \"\"\"Common method for Dataset.__eq__ and FileDataset.__eq__\n189 \n190     Uses .keys() as needed because Dataset iter return items not keys\n191     `exclude` is used in FileDataset__eq__ ds.__dict__ compare, which\n192     would also compare the wrapped _dict member (entire dataset) again.\n193     \"\"\"\n194     return (len(a) == len(b) and\n195             all(key in b for key in a.keys()) and\n196             all(a[key] == b[key] for key in a.keys()\n197                 if exclude is None or key not in exclude)\n198             )\n199 \n200 \n201 class Dataset(dict):\n202     \"\"\"Contains a collection (dictionary) of DICOM DataElements.\n203     Behaves like a dictionary.\n204 \n205     .. note::\n206 \n207         `Dataset` is derived from `dict` only to make it work in a NumPy\n208         array. The parent dict class is never called, as all `dict` methods\n209         are overridden.\n210 \n211     Examples\n212     --------\n213     Add DataElements to the Dataset (for elements in the DICOM dictionary):\n214 \n215     >>> ds = Dataset()\n216     >>> ds.PatientName = \"CITIZEN^Joan\"\n217     >>> ds.add_new(0x00100020, 'LO', '12345')\n218     >>> ds[0x0010, 0x0030] = DataElement(0x00100030, 'DA', '20010101')\n219 \n220     Add Sequence DataElement to the Dataset:\n221 \n222     >>> ds.BeamSequence = [Dataset(), Dataset(), Dataset()]\n223     >>> ds.BeamSequence[0].Manufacturer = \"Linac, co.\"\n224     >>> ds.BeamSequence[1].Manufacturer = \"Linac and Sons, co.\"\n225     >>> ds.BeamSequence[2].Manufacturer = \"Linac and Daughters, co.\"\n226 \n227     Add private DataElements to the Dataset:\n228 \n229     >>> block = ds.private_block(0x0041, 'My Creator', create=True)\n230     >>> block.add_new(0x01, 'LO', '12345')\n231 \n232     Updating and retrieving DataElement values:\n233 \n234     >>> ds.PatientName = \"CITIZEN^Joan\"\n235     >>> ds.PatientName\n236     'CITIZEN^Joan'\n237     >>> ds.PatientName = \"CITIZEN^John\"\n238     >>> ds.PatientName\n239     'CITIZEN^John'\n240 \n241     Retrieving a DataElement's value from a Sequence:\n242 \n243     >>> ds.BeamSequence[0].Manufacturer\n244     'Linac, co.'\n245     >>> ds.BeamSequence[1].Manufacturer\n246     'Linac and Sons, co.'\n247 \n248     Retrieving DataElements:\n249 \n250     >>> elem = ds[0x00100010]\n251     >>> elem\n252     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'\n253     >>> elem = ds.data_element('PatientName')\n254     >>> elem\n255     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'\n256 \n257     Retrieving a private DataElement:\n258 \n259     >>> block = ds.private_block(0x0041, 'My Creator')\n260     >>> elem = block[0x01]\n261     >>> elem\n262     (0041, 1001) Private tag data                    LO: '12345'\n263 \n264     >>> elem.value\n265     '12345'\n266 \n267     Alternatively:\n268     >>> ds.get_private_item(0x0041, 0x01, 'My Creator').value\n269     '12345'\n270 \n271     Deleting a DataElement from the Dataset:\n272 \n273     >>> del ds.PatientID\n274     >>> del ds.BeamSequence[1].Manufacturer\n275     >>> del ds.BeamSequence[2]\n276 \n277     Deleting a private DataElement from the Dataset:\n278 \n279     >>> block = ds.private_block(0x0041, 'My Creator')\n280     >>> if 0x01 in block:\n281     ...     del block[0x01]\n282 \n283     Determining if a DataElement is present in the Dataset:\n284 \n285     >>> 'PatientName' in ds\n286     True\n287     >>> 'PatientID' in ds\n288     False\n289     >>> (0x0010, 0x0030) in ds\n290     True\n291     >>> 'Manufacturer' in ds.BeamSequence[0]\n292     True\n293 \n294     Iterating through the top level of a Dataset only (excluding Sequences):\n295 \n296     >>> for elem in ds:\n297     ...    print(elem)   #doctest: +ELLIPSIS\n298     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'...\n299 \n300     Iterating through the entire Dataset (including Sequences):\n301 \n302     >>> for elem in ds.iterall():\n303     ...     print(elem)  #doctest: +ELLIPSIS\n304     (0010, 0010) Patient's Name                      PN: 'CITIZEN^John'...\n305 \n306     Recursively iterate through a Dataset (including Sequences):\n307 \n308     >>> def recurse(ds):\n309     ...     for elem in ds:\n310     ...         if elem.VR == 'SQ':\n311     ...             [recurse(item) for item in elem]\n312     ...         else:\n313     ...             # Do something useful with each DataElement\n314 \n315     Converting a dataset to and from json:\n316 \n317     >>> ds = Dataset()\n318     >>> ds.PatientName = \"Some^Name\"\n319     >>> jsonmodel = ds.to_json()\n320     >>> ds2 = Dataset()\n321     >>> ds2.from_json(jsonmodel)\n322     (0010, 0010) Patient's Name                      PN: u'Some^Name'\n323 \n324     Attributes\n325     ----------\n326     default_element_format : str\n327         The default formatting for string display.\n328     default_sequence_element_format : str\n329         The default formatting for string display of sequences.\n330     indent_chars : str\n331         For string display, the characters used to indent nested Sequences.\n332         Default is \"   \".\n333     is_little_endian : bool\n334         Shall be set before writing with `write_like_original=False`.\n335         The written dataset (excluding the pixel data) will be written using\n336         the given endianess.\n337     is_implicit_VR : bool\n338         Shall be set before writing with `write_like_original=False`.\n339         The written dataset will be written using the transfer syntax with\n340         the given VR handling, e.g LittleEndianImplicit if True,\n341         and LittleEndianExplicit or BigEndianExplicit (depending on\n342         `is_little_endian`) if False.\n343     \"\"\"\n344     indent_chars = \"   \"\n345 \n346     _BINARY_VR_VALUES = ['OW', 'OB', 'OD', 'OF', 'OL', 'UN',\n347                          'OB or OW', 'US or OW', 'US or SS or OW']\n348     _VRs_TO_BE_FLOATS = ['DS', 'FL', 'FD', ]\n349     _VRs_TO_BE_INTS = ['IS', 'SL', 'SS', 'UL', 'US', 'US or SS']\n350 \n351     # Order of keys is significant!\n352     _JSON_VALUE_KEYS = ('Value', 'BulkDataURI', 'InlineBinary', )\n353 \n354     # Python 2: Classes defining __eq__ should flag themselves as unhashable\n355     __hash__ = None\n356 \n357     def __init__(self, *args, **kwargs):\n358         \"\"\"Create a new Dataset instance.\"\"\"\n359         self._parent_encoding = kwargs.get('parent_encoding', default_encoding)\n360         if not args:\n361             self._dict = {}\n362         elif isinstance(args[0], Dataset):\n363             self._dict = args[0]._dict\n364         else:\n365             self._dict = args[0]\n366         self.is_decompressed = False\n367 \n368         # the following read_XXX attributes are used internally to store\n369         # the properties of the dataset after read from a file\n370 \n371         # set depending on the endianess of the read dataset\n372         self.read_little_endian = None\n373         # set depending on the VR handling of the read dataset\n374         self.read_implicit_vr = None\n375         # set to the encoding the dataset had originally\n376         self.read_encoding = None\n377 \n378         self.is_little_endian = None\n379         self.is_implicit_VR = None\n380 \n381         # the parent data set, if this dataset is a sequence item\n382         self.parent = None\n383 \n384         # known private creator blocks\n385         self._private_blocks = {}\n386 \n387     def __enter__(self):\n388         \"\"\"Method invoked on entry to a with statement.\"\"\"\n389         return self\n390 \n391     def __exit__(self, exc_type, exc_val, exc_tb):\n392         \"\"\"Method invoked on exit from a with statement.\"\"\"\n393         # Returning False will re-raise any exceptions that occur\n394         return False\n395 \n396     def add(self, data_element):\n397         \"\"\"Add a DataElement to the Dataset.\n398 \n399         Equivalent to ds[data_element.tag] = data_element\n400 \n401         Parameters\n402         ----------\n403         data_element : pydicom.dataelem.DataElement\n404             The DataElement to add to the Dataset.\n405         \"\"\"\n406         self[data_element.tag] = data_element\n407 \n408     def add_new(self, tag, VR, value):\n409         \"\"\"Add a DataElement to the Dataset.\n410 \n411         Parameters\n412         ----------\n413         tag\n414             The DICOM (group, element) tag in any form accepted by\n415             pydicom.tag.Tag such as [0x0010, 0x0010], (0x10, 0x10), 0x00100010,\n416             etc.\n417         VR : str\n418             The 2 character DICOM value representation (see DICOM standard part\n419             5, Section 6.2).\n420         value\n421             The value of the data element. One of the following:\n422             * a single string or number\n423             * a list or tuple with all strings or all numbers\n424             * a multi-value string with backslash separator\n425             * for a sequence DataElement, an empty list or list of Dataset\n426         \"\"\"\n427 \n428         data_element = DataElement(tag, VR, value)\n429         # use data_element.tag since DataElement verified it\n430         self._dict[data_element.tag] = data_element\n431 \n432     def data_element(self, name):\n433         \"\"\"Return the DataElement corresponding to the element keyword `name`.\n434 \n435         Parameters\n436         ----------\n437         name : str\n438             A DICOM element keyword.\n439 \n440         Returns\n441         -------\n442         pydicom.dataelem.DataElement or None\n443             For the given DICOM element `keyword`, return the corresponding\n444             Dataset DataElement if present, None otherwise.\n445         \"\"\"\n446         tag = tag_for_keyword(name)\n447         # Test against None as (0000,0000) is a possible tag\n448         if tag is not None:\n449             return self[tag]\n450         return None\n451 \n452     def __contains__(self, name):\n453         \"\"\"Simulate dict.__contains__() to handle DICOM keywords.\n454 \n455         This is called for code like:\n456         >>> ds = Dataset()\n457         >>> ds.SliceLocation = '2'\n458         >>> 'SliceLocation' in ds\n459         True\n460 \n461         Parameters\n462         ----------\n463         name : str or int or 2-tuple\n464             The Element keyword or tag to search for.\n465 \n466         Returns\n467         -------\n468         bool\n469             True if the DataElement is in the Dataset, False otherwise.\n470         \"\"\"\n471         try:\n472             tag = Tag(name)\n473         except (ValueError, OverflowError):\n474             return False\n475         # Test against None as (0000,0000) is a possible tag\n476         if tag is not None:\n477             return tag in self._dict\n478         return name in self._dict  # will no doubt raise an exception\n479 \n480     def decode(self):\n481         \"\"\"Apply character set decoding to all DataElements in the Dataset.\n482 \n483         See DICOM PS3.5-2008 6.1.1.\n484         \"\"\"\n485         # Find specific character set. 'ISO_IR 6' is default\n486         # May be multi-valued, but let pydicom.charset handle all logic on that\n487         dicom_character_set = self._character_set\n488 \n489         # Shortcut to the decode function in pydicom.charset\n490         decode_data_element = pydicom.charset.decode\n491 \n492         # Callback for walk(), to decode the chr strings if necessary\n493         # This simply calls the pydicom.charset.decode function\n494         def decode_callback(ds, data_element):\n495             \"\"\"Callback to decode `data_element`.\"\"\"\n496             if data_element.VR == 'SQ':\n497                 for dset in data_element.value:\n498                     dset._parent_encoding = dicom_character_set\n499                     dset.decode()\n500             else:\n501                 decode_data_element(data_element, dicom_character_set)\n502 \n503         self.walk(decode_callback, recursive=False)\n504 \n505     def __delattr__(self, name):\n506         \"\"\"Intercept requests to delete an attribute by `name`.\n507 \n508         >>> ds = Dataset()\n509         >>> ds.PatientName = 'foo'\n510         >>> ds.some_attribute = True\n511 \n512         If `name` is a DICOM keyword - delete the corresponding DataElement\n513         >>> del ds.PatientName\n514         >>> 'PatientName' in ds\n515         False\n516 \n517         If `name` is another attribute - delete it\n518         >>> del ds.some_attribute\n519         >>> hasattr(ds, 'some_attribute')\n520         False\n521 \n522         Parameters\n523         ----------\n524         name : str\n525             The keyword for the DICOM element or the class attribute to delete.\n526         \"\"\"\n527         # First check if a valid DICOM keyword and if we have that data element\n528         tag = tag_for_keyword(name)\n529         if tag is not None and tag in self._dict:\n530             del self._dict[tag]\n531         # If not a DICOM name in this dataset, check for regular instance name\n532         #   can't do delete directly, that will call __delattr__ again\n533         elif name in self.__dict__:\n534             del self.__dict__[name]\n535         # Not found, raise an error in same style as python does\n536         else:\n537             raise AttributeError(name)\n538 \n539     def __delitem__(self, key):\n540         \"\"\"Intercept requests to delete an attribute by key.\n541 \n542         Examples\n543         --------\n544         Indexing using DataElement tag\n545         >>> ds = Dataset()\n546         >>> ds.CommandGroupLength = 100\n547         >>> ds.PatientName = 'CITIZEN^Jan'\n548         >>> del ds[0x00000000]\n549         >>> ds\n550         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'\n551 \n552         Slicing using DataElement tag\n553         >>> ds = Dataset()\n554         >>> ds.CommandGroupLength = 100\n555         >>> ds.SOPInstanceUID = '1.2.3'\n556         >>> ds.PatientName = 'CITIZEN^Jan'\n557         >>> del ds[:0x00100000]\n558         >>> ds\n559         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'\n560 \n561         Parameters\n562         ----------\n563         key\n564             The key for the attribute to be deleted. If a slice is used then\n565             the tags matching the slice conditions will be deleted.\n566         \"\"\"\n567         # If passed a slice, delete the corresponding DataElements\n568         if isinstance(key, slice):\n569             for tag in self._slice_dataset(key.start, key.stop, key.step):\n570                 del self._dict[tag]\n571         else:\n572             # Assume is a standard tag (for speed in common case)\n573             try:\n574                 del self._dict[key]\n575             # If not a standard tag, than convert to Tag and try again\n576             except KeyError:\n577                 tag = Tag(key)\n578                 del self._dict[tag]\n579 \n580     def __dir__(self):\n581         \"\"\"Give a list of attributes available in the Dataset.\n582 \n583         List of attributes is used, for example, in auto-completion in editors\n584         or command-line environments.\n585         \"\"\"\n586         # Force zip object into a list in case of python3. Also backwards\n587         # compatible\n588         meths = set(list(zip(\n589             *inspect.getmembers(self.__class__, inspect.isroutine)))[0])\n590         props = set(list(zip(\n591             *inspect.getmembers(self.__class__, inspect.isdatadescriptor)))[0])\n592         dicom_names = set(self.dir())\n593         alldir = sorted(props | meths | dicom_names)\n594         return alldir\n595 \n596     def dir(self, *filters):\n597         \"\"\"Return an alphabetical list of DataElement keywords in the Dataset.\n598 \n599         Intended mainly for use in interactive Python sessions. Only lists the\n600         DataElement keywords in the current level of the Dataset (i.e. the\n601         contents of any Sequence elements are ignored).\n602 \n603         Parameters\n604         ----------\n605         filters : str\n606             Zero or more string arguments to the function. Used for\n607             case-insensitive match to any part of the DICOM keyword.\n608 \n609         Returns\n610         -------\n611         list of str\n612             The matching DataElement keywords in the dataset. If no filters are\n613             used then all DataElement keywords are returned.\n614         \"\"\"\n615         allnames = [keyword_for_tag(tag) for tag in self._dict.keys()]\n616         # remove blanks - tags without valid names (e.g. private tags)\n617         allnames = [x for x in allnames if x]\n618         # Store found names in a dict, so duplicate names appear only once\n619         matches = {}\n620         for filter_ in filters:\n621             filter_ = filter_.lower()\n622             match = [x for x in allnames if x.lower().find(filter_) != -1]\n623             matches.update(dict([(x, 1) for x in match]))\n624         if filters:\n625             names = sorted(matches.keys())\n626             return names\n627         else:\n628             return sorted(allnames)\n629 \n630     def __eq__(self, other):\n631         \"\"\"Compare `self` and `other` for equality.\n632 \n633         Returns\n634         -------\n635         bool\n636             The result if `self` and `other` are the same class\n637         NotImplemented\n638             If `other` is not the same class as `self` then returning\n639             NotImplemented delegates the result to superclass.__eq__(subclass)\n640         \"\"\"\n641         # When comparing against self this will be faster\n642         if other is self:\n643             return True\n644 \n645         if isinstance(other, self.__class__):\n646             return _dict_equal(self, other)\n647 \n648         return NotImplemented\n649 \n650     def get(self, key, default=None):\n651         \"\"\"Simulate dict.get() to handle DICOM DataElement tags and keywords.\n652 \n653         Parameters\n654         ----------\n655         key : str or pydicom.tag.Tag\n656             The element keyword or Tag or the class attribute name to get.\n657         default : obj or None\n658             If the DataElement or class attribute is not present, return\n659             `default` (default None).\n660 \n661         Returns\n662         -------\n663         value\n664             If `key` is the keyword for a DataElement in the Dataset then\n665             return the DataElement's value.\n666         pydicom.dataelem.DataElement\n667             If `key` is a tag for a DataElement in the Dataset then return the\n668             DataElement instance.\n669         value\n670             If `key` is a class attribute then return its value.\n671         \"\"\"\n672         if isinstance(key, (str, compat.text_type)):\n673             try:\n674                 return getattr(self, key)\n675             except AttributeError:\n676                 return default\n677         else:\n678             # is not a string, try to make it into a tag and then hand it\n679             # off to the underlying dict\n680             if not isinstance(key, BaseTag):\n681                 try:\n682                     key = Tag(key)\n683                 except Exception:\n684                     raise TypeError(\"Dataset.get key must be a string or tag\")\n685         try:\n686             return self.__getitem__(key)\n687         except KeyError:\n688             return default\n689 \n690     def items(self):\n691         \"\"\"Return the elements in the Dataset as a list of tuple.\n692 \n693         Returns\n694         -------\n695         list of tuple\n696             The top-level (element tag, element) for the Dataset.\n697         \"\"\"\n698         return self._dict.items()\n699 \n700     def keys(self):\n701         \"\"\"Return the DICOM tag keys to simulate dict.\"\"\"\n702         return self._dict.keys()\n703 \n704     def values(self):\n705         \"\"\"Return the DICOM tag values to simulate dict.\"\"\"\n706         return self._dict.values()\n707 \n708     if compat.in_py2:\n709         def iterkeys(self):\n710             return self._dict.iterkeys()\n711 \n712         def itervalues(self):\n713             return self._dict.itervalues()\n714 \n715         def iteritems(self):\n716             return self._dict.iteritems()\n717 \n718     def __getattr__(self, name):\n719         \"\"\"Intercept requests for Dataset attribute names.\n720 \n721         If `name` matches a DICOM keyword, return the value for the\n722         DataElement with the corresponding tag.\n723 \n724         Parameters\n725         ----------\n726         name\n727             A DataElement keyword or tag or a class attribute name.\n728 \n729         Returns\n730         -------\n731         value\n732               If `name` matches a DICOM keyword, returns the corresponding\n733               DataElement's value. Otherwise returns the class attribute's\n734               value (if present).\n735         \"\"\"\n736         tag = tag_for_keyword(name)\n737         if tag is None:  # `name` isn't a DICOM element keyword\n738             # Try the base class attribute getter (fix for issue 332)\n739             return object.__getattribute__(self, name)\n740         tag = Tag(tag)\n741         if tag not in self._dict:  # DICOM DataElement not in the Dataset\n742             # Try the base class attribute getter (fix for issue 332)\n743             return object.__getattribute__(self, name)\n744         else:\n745             data_elem = self[tag]\n746             value = data_elem.value\n747             if data_elem.VR == 'SQ':\n748                 # let a sequence know its parent dataset, as sequence items\n749                 # may need parent dataset tags to resolve ambiguous tags\n750                 value.parent = self\n751             return value\n752 \n753     @property\n754     def _character_set(self):\n755         \"\"\"The Dataset's SpecificCharacterSet value (if present).\"\"\"\n756         char_set = self.get(BaseTag(0x00080005), None)\n757 \n758         if not char_set:\n759             char_set = self._parent_encoding\n760         else:\n761             char_set = convert_encodings(char_set)\n762 \n763         return char_set\n764 \n765     def __getitem__(self, key):\n766         \"\"\"Operator for Dataset[key] request.\n767 \n768         Any deferred data elements will be read in and an attempt will be made\n769         to correct any elements with ambiguous VRs.\n770 \n771         Examples\n772         --------\n773         Indexing using DataElement tag\n774         >>> ds = Dataset()\n775         >>> ds.SOPInstanceUID = '1.2.3'\n776         >>> ds.PatientName = 'CITIZEN^Jan'\n777         >>> ds.PatientID = '12345'\n778         >>> ds[0x00100010].value\n779         'CITIZEN^Jan'\n780 \n781         Slicing using DataElement tag\n782         All group 0x0010 elements in the dataset\n783         >>> ds[0x00100000:0x00110000]\n784         (0010, 0010) Patient's Name                      PN: 'CITIZEN^Jan'\n785         (0010, 0020) Patient ID                          LO: '12345'\n786 \n787         All group 0x0002 elements in the dataset\n788         >>> ds[(0x0002, 0x0000):(0x0003, 0x0000)]\n789         <BLANKLINE>\n790 \n791         Parameters\n792         ----------\n793         key\n794             The DICOM (group, element) tag in any form accepted by\n795             pydicom.tag.Tag such as [0x0010, 0x0010], (0x10, 0x10), 0x00100010,\n796             etc. May also be a slice made up of DICOM tags.\n797 \n798         Returns\n799         -------\n800         pydicom.dataelem.DataElement or pydicom.dataset.Dataset\n801             If a single DICOM element tag is used then returns the\n802             corresponding DataElement. If a slice is used then returns a\n803             Dataset object containing the corresponding DataElements.\n804         \"\"\"\n805         # If passed a slice, return a Dataset containing the corresponding\n806         #   DataElements\n807         if isinstance(key, slice):\n808             return self._dataset_slice(key)\n809 \n810         if isinstance(key, BaseTag):\n811             tag = key\n812         else:\n813             tag = Tag(key)\n814         data_elem = self._dict[tag]\n815 \n816         if isinstance(data_elem, DataElement):\n817             return data_elem\n818         elif isinstance(data_elem, tuple):\n819             # If a deferred read, then go get the value now\n820             if data_elem.value is None:\n821                 from pydicom.filereader import read_deferred_data_element\n822                 data_elem = read_deferred_data_element(\n823                     self.fileobj_type, self.filename, self.timestamp,\n824                     data_elem)\n825 \n826             if tag != BaseTag(0x00080005):\n827                 character_set = self.read_encoding or self._character_set\n828             else:\n829                 character_set = default_encoding\n830             # Not converted from raw form read from file yet; do so now\n831             self[tag] = DataElement_from_raw(data_elem, character_set)\n832 \n833             # If the Element has an ambiguous VR, try to correct it\n834             if 'or' in self[tag].VR:\n835                 from pydicom.filewriter import correct_ambiguous_vr_element\n836                 self[tag] = correct_ambiguous_vr_element(\n837                     self[tag], self, data_elem[6])\n838 \n839         return self._dict.get(tag)\n840 \n841     def private_block(self, group, private_creator, create=False):\n842         \"\"\"Return the block for the given tag and private creator.\n843 \n844         If `create` is set and the private creator does not exist,\n845         the private creator tag is added.\n846         Note: We ignore the unrealistic case that no free block is\n847         available.\n848 \n849         Parameters\n850         ----------\n851         group : 32 bit int\n852             The group of the private tag to be found. Must be an odd number\n853             (e.g. a private group).\n854         private_creator : str\n855             The private creator string associated with the tag.\n856         create : bool\n857             If `True` and `private_creator` does not exist, a new private\n858             creator tag is added at the next free block.\n859             If `False` (the default) and `private_creator` does not exist,\n860             `KeyError` is raised instead.\n861 \n862         Returns\n863         -------\n864         32 bit int\n865             Element base for the given tag (the last 2 hex digits are always 0)\n866 \n867         Raises\n868         ------\n869         ValueError\n870             If `tag` is not a private tag or `private_creator` is empty.\n871         KeyError\n872             If the private creator tag is not found in the given group and\n873             the `create` parameter is not set.\n874         \"\"\"\n875         def new_block():\n876             block = PrivateBlock(key, self, element)\n877             self._private_blocks[key] = block\n878             return block\n879 \n880         key = (group, private_creator)\n881         if key in self._private_blocks:\n882             return self._private_blocks[key]\n883 \n884         if not private_creator:\n885             raise ValueError('Private creator must have a value')\n886 \n887         if group % 2 == 0:\n888             raise ValueError(\n889                 'Tag must be private if private creator is given')\n890 \n891         for element in range(0x10, 0x100):\n892             private_creator_tag = Tag(group, element)\n893             if private_creator_tag not in self._dict:\n894                 if create:\n895                     self.add_new(private_creator_tag, 'LO', private_creator)\n896                     return new_block()\n897                 else:\n898                     break\n899             if self._dict[private_creator_tag].value == private_creator:\n900                 return new_block()\n901 \n902         raise KeyError(\n903             \"Private creator '{}' not found\".format(private_creator))\n904 \n905     def private_creators(self, group):\n906         \"\"\"Return a list of private creator names in the given group.\n907 \n908         This can be used to check if a given private creator exists in\n909         the group of the dataset:\n910         >>> ds = Dataset()\n911         >>> if 'My Creator' in ds.private_creators(0x0041):\n912         ...     block = ds.private_block(0x0041, 'My Creator')\n913 \n914         Parameters\n915         ----------\n916         group : 32 bit int\n917             The private group. Must be an odd number.\n918 \n919         Returns\n920         -------\n921         list of str\n922             List of all private creator names for private blocks in the group.\n923 \n924         Raises\n925         ------\n926         ValueError\n927             If `group` is not a private group.\n928         \"\"\"\n929         if group % 2 == 0:\n930             raise ValueError('Group must be an odd number')\n931 \n932         private_creators = []\n933         for element in range(0x10, 0x100):\n934             private_creator_tag = Tag(group, element)\n935             if private_creator_tag not in self._dict:\n936                 break\n937             private_creators.append(self._dict[private_creator_tag].value)\n938         return private_creators\n939 \n940     def get_private_item(self, group, element_offset, private_creator):\n941         \"\"\"Return the data element for the given private tag.\n942 \n943         This is analogous to `__getitem__`, but only for private tags.\n944         This allows to find the private tag for the correct private creator\n945         without the need to add the tag to the private dictionary first.\n946 \n947         Parameters\n948         ----------\n949         group : 32 bit int\n950             The private group where the item is located.\n951         element_offset : 16 bit int\n952             The lower 16 bit (e.g. 2 hex numbers) of the element tag.\n953         private_creator : str\n954             The private creator for the tag. Must match the private creator\n955             for the tag to be returned.\n956 \n957         Returns\n958         -------\n959         pydicom.dataelem.DataElement\n960 \n961         Raises\n962         ------\n963         ValueError\n964             If `tag` is not a private tag or `private_creator` is empty.\n965         KeyError\n966             If the private creator tag is not found in the given group.\n967             If the private tag is not found.\n968         \"\"\"\n969         block = self.private_block(group, private_creator)\n970         return self.__getitem__(block.get_tag(element_offset))\n971 \n972     def get_item(self, key):\n973         \"\"\"Return the raw data element if possible.\n974 \n975         It will be raw if the user has never accessed the value, or set their\n976         own value. Note if the data element is a deferred-read element,\n977         then it is read and converted before being returned.\n978 \n979         Parameters\n980         ----------\n981         key\n982             The DICOM (group, element) tag in any form accepted by\n983             pydicom.tag.Tag such as [0x0010, 0x0010], (0x10, 0x10), 0x00100010,\n984             etc. May also be a slice made up of DICOM tags.\n985 \n986         Returns\n987         -------\n988         pydicom.dataelem.DataElement\n989         \"\"\"\n990         if isinstance(key, slice):\n991             return self._dataset_slice(key)\n992 \n993         if isinstance(key, BaseTag):\n994             tag = key\n995         else:\n996             tag = Tag(key)\n997         data_elem = self._dict.get(tag)\n998         # If a deferred read, return using __getitem__ to read and convert it\n999         if isinstance(data_elem, tuple) and data_elem.value is None:\n1000             return self[key]\n1001         return data_elem\n1002 \n1003     def _dataset_slice(self, slice):\n1004         \"\"\"Return a slice that has the same properties as the original\n1005         dataset. That includes properties related to endianess and VR handling,\n1006         and the specific character set. No element conversion is done, e.g.\n1007         elements of type RawDataElement are kept.\n1008         \"\"\"\n1009         tags = self._slice_dataset(slice.start, slice.stop, slice.step)\n1010         dataset = Dataset({tag: self.get_item(tag) for tag in tags})\n1011         dataset.is_little_endian = self.is_little_endian\n1012         dataset.is_implicit_VR = self.is_implicit_VR\n1013         dataset.set_original_encoding(self.read_implicit_vr,\n1014                                       self.read_little_endian,\n1015                                       self.read_encoding)\n1016         return dataset\n1017 \n1018     @property\n1019     def is_original_encoding(self):\n1020         \"\"\"Return True if the properties to be used for writing are set and\n1021         have the same value as the ones in the dataset after reading it.\n1022         This includes properties related to endianess, VR handling and the\n1023         specific character set.\n1024         \"\"\"\n1025         return (self.is_implicit_VR is not None and\n1026                 self.is_little_endian is not None and\n1027                 self.read_implicit_vr == self.is_implicit_VR and\n1028                 self.read_little_endian == self.is_little_endian and\n1029                 self.read_encoding == self._character_set)\n1030 \n1031     def set_original_encoding(self, is_implicit_vr, is_little_endian,\n1032                               character_encoding):\n1033         \"\"\"Set the values for the original transfer syntax and encoding.\n1034         Can be used for a dataset with raw data elements to enable\n1035         optimized writing (e.g. without decoding the data elements).\n1036         \"\"\"\n1037         self.read_implicit_vr = is_implicit_vr\n1038         self.read_little_endian = is_little_endian\n1039         self.read_encoding = character_encoding\n1040 \n1041     def group_dataset(self, group):\n1042         \"\"\"Return a Dataset containing only DataElements of a certain group.\n1043 \n1044         Parameters\n1045         ----------\n1046         group : int\n1047             The group part of a DICOM (group, element) tag.\n1048 \n1049         Returns\n1050         -------\n1051         pydicom.dataset.Dataset\n1052             A dataset instance containing elements of the group specified.\n1053         \"\"\"\n1054         return self[(group, 0x0000):(group + 1, 0x0000)]\n1055 \n1056     def __iter__(self):\n1057         \"\"\"Iterate through the top-level of the Dataset, yielding DataElements.\n1058 \n1059         >>> ds = Dataset()\n1060         >>> for elem in ds:\n1061         ...     print(elem)\n1062 \n1063         The DataElements are returned in increasing tag value order.\n1064         Sequence items are returned as a single DataElement, so it is up to the\n1065         calling code to recurse into the Sequence items if desired.\n1066 \n1067         Yields\n1068         ------\n1069         pydicom.dataelem.DataElement\n1070             The Dataset's DataElements, sorted by increasing tag order.\n1071         \"\"\"\n1072         # Note this is different than the underlying dict class,\n1073         #        which returns the key of the key:value mapping.\n1074         #   Here the value is returned (but data_element.tag has the key)\n1075         taglist = sorted(self._dict.keys())\n1076         for tag in taglist:\n1077             yield self[tag]\n1078 \n1079     def elements(self):\n1080         \"\"\"Iterate through the top-level of the Dataset, yielding DataElements\n1081         or RawDataElements (no conversion done).\n1082 \n1083         >>> ds = Dataset()\n1084         >>> for elem in ds.elements():\n1085         ...     print(elem)\n1086 \n1087         The elements are returned in the same way as in __getitem__.\n1088 \n1089         Yields\n1090         ------\n1091         pydicom.dataelem.DataElement or pydicom.dataelem.RawDataElement\n1092             The Dataset's DataElements, sorted by increasing tag order.\n1093         \"\"\"\n1094         taglist = sorted(self._dict.keys())\n1095         for tag in taglist:\n1096             yield self.get_item(tag)\n1097 \n1098     def __len__(self):\n1099         return len(self._dict)\n1100 \n1101     def __ne__(self, other):\n1102         \"\"\"Compare `self` and `other` for inequality.\"\"\"\n1103         return not self == other\n1104 \n1105     def clear(self):\n1106         \"\"\"Delete all data elements.\"\"\"\n1107         self._dict.clear()\n1108 \n1109     def pop(self, key, *args):\n1110         \"\"\"Emulate dictionary `pop`, but additionally support tag ID tuple\n1111         and DICOM keyword.\n1112 \n1113         Removes the data element for `key` if it exists and returns it,\n1114         otherwise returns a default value if given or raises `KeyError`.\n1115 \n1116         Parameters\n1117         ----------\n1118         key: int or str or 2-tuple\n1119             if tuple - the group and element number of the DICOM tag\n1120             if int - the combined group/element number\n1121             if str - the DICOM keyword of the tag\n1122 \n1123         *args: zero or one argument\n1124             defines the behavior if no tag exists for `key`: if given,\n1125             it defines the return value, if not given, `KeyError` is raised\n1126 \n1127         Returns\n1128         -------\n1129         The data element for `key` if it exists, or the default value if given.\n1130 \n1131         Raises\n1132         ------\n1133         KeyError\n1134             If the key is not a valid tag ID or keyword.\n1135             If the tag does not exist and no default is given.\n1136         \"\"\"\n1137         try:\n1138             tag = Tag(key)\n1139         except (ValueError, OverflowError):\n1140             return self._dict.pop(key, *args)\n1141         return self._dict.pop(tag, *args)\n1142 \n1143     def popitem(self):\n1144         return self._dict.popitem()\n1145 \n1146     def setdefault(self, key, default=None):\n1147         \"\"\"Emulate dictionary `setdefault`, but additionally support\n1148         tag ID tuple and DICOM keyword for `key`, and data element value\n1149         for `default`.\n1150 \n1151         .. usage:\n1152 \n1153         >>> ds = Dataset()\n1154         >>> pname = ds.setdefault((0x0010, 0x0010), \"Test\")\n1155         >>> pname\n1156         (0010, 0010) Patient's Name                      PN: 'Test'\n1157         >>> pname.value\n1158         'Test'\n1159         >>> psex = ds.setdefault('PatientSex',\n1160         ...     DataElement(0x00100040, 'CS', 'F'))\n1161         >>> psex.value\n1162         'F'\n1163 \n1164         Parameters\n1165         ----------\n1166         key: int or str or 2-tuple\n1167             if tuple - the group and element number of the DICOM tag\n1168             if int - the combined group/element number\n1169             if str - the DICOM keyword of the tag\n1170 \n1171         default: DataElement or value type or None\n1172             The default value that is inserted and returned if no data\n1173             element exists for the given key.\n1174             If it is not of type DataElement, a DataElement is constructed\n1175             instead for the given tag ID and default as value. This is only\n1176             possible for known tags (e.g. tags found via the dictionary\n1177             lookup).\n1178 \n1179         Returns\n1180         -------\n1181         The data element for `key` if it exists, or the default value if\n1182         it is a DataElement or None, or a DataElement constructed with\n1183         `default` as value.\n1184 \n1185         Raises\n1186         ------\n1187         KeyError\n1188             If the key is not a valid tag ID or keyword.\n1189             If no tag exists for `key`, default is not a DataElement\n1190             and not None, and key is not a known DICOM tag.\n1191         \"\"\"\n1192         if key in self:\n1193             return self[key]\n1194         if default is not None:\n1195             if not isinstance(default, DataElement):\n1196                 tag = Tag(key)\n1197                 vr = datadict.dictionary_VR(tag)\n1198                 default = DataElement(Tag(key), vr, default)\n1199             self[key] = default\n1200         return default\n1201 \n1202     def convert_pixel_data(self):\n1203         \"\"\"Convert the Pixel Data to a numpy array internally.\n1204 \n1205         Returns\n1206         -------\n1207         None\n1208             Converted pixel data is stored internally in the dataset.\n1209 \n1210         Notes\n1211         -----\n1212         If the pixel data is in a compressed image format, the data is\n1213         decompressed and any related data elements are changed accordingly.\n1214         \"\"\"\n1215         # Check if already have converted to a NumPy array\n1216         # Also check if self.PixelData has changed. If so, get new NumPy array\n1217         already_have = True\n1218         if not hasattr(self, \"_pixel_array\"):\n1219             already_have = False\n1220         elif self._pixel_id != id(self.PixelData):\n1221             already_have = False\n1222 \n1223         if already_have:\n1224             return\n1225 \n1226         # Find all possible handlers that support the transfer syntax\n1227         transfer_syntax = self.file_meta.TransferSyntaxUID\n1228         possible_handlers = [hh for hh in pydicom.config.pixel_data_handlers\n1229                              if hh.supports_transfer_syntax(transfer_syntax)]\n1230 \n1231         # No handlers support the transfer syntax\n1232         if not possible_handlers:\n1233             raise NotImplementedError(\n1234                 \"Unable to decode pixel data with a transfer syntax UID of \"\n1235                 \"'{0}' ({1}) as there are no pixel data handlers \"\n1236                 \"available that support it. Please see the pydicom \"\n1237                 \"documentation for information on supported transfer syntaxes \"\n1238                 .format(self.file_meta.TransferSyntaxUID,\n1239                         self.file_meta.TransferSyntaxUID.name)\n1240             )\n1241 \n1242         # Handlers that both support the transfer syntax and have their\n1243         #   dependencies met\n1244         available_handlers = [hh for hh in possible_handlers if\n1245                               hh.is_available()]\n1246 \n1247         # There are handlers that support the transfer syntax but none of them\n1248         #   can be used as missing dependencies\n1249         if not available_handlers:\n1250             # For each of the possible handlers we want to find which\n1251             #   dependencies are missing\n1252             msg = (\n1253                 \"The following handlers are available to decode the pixel \"\n1254                 \"data however they are missing required dependencies: \"\n1255             )\n1256             pkg_msg = []\n1257             for hh in possible_handlers:\n1258                 hh_deps = hh.DEPENDENCIES\n1259                 # Missing packages\n1260                 missing = [dd for dd in hh_deps if have_package(dd) is None]\n1261                 # Package names\n1262                 names = [hh_deps[name][1] for name in missing]\n1263                 pkg_msg.append(\n1264                     \"{} (req. {})\"\n1265                     .format(hh.HANDLER_NAME, ', '.join(names))\n1266                 )\n1267 \n1268             raise RuntimeError(msg + ', '.join(pkg_msg))\n1269 \n1270         last_exception = None\n1271         for handler in available_handlers:\n1272             try:\n1273                 # Use the handler to get a 1D numpy array of the pixel data\n1274                 arr = handler.get_pixeldata(self)\n1275                 self._pixel_array = reshape_pixel_array(self, arr)\n1276 \n1277                 # Some handler/transfer syntax combinations may need to\n1278                 #   convert the color space from YCbCr to RGB\n1279                 if handler.needs_to_convert_to_RGB(self):\n1280                     self._pixel_array = convert_color_space(self._pixel_array,\n1281                                                             'YBR_FULL',\n1282                                                             'RGB')\n1283 \n1284                 self._pixel_id = id(self.PixelData)\n1285 \n1286                 return\n1287             except Exception as exc:\n1288                 logger.debug(\n1289                     \"Exception raised by pixel data handler\", exc_info=exc\n1290                 )\n1291                 last_exception = exc\n1292 \n1293         # The only way to get to this point is if we failed to get the pixel\n1294         #   array because all suitable handlers raised exceptions\n1295         self._pixel_array = None\n1296         self._pixel_id = None\n1297 \n1298         logger.info(\n1299             \"Unable to decode the pixel data using the following handlers: {}.\"\n1300             \"Please see the list of supported Transfer Syntaxes in the \"\n1301             \"pydicom documentation for alternative packages that might \"\n1302             \"be able to decode the data\"\n1303             .format(\", \".join([str(hh) for hh in available_handlers]))\n1304         )\n1305 \n1306         raise last_exception\n1307 \n1308     def decompress(self):\n1309         \"\"\"Decompresses pixel data and modifies the Dataset in-place\n1310 \n1311         If not a compressed tranfer syntax, then pixel data is converted\n1312         to a numpy array internally, but not returned.\n1313 \n1314         If compressed pixel data, then is decompressed using an image handler,\n1315         and internal state is updated appropriately:\n1316             - TransferSyntax is updated to non-compressed form\n1317             - is_undefined_length for pixel data is set False\n1318 \n1319         Returns\n1320         -------\n1321         None\n1322 \n1323         Raises\n1324         ------\n1325         NotImplementedError\n1326             If the pixel data was originally compressed but file is not\n1327             ExplicitVR LittleEndian as required by Dicom standard\n1328         \"\"\"\n1329         self.convert_pixel_data()\n1330         self.is_decompressed = True\n1331         # May have been undefined length pixel data, but won't be now\n1332         if 'PixelData' in self:\n1333             self[0x7fe00010].is_undefined_length = False\n1334 \n1335         # Make sure correct Transfer Syntax is set\n1336         # According to the dicom standard PS3.5 section A.4,\n1337         # all compressed files must have been explicit VR, little endian\n1338         # First check if was a compressed file\n1339         if (hasattr(self, 'file_meta') and\n1340                 self.file_meta.TransferSyntaxUID.is_compressed):\n1341             # Check that current file as read does match expected\n1342             if not self.is_little_endian or self.is_implicit_VR:\n1343                 msg = (\"Current dataset does not match expected ExplicitVR \"\n1344                        \"LittleEndian transfer syntax from a compressed \"\n1345                        \"transfer syntax\")\n1346                 raise NotImplementedError(msg)\n1347 \n1348             # All is as expected, updated the Transfer Syntax\n1349             self.file_meta.TransferSyntaxUID = ExplicitVRLittleEndian\n1350 \n1351     @property\n1352     def pixel_array(self):\n1353         \"\"\"Return the Pixel Data as a NumPy array.\n1354 \n1355         Returns\n1356         -------\n1357         numpy.ndarray\n1358             The Pixel Data (7FE0,0010) as a NumPy ndarray.\n1359         \"\"\"\n1360         self.convert_pixel_data()\n1361         return self._pixel_array\n1362 \n1363     # Format strings spec'd according to python string formatting options\n1364     #    See http://docs.python.org/library/stdtypes.html#string-formatting-operations # noqa\n1365     default_element_format = \"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\"\n1366     default_sequence_element_format = \"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\"  # noqa\n1367 \n1368     def formatted_lines(\n1369             self,\n1370             element_format=default_element_format,\n1371             sequence_element_format=default_sequence_element_format,\n1372             indent_format=None):\n1373         \"\"\"Iterate through the Dataset yielding formatted str for each element.\n1374 \n1375         Parameters\n1376         ----------\n1377         element_format : str\n1378             The string format to use for non-sequence elements. Formatting uses\n1379             the attributes of DataElement. Default is\n1380             \"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\".\n1381         sequence_element_format : str\n1382             The string format to use for sequence elements. Formatting uses\n1383             the attributes of DataElement. Default is\n1384             \"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\"\n1385         indent_format : str or None\n1386             Placeholder for future functionality.\n1387 \n1388         Yields\n1389         ------\n1390         str\n1391             A string representation of a DataElement.\n1392         \"\"\"\n1393         for data_element in self.iterall():\n1394             # Get all the attributes possible for this data element (e.g.\n1395             #   gets descriptive text name too)\n1396             # This is the dictionary of names that can be used in the format\n1397             #   string\n1398             elem_dict = dict([(x, getattr(data_element, x)()\n1399                                if callable(getattr(data_element, x)) else\n1400                                getattr(data_element, x))\n1401                               for x in dir(data_element)\n1402                               if not x.startswith(\"_\")\n1403                               and x != \"from_json\"])\n1404             if data_element.VR == \"SQ\":\n1405                 yield sequence_element_format % elem_dict\n1406             else:\n1407                 yield element_format % elem_dict\n1408 \n1409     def _pretty_str(self, indent=0, top_level_only=False):\n1410         \"\"\"Return a string of the DataElements in the Dataset, with indented\n1411         levels.\n1412 \n1413         This private method is called by the __str__() method for handling\n1414         print statements or str(dataset), and the __repr__() method.\n1415         It is also used by top(), therefore the top_level_only flag.\n1416         This function recurses, with increasing indentation levels.\n1417 \n1418         Parameters\n1419         ----------\n1420         indent : int\n1421             The indent level offset (default 0)\n1422         top_level_only : bool\n1423             When True, only create a string for the top level elements, i.e.\n1424             exclude elements within any Sequences (default False).\n1425 \n1426         Returns\n1427         -------\n1428         str\n1429             A string representation of the Dataset.\n1430         \"\"\"\n1431         strings = []\n1432         indent_str = self.indent_chars * indent\n1433         nextindent_str = self.indent_chars * (indent + 1)\n1434         for data_element in self:\n1435             with tag_in_exception(data_element.tag):\n1436                 if data_element.VR == \"SQ\":  # a sequence\n1437                     strings.append(indent_str + str(data_element.tag) +\n1438                                    \"  %s   %i item(s) ---- \" %\n1439                                    (data_element.description(),\n1440                                     len(data_element.value)))\n1441                     if not top_level_only:\n1442                         for dataset in data_element.value:\n1443                             strings.append(dataset._pretty_str(indent + 1))\n1444                             strings.append(nextindent_str + \"---------\")\n1445                 else:\n1446                     strings.append(indent_str + repr(data_element))\n1447         return \"\\n\".join(strings)\n1448 \n1449     def remove_private_tags(self):\n1450         \"\"\"Remove all private DataElements in the Dataset.\"\"\"\n1451 \n1452         def RemoveCallback(dataset, data_element):\n1453             \"\"\"Internal method to use as callback to walk() method.\"\"\"\n1454             if data_element.tag.is_private:\n1455                 # can't del self[tag] - won't be right dataset on recursion\n1456                 del dataset[data_element.tag]\n1457 \n1458         self.walk(RemoveCallback)\n1459 \n1460     def save_as(self, filename, write_like_original=True):\n1461         \"\"\"Write the Dataset to `filename`.\n1462 \n1463         Saving a Dataset requires that the Dataset.is_implicit_VR and\n1464         Dataset.is_little_endian attributes exist and are set appropriately. If\n1465         Dataset.file_meta.TransferSyntaxUID is present then it should be set to\n1466         a consistent value to ensure conformance.\n1467 \n1468         Conformance with DICOM File Format\n1469         ----------------------------------\n1470         If `write_like_original` is False, the Dataset will be stored in the\n1471         DICOM File Format in accordance with DICOM Standard Part 10 Section 7.\n1472         To do so requires that the `Dataset.file_meta` attribute exists and\n1473         contains a Dataset with the required (Type 1) File Meta Information\n1474         Group elements (see pydicom.filewriter.dcmwrite and\n1475         pydicom.filewriter.write_file_meta_info for more information).\n1476 \n1477         If `write_like_original` is True then the Dataset will be written as is\n1478         (after minimal validation checking) and may or may not contain all or\n1479         parts of the File Meta Information (and hence may or may not be\n1480         conformant with the DICOM File Format).\n1481 \n1482         Parameters\n1483         ----------\n1484         filename : str or file-like\n1485             Name of file or the file-like to write the new DICOM file to.\n1486         write_like_original : bool\n1487             If True (default), preserves the following information from\n1488             the Dataset (and may result in a non-conformant file):\n1489             - preamble -- if the original file has no preamble then none will\n1490                 be written.\n1491             - file_meta -- if the original file was missing any required File\n1492                 Meta Information Group elements then they will not be added or\n1493                 written.\n1494                 If (0002,0000) 'File Meta Information Group Length' is present\n1495                 then it may have its value updated.\n1496             - seq.is_undefined_length -- if original had delimiters, write them\n1497                 now too, instead of the more sensible length characters\n1498             - is_undefined_length_sequence_item -- for datasets that belong to\n1499                 a sequence, write the undefined length delimiters if that is\n1500                 what the original had.\n1501             If False, produces a file conformant with the DICOM File Format,\n1502             with explicit lengths for all elements.\n1503 \n1504         See Also\n1505         --------\n1506         pydicom.filewriter.write_dataset\n1507             Write a DICOM Dataset to a file.\n1508         pydicom.filewriter.write_file_meta_info\n1509             Write the DICOM File Meta Information Group elements to a file.\n1510         pydicom.filewriter.dcmwrite\n1511             Write a DICOM file from a FileDataset instance.\n1512         \"\"\"\n1513         # Ensure is_little_endian and is_implicit_VR are set\n1514         if self.is_little_endian is None or self.is_implicit_VR is None:\n1515             raise AttributeError(\n1516                 \"'{0}.is_little_endian' and '{0}.is_implicit_VR' must be \"\n1517                 \"set appropriately before saving.\".format(\n1518                     self.__class__.__name__))\n1519 \n1520         pydicom.dcmwrite(filename, self, write_like_original)\n1521 \n1522     def ensure_file_meta(self):\n1523         \"\"\"Create an empty file meta dataset if none exists.\"\"\"\n1524         self.file_meta = getattr(self, 'file_meta', Dataset())\n1525 \n1526     def fix_meta_info(self, enforce_standard=True):\n1527         \"\"\"Ensure the file meta info exists and has the correct values\n1528         for transfer syntax and media storage uids.\n1529 \n1530         .. warning::\n1531 \n1532             The transfer syntax for is_implicit_VR = False and\n1533             is_little_endian = True is ambiguous and will therefore not be set.\n1534 \n1535         Parameters\n1536         ----------\n1537         enforce_standard : boolean\n1538             If True, a check for incorrect and missing elements is performed.\n1539             (see pydicom.filewriter.validate_file_meta)\n1540 \n1541         \"\"\"\n1542         self.ensure_file_meta()\n1543 \n1544         if self.is_little_endian and self.is_implicit_VR:\n1545             self.file_meta.TransferSyntaxUID = ImplicitVRLittleEndian\n1546         elif not self.is_little_endian and not self.is_implicit_VR:\n1547             self.file_meta.TransferSyntaxUID = ExplicitVRBigEndian\n1548         elif not self.is_little_endian and self.is_implicit_VR:\n1549             raise NotImplementedError(\"Implicit VR Big Endian is not a \"\n1550                                       \"supported Transfer Syntax.\")\n1551 \n1552         if 'SOPClassUID' in self:\n1553             self.file_meta.MediaStorageSOPClassUID = self.SOPClassUID\n1554         if 'SOPInstanceUID' in self:\n1555             self.file_meta.MediaStorageSOPInstanceUID = self.SOPInstanceUID\n1556         if enforce_standard:\n1557             validate_file_meta(self.file_meta, enforce_standard=True)\n1558 \n1559     def __setattr__(self, name, value):\n1560         \"\"\"Intercept any attempts to set a value for an instance attribute.\n1561 \n1562         If name is a DICOM keyword, set the corresponding tag and DataElement.\n1563         Else, set an instance (python) attribute as any other class would do.\n1564 \n1565         Parameters\n1566         ----------\n1567         name : str\n1568             The element keyword for the DataElement you wish to add/change. If\n1569             `name` is not a DICOM element keyword then this will be the\n1570             name of the attribute to be added/changed.\n1571         value\n1572             The value for the attribute to be added/changed.\n1573         \"\"\"\n1574         tag = tag_for_keyword(name)\n1575         if tag is not None:  # successfully mapped name to a tag\n1576             if tag not in self:\n1577                 # don't have this tag yet->create the data_element instance\n1578                 VR = dictionary_VR(tag)\n1579                 data_element = DataElement(tag, VR, value)\n1580                 if VR == 'SQ':\n1581                     # let a sequence know its parent dataset to pass it\n1582                     # to its items, who may need parent dataset tags\n1583                     # to resolve ambiguous tags\n1584                     data_element.parent = self\n1585             else:\n1586                 # already have this data_element, just changing its value\n1587                 data_element = self[tag]\n1588                 data_element.value = value\n1589             # Now have data_element - store it in this dict\n1590             self[tag] = data_element\n1591         elif repeater_has_keyword(name):\n1592             # Check if `name` is repeaters element\n1593             raise ValueError('{} is a DICOM repeating group '\n1594                              'element and must be added using '\n1595                              'the add() or add_new() methods.'\n1596                              .format(name))\n1597         else:\n1598             # name not in dicom dictionary - setting a non-dicom instance\n1599             # attribute\n1600             # XXX note if user mis-spells a dicom data_element - no error!!!\n1601             object.__setattr__(self, name, value)\n1602 \n1603     def __setitem__(self, key, value):\n1604         \"\"\"Operator for Dataset[key] = value.\n1605 \n1606         Check consistency, and deal with private tags.\n1607 \n1608         Parameters\n1609         ----------\n1610         key : int\n1611             The tag for the element to be added to the Dataset.\n1612         value : pydicom.dataelem.DataElement or pydicom.dataelem.RawDataElement\n1613             The element to add to the Dataset.\n1614 \n1615         Raises\n1616         ------\n1617         NotImplementedError\n1618             If `key` is a slice.\n1619         ValueError\n1620             If the `key` value doesn't match DataElement.tag.\n1621         \"\"\"\n1622         if isinstance(key, slice):\n1623             raise NotImplementedError('Slicing is not supported for setting '\n1624                                       'Dataset elements.')\n1625 \n1626         # OK if is subclass, e.g. DeferredDataElement\n1627         if not isinstance(value, (DataElement, RawDataElement)):\n1628             raise TypeError(\"Dataset contents must be DataElement instances.\")\n1629         if isinstance(value.tag, BaseTag):\n1630             tag = value.tag\n1631         else:\n1632             tag = Tag(value.tag)\n1633         if key != tag:\n1634             raise ValueError(\"DataElement.tag must match the dictionary key\")\n1635 \n1636         data_element = value\n1637         if tag.is_private:\n1638             # See PS 3.5-2008 section 7.8.1 (p. 44) for how blocks are reserved\n1639             logger.debug(\"Setting private tag %r\" % tag)\n1640             private_block = tag.elem >> 8\n1641             private_creator_tag = Tag(tag.group, private_block)\n1642             if private_creator_tag in self and tag != private_creator_tag:\n1643                 if data_element.is_raw:\n1644                     data_element = DataElement_from_raw(\n1645                         data_element, self._character_set)\n1646                 data_element.private_creator = self[private_creator_tag].value\n1647         self._dict[tag] = data_element\n1648 \n1649     def _slice_dataset(self, start, stop, step):\n1650         \"\"\"Return the element tags in the Dataset that match the slice.\n1651 \n1652         Parameters\n1653         ----------\n1654         start : int or 2-tuple of int or None\n1655             The slice's starting element tag value, in any format accepted by\n1656             pydicom.tag.Tag.\n1657         stop : int or 2-tuple of int or None\n1658             The slice's stopping element tag value, in any format accepted by\n1659             pydicom.tag.Tag.\n1660         step : int or None\n1661             The slice's step size.\n1662 \n1663         Returns\n1664         ------\n1665         list of pydicom.tag.Tag\n1666             The tags in the Dataset that meet the conditions of the slice.\n1667         \"\"\"\n1668         # Check the starting/stopping Tags are valid when used\n1669         if start is not None:\n1670             start = Tag(start)\n1671         if stop is not None:\n1672             stop = Tag(stop)\n1673 \n1674         all_tags = sorted(self._dict.keys())\n1675         # If the Dataset is empty, return an empty list\n1676         if not all_tags:\n1677             return []\n1678 \n1679         # Special case the common situations:\n1680         #   - start and/or stop are None\n1681         #   - step is 1\n1682 \n1683         if start is None:\n1684             if stop is None:\n1685                 # For step=1 avoid copying the list\n1686                 return all_tags if step == 1 else all_tags[::step]\n1687             else:  # Have a stop value, get values until that point\n1688                 step1_list = list(takewhile(lambda x: x < stop, all_tags))\n1689                 return step1_list if step == 1 else step1_list[::step]\n1690 \n1691         # Have a non-None start value.  Find its index\n1692         i_start = bisect_left(all_tags, start)\n1693         if stop is None:\n1694             return all_tags[i_start::step]\n1695         else:\n1696             i_stop = bisect_left(all_tags, stop)\n1697             return all_tags[i_start:i_stop:step]\n1698 \n1699     def __str__(self):\n1700         \"\"\"Handle str(dataset).\"\"\"\n1701         return self._pretty_str()\n1702 \n1703     def top(self):\n1704         \"\"\"Return a str of the Dataset's top level DataElements only.\"\"\"\n1705         return self._pretty_str(top_level_only=True)\n1706 \n1707     def trait_names(self):\n1708         \"\"\"Return a list of valid names for auto-completion code.\n1709 \n1710         Used in IPython, so that data element names can be found and offered\n1711         for autocompletion on the IPython command line.\n1712         \"\"\"\n1713         return dir(self)  # only valid python >=2.6, else use self.__dir__()\n1714 \n1715     def update(self, dictionary):\n1716         \"\"\"Extend dict.update() to handle DICOM keywords.\n1717 \n1718         Parameters\n1719         ----------\n1720         dictionary : dict or Dataset\n1721             The dict or Dataset to use when updating the current object.\n1722         \"\"\"\n1723         for key, value in list(dictionary.items()):\n1724             if isinstance(key, (str, compat.text_type)):\n1725                 setattr(self, key, value)\n1726             else:\n1727                 self[Tag(key)] = value\n1728 \n1729     def iterall(self):\n1730         \"\"\"Iterate through the Dataset, yielding all DataElements.\n1731 \n1732         Unlike Dataset.__iter__, this *does* recurse into sequences,\n1733         and so returns all data elements as if the file were \"flattened\".\n1734 \n1735         Yields\n1736         ------\n1737         pydicom.dataelem.DataElement\n1738         \"\"\"\n1739         for data_element in self:\n1740             yield data_element\n1741             if data_element.VR == \"SQ\":\n1742                 sequence = data_element.value\n1743                 for dataset in sequence:\n1744                     for elem in dataset.iterall():\n1745                         yield elem\n1746 \n1747     def walk(self, callback, recursive=True):\n1748         \"\"\"Iterate through the DataElements and run `callback` on each.\n1749 \n1750         Visit all DataElements, possibly recursing into sequences and their\n1751         datasets. The callback function is called for each DataElement\n1752         (including SQ element). Can be used to perform an operation on certain\n1753         types of DataElements. E.g., `remove_private_tags`() finds all private\n1754         tags and deletes them. DataElement`s will come back in DICOM order (by\n1755         increasing tag number within their dataset).\n1756 \n1757         Parameters\n1758         ----------\n1759         callback\n1760             A callable that takes two arguments:\n1761                 * a Dataset\n1762                 * a DataElement belonging to that Dataset\n1763         recursive : bool\n1764             Flag to indicate whether to recurse into Sequences.\n1765         \"\"\"\n1766         taglist = sorted(self._dict.keys())\n1767         for tag in taglist:\n1768 \n1769             with tag_in_exception(tag):\n1770                 data_element = self[tag]\n1771                 callback(self, data_element)  # self = this Dataset\n1772                 # 'tag in self' below needed in case callback deleted\n1773                 # data_element\n1774                 if recursive and tag in self and data_element.VR == \"SQ\":\n1775                     sequence = data_element.value\n1776                     for dataset in sequence:\n1777                         dataset.walk(callback)\n1778 \n1779     @classmethod\n1780     def _convert_to_python_number(cls, value, vr):\n1781         \"\"\"Makes sure that values are either ints or floats\n1782         based on their value representation.\n1783 \n1784         Parameters\n1785         ----------\n1786         value: Union[Union[str, int, float], List[Union[str, int, float]]]\n1787             value of data element\n1788         vr: str\n1789             value representation of data element\n1790 \n1791         Returns\n1792         -------\n1793         Union[Union[str, int, float], List[Union[str, int, float]]]\n1794 \n1795         \"\"\"\n1796         if value is None:\n1797             return None\n1798         number_type = None\n1799         if vr in cls._VRs_TO_BE_INTS:\n1800             number_type = int\n1801         if vr in cls._VRs_TO_BE_FLOATS:\n1802             number_type = float\n1803         if number_type is not None:\n1804             if isinstance(value, (list, tuple, )):\n1805                 value = [number_type(e) for e in value]\n1806             else:\n1807                 value = number_type(value)\n1808         return value\n1809 \n1810     @classmethod\n1811     def from_json(cls, json_dataset, bulk_data_uri_handler=None,\n1812                   encodings=None):\n1813         \"\"\"Loads DICOM Data Set in DICOM JSON format.\n1814         See:\n1815         http://dicom.nema.org/medical/dicom/current/output/chtml/part18/chapter_F.html\n1816 \n1817         Parameters\n1818         ----------\n1819         json_dataset: Union[dict, str]\n1820             dictionary or string representing a DICOM Data Set formatted based\n1821             on the DICOM JSON Model (Annex F)\n1822         bulk_data_uri_handler: Union[Callable, None]\n1823             callable that accepts the \"BulkDataURI\" of the JSON representation\n1824             of a data element and returns the actual value of data element\n1825             (retrieved via DICOMweb WADO-RS)\n1826         encodings: Union[list, None]\n1827             encodings from SpecificCharacterSet, or None for default\n1828         Returns\n1829         -------\n1830         pydicom.dataset.Dataset\n1831 \n1832         \"\"\"\n1833         if not isinstance(json_dataset, dict):\n1834             json_dataset = json.loads(json_dataset)\n1835         dataset = cls()\n1836         for tag, mapping in json_dataset.items():\n1837             vr = mapping['vr']\n1838             unique_value_keys = tuple(\n1839                 set(mapping.keys()) & set(cls._JSON_VALUE_KEYS)\n1840             )\n1841             if len(unique_value_keys) == 0:\n1842                 value_key = None\n1843                 value = ['']\n1844             else:\n1845                 value_key = unique_value_keys[0]\n1846                 value = mapping[value_key]\n1847             data_element = DataElement.from_json(\n1848                 cls, tag, vr, value, value_key\n1849             )\n1850             dataset.add(data_element)\n1851         return dataset\n1852 \n1853     def _data_element_to_json(self, data_element,\n1854                               bulk_data_element_handler,\n1855                               bulk_data_threshold, dump_handler):\n1856         \"\"\"Converts a DataElement to JSON representation.\n1857 \n1858         Parameters\n1859         ----------\n1860         data_element: pydicom.dataelem.DataElement\n1861             data element\n1862         bulk_data_element_handler: Union[Callable, None]\n1863             callable that accepts a bulk data element and returns the\n1864             \"BulkDataURI\" for retrieving the value of the data element\n1865             via DICOMweb WADO-RS\n1866         bulk_data_threshold: int\n1867             size of base64 encoded data element above which a value will be\n1868             provided in form of a \"BulkDataURI\" rather than \"InlineBinary\"\n1869 \n1870         Returns\n1871         -------\n1872         dict\n1873             mapping representing a JSON encoded data element\n1874 \n1875         Raises\n1876         ------\n1877         TypeError\n1878             when size of encoded data element exceeds `bulk_data_threshold`\n1879             but `bulk_data_element_handler` is ``None`` and hence not callable\n1880 \n1881         \"\"\"\n1882         # TODO: Determine whether more VRs need to be converted to strings\n1883         _VRs_TO_QUOTE = ['AT', ]\n1884         json_element = {'vr': data_element.VR, }\n1885         if data_element.VR in Dataset._BINARY_VR_VALUES:\n1886             if data_element.value is not None:\n1887                 binary_value = data_element.value\n1888                 encoded_value = base64.b64encode(binary_value).decode('utf-8')\n1889                 if len(encoded_value) > bulk_data_threshold:\n1890                     if bulk_data_element_handler is None:\n1891                         raise TypeError(\n1892                             'No bulk data element handler provided to generate '\n1893                             'URL for value of data element \"{}\".'.format(\n1894                                 data_element.name\n1895                             )\n1896                         )\n1897                     json_element['BulkDataURI'] = bulk_data_element_handler(\n1898                         data_element\n1899                     )\n1900                 else:\n1901                     logger.info(\n1902                         'encode bulk data element \"{}\" inline'.format(\n1903                             data_element.name\n1904                         )\n1905                     )\n1906                     json_element['InlineBinary'] = encoded_value\n1907         elif data_element.VR == 'SQ':\n1908             # recursive call to co-routine to format sequence contents\n1909             value = [\n1910                 json.loads(e.to_json(\n1911                     bulk_data_element_handler=bulk_data_element_handler,\n1912                     bulk_data_threshold=bulk_data_threshold,\n1913                     dump_handler=dump_handler\n1914                 ))\n1915                 for e in data_element\n1916             ]\n1917             json_element['Value'] = value\n1918         elif data_element.VR == 'PN':\n1919             elem_value = data_element.value\n1920             if elem_value is not None:\n1921                 if compat.in_py2:\n1922                     elem_value = PersonNameUnicode(elem_value, 'UTF8')\n1923                 if len(elem_value.components) > 2:\n1924                     json_element['Value'] = [\n1925                         {'Phonetic': elem_value.components[2], },\n1926                     ]\n1927                 elif len(elem_value.components) > 1:\n1928                     json_element['Value'] = [\n1929                         {'Ideographic': elem_value.components[1], },\n1930                     ]\n1931                 else:\n1932                     json_element['Value'] = [\n1933                         {'Alphabetic': elem_value.components[0], },\n1934                     ]\n1935         else:\n1936             if data_element.value is not None:\n1937                 is_multivalue = isinstance(data_element.value, MultiValue)\n1938                 if data_element.VM > 1 or is_multivalue:\n1939                     value = data_element.value\n1940                 else:\n1941                     value = [data_element.value]\n1942                 # ensure it's a list and not another iterable\n1943                 # (e.g. tuple), which would not be JSON serializable\n1944                 if data_element.VR in _VRs_TO_QUOTE:\n1945                     json_element['Value'] = [str(v) for v in value]\n1946                 else:\n1947                     json_element['Value'] = [v for v in value]\n1948         if hasattr(json_element, 'Value'):\n1949             json_element['Value'] = self._convert_to_python_number(\n1950                 json_element['Value'], data_element.VR\n1951             )\n1952         return json_element\n1953 \n1954     def to_json(self, bulk_data_threshold=1, bulk_data_element_handler=None,\n1955                 dump_handler=None):\n1956         \"\"\"Converts the data set into JSON representation based on the\n1957         DICOM JSON Model\n1958         http://dicom.nema.org/medical/dicom/current/output/chtml/part18/chapter_F.html.\n1959 \n1960         Parameters\n1961         ----------\n1962         bulk_data_threshold: int, optional\n1963             threshold for the length of a base64-encoded binary data element\n1964             above which the element should be considered bulk data and the value\n1965             provided as a URI rather than included inline (default: ``1``)\n1966         bulk_data_element_handler: Union[Callable, None], optional\n1967             callable that accepts a bulk data element and returns a JSON\n1968             representation of the data element (dictionary including the \"vr\"\n1969             key and either the \"InlineBinary\" or the \"BulkDataURI\" key)\n1970         dump_handler: Union[Callable, None], optional\n1971             callable that accepts a dict and returns the serialized (dumped)\n1972             JSON string (by default uses ``json.dumps()``)\n1973 \n1974         Returns\n1975         -------\n1976         str\n1977             data set serialized into a string based on the DICOM JSON Model\n1978 \n1979         Examples\n1980         --------\n1981         >>> def my_json_dumps(data):\n1982         ...     return json.dumps(data, indent=4)\n1983         >>> dataset.to_json(dump_handler=my_json_dumps)\n1984 \n1985         \"\"\"\n1986         if dump_handler is None:\n1987             logger.debug('using default json.dumps function')\n1988             dump_handler = json.dumps\n1989         json_dataset = {}\n1990         for key in self.keys():\n1991             json_key = '{0:04x}{1:04x}'.format(key.group, key.element).upper()\n1992             # FIXME: with pydicom 1.x the referenced image sequence\n1993             # causes a recursion error\n1994             if json_key == '00081140':\n1995                 logger.warning(\n1996                     'currently can\\'t serialize data element \"{}\"'.format(key)\n1997                 )\n1998                 continue\n1999             data_element = self[key]\n2000             json_dataset[json_key] = self._data_element_to_json(\n2001                 data_element,\n2002                 bulk_data_element_handler=bulk_data_element_handler,\n2003                 bulk_data_threshold=bulk_data_threshold,\n2004                 dump_handler=dump_handler\n2005             )\n2006         return dump_handler(json_dataset)\n2007 \n2008     __repr__ = __str__\n2009 \n2010 \n2011 class FileDataset(Dataset):\n2012     \"\"\"An extension of Dataset to make reading and writing to file-like easier.\n2013 \n2014     Attributes\n2015     ----------\n2016     preamble : str or bytes or None\n2017         The optional DICOM preamble prepended to the dataset, if available.\n2018     file_meta : pydicom.dataset.Dataset or None\n2019         The Dataset's file meta information as a Dataset, if available (None if\n2020         not present). Consists of group 0002 elements.\n2021     filename : str or None\n2022         The filename that the dataset was read from (if read from file) or None\n2023         if the filename is not available (if read from a BytesIO or similar).\n2024     fileobj_type\n2025         The object type of the file-like the Dataset was read from.\n2026     is_implicit_VR : bool\n2027         True if the dataset encoding is implicit VR, False otherwise.\n2028     is_little_endian : bool\n2029         True if the dataset encoding is little endian byte ordering, False\n2030         otherwise.\n2031     timestamp : float or None\n2032         The modification time of the file the dataset was read from, None if\n2033         the modification time is not available.\n2034     \"\"\"\n2035 \n2036     def __init__(self,\n2037                  filename_or_obj,\n2038                  dataset,\n2039                  preamble=None,\n2040                  file_meta=None,\n2041                  is_implicit_VR=True,\n2042                  is_little_endian=True):\n2043         \"\"\"Initialize a Dataset read from a DICOM file.\n2044 \n2045         Parameters\n2046         ----------\n2047         filename_or_obj : str or BytesIO or None\n2048             Full path and filename to the file, memory buffer object, or None\n2049             if is a BytesIO.\n2050         dataset : Dataset or dict\n2051             Some form of dictionary, usually a Dataset from read_dataset().\n2052         preamble : bytes or str, optional\n2053             The 128-byte DICOM preamble.\n2054         file_meta : Dataset, optional\n2055             The file meta info dataset, as returned by _read_file_meta,\n2056             or an empty dataset if no file meta information is in the file.\n2057         is_implicit_VR : bool, optional\n2058             True (default) if implicit VR transfer syntax used; False if\n2059             explicit VR.\n2060         is_little_endian : boolean\n2061             True (default) if little-endian transfer syntax used; False if\n2062             big-endian.\n2063         \"\"\"\n2064         Dataset.__init__(self, dataset)\n2065         self.preamble = preamble\n2066         self.file_meta = file_meta\n2067         self.is_implicit_VR = is_implicit_VR\n2068         self.is_little_endian = is_little_endian\n2069         if isinstance(filename_or_obj, compat.string_types):\n2070             self.filename = filename_or_obj\n2071             self.fileobj_type = open\n2072         elif isinstance(filename_or_obj, io.BufferedReader):\n2073             self.filename = filename_or_obj.name\n2074             # This is the appropriate constructor for io.BufferedReader\n2075             self.fileobj_type = open\n2076         else:\n2077             # use __class__ python <2.7?;\n2078             # http://docs.python.org/reference/datamodel.html\n2079             self.fileobj_type = filename_or_obj.__class__\n2080             if getattr(filename_or_obj, \"name\", False):\n2081                 self.filename = filename_or_obj.name\n2082             elif getattr(filename_or_obj, \"filename\",\n2083                          False):  # gzip python <2.7?\n2084                 self.filename = filename_or_obj.filename\n2085             else:\n2086                 # e.g. came from BytesIO or something file-like\n2087                 self.filename = None\n2088         self.timestamp = None\n2089         if self.filename and os.path.exists(self.filename):\n2090             statinfo = os.stat(self.filename)\n2091             self.timestamp = statinfo.st_mtime\n2092 \n2093     def __eq__(self, other):\n2094         \"\"\"Compare `self` and `other` for equality.\n2095 \n2096         Returns\n2097         -------\n2098         bool\n2099             The result if `self` and `other` are the same class\n2100         NotImplemented\n2101             If `other` is not the same class as `self` then returning\n2102             NotImplemented delegates the result to superclass.__eq__(subclass)\n2103         \"\"\"\n2104         # When comparing against self this will be faster\n2105         if other is self:\n2106             return True\n2107 \n2108         if isinstance(other, self.__class__):\n2109             return (_dict_equal(self, other) and\n2110                     _dict_equal(self.__dict__, other.__dict__,\n2111                                 exclude=['_dict'])\n2112                     )\n2113 \n2114         return NotImplemented\n2115 \n2116 \n2117 def validate_file_meta(file_meta, enforce_standard=True):\n2118     \"\"\"Validates the File Meta Information elements in `file_meta` and\n2119     adds some tags if missing and `enforce_standard` is True.\n2120 \n2121     Parameters\n2122     ----------\n2123     file_meta : pydicom.dataset.Dataset\n2124         The File Meta Information data elements.\n2125     enforce_standard : bool\n2126         If False, then only a check for invalid elements is performed.\n2127         If True, the following elements will be added if not already present:\n2128             * (0002,0001) FileMetaInformationVersion\n2129             * (0002,0012) ImplementationClassUID\n2130             * (0002,0013) ImplementationVersionName\n2131         and the following elements will be checked:\n2132             * (0002,0002) MediaStorageSOPClassUID\n2133             * (0002,0003) MediaStorageSOPInstanceUID\n2134             * (0002,0010) TransferSyntaxUID\n2135 \n2136     Raises\n2137     ------\n2138     ValueError\n2139         If `enforce_standard` is True and any of the checked File Meta\n2140         Information elements are missing from `file_meta`.\n2141     ValueError\n2142         If any non-Group 2 Elements are present in `file_meta`.\n2143     \"\"\"\n2144     # Check that no non-Group 2 Elements are present\n2145     for elem in file_meta.elements():\n2146         if elem.tag.group != 0x0002:\n2147             raise ValueError(\"Only File Meta Information Group (0002,eeee) \"\n2148                              \"elements must be present in 'file_meta'.\")\n2149 \n2150     if enforce_standard:\n2151         if 'FileMetaInformationVersion' not in file_meta:\n2152             file_meta.FileMetaInformationVersion = b'\\x00\\x01'\n2153 \n2154         if 'ImplementationClassUID' not in file_meta:\n2155             file_meta.ImplementationClassUID = PYDICOM_IMPLEMENTATION_UID\n2156 \n2157         if 'ImplementationVersionName' not in file_meta:\n2158             file_meta.ImplementationVersionName = (\n2159                 'PYDICOM ' + \".\".join(str(x) for x in __version_info__))\n2160 \n2161         # Check that required File Meta Information elements are present\n2162         missing = []\n2163         for element in [0x0002, 0x0003, 0x0010]:\n2164             if Tag(0x0002, element) not in file_meta:\n2165                 missing.append(Tag(0x0002, element))\n2166         if missing:\n2167             msg = (\"Missing required File Meta Information elements from \"\n2168                    \"'file_meta':\\n\")\n2169             for tag in missing:\n2170                 msg += '\\t{0} {1}\\n'.format(tag, keyword_for_tag(tag))\n2171             raise ValueError(msg[:-1])  # Remove final newline\n2172 \n[end of pydicom/dataset.py]\n[start of pydicom/util/codify.py]\n1 # Copyright 2008-2018 pydicom authors. See LICENSE file for details.\n2 \"\"\"\n3 Produce runnable python code which can recreate DICOM objects or files.\n4 \n5 Can run as a script to produce code for an entire file,\n6 or import and use specific functions to provide code for pydicom DICOM classes\n7 \n8 \"\"\"\n9 \n10 # Run this from the same directory as a \"base\" dicom file and\n11 # this code will output to screen the dicom parameters like:\n12 #    ds.PatientName = 'TEST'\n13 # etc for all parameters in the file.\n14 # This can then be pasted into a python file and parameters edited as necessary\n15 # to create a DICOM file from scratch\n16 \n17 import sys\n18 import os.path\n19 import pydicom\n20 from pydicom.datadict import dictionary_keyword\n21 from pydicom.compat import int_type\n22 \n23 import re\n24 \n25 line_term = \"\\n\"\n26 \n27 # Helper functions first\n28 \n29 # Precompiled search patterns for camel_to_underscore()\n30 first_cap_re = re.compile('(.)([A-Z][a-z]+)')\n31 all_cap_re = re.compile('([a-z0-9])([A-Z])')\n32 \n33 byte_VRs = [\n34     'OB', 'OW', 'OW/OB', 'OW or OB', 'OB or OW', 'US or SS or OW', 'US or SS',\n35     'OD', 'OL'\n36 ]\n37 \n38 \n39 def camel_to_underscore(name):\n40     \"\"\"Convert name from CamelCase to lower_case_with_underscores\"\"\"\n41     # From http://stackoverflow.com/questions/1175208\n42     s1 = first_cap_re.sub(r'\\1_\\2', name)\n43     return all_cap_re.sub(r'\\1_\\2', s1).lower()\n44 \n45 \n46 def tag_repr(tag):\n47     \"\"\"String of tag value as (0xgggg, 0xeeee)\"\"\"\n48     return \"(0x{group:04x}, 0x{elem:04x})\".format(\n49         group=tag.group, elem=tag.element)\n50 \n51 \n52 def default_name_filter(name):\n53     \"\"\"Callable to reduce some names in code to more readable short form\n54 \n55     :arg name: a sequence variable name or sequence item name\n56     :return: a shorter version of name if a known conversion,\n57              else return original name\n58 \n59     \"\"\"\n60     name = camel_to_underscore(name)\n61     name = name.replace(\"control_point\", \"cp\")\n62     name = name.replace(\"reference\", \"ref\")\n63     name = name.replace(\"fraction_group\", \"frxn_gp\")\n64     return name\n65 \n66 \n67 # Functions to produce python code\n68 def code_imports():\n69     \"\"\"Code the import statements needed by other codify results\n70 \n71     :return: a string of import statement lines\n72 \n73     \"\"\"\n74     line0 = \"from __future__ import unicode_literals\"\n75     line0 += \"  # Only for python2.7 and save_as unicode filename\"\n76     line1 = \"import pydicom\"\n77     line2 = \"from pydicom.dataset import Dataset\"\n78     line3 = \"from pydicom.sequence import Sequence\"\n79     return line_term.join((line0, line1, line2, line3))\n80 \n81 \n82 def code_dataelem(dataelem,\n83                   dataset_name=\"ds\",\n84                   exclude_size=None,\n85                   include_private=False):\n86     \"\"\"Code lines for a single DICOM data element\n87 \n88     :arg dataelem: the DataElement instance to turn into code\n89     :arg dataset_name: variable name of the Dataset containing dataelem\n90     :arg exclude_size: if specified, values longer than this (in bytes)\n91                        will only have a commented string for a value,\n92                        causing a syntax error when the code is run,\n93                        and thus prompting the user to remove or fix that line.\n94     :return: a string containing code to recreate the data element\n95              If the data element is a sequence, calls code_sequence\n96 \n97     \"\"\"\n98 \n99     if dataelem.VR == \"SQ\":\n100         return code_sequence(dataelem, dataset_name, exclude_size,\n101                              include_private)\n102 \n103     # If in DICOM dictionary, set using the keyword\n104     # If not (e.g. is private element), set using add_new method\n105     have_keyword = True\n106     try:\n107         keyword = dictionary_keyword(dataelem.tag)\n108     except KeyError:\n109         have_keyword = False\n110 \n111     valuerep = repr(dataelem.value)\n112 \n113     if exclude_size:\n114         if (dataelem.VR in byte_VRs and\n115                 len(dataelem.value) > exclude_size):\n116             valuerep = (\n117                 \"# XXX Array of %d bytes excluded\" % len(dataelem.value))\n118 \n119     if have_keyword:\n120         format_str = \"{ds_name}.{keyword} = {valuerep}\"\n121         line = format_str.format(\n122             ds_name=dataset_name, keyword=keyword, valuerep=valuerep)\n123     else:\n124         format_str = \"{ds_name}.add_new({tag}, '{VR}', {valuerep})\"\n125         line = format_str.format(\n126             ds_name=dataset_name,\n127             tag=tag_repr(dataelem.tag),\n128             VR=dataelem.VR,\n129             valuerep=valuerep)\n130     return line\n131 \n132 \n133 def code_sequence(dataelem,\n134                   dataset_name=\"ds\",\n135                   exclude_size=None,\n136                   include_private=False,\n137                   name_filter=default_name_filter):\n138     \"\"\"Code lines for recreating a Sequence data element\n139 \n140     :arg dataelem: the DataElement instance of the Sequence\n141     :arg dataset_name: variable name of the dataset containing the Sequence\n142     :arg exclude_size: if specified, values longer than this (in bytes)\n143                        will only have a commented string for a value,\n144                        causing a syntax error when the code is run,\n145                        and thus prompting the user to remove or fix that line.\n146     :arg include_private: If True, private data elements will be coded.\n147                           If False, private elements are skipped\n148     :arg name_filter: a callable taking a sequence name or sequence item name,\n149                       and returning a shorter name for easier code reading\n150     :return: a string containing code lines to recreate a DICOM sequence\n151 \n152     \"\"\"\n153     lines = []\n154     seq = dataelem.value\n155     seq_name = dataelem.name\n156     seq_item_name = seq_name.replace(' Sequence', '')\n157     seq_keyword = dictionary_keyword(dataelem.tag)\n158 \n159     # Create comment line to document the start of Sequence\n160     lines.append('')\n161     lines.append(\"# \" + seq_name)\n162 \n163     # Code line to create a new Sequence object\n164     if name_filter:\n165         seq_var = name_filter(seq_keyword)\n166     lines.append(seq_var + \" = Sequence()\")\n167 \n168     # Code line to add the sequence to its parent\n169     lines.append(dataset_name + \".\" + seq_keyword + \" = \" + seq_var)\n170 \n171     # Code lines to add sequence items to the Sequence\n172     for i, ds in enumerate(seq):\n173         # Determine index to use. If seq item has a data element with 'Index',\n174         #    use that; if one with 'Number', use that, else start at 1\n175         index_keyword = seq_keyword.replace(\"Sequence\", \"\") + \"Index\"\n176         number_keyword = seq_keyword.replace(\"Sequence\", \"\") + \"Number\"\n177         if index_keyword in ds:\n178             index_str = str(getattr(ds, index_keyword))\n179         elif number_keyword in ds:\n180             index_str = str(getattr(ds, number_keyword))\n181         else:\n182             index_str = str(i + 1)\n183 \n184         # Code comment line to mark start of sequence item\n185         lines.append('')\n186         lines.append(\"# \" + seq_name + \": \" + seq_item_name + \" \" + index_str)\n187 \n188         # Determine the variable name to use for the sequence item (dataset)\n189         ds_name = seq_var.replace(\"_sequence\", \"\") + index_str\n190 \n191         # Code the sequence item\n192         code_item = code_dataset(ds, ds_name, exclude_size, include_private)\n193         lines.append(code_item)\n194 \n195         # Code the line to append the item to its parent sequence\n196         lines.append(seq_var + \".append(\" + ds_name + \")\")\n197 \n198     # Join the lines and return a single string\n199     return line_term.join(lines)\n200 \n201 \n202 def code_dataset(ds,\n203                  dataset_name=\"ds\",\n204                  exclude_size=None,\n205                  include_private=False):\n206     \"\"\"Return python code lines for import statements needed by other code\n207 \n208     :arg exclude_size: if specified, values longer than this (in bytes)\n209                        will only have a commented string for a value,\n210                        causing a syntax error when the code is run,\n211                        and thus prompting the user to remove or fix that line.\n212     :arg include_private: If True, private data elements will be coded.\n213                           If False, private elements are skipped\n214     :return: a list of code lines containing import statements\n215 \n216     \"\"\"\n217     lines = []\n218     lines.append(dataset_name + \" = Dataset()\")\n219     for dataelem in ds:\n220         # If a private data element and flag says so, skip it and go to next\n221         if not include_private and dataelem.tag.is_private:\n222             continue\n223         # Otherwise code the line and add it to the lines list\n224         code_line = code_dataelem(dataelem, dataset_name, exclude_size,\n225                                   include_private)\n226         lines.append(code_line)\n227         # Add blank line if just coded a sequence\n228         if dataelem.VR == \"SQ\":\n229             lines.append('')\n230     # If sequence was end of this dataset, remove the extra blank line\n231     if len(lines) and lines[-1] == '':\n232         lines.pop()\n233     # Join all the code lines and return them\n234     return line_term.join(lines)\n235 \n236 \n237 def code_file(filename, exclude_size=None, include_private=False):\n238     \"\"\"Write a complete source code file to recreate a DICOM file\n239 \n240     :arg filename: complete path and filename of a DICOM file to convert\n241     :arg exclude_size: if specified, values longer than this (in bytes)\n242                        will only have a commented string for a value,\n243                        causing a syntax error when the code is run,\n244                        and thus prompting the user to remove or fix that line.\n245     :arg include_private: If True, private data elements will be coded.\n246                           If False, private elements are skipped\n247     :return: a string containing code lines to recreate entire file\n248 \n249     \"\"\"\n250     lines = []\n251 \n252     ds = pydicom.dcmread(filename, force=True)\n253 \n254     # Code a nice header for the python file\n255     lines.append(\"# Coded version of DICOM file '{0}'\".format(filename))\n256     lines.append(\"# Produced by pydicom codify utility script\")\n257 \n258     # Code the necessary imports\n259     lines.append(code_imports())\n260     lines.append('')\n261 \n262     # Code the file_meta information\n263     lines.append(\"# File meta info data elements\")\n264     code_meta = code_dataset(ds.file_meta, \"file_meta\", exclude_size,\n265                              include_private)\n266     lines.append(code_meta)\n267     lines.append('')\n268 \n269     # Code the main dataset\n270     lines.append(\"# Main data elements\")\n271     code_ds = code_dataset(\n272         ds, exclude_size=exclude_size, include_private=include_private)\n273     lines.append(code_ds)\n274     lines.append('')\n275 \n276     # Add the file meta to the dataset, and set transfer syntax\n277     lines.append(\"ds.file_meta = file_meta\")\n278     lines.append(\"ds.is_implicit_VR = \" + str(ds.is_implicit_VR))\n279     lines.append(\"ds.is_little_endian = \" + str(ds.is_little_endian))\n280 \n281     # Return the complete code string\n282     return line_term.join(lines)\n283 \n284 \n285 def main(default_exclude_size, args=None):\n286     \"\"\"Create python code according to user options\n287 \n288     Parameters:\n289     -----------\n290     default_exclude_size:  int\n291         Values longer than this will be coded as a commented syntax error\n292 \n293     args: list\n294         Command-line arguments to parse.  If None, then sys.argv is used\n295     \"\"\"\n296 \n297     try:\n298         import argparse\n299     except ImportError:\n300         print(\"The argparse module is required to run this script\")\n301         print(\"argparse is standard in python >= 2.7,\")\n302         print(\"   or can be installed with 'pip install argparse'\")\n303         sys.exit(-1)\n304 \n305     parser = argparse.ArgumentParser(\n306         description=\"Produce python/pydicom code from a DICOM file\",\n307         epilog=\"Binary data (e.g. pixels) larger than --exclude-size \"\n308         \"(default %d bytes) is not included. A dummy line \"\n309         \"with a syntax error is produced. \"\n310         \"Private data elements are not included \"\n311         \"by default.\" % default_exclude_size)\n312     parser.add_argument(\n313         'infile', help=\"DICOM file from which to produce code lines\")\n314     parser.add_argument(\n315         'outfile',\n316         nargs='?',\n317         type=argparse.FileType('w'),\n318         help=(\"Filename to write python code to. \"\n319               \"If not specified, code is written to stdout\"),\n320         default=sys.stdout)\n321     help_exclude_size = 'Exclude binary data larger than specified (bytes). '\n322     help_exclude_size += 'Default is %d bytes' % default_exclude_size\n323     parser.add_argument(\n324         '-e',\n325         '--exclude-size',\n326         type=int_type,\n327         default=default_exclude_size,\n328         help=help_exclude_size)\n329     parser.add_argument(\n330         '-p',\n331         '--include-private',\n332         action=\"store_true\",\n333         help='Include private data elements '\n334         '(default is to exclude them)')\n335     parser.add_argument(\n336         '-s',\n337         '--save-as',\n338         help=(\"Specify the filename for ds.save_as(save_filename); \"\n339               \"otherwise the input name + '_from_codify' will be used\"))\n340 \n341     args = parser.parse_args(args)\n342 \n343     # Read the requested file and convert to python/pydicom code lines\n344     filename = args.infile  # name\n345     code_lines = code_file(filename, args.exclude_size, args.include_private)\n346 \n347     # If requested, write a code line to save the dataset\n348     if args.save_as:\n349         save_as_filename = args.save_as\n350     else:\n351         base, ext = os.path.splitext(filename)\n352         save_as_filename = base + \"_from_codify\" + \".dcm\"\n353     line = \"\\nds.save_as(r'{filename}', write_like_original=False)\"\n354     save_line = line.format(filename=save_as_filename)\n355     code_lines += save_line\n356 \n357     # Write the code lines to specified file or to standard output\n358     # For test_util, captured output .name throws error, ignore it:\n359     try:\n360         if args.outfile.name != \"<stdout>\":\n361             print(\"Writing code to file '%s'\" % args.outfile.name)\n362     except AttributeError:\n363         pass\n364     args.outfile.write(code_lines)\n365 \n366 \n367 if __name__ == \"__main__\":\n368     main(default_exclude_size=100)\n369 \n[end of pydicom/util/codify.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -1,5 +1,6 @@\n import logging\n import numpy as np\n from pydicom.dataelem import DataElement\n+from pydicom.errors import InvalidDicomError\n \n logger = logging.getLogger(\"pydicom\")\n \n@@ -1234,6 +1235,15 @@\n         # Check if the value length exceeds the maximum for explicit VR\n         if isinstance(value, (list, tuple)) and len(value) > 65534:\n             logger.warning(\"Value length exceeds maximum for explicit VR. Truncating.\")\n             value = value[:32767]  # Truncate to maximum allowed length\n+            raise ValueError(\"Value length exceeds maximum for explicit VR. Truncated.\")\n \n         # Proceed with normal assignment\n         self._dict[tag] = DataElement(tag, VR, value)\n \n+    def validate_ds_length(self):\n+        \"\"\"Validate the length of the dataset for explicit VR.\"\"\"\n+        for elem in self:\n+            if elem.VR == 'DS' and isinstance(elem.value, (list, tuple)):\n+                total_length = sum(len(str(v)) for v in elem.value)\n+                if total_length > 65534:\n+                    raise InvalidDicomError(f\"Data Element {elem.tag} exceeds maximum length for explicit VR.\")\n \n     def __setitem__(self, key, value):\n         \"\"\"Operator for Dataset[key] = value.\n``` \n\nThis patch modifies the `Dataset` class to include validation for the length of data elements with the VR of 'DS'. If the total length exceeds the maximum allowed for explicit VR, it raises a `ValueError` and logs a warning.", "model_patch": "--- a/pydicom/dataset.py\n+++ b/pydicom/dataset.py\n@@ -1,5 +1,6 @@\n import logging\n import numpy as np\n from pydicom.dataelem import DataElement\n+from pydicom.errors import InvalidDicomError\n \n logger = logging.getLogger(\"pydicom\")\n \n@@ -1234,6 +1235,15 @@\n         # Check if the value length exceeds the maximum for explicit VR\n         if isinstance(value, (list, tuple)) and len(value) > 65534:\n             logger.warning(\"Value length exceeds maximum for explicit VR. Truncating.\")\n             value = value[:32767]  # Truncate to maximum allowed length\n+            raise ValueError(\"Value length exceeds maximum for explicit VR. Truncated.\")\n \n         # Proceed with normal assignment\n         self._dict[tag] = DataElement(tag, VR, value)\n \n+    def validate_ds_length(self):\n+        \"\"\"Validate the length of the dataset for explicit VR.\"\"\"\n+        for elem in self:\n+            if elem.VR == 'DS' and isinstance(elem.value, (list, tuple)):\n+                total_length = sum(len(str(v)) for v in elem.value)\n+                if total_length > 65534:\n+                    raise InvalidDicomError(f\"Data Element {elem.tag} exceeds maximum length for explicit VR.\")\n \n     def __setitem__(self, key, value):\n         \"\"\"Operator for Dataset[key] = value.\n"}
