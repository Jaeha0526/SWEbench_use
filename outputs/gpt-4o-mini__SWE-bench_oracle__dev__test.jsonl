{"instance_id": "sqlfluff__sqlfluff-4764", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nEnable quiet mode/no-verbose in CLI for use in pre-commit hook\nThere seems to be only an option to increase the level of verbosity when using SQLFluff [CLI](https://docs.sqlfluff.com/en/stable/cli.html), not to limit it further.\r\n\r\nIt would be great to have an option to further limit the amount of prints when running `sqlfluff fix`, especially in combination with deployment using a pre-commit hook. For example, only print the return status and the number of fixes applied, similar to how it is when using `black` in a pre-commit hook:\r\n![image](https://user-images.githubusercontent.com/10177212/140480676-dc98d00b-4383-44f2-bb90-3301a6eedec2.png)\r\n\r\nThis hides the potentially long list of fixes that are being applied to the SQL files, which can get quite verbose.\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![Coveralls](https://img.shields.io/coverallsCoverage/github/sqlfluff/sqlfluff?logo=coveralls&style=flat-square)](https://coveralls.io/github/sqlfluff/sqlfluff?branch=main)\n13 [![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/sqlfluff/sqlfluff/.github/workflows/ci-tests.yml?logo=github&style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)\n14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n16 [![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)\n17 \n18 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed\n19 with ELT applications in mind, **SQLFluff** also works with Jinja templating\n20 and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus\n21 your time on what matters.\n22 \n23 ## Dialects Supported\n24 \n25 Although SQL is reasonably consistent in its implementations, there are several\n26 different dialects available with variations of syntax and grammar. **SQLFluff**\n27 currently supports the following SQL dialects (though perhaps not in full):\n28 \n29 - ANSI SQL - this is the base version and on occasion may not strictly follow\n30   the ANSI/ISO SQL definition\n31 - [Athena](https://aws.amazon.com/athena/)\n32 - [BigQuery](https://cloud.google.com/bigquery/)\n33 - [ClickHouse](https://clickhouse.com/)\n34 - [Databricks](https://databricks.com/) (note: this extends the `sparksql` dialect with\n35   [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) syntax).\n36 - [Db2](https://www.ibm.com/analytics/db2)\n37 - [DuckDB](https://duckdb.org/)\n38 - [Exasol](https://www.exasol.com/)\n39 - [Greenplum](https://greenplum.org/)\n40 - [Hive](https://hive.apache.org/)\n41 - [Materialize](https://materialize.com/)\n42 - [MySQL](https://www.mysql.com/)\n43 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n44 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n45 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n46 - [Snowflake](https://www.snowflake.com/)\n47 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n48 - [SparkSQL](https://spark.apache.org/docs/latest/)\n49 - [SQLite](https://www.sqlite.org/)\n50 - [Teradata](https://www.teradata.com/)\n51 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n52 \n53 We aim to make it easy to expand on the support of these dialects and also\n54 add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues)\n55 (or upvote any existing issues) to let us know of demand for missing support.\n56 \n57 Pull requests from those that know the missing syntax or dialects are especially\n58 welcomed and are the question way for you to get support added. We are happy\n59 to work with any potential contributors on this to help them add this support.\n60 Please raise an issue first for any large feature change to ensure it is a good\n61 fit for this project before spending time on this work.\n62 \n63 ## Templates Supported\n64 \n65 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity),\n66 so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor)\n67 as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n68 \n69 **SQLFluff** supports the following templates:\n70 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n71 - [dbt](https://www.getdbt.com/)\n72 \n73 Again, please raise issues if you wish to support more templating languages/syntaxes.\n74 \n75 ## VS Code Extension\n76 \n77 We also have a VS Code extension:\n78 \n79 - [Github Repository](https://github.com/sqlfluff/vscode-sqlfluff)\n80 - [Extension in VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff)\n81 \n82 # Getting Started\n83 \n84 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n85 \n86 ```shell\n87 $ pip install sqlfluff\n88 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n89 $ sqlfluff lint test.sql --dialect ansi\n90 == [test.sql] FAIL\n91 L:   1 | P:   1 | LT01 | Expected only single space before 'SELECT' keyword.\n92                        | Found '  '. [layout.spacing]\n93 L:   1 | P:   1 | LT02 | First line should not be indented.\n94                        | [layout.indent]\n95 L:   1 | P:   1 | LT13 | Files must not begin with newlines or whitespace.\n96                        | [layout.start_of_file]\n97 L:   1 | P:  11 | LT01 | Expected only single space before binary operator '+'.\n98                        | Found '  '. [layout.spacing]\n99 L:   1 | P:  14 | LT01 | Expected only single space before naked identifier.\n100                        | Found '  '. [layout.spacing]\n101 L:   1 | P:  27 | LT01 | Unnecessary trailing whitespace at end of file.\n102                        | [layout.spacing]\n103 L:   1 | P:  27 | LT12 | Files must end with a single trailing newline.\n104                        | [layout.end_of_file]\n105 All Finished \ud83d\udcdc \ud83c\udf89!\n106 ```\n107 \n108 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff)\n109 or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n110 \n111 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and\n112 [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see\n113 [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n114 \n115 # Documentation\n116 \n117 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/).\n118 This documentation is generated from this repository so please raise\n119 [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests\n120 for any additions, corrections, or clarifications.\n121 \n122 # Releases\n123 \n124 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),\n125 so breaking changes should be restricted to major versions releases. Some\n126 elements (such as the python API) are in a less stable state and may see more\n127 significant changes more often. For details on breaking changes and how\n128 to migrate between versions, see our\n129 [release notes](https://docs.sqlfluff.com/en/latest/releasenotes.html). See the\n130 [changelog](CHANGELOG.md) for more details. If you would like to join in, please\n131 consider [contributing](CONTRIBUTING.md).\n132 \n133 New releases are made monthly. For more information, visit\n134 [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n135 \n136 # SQLFluff on Slack\n137 \n138 We have a fast-growing community\n139 [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g),\n140 come and join us!\n141 \n142 # SQLFluff on Twitter\n143 \n144 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements\n145 and other related posts.\n146 \n147 # Contributing\n148 \n149 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors).\n150 There is a lot to do in this project, and we are just getting started.\n151 \n152 If you want to understand more about the architecture of **SQLFluff**, you can\n153 find [more here](https://docs.sqlfluff.com/en/latest/internals.html#architecture).\n154 \n155 If you would like to contribute, check out the\n156 [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see\n157 the guide to [contributing](CONTRIBUTING.md).\n158 \n159 # Sponsors\n160 \n161 <img src=\"images/datacoves.png\" alt=\"Datacoves\" width=\"150\"/><br>\n162 The turnkey analytics stack, find out more at [Datacoves.com](https://datacoves.com/).\n163 \n[end of README.md]\n[start of src/sqlfluff/cli/commands.py]\n1 \"\"\"Contains the CLI.\"\"\"\n2 \n3 from itertools import chain\n4 import os\n5 import sys\n6 import json\n7 import logging\n8 import time\n9 from logging import LogRecord\n10 from typing import Callable, Tuple, Optional, cast\n11 \n12 import yaml\n13 \n14 import click\n15 \n16 # For the profiler\n17 import pstats\n18 from io import StringIO\n19 \n20 # To enable colour cross platform\n21 import colorama\n22 from tqdm import tqdm\n23 from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n24 \n25 from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n26 from sqlfluff.cli.click_deprecated_option import (\n27     DeprecatedOption,\n28     DeprecatedOptionsCommand,\n29 )\n30 from sqlfluff.cli.formatters import (\n31     format_linting_result_header,\n32     OutputStreamFormatter,\n33 )\n34 from sqlfluff.cli.helpers import get_package_version\n35 from sqlfluff.cli.outputstream import make_output_stream, OutputStream\n36 \n37 # Import from sqlfluff core.\n38 from sqlfluff.core import (\n39     Linter,\n40     FluffConfig,\n41     SQLLintError,\n42     SQLTemplaterError,\n43     SQLFluffUserError,\n44     dialect_selector,\n45     dialect_readout,\n46 )\n47 from sqlfluff.core.config import progress_bar_configuration\n48 \n49 from sqlfluff.core.enums import FormatType, Color\n50 from sqlfluff.core.plugin.host import get_plugin_manager\n51 \n52 \n53 class StreamHandlerTqdm(logging.StreamHandler):\n54     \"\"\"Modified StreamHandler which takes care of writing within `tqdm` context.\n55 \n56     It uses `tqdm` write which takes care of conflicting prints with progressbar.\n57     Without it, there were left artifacts in DEBUG mode (not sure about another ones,\n58     but probably would happen somewhere).\n59     \"\"\"\n60 \n61     def emit(self, record: LogRecord) -> None:\n62         \"\"\"Behaves like original one except uses `tqdm` to write.\"\"\"\n63         try:\n64             msg = self.format(record)\n65             tqdm.write(msg, file=self.stream)\n66             self.flush()\n67         except Exception:  # pragma: no cover\n68             self.handleError(record)\n69 \n70 \n71 def set_logging_level(\n72     verbosity: int,\n73     formatter: OutputStreamFormatter,\n74     logger: Optional[logging.Logger] = None,\n75     stderr_output: bool = False,\n76 ) -> None:\n77     \"\"\"Set up logging for the CLI.\n78 \n79     We either set up global logging based on the verbosity\n80     or, if `logger` is specified, we only limit to a single\n81     sqlfluff logger. Verbosity is applied in the same way.\n82 \n83     Implementation: If `logger` is not specified, the handler\n84     is attached to the `sqlfluff` logger. If it is specified\n85     then it attaches the the logger in question. In addition\n86     if `logger` is specified, then that logger will also\n87     not propagate.\n88     \"\"\"\n89     fluff_logger = logging.getLogger(\"sqlfluff\")\n90     # Don't propagate logging\n91     fluff_logger.propagate = False\n92 \n93     # Enable colorama\n94     colorama.init()\n95 \n96     # Set up the log handler which is able to print messages without overlapping\n97     # with progressbars.\n98     handler = StreamHandlerTqdm(stream=sys.stderr if stderr_output else sys.stdout)\n99     # NB: the unicode character at the beginning is to squash any badly\n100     # tamed ANSI colour statements, and return us to normality.\n101     handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n102 \n103     # Set up a handler to colour warnings red.\n104     # See: https://docs.python.org/3/library/logging.html#filter-objects\n105     def red_log_filter(record: logging.LogRecord) -> bool:\n106         if record.levelno >= logging.WARNING:\n107             record.msg = f\"{formatter.colorize(record.msg, Color.red)} \"\n108         return True\n109 \n110     handler.addFilter(red_log_filter)\n111 \n112     if logger:\n113         focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n114         focus_logger.addHandler(handler)\n115     else:\n116         fluff_logger.addHandler(handler)\n117 \n118     # NB: We treat the parser logger slightly differently because it's noisier.\n119     # It's important that we set levels for all each time so\n120     # that we don't break tests by changing the granularity\n121     # between tests.\n122     parser_logger = logging.getLogger(\"sqlfluff.parser\")\n123     if verbosity < 3:\n124         fluff_logger.setLevel(logging.WARNING)\n125         parser_logger.setLevel(logging.NOTSET)\n126     elif verbosity == 3:\n127         fluff_logger.setLevel(logging.INFO)\n128         parser_logger.setLevel(logging.WARNING)\n129     elif verbosity == 4:\n130         fluff_logger.setLevel(logging.DEBUG)\n131         parser_logger.setLevel(logging.INFO)\n132     elif verbosity > 4:\n133         fluff_logger.setLevel(logging.DEBUG)\n134         parser_logger.setLevel(logging.DEBUG)\n135 \n136 \n137 class PathAndUserErrorHandler:\n138     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n139 \n140     def __init__(self, formatter):\n141         self.formatter = formatter\n142 \n143     def __enter__(self):\n144         return self\n145 \n146     def __exit__(self, exc_type, exc_val, exc_tb):\n147         if exc_type is SQLFluffUserError:\n148             click.echo(\n149                 \"\\nUser Error: \"\n150                 + self.formatter.colorize(\n151                     str(exc_val),\n152                     Color.red,\n153                 )\n154             )\n155             sys.exit(EXIT_ERROR)\n156 \n157 \n158 def common_options(f: Callable) -> Callable:\n159     \"\"\"Add common options to commands via a decorator.\n160 \n161     These are applied to all of the cli commands.\n162     \"\"\"\n163     f = click.version_option()(f)\n164     f = click.option(\n165         \"-v\",\n166         \"--verbose\",\n167         count=True,\n168         default=None,\n169         help=(\n170             \"Verbosity, how detailed should the output be. This is *stackable*, so \"\n171             \"`-vv` is more verbose than `-v`. For the most verbose option try `-vvvv` \"\n172             \"or `-vvvvv`.\"\n173         ),\n174     )(f)\n175     f = click.option(\n176         \"-n\",\n177         \"--nocolor\",\n178         is_flag=True,\n179         default=None,\n180         help=\"No color - output will be without ANSI color codes.\",\n181     )(f)\n182 \n183     return f\n184 \n185 \n186 def core_options(f: Callable) -> Callable:\n187     \"\"\"Add core operation options to commands via a decorator.\n188 \n189     These are applied to the main (but not all) cli commands like\n190     `parse`, `lint` and `fix`.\n191     \"\"\"\n192     # Only enable dialect completion if on version of click\n193     # that supports it\n194     if shell_completion_enabled:\n195         f = click.option(\n196             \"-d\",\n197             \"--dialect\",\n198             default=None,\n199             help=\"The dialect of SQL to lint\",\n200             shell_complete=dialect_shell_complete,\n201         )(f)\n202     else:  # pragma: no cover\n203         f = click.option(\n204             \"-d\",\n205             \"--dialect\",\n206             default=None,\n207             help=\"The dialect of SQL to lint\",\n208         )(f)\n209     f = click.option(\n210         \"-t\",\n211         \"--templater\",\n212         default=None,\n213         help=\"The templater to use (default=jinja)\",\n214         type=click.Choice(\n215             [\n216                 templater.name\n217                 for templater in chain.from_iterable(\n218                     get_plugin_manager().hook.get_templaters()\n219                 )\n220             ]\n221         ),\n222     )(f)\n223     f = click.option(\n224         \"-r\",\n225         \"--rules\",\n226         default=None,\n227         help=(\n228             \"Narrow the search to only specific rules. For example \"\n229             \"specifying `--rules LT01` will only search for rule `LT01` (Unnecessary \"\n230             \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n231             \"`--rules LT01,LT02` will specify only looking for violations of rule \"\n232             \"`LT01` and rule `LT02`.\"\n233         ),\n234     )(f)\n235     f = click.option(\n236         \"-e\",\n237         \"--exclude-rules\",\n238         default=None,\n239         help=(\n240             \"Exclude specific rules. For example \"\n241             \"specifying `--exclude-rules LT01` will remove rule `LT01` (Unnecessary \"\n242             \"trailing whitespace) from the set of considered rules. This could either \"\n243             \"be the allowlist, or the general set if there is no specific allowlist. \"\n244             \"Multiple rules can be specified with commas e.g. \"\n245             \"`--exclude-rules LT01,LT02` will exclude violations of rule \"\n246             \"`LT01` and rule `LT02`.\"\n247         ),\n248     )(f)\n249     f = click.option(\n250         \"--config\",\n251         \"extra_config_path\",\n252         default=None,\n253         help=(\n254             \"Include additional config file. By default the config is generated \"\n255             \"from the standard configuration files described in the documentation. \"\n256             \"This argument allows you to specify an additional configuration file that \"\n257             \"overrides the standard configuration files. N.B. cfg format is required.\"\n258         ),\n259         type=click.Path(),\n260     )(f)\n261     f = click.option(\n262         \"--ignore-local-config\",\n263         is_flag=True,\n264         help=(\n265             \"Ignore config files in default search path locations. \"\n266             \"This option allows the user to lint with the default config \"\n267             \"or can be used in conjunction with --config to only \"\n268             \"reference the custom config file.\"\n269         ),\n270     )(f)\n271     f = click.option(\n272         \"--encoding\",\n273         default=None,\n274         help=(\n275             \"Specify encoding to use when reading and writing files. Defaults to \"\n276             \"autodetect.\"\n277         ),\n278     )(f)\n279     f = click.option(\n280         \"-i\",\n281         \"--ignore\",\n282         default=None,\n283         help=(\n284             \"Ignore particular families of errors so that they don't cause a failed \"\n285             \"run. For example `--ignore parsing` would mean that any parsing errors \"\n286             \"are ignored and don't influence the success or fail of a run. \"\n287             \"`--ignore` behaves somewhat like `noqa` comments, except it \"\n288             \"applies globally. Multiple options are possible if comma separated: \"\n289             \"e.g. `--ignore parsing,templating`.\"\n290         ),\n291     )(f)\n292     f = click.option(\n293         \"--bench\",\n294         is_flag=True,\n295         help=\"Set this flag to engage the benchmarking tool output.\",\n296     )(f)\n297     f = click.option(\n298         \"--logger\",\n299         type=click.Choice(\n300             [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\", \"config\"],\n301             case_sensitive=False,\n302         ),\n303         help=\"Choose to limit the logging to one of the loggers.\",\n304     )(f)\n305     f = click.option(\n306         \"--disable-noqa\",\n307         is_flag=True,\n308         default=None,\n309         help=\"Set this flag to ignore inline noqa comments.\",\n310     )(f)\n311     return f\n312 \n313 \n314 def lint_options(f: Callable) -> Callable:\n315     \"\"\"Add lint operation options to commands via a decorator.\n316 \n317     These are cli commands that do linting, i.e. `lint` and `fix`.\n318     \"\"\"\n319     f = click.option(\n320         \"-p\",\n321         \"--processes\",\n322         type=int,\n323         default=None,\n324         help=(\n325             \"The number of parallel processes to run. Positive numbers work as \"\n326             \"expected. Zero and negative numbers will work as number_of_cpus - \"\n327             \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n328         ),\n329     )(f)\n330     f = click.option(\n331         \"--disable_progress_bar\",\n332         \"--disable-progress-bar\",\n333         is_flag=True,\n334         help=\"Disables progress bars.\",\n335         cls=DeprecatedOption,\n336         deprecated=[\"--disable_progress_bar\"],\n337     )(f)\n338     f = click.option(\n339         \"--persist-timing\",\n340         default=None,\n341         help=(\n342             \"A filename to persist the timing information for a linting run to \"\n343             \"in csv format for external analysis. NOTE: This feature should be \"\n344             \"treated as beta, and the format of the csv file may change in \"\n345             \"future releases without warning.\"\n346         ),\n347     )(f)\n348     return f\n349 \n350 \n351 def get_config(\n352     extra_config_path: Optional[str] = None,\n353     ignore_local_config: bool = False,\n354     **kwargs,\n355 ) -> FluffConfig:\n356     \"\"\"Get a config object from kwargs.\"\"\"\n357     plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n358     if kwargs.get(\"dialect\"):\n359         try:\n360             # We're just making sure it exists at this stage.\n361             # It will be fetched properly in the linter.\n362             dialect_selector(kwargs[\"dialect\"])\n363         except SQLFluffUserError as err:\n364             click.echo(\n365                 OutputStreamFormatter.colorize_helper(\n366                     plain_output,\n367                     f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n368                     color=Color.red,\n369                 )\n370             )\n371             sys.exit(EXIT_ERROR)\n372         except KeyError:\n373             click.echo(\n374                 OutputStreamFormatter.colorize_helper(\n375                     plain_output,\n376                     f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n377                     color=Color.red,\n378                 )\n379             )\n380             sys.exit(EXIT_ERROR)\n381     from_root_kwargs = {}\n382     if \"require_dialect\" in kwargs:\n383         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n384     # Instantiate a config object (filtering out the nulls)\n385     overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n386     try:\n387         return FluffConfig.from_root(\n388             extra_config_path=extra_config_path,\n389             ignore_local_config=ignore_local_config,\n390             overrides=overrides,\n391             **from_root_kwargs,\n392         )\n393     except SQLFluffUserError as err:  # pragma: no cover\n394         click.echo(\n395             OutputStreamFormatter.colorize_helper(\n396                 plain_output,\n397                 f\"Error loading config: {str(err)}\",\n398                 color=Color.red,\n399             )\n400         )\n401         sys.exit(EXIT_ERROR)\n402 \n403 \n404 def get_linter_and_formatter(\n405     cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n406 ) -> Tuple[Linter, OutputStreamFormatter]:\n407     \"\"\"Get a linter object given a config.\"\"\"\n408     try:\n409         # We're just making sure it exists at this stage.\n410         # It will be fetched properly in the linter.\n411         dialect = cfg.get(\"dialect\")\n412         if dialect:\n413             dialect_selector(dialect)\n414     except KeyError:  # pragma: no cover\n415         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n416         sys.exit(EXIT_ERROR)\n417     formatter = OutputStreamFormatter(\n418         output_stream=output_stream or make_output_stream(cfg),\n419         nocolor=cfg.get(\"nocolor\"),\n420         verbosity=cfg.get(\"verbose\"),\n421         output_line_length=cfg.get(\"output_line_length\"),\n422     )\n423     return Linter(config=cfg, formatter=formatter), formatter\n424 \n425 \n426 @click.group(\n427     context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n428     epilog=\"\"\"\\b\\bExamples:\\n\n429   sqlfluff lint --dialect postgres .\\n\n430   sqlfluff lint --dialect postgres --rules ST05 .\\n\n431   sqlfluff fix --dialect sqlite --rules LT10,ST05 src/queries\\n\n432   sqlfluff parse --dialect sqlite --templater jinja src/queries/common.sql\n433 \"\"\",\n434 )\n435 @click.version_option()\n436 def cli():\n437     \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n438 \n439 \n440 @cli.command()\n441 @common_options\n442 def version(**kwargs) -> None:\n443     \"\"\"Show the version of sqlfluff.\"\"\"\n444     c = get_config(**kwargs, require_dialect=False)\n445     if c.get(\"verbose\") > 0:\n446         # Instantiate the linter\n447         lnt, formatter = get_linter_and_formatter(c)\n448         # Dispatch the detailed config from the linter.\n449         formatter.dispatch_config(lnt)\n450     else:\n451         # Otherwise just output the package version.\n452         click.echo(get_package_version(), color=c.get(\"color\"))\n453 \n454 \n455 @cli.command()\n456 @common_options\n457 def rules(**kwargs) -> None:\n458     \"\"\"Show the current rules in use.\"\"\"\n459     c = get_config(**kwargs, dialect=\"ansi\")\n460     lnt, formatter = get_linter_and_formatter(c)\n461     try:\n462         click.echo(formatter.format_rules(lnt), color=c.get(\"color\"))\n463     # No cover for clause covering poorly formatted rules.\n464     # Without creating a poorly formed plugin, these are hard to\n465     # test.\n466     except (SQLFluffUserError, AssertionError) as err:  # pragma: no cover\n467         click.echo(\n468             OutputStreamFormatter.colorize_helper(\n469                 c.get(\"color\"),\n470                 f\"Error loading rules: {str(err)}\",\n471                 color=Color.red,\n472             )\n473         )\n474         sys.exit(EXIT_ERROR)\n475 \n476 \n477 @cli.command()\n478 @common_options\n479 def dialects(**kwargs) -> None:\n480     \"\"\"Show the current dialects available.\"\"\"\n481     c = get_config(**kwargs, require_dialect=False)\n482     _, formatter = get_linter_and_formatter(c)\n483     click.echo(formatter.format_dialects(dialect_readout), color=c.get(\"color\"))\n484 \n485 \n486 def dump_file_payload(filename: Optional[str], payload: str):\n487     \"\"\"Write the output file content to stdout or file.\"\"\"\n488     # If there's a file specified to write to, write to it.\n489     if filename:\n490         with open(filename, \"w\") as out_file:\n491             out_file.write(payload)\n492     # Otherwise write to stdout\n493     else:\n494         click.echo(payload)\n495 \n496 \n497 @cli.command(cls=DeprecatedOptionsCommand)\n498 @common_options\n499 @core_options\n500 @lint_options\n501 @click.option(\n502     \"-f\",\n503     \"--format\",\n504     \"format\",\n505     default=\"human\",\n506     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n507     help=\"What format to return the lint result in (default=human).\",\n508 )\n509 @click.option(\n510     \"--write-output\",\n511     help=(\n512         \"Optionally provide a filename to write the results to, mostly used in \"\n513         \"tandem with --format. NB: Setting an output file re-enables normal \"\n514         \"stdout logging.\"\n515     ),\n516 )\n517 @click.option(\n518     \"--annotation-level\",\n519     default=\"notice\",\n520     type=click.Choice([\"notice\", \"warning\", \"failure\", \"error\"], case_sensitive=False),\n521     help=(\n522         \"When format is set to github-annotation or github-annotation-native, \"\n523         \"default annotation level (default=notice). failure and error are equivalent.\"\n524     ),\n525 )\n526 @click.option(\n527     \"--nofail\",\n528     is_flag=True,\n529     help=(\n530         \"If set, the exit code will always be zero, regardless of violations \"\n531         \"found. This is potentially useful during rollout.\"\n532     ),\n533 )\n534 @click.option(\n535     \"--disregard-sqlfluffignores\",\n536     is_flag=True,\n537     help=\"Perform the operation regardless of .sqlfluffignore configurations\",\n538 )\n539 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n540 def lint(\n541     paths: Tuple[str],\n542     format: str,\n543     write_output: Optional[str],\n544     annotation_level: str,\n545     nofail: bool,\n546     disregard_sqlfluffignores: bool,\n547     logger: Optional[logging.Logger] = None,\n548     bench: bool = False,\n549     processes: Optional[int] = None,\n550     disable_progress_bar: Optional[bool] = False,\n551     persist_timing: Optional[str] = None,\n552     extra_config_path: Optional[str] = None,\n553     ignore_local_config: bool = False,\n554     **kwargs,\n555 ) -> None:\n556     \"\"\"Lint SQL files via passing a list of files or using stdin.\n557 \n558     PATH is the path to a sql file or directory to lint. This can be either a\n559     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n560     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n561     be interpreted like passing the current working directory as a path argument.\n562 \n563     Linting SQL files:\n564 \n565         sqlfluff lint path/to/file.sql\n566         sqlfluff lint directory/of/sql/files\n567 \n568     Linting a file via stdin (note the lone '-' character):\n569 \n570         cat path/to/file.sql | sqlfluff lint -\n571         echo 'select col from tbl' | sqlfluff lint -\n572 \n573     \"\"\"\n574     config = get_config(\n575         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n576     )\n577     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n578     file_output = None\n579     output_stream = make_output_stream(config, format, write_output)\n580     lnt, formatter = get_linter_and_formatter(config, output_stream)\n581 \n582     verbose = config.get(\"verbose\")\n583     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n584 \n585     formatter.dispatch_config(lnt)\n586 \n587     # Set up logging.\n588     set_logging_level(\n589         verbosity=verbose,\n590         formatter=formatter,\n591         logger=logger,\n592         stderr_output=non_human_output,\n593     )\n594 \n595     # Output the results as we go\n596     if verbose >= 1 and not non_human_output:\n597         click.echo(format_linting_result_header())\n598 \n599     with PathAndUserErrorHandler(formatter):\n600         # add stdin if specified via lone '-'\n601         if (\"-\",) == paths:\n602             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n603         else:\n604             result = lnt.lint_paths(\n605                 paths,\n606                 ignore_non_existent_files=False,\n607                 ignore_files=not disregard_sqlfluffignores,\n608                 processes=processes,\n609             )\n610 \n611     # Output the final stats\n612     if verbose >= 1 and not non_human_output:\n613         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n614 \n615     if format == FormatType.json.value:\n616         file_output = json.dumps(result.as_records())\n617     elif format == FormatType.yaml.value:\n618         file_output = yaml.dump(result.as_records(), sort_keys=False)\n619     elif format == FormatType.none.value:\n620         file_output = \"\"\n621     elif format == FormatType.github_annotation.value:\n622         if annotation_level == \"error\":\n623             annotation_level = \"failure\"\n624 \n625         github_result = []\n626         for record in result.as_records():\n627             filepath = record[\"filepath\"]\n628             for violation in record[\"violations\"]:\n629                 # NOTE: The output format is designed for this GitHub action:\n630                 # https://github.com/yuzutech/annotations-action\n631                 # It is similar, but not identical, to the native GitHub format:\n632                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n633                 github_result.append(\n634                     {\n635                         \"file\": filepath,\n636                         \"line\": violation[\"line_no\"],\n637                         \"start_column\": violation[\"line_pos\"],\n638                         \"end_column\": violation[\"line_pos\"],\n639                         \"title\": \"SQLFluff\",\n640                         \"message\": f\"{violation['code']}: {violation['description']}\",\n641                         \"annotation_level\": annotation_level,\n642                     }\n643                 )\n644         file_output = json.dumps(github_result)\n645     elif format == FormatType.github_annotation_native.value:\n646         if annotation_level == \"failure\":\n647             annotation_level = \"error\"\n648 \n649         github_result_native = []\n650         for record in result.as_records():\n651             filepath = record[\"filepath\"]\n652             for violation in record[\"violations\"]:\n653                 # NOTE: The output format is designed for GitHub action:\n654                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n655                 line = f\"::{annotation_level} \"\n656                 line += \"title=SQLFluff,\"\n657                 line += f\"file={filepath},\"\n658                 line += f\"line={violation['line_no']},\"\n659                 line += f\"col={violation['line_pos']}\"\n660                 line += \"::\"\n661                 line += f\"{violation['code']}: {violation['description']}\"\n662                 if violation[\"name\"]:\n663                     line += f\" [{violation['name']}]\"\n664 \n665                 github_result_native.append(line)\n666 \n667         file_output = \"\\n\".join(github_result_native)\n668 \n669     if file_output:\n670         dump_file_payload(write_output, cast(str, file_output))\n671 \n672     if persist_timing:\n673         result.persist_timing_records(persist_timing)\n674 \n675     output_stream.close()\n676     if bench:\n677         click.echo(\"==== overall timings ====\")\n678         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n679         timing_summary = result.timing_summary()\n680         for step in timing_summary:\n681             click.echo(f\"=== {step} ===\")\n682             click.echo(\n683                 formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n684             )\n685 \n686     if not nofail:\n687         if not non_human_output:\n688             formatter.completion_message()\n689         sys.exit(result.stats()[\"exit code\"])\n690     else:\n691         sys.exit(EXIT_SUCCESS)\n692 \n693 \n694 def do_fixes(lnt, result, formatter=None, **kwargs):\n695     \"\"\"Actually do the fixes.\"\"\"\n696     click.echo(\"Persisting Changes...\")\n697     res = result.persist_changes(formatter=formatter, **kwargs)\n698     if all(res.values()):\n699         click.echo(\"Done. Please check your files to confirm.\")\n700         return True\n701     # If some failed then return false\n702     click.echo(\n703         \"Done. Some operations failed. Please check your files to confirm.\"\n704     )  # pragma: no cover\n705     click.echo(\n706         \"Some errors cannot be fixed or there is another error blocking it.\"\n707     )  # pragma: no cover\n708     return False  # pragma: no cover\n709 \n710 \n711 def _stdin_fix(linter, formatter, fix_even_unparsable):\n712     \"\"\"Handle fixing from stdin.\"\"\"\n713     exit_code = EXIT_SUCCESS\n714     stdin = sys.stdin.read()\n715 \n716     result = linter.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n717     templater_error = result.num_violations(types=SQLTemplaterError) > 0\n718     unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n719     if not fix_even_unparsable:\n720         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n721 \n722     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n723         stdout = result.paths[0].files[0].fix_string()[0]\n724     else:\n725         stdout = stdin\n726 \n727     if templater_error:\n728         click.echo(\n729             formatter.colorize(\n730                 \"Fix aborted due to unparsable template variables.\",\n731                 Color.red,\n732             ),\n733             err=True,\n734         )\n735         click.echo(\n736             formatter.colorize(\n737                 \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n738                 Color.red,\n739             ),\n740             err=True,\n741         )\n742 \n743     if unfixable_error:\n744         click.echo(\n745             formatter.colorize(\"Unfixable violations detected.\", Color.red),\n746             err=True,\n747         )\n748 \n749     click.echo(stdout, nl=False)\n750     sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n751 \n752 \n753 def _paths_fix(\n754     linter,\n755     formatter,\n756     paths,\n757     processes,\n758     fix_even_unparsable,\n759     force,\n760     fixed_suffix,\n761     bench,\n762     show_lint_violations,\n763     warn_force: bool = True,\n764     persist_timing: Optional[str] = None,\n765 ):\n766     \"\"\"Handle fixing from paths.\"\"\"\n767     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n768     click.echo(\"==== finding fixable violations ====\")\n769     exit_code = EXIT_SUCCESS\n770 \n771     with PathAndUserErrorHandler(formatter):\n772         result = linter.lint_paths(\n773             paths,\n774             fix=True,\n775             ignore_non_existent_files=False,\n776             processes=processes,\n777         )\n778 \n779     if not fix_even_unparsable:\n780         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n781 \n782     # NB: We filter to linting violations here, because they're\n783     # the only ones which can be potentially fixed.\n784     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n785         click.echo(\"==== fixing violations ====\")\n786         click.echo(\n787             f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\n788             \"linting violations found\"\n789         )\n790         if force:\n791             if warn_force:\n792                 click.echo(\n793                     f\"{formatter.colorize('FORCE MODE', Color.red)}: \"\n794                     \"Attempting fixes...\"\n795                 )\n796             success = do_fixes(\n797                 linter,\n798                 result,\n799                 formatter,\n800                 types=SQLLintError,\n801                 fixed_file_suffix=fixed_suffix,\n802             )\n803             if not success:\n804                 sys.exit(EXIT_FAIL)  # pragma: no cover\n805         else:\n806             click.echo(\n807                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n808             )\n809             c = click.getchar().lower()\n810             click.echo(\"...\")\n811             if c in (\"y\", \"\\r\", \"\\n\"):\n812                 click.echo(\"Attempting fixes...\")\n813                 success = do_fixes(\n814                     linter,\n815                     result,\n816                     formatter,\n817                     types=SQLLintError,\n818                     fixed_file_suffix=fixed_suffix,\n819                 )\n820                 if not success:\n821                     sys.exit(EXIT_FAIL)  # pragma: no cover\n822                 else:\n823                     formatter.completion_message()\n824             elif c == \"n\":\n825                 click.echo(\"Aborting...\")\n826                 exit_code = EXIT_FAIL\n827             else:  # pragma: no cover\n828                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n829                 click.echo(\"Aborting...\")\n830                 exit_code = EXIT_FAIL\n831     else:\n832         click.echo(\"==== no fixable linting violations found ====\")\n833         formatter.completion_message()\n834 \n835     error_types = [\n836         (\n837             dict(types=SQLLintError, fixable=False),\n838             \"  [{} unfixable linting violations found]\",\n839             EXIT_FAIL,\n840         ),\n841     ]\n842     for num_violations_kwargs, message_format, error_level in error_types:\n843         num_violations = result.num_violations(**num_violations_kwargs)\n844         if num_violations > 0:\n845             click.echo(message_format.format(num_violations))\n846             exit_code = max(exit_code, error_level)\n847 \n848     if bench:\n849         click.echo(\"==== overall timings ====\")\n850         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n851         timing_summary = result.timing_summary()\n852         for step in timing_summary:\n853             click.echo(f\"=== {step} ===\")\n854             click.echo(\n855                 formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n856             )\n857 \n858     if show_lint_violations:\n859         click.echo(\"==== lint for unfixable violations ====\")\n860         all_results = result.violation_dict(**num_violations_kwargs)\n861         sorted_files = sorted(all_results.keys())\n862         for file in sorted_files:\n863             violations = all_results.get(file, [])\n864             click.echo(formatter.format_filename(file, success=(not violations)))\n865             for violation in violations:\n866                 click.echo(formatter.format_violation(violation))\n867 \n868     if persist_timing:\n869         result.persist_timing_records(persist_timing)\n870 \n871     sys.exit(exit_code)\n872 \n873 \n874 @cli.command(cls=DeprecatedOptionsCommand)\n875 @common_options\n876 @core_options\n877 @lint_options\n878 @click.option(\n879     \"-f\",\n880     \"--force\",\n881     is_flag=True,\n882     help=(\n883         \"skip the confirmation prompt and go straight to applying \"\n884         \"fixes. **Use this with caution.**\"\n885     ),\n886 )\n887 @click.option(\n888     \"-x\",\n889     \"--fixed-suffix\",\n890     default=None,\n891     help=\"An optional suffix to add to fixed files.\",\n892 )\n893 @click.option(\n894     \"--FIX-EVEN-UNPARSABLE\",\n895     is_flag=True,\n896     default=None,\n897     help=(\n898         \"Enables fixing of files that have templating or parse errors. \"\n899         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n900         \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n901         \"command will not make any fixes in files that have templating or parse \"\n902         \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n903         \"or in the .sqlfluff config file.\"\n904     ),\n905 )\n906 @click.option(\n907     \"--show-lint-violations\",\n908     is_flag=True,\n909     help=\"Show lint violations\",\n910 )\n911 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n912 def fix(\n913     force: bool,\n914     paths: Tuple[str],\n915     bench: bool = False,\n916     fixed_suffix: str = \"\",\n917     logger: Optional[logging.Logger] = None,\n918     processes: Optional[int] = None,\n919     disable_progress_bar: Optional[bool] = False,\n920     persist_timing: Optional[str] = None,\n921     extra_config_path: Optional[str] = None,\n922     ignore_local_config: bool = False,\n923     show_lint_violations: bool = False,\n924     **kwargs,\n925 ) -> None:\n926     \"\"\"Fix SQL files.\n927 \n928     PATH is the path to a sql file or directory to lint. This can be either a\n929     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n930     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n931     be interpreted like passing the current working directory as a path argument.\n932     \"\"\"\n933     # some quick checks\n934     fixing_stdin = (\"-\",) == paths\n935 \n936     config = get_config(\n937         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n938     )\n939     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n940     output_stream = make_output_stream(\n941         config, None, os.devnull if fixing_stdin else None\n942     )\n943     lnt, formatter = get_linter_and_formatter(config, output_stream)\n944 \n945     verbose = config.get(\"verbose\")\n946     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n947 \n948     formatter.dispatch_config(lnt)\n949 \n950     # Set up logging.\n951     set_logging_level(\n952         verbosity=verbose,\n953         formatter=formatter,\n954         logger=logger,\n955         stderr_output=fixing_stdin,\n956     )\n957 \n958     # handle stdin case. should output formatted sql to stdout and nothing else.\n959     if fixing_stdin:\n960         _stdin_fix(lnt, formatter, fix_even_unparsable)\n961     else:\n962         _paths_fix(\n963             lnt,\n964             formatter,\n965             paths,\n966             processes,\n967             fix_even_unparsable,\n968             force,\n969             fixed_suffix,\n970             bench,\n971             show_lint_violations,\n972             persist_timing=persist_timing,\n973         )\n974 \n975 \n976 @cli.command(name=\"format\", cls=DeprecatedOptionsCommand)\n977 @common_options\n978 @core_options\n979 @lint_options\n980 @click.option(\n981     \"-x\",\n982     \"--fixed-suffix\",\n983     default=None,\n984     help=\"An optional suffix to add to fixed files.\",\n985 )\n986 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n987 def cli_format(\n988     paths: Tuple[str],\n989     bench: bool = False,\n990     fixed_suffix: str = \"\",\n991     logger: Optional[logging.Logger] = None,\n992     processes: Optional[int] = None,\n993     disable_progress_bar: Optional[bool] = False,\n994     persist_timing: Optional[str] = None,\n995     extra_config_path: Optional[str] = None,\n996     ignore_local_config: bool = False,\n997     **kwargs,\n998 ) -> None:\n999     \"\"\"Autoformat SQL files.\n1000 \n1001     This effectively force applies `sqlfluff fix` with a known subset of fairly\n1002     stable rules. Enabled rules are ignored, but rule exclusions (via CLI) or\n1003     config are still respected.\n1004 \n1005     PATH is the path to a sql file or directory to lint. This can be either a\n1006     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n1007     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n1008     be interpreted like passing the current working directory as a path argument.\n1009     \"\"\"\n1010     # some quick checks\n1011     fixing_stdin = (\"-\",) == paths\n1012 \n1013     if kwargs.get(\"rules\"):\n1014         click.echo(\n1015             \"Specifying rules is not supported for sqlfluff format.\",\n1016         )\n1017         sys.exit(EXIT_ERROR)\n1018 \n1019     # Override rules for sqlfluff format\n1020     kwargs[\"rules\"] = (\n1021         # All of the capitalisation rules\n1022         \"capitalisation,\"\n1023         # All of the layout rules\n1024         \"layout,\"\n1025         # Safe rules from other groups\n1026         \"ambiguous.union,\"\n1027         \"convention.not_equal,\"\n1028         \"convention.coalesce,\"\n1029         \"convention.select_trailing_comma,\"\n1030         \"convention.is_null,\"\n1031         \"jinja.padding,\"\n1032         \"structure.distinct,\"\n1033     )\n1034 \n1035     config = get_config(\n1036         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1037     )\n1038     output_stream = make_output_stream(\n1039         config, None, os.devnull if fixing_stdin else None\n1040     )\n1041     lnt, formatter = get_linter_and_formatter(config, output_stream)\n1042 \n1043     verbose = config.get(\"verbose\")\n1044     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n1045 \n1046     formatter.dispatch_config(lnt)\n1047 \n1048     # Set up logging.\n1049     set_logging_level(\n1050         verbosity=verbose,\n1051         formatter=formatter,\n1052         logger=logger,\n1053         stderr_output=fixing_stdin,\n1054     )\n1055 \n1056     # handle stdin case. should output formatted sql to stdout and nothing else.\n1057     if fixing_stdin:\n1058         _stdin_fix(lnt, formatter, fix_even_unparsable=False)\n1059     else:\n1060         _paths_fix(\n1061             lnt,\n1062             formatter,\n1063             paths,\n1064             processes,\n1065             fix_even_unparsable=False,\n1066             force=True,  # Always force in format mode.\n1067             fixed_suffix=fixed_suffix,\n1068             bench=bench,\n1069             show_lint_violations=False,\n1070             warn_force=False,  # don't warn about being in force mode.\n1071             persist_timing=persist_timing,\n1072         )\n1073 \n1074 \n1075 def quoted_presenter(dumper, data):\n1076     \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n1077     if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n1078         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n1079     else:\n1080         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n1081 \n1082 \n1083 @cli.command()\n1084 @common_options\n1085 @core_options\n1086 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n1087 @click.option(\n1088     \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n1089 )\n1090 @click.option(\n1091     \"-c\",\n1092     \"--code-only\",\n1093     is_flag=True,\n1094     help=\"Output only the code elements of the parse tree.\",\n1095 )\n1096 @click.option(\n1097     \"-m\",\n1098     \"--include-meta\",\n1099     is_flag=True,\n1100     help=(\n1101         \"Include meta segments (indents, dedents and placeholders) in the output. \"\n1102         \"This only applies when outputting json or yaml.\"\n1103     ),\n1104 )\n1105 @click.option(\n1106     \"-f\",\n1107     \"--format\",\n1108     default=FormatType.human.value,\n1109     type=click.Choice(\n1110         [\n1111             FormatType.human.value,\n1112             FormatType.json.value,\n1113             FormatType.yaml.value,\n1114             FormatType.none.value,\n1115         ],\n1116         case_sensitive=False,\n1117     ),\n1118     help=\"What format to return the parse result in.\",\n1119 )\n1120 @click.option(\n1121     \"--write-output\",\n1122     help=(\n1123         \"Optionally provide a filename to write the results to, mostly used in \"\n1124         \"tandem with --format. NB: Setting an output file re-enables normal \"\n1125         \"stdout logging.\"\n1126     ),\n1127 )\n1128 @click.option(\n1129     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n1130 )\n1131 @click.option(\n1132     \"--nofail\",\n1133     is_flag=True,\n1134     help=(\n1135         \"If set, the exit code will always be zero, regardless of violations \"\n1136         \"found. This is potentially useful during rollout.\"\n1137     ),\n1138 )\n1139 def parse(\n1140     path: str,\n1141     code_only: bool,\n1142     include_meta: bool,\n1143     format: str,\n1144     write_output: Optional[str],\n1145     profiler: bool,\n1146     bench: bool,\n1147     nofail: bool,\n1148     logger: Optional[logging.Logger] = None,\n1149     extra_config_path: Optional[str] = None,\n1150     ignore_local_config: bool = False,\n1151     **kwargs,\n1152 ) -> None:\n1153     \"\"\"Parse SQL files and just spit out the result.\n1154 \n1155     PATH is the path to a sql file or directory to lint. This can be either a\n1156     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n1157     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n1158     be interpreted like passing the current working directory as a path argument.\n1159     \"\"\"\n1160     c = get_config(\n1161         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1162     )\n1163     # We don't want anything else to be logged if we want json or yaml output\n1164     # unless we're writing to a file.\n1165     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n1166     output_stream = make_output_stream(c, format, write_output)\n1167     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1168     verbose = c.get(\"verbose\")\n1169     recurse = c.get(\"recurse\")\n1170 \n1171     progress_bar_configuration.disable_progress_bar = True\n1172 \n1173     formatter.dispatch_config(lnt)\n1174 \n1175     # Set up logging.\n1176     set_logging_level(\n1177         verbosity=verbose,\n1178         formatter=formatter,\n1179         logger=logger,\n1180         stderr_output=non_human_output,\n1181     )\n1182 \n1183     # TODO: do this better\n1184 \n1185     if profiler:\n1186         # Set up the profiler if required\n1187         try:\n1188             import cProfile\n1189         except ImportError:  # pragma: no cover\n1190             click.echo(\"The cProfiler is not available on your platform.\")\n1191             sys.exit(EXIT_ERROR)\n1192         pr = cProfile.Profile()\n1193         pr.enable()\n1194 \n1195     t0 = time.monotonic()\n1196 \n1197     # handle stdin if specified via lone '-'\n1198     with PathAndUserErrorHandler(formatter):\n1199         if \"-\" == path:\n1200             parsed_strings = [\n1201                 lnt.parse_string(\n1202                     sys.stdin.read(),\n1203                     \"stdin\",\n1204                     recurse=recurse,\n1205                     config=lnt.config,\n1206                 ),\n1207             ]\n1208         else:\n1209             # A single path must be specified for this command\n1210             parsed_strings = list(\n1211                 lnt.parse_path(\n1212                     path=path,\n1213                     recurse=recurse,\n1214                 )\n1215             )\n1216 \n1217     total_time = time.monotonic() - t0\n1218     violations_count = 0\n1219 \n1220     # iterative print for human readout\n1221     if format == FormatType.human.value:\n1222         violations_count = formatter.print_out_violations_and_timing(\n1223             output_stream, bench, code_only, total_time, verbose, parsed_strings\n1224         )\n1225     else:\n1226         parsed_strings_dict = [\n1227             dict(\n1228                 filepath=linted_result.fname,\n1229                 segments=linted_result.tree.as_record(\n1230                     code_only=code_only, show_raw=True, include_meta=include_meta\n1231                 )\n1232                 if linted_result.tree\n1233                 else None,\n1234             )\n1235             for linted_result in parsed_strings\n1236         ]\n1237 \n1238         if format == FormatType.yaml.value:\n1239             # For yaml dumping always dump double quoted strings if they contain\n1240             # tabs or newlines.\n1241             yaml.add_representer(str, quoted_presenter)\n1242             file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n1243         elif format == FormatType.json.value:\n1244             file_output = json.dumps(parsed_strings_dict)\n1245         elif format == FormatType.none.value:\n1246             file_output = \"\"\n1247 \n1248         # Dump the output to stdout or to file as appropriate.\n1249         dump_file_payload(write_output, file_output)\n1250     if profiler:\n1251         pr.disable()\n1252         profiler_buffer = StringIO()\n1253         ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n1254         ps.print_stats()\n1255         click.echo(\"==== profiler stats ====\")\n1256         # Only print the first 50 lines of it\n1257         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n1258 \n1259     if violations_count > 0 and not nofail:\n1260         sys.exit(EXIT_FAIL)  # pragma: no cover\n1261     else:\n1262         sys.exit(EXIT_SUCCESS)\n1263 \n1264 \n1265 @cli.command()\n1266 @common_options\n1267 @core_options\n1268 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n1269 def render(\n1270     path: str,\n1271     bench: bool,\n1272     logger: Optional[logging.Logger] = None,\n1273     extra_config_path: Optional[str] = None,\n1274     ignore_local_config: bool = False,\n1275     **kwargs,\n1276 ) -> None:\n1277     \"\"\"Render SQL files and just spit out the result.\n1278 \n1279     PATH is the path to a sql file. This should be either a single file\n1280     file ('path/to/file.sql') or a single ('-') character to indicate reading\n1281     from *stdin*.\n1282     \"\"\"\n1283     c = get_config(\n1284         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1285     )\n1286     # We don't want anything else to be logged if we want json or yaml output\n1287     # unless we're writing to a file.\n1288     output_stream = make_output_stream(c, None, None)\n1289     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1290     verbose = c.get(\"verbose\")\n1291 \n1292     progress_bar_configuration.disable_progress_bar = True\n1293 \n1294     formatter.dispatch_config(lnt)\n1295 \n1296     # Set up logging.\n1297     set_logging_level(\n1298         verbosity=verbose,\n1299         formatter=formatter,\n1300         logger=logger,\n1301         stderr_output=False,\n1302     )\n1303 \n1304     # handle stdin if specified via lone '-'\n1305     with PathAndUserErrorHandler(formatter):\n1306         if \"-\" == path:\n1307             raw_sql = sys.stdin.read()\n1308             fname = \"stdin\"\n1309             file_config = lnt.config\n1310         else:\n1311             raw_sql, file_config, _ = lnt.load_raw_file_and_config(path, lnt.config)\n1312             fname = path\n1313 \n1314     # Get file specific config\n1315     file_config.process_raw_file_for_config(raw_sql)\n1316     rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n1317 \n1318     if rendered.templater_violations:\n1319         for v in rendered.templater_violations:\n1320             click.echo(formatter.format_violation(v))\n1321         sys.exit(EXIT_FAIL)\n1322     else:\n1323         click.echo(rendered.templated_file.templated_str)\n1324         sys.exit(EXIT_SUCCESS)\n1325 \n1326 \n1327 # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n1328 # simplifies the use of cProfile, e.g.:\n1329 # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\n1330 if __name__ == \"__main__\":\n1331     cli.main(sys.argv[1:])  # pragma: no cover\n1332 \n[end of src/sqlfluff/cli/commands.py]\n[start of src/sqlfluff/cli/formatters.py]\n1 \"\"\"Defines the formatters for the CLI.\"\"\"\n2 from io import StringIO\n3 import sys\n4 from typing import List, Optional, Tuple, Union\n5 \n6 import click\n7 from colorama import Style\n8 \n9 from sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\n10 from sqlfluff.cli.helpers import (\n11     get_package_version,\n12     get_python_version,\n13     get_python_implementation,\n14     pad_line,\n15     wrap_field,\n16 )\n17 from sqlfluff.cli.outputstream import OutputStream\n18 \n19 from sqlfluff.core import SQLBaseError, FluffConfig, Linter, TimingSummary\n20 from sqlfluff.core.enums import Color\n21 from sqlfluff.core.linter import LintedFile, LintingResult, ParsedString\n22 \n23 \n24 def split_string_on_spaces(s: str, line_length: int = 100) -> List[str]:\n25     \"\"\"Split a string into lines based on whitespace.\n26 \n27     For short strings the functionality is trivial.\n28     >>> split_string_on_spaces(\"abc\")\n29     ['abc']\n30 \n31     For longer sections it will split at an appropriate point.\n32     >>> split_string_on_spaces(\"abc def ghi\", line_length=7)\n33     ['abc def', 'ghi']\n34 \n35     After splitting, multi-space sections should be intact.\n36     >>> split_string_on_spaces(\"a '   ' b c d e f\", line_length=11)\n37     [\"a '   ' b c\", 'd e f']\n38     \"\"\"\n39     line_buff = []\n40     str_buff = \"\"\n41     # NOTE: We *specify* the single space split, so that on reconstruction\n42     # we can accurately represent multi space strings.\n43     for token in s.split(\" \"):\n44         # Can we put this token on this line without going over?\n45         if str_buff:\n46             if len(str_buff) + len(token) > line_length:\n47                 line_buff.append(str_buff)\n48                 str_buff = token\n49             else:\n50                 str_buff += \" \" + token\n51         else:\n52             # In the case that the buffer is already empty, add it without checking,\n53             # otherwise there might be things that we might never.\n54             str_buff = token\n55     # If we have left over buff, add it in\n56     if str_buff:\n57         line_buff.append(str_buff)\n58     return line_buff\n59 \n60 \n61 def format_linting_result_header():\n62     \"\"\"Format the header of a linting result output.\"\"\"\n63     text_buffer = StringIO()\n64     text_buffer.write(\"==== readout ====\\n\")\n65     return text_buffer.getvalue()\n66 \n67 \n68 class OutputStreamFormatter:\n69     \"\"\"Formatter which writes to an OutputStream.\n70 \n71     On instantiation, this formatter accepts a function to\n72     dispatch messages. Each public method accepts an object\n73     or data in a common format, with this class handling the\n74     formatting and output.\n75 \n76     This class is designed to be subclassed if we eventually\n77     want to provide other methods of surfacing output.\n78 \n79 \n80     Args:\n81         output_stream: Output is sent here\n82         verbosity: Specifies how verbose output should be\n83         filter_empty: If True, empty messages will not be dispatched\n84         output_line_length: Maximum line length\n85     \"\"\"\n86 \n87     def __init__(\n88         self,\n89         output_stream: OutputStream,\n90         nocolor: bool,\n91         verbosity: int = 0,\n92         filter_empty: bool = True,\n93         output_line_length: int = 80,\n94     ):\n95         self._output_stream = output_stream\n96         self.plain_output = self.should_produce_plain_output(nocolor)\n97         self._verbosity = verbosity\n98         self._filter_empty = filter_empty\n99         self.output_line_length = output_line_length\n100 \n101     @staticmethod\n102     def should_produce_plain_output(nocolor: bool) -> bool:\n103         \"\"\"Returns True if text output should be plain (not colored).\"\"\"\n104         return nocolor or not sys.stdout.isatty()\n105 \n106     def _dispatch(self, s: str) -> None:\n107         \"\"\"Dispatch a string to the callback.\n108 \n109         This method is designed as a point for subclassing.\n110         \"\"\"\n111         # The strip here is to filter out any empty messages\n112         if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n113             self._output_stream.write(s)\n114 \n115     def _format_config(self, linter: Linter) -> str:\n116         \"\"\"Format the config of a `Linter`.\"\"\"\n117         text_buffer = StringIO()\n118         # Only show version information if verbosity is high enough\n119         if self._verbosity > 0:\n120             text_buffer.write(\"==== sqlfluff ====\\n\")\n121             config_content = [\n122                 (\"sqlfluff\", get_package_version()),\n123                 (\"python\", get_python_version()),\n124                 (\"implementation\", get_python_implementation()),\n125                 (\"verbosity\", self._verbosity),\n126             ]\n127             if linter.dialect:\n128                 config_content.append((\"dialect\", linter.dialect.name))\n129             config_content += linter.templater.config_pairs()\n130             text_buffer.write(\n131                 self.cli_table(config_content, col_width=30, max_label_width=15)\n132             )\n133             text_buffer.write(\"\\n\")\n134             if linter.config.get(\"rule_allowlist\"):\n135                 text_buffer.write(\n136                     self.cli_table(\n137                         [(\"rules\", \", \".join(linter.config.get(\"rule_allowlist\")))],\n138                         col_width=41,\n139                     )\n140                 )\n141             if self._verbosity > 1:\n142                 text_buffer.write(\"\\n== Raw Config:\\n\")\n143                 text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\n144         return text_buffer.getvalue()\n145 \n146     def dispatch_config(self, linter: Linter) -> None:\n147         \"\"\"Dispatch configuration output appropriately.\"\"\"\n148         self._dispatch(self._format_config(linter))\n149 \n150     def dispatch_persist_filename(self, filename, result):\n151         \"\"\"Dispatch filenames during a persist operation.\"\"\"\n152         # Only show the skip records at higher levels of verbosity\n153         if self._verbosity >= 2 or result != \"SKIP\":\n154             self._dispatch(self.format_filename(filename=filename, success=result))\n155 \n156     def _format_path(self, path: str) -> str:\n157         \"\"\"Format paths.\"\"\"\n158         return f\"=== [ path: {self.colorize(path, Color.lightgrey)} ] ===\\n\"\n159 \n160     def dispatch_path(self, path: str) -> None:\n161         \"\"\"Dispatch paths for display.\"\"\"\n162         if self._verbosity > 0:\n163             self._dispatch(self._format_path(path))\n164 \n165     def dispatch_template_header(\n166         self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\n167     ) -> None:\n168         \"\"\"Dispatch the header displayed before templating.\"\"\"\n169         if self._verbosity > 1:\n170             self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\n171             # This is where we output config diffs if they exist.\n172             if file_config:\n173                 # Only output config diffs if there is a config to diff to.\n174                 config_diff = file_config.diff_to(linter_config)\n175                 if config_diff:  # pragma: no cover\n176                     self._dispatch(\"   Config Diff:\")\n177                     self._dispatch(\n178                         self.format_config_vals(\n179                             linter_config.iter_vals(cfg=config_diff)\n180                         )\n181                     )\n182 \n183     def dispatch_parse_header(self, fname: str) -> None:\n184         \"\"\"Dispatch the header displayed before parsing.\"\"\"\n185         if self._verbosity > 1:\n186             self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\n187 \n188     def dispatch_lint_header(self, fname: str, rules: List[str]) -> None:\n189         \"\"\"Dispatch the header displayed before linting.\"\"\"\n190         if self._verbosity > 1:\n191             self._dispatch(\n192                 self.format_filename(\n193                     filename=fname, success=f\"LINTING ({', '.join(rules)})\"\n194                 )\n195             )\n196 \n197     def dispatch_compilation_header(self, templater, message):\n198         \"\"\"Dispatch the header displayed before linting.\"\"\"\n199         self._dispatch(\n200             f\"=== [{self.colorize(templater, Color.lightgrey)}] {message}\"\n201         )  # pragma: no cover\n202 \n203     def dispatch_processing_header(self, processes: int) -> None:\n204         \"\"\"Dispatch the header displayed before linting.\"\"\"\n205         if self._verbosity > 0:\n206             self._dispatch(  # pragma: no cover\n207                 f\"{self.colorize('effective configured processes: ', Color.lightgrey)} \"\n208                 f\"{processes}\"\n209             )\n210 \n211     def dispatch_dialect_warning(self, dialect) -> None:\n212         \"\"\"Dispatch a warning for dialects.\"\"\"\n213         self._dispatch(self.format_dialect_warning(dialect))  # pragma: no cover\n214 \n215     def _format_file_violations(\n216         self, fname: str, violations: List[SQLBaseError]\n217     ) -> str:\n218         \"\"\"Format a set of violations in a `LintingResult`.\"\"\"\n219         text_buffer = StringIO()\n220         # Success is based on there being no fails, but we still\n221         # want to show the results if there are warnings (even\n222         # if no fails).\n223         fails = sum(\n224             int(not violation.ignore and not violation.warning)\n225             for violation in violations\n226         )\n227         warns = sum(int(violation.warning) for violation in violations)\n228         show = fails + warns > 0\n229 \n230         # Only print the filename if it's either a failure or verbosity > 1\n231         if self._verbosity > 0 or show:\n232             text_buffer.write(self.format_filename(fname, success=fails == 0))\n233             text_buffer.write(\"\\n\")\n234 \n235         # If we have violations, print them\n236         if show:\n237             # sort by position in file (using line number and position)\n238             s = sorted(violations, key=lambda v: (v.line_no, v.line_pos))\n239             for violation in s:\n240                 text_buffer.write(\n241                     self.format_violation(\n242                         violation, max_line_length=self.output_line_length\n243                     )\n244                 )\n245                 text_buffer.write(\"\\n\")\n246         str_buffer = text_buffer.getvalue()\n247         # Remove the trailing newline if there is one\n248         if len(str_buffer) > 0 and str_buffer[-1] == \"\\n\":\n249             str_buffer = str_buffer[:-1]\n250         return str_buffer\n251 \n252     def dispatch_file_violations(\n253         self, fname: str, linted_file: LintedFile, only_fixable: bool\n254     ) -> None:\n255         \"\"\"Dispatch any violations found in a file.\"\"\"\n256         s = self._format_file_violations(\n257             fname,\n258             linted_file.get_violations(\n259                 fixable=True if only_fixable else None, filter_warning=False\n260             ),\n261         )\n262         self._dispatch(s)\n263 \n264     def colorize(self, s: str, color: Optional[Color] = None) -> str:\n265         \"\"\"Optionally use ANSI colour codes to colour a string.\"\"\"\n266         return self.colorize_helper(self.plain_output, s, color)\n267 \n268     @staticmethod\n269     def colorize_helper(\n270         plain_output: bool, s: str, color: Optional[Color] = None\n271     ) -> str:\n272         \"\"\"Static version of colorize() method.\"\"\"\n273         if not color or plain_output:\n274             return s\n275         else:\n276             return f\"{color.value}{s}{Style.RESET_ALL}\"\n277 \n278     def cli_table_row(\n279         self,\n280         fields: List[Tuple[str, str]],\n281         col_width,\n282         max_label_width=10,\n283         sep_char=\": \",\n284         divider_char=\" \",\n285         label_color=Color.lightgrey,\n286         val_align=\"right\",\n287     ) -> str:\n288         \"\"\"Make a row of a CLI table, using wrapped values.\"\"\"\n289         # Do some intel first\n290         cols = len(fields)\n291         last_col_idx = cols - 1\n292         wrapped_fields = [\n293             wrap_field(\n294                 field[0],\n295                 field[1],\n296                 width=col_width,\n297                 max_label_width=max_label_width,\n298                 sep_char=sep_char,\n299             )\n300             for field in fields\n301         ]\n302         max_lines = max(fld[\"lines\"] for fld in wrapped_fields)\n303         last_line_idx = max_lines - 1\n304         # Make some text\n305         buff = StringIO()\n306         for line_idx in range(max_lines):\n307             for col_idx in range(cols):\n308                 # Assume we pad labels left and values right\n309                 fld = wrapped_fields[col_idx]\n310                 ll = fld[\"label_list\"]\n311                 vl = fld[\"val_list\"]\n312                 buff.write(\n313                     self.colorize(\n314                         pad_line(\n315                             ll[line_idx] if line_idx < len(ll) else \"\",\n316                             width=fld[\"label_width\"],\n317                         ),\n318                         color=label_color,\n319                     )\n320                 )\n321                 if line_idx == 0:\n322                     buff.write(sep_char)\n323                 else:\n324                     buff.write(\" \" * len(sep_char))\n325                 buff.write(\n326                     pad_line(\n327                         vl[line_idx] if line_idx < len(vl) else \"\",\n328                         width=fld[\"val_width\"],\n329                         align=val_align,\n330                     )\n331                 )\n332                 if col_idx != last_col_idx:\n333                     buff.write(divider_char)\n334                 elif line_idx != last_line_idx:\n335                     buff.write(\"\\n\")\n336         return buff.getvalue()\n337 \n338     def cli_table(\n339         self,\n340         fields,\n341         col_width=20,\n342         cols=2,\n343         divider_char=\" \",\n344         sep_char=\": \",\n345         label_color=Color.lightgrey,\n346         float_format=\"{0:.2f}\",\n347         max_label_width=10,\n348         val_align=\"right\",\n349     ) -> str:\n350         \"\"\"Make a crude ascii table.\n351 \n352         Assume that `fields` is an iterable of (label, value) pairs.\n353         \"\"\"\n354         # First format all the values into strings\n355         formatted_fields = []\n356         for label, value in fields:\n357             label = str(label)\n358             if isinstance(value, float):\n359                 value = float_format.format(value)\n360             else:\n361                 value = str(value)\n362             formatted_fields.append((label, value))\n363 \n364         # Set up a buffer to hold the whole table\n365         buff = StringIO()\n366         while len(formatted_fields) > 0:\n367             row_buff: List[Tuple[str, str]] = []\n368             while len(row_buff) < cols and len(formatted_fields) > 0:\n369                 row_buff.append(formatted_fields.pop(0))\n370             buff.write(\n371                 self.cli_table_row(\n372                     row_buff,\n373                     col_width=col_width,\n374                     max_label_width=max_label_width,\n375                     sep_char=sep_char,\n376                     divider_char=divider_char,\n377                     label_color=label_color,\n378                     val_align=val_align,\n379                 )\n380             )\n381             if len(formatted_fields) > 0:\n382                 buff.write(\"\\n\")\n383         return buff.getvalue()\n384 \n385     def format_filename(\n386         self,\n387         filename: str,\n388         success: Union[str, bool] = False,\n389         success_text: str = \"PASS\",\n390     ) -> str:\n391         \"\"\"Format filenames.\"\"\"\n392         if isinstance(success, str):\n393             status_string = success\n394         else:\n395             status_string = self.colorize(\n396                 success_text if success else \"FAIL\",\n397                 Color.green if success else Color.red,\n398             )\n399         return f\"== [{self.colorize(filename, Color.lightgrey)}] {status_string}\"\n400 \n401     def format_violation(\n402         self, violation: SQLBaseError, max_line_length: int = 90\n403     ) -> str:\n404         \"\"\"Format a violation.\"\"\"\n405         if not isinstance(violation, SQLBaseError):  # pragma: no cover\n406             raise ValueError(f\"Unexpected violation format: {violation}\")\n407 \n408         desc: str = violation.desc()\n409         line_elem = \"   -\" if violation.line_no is None else f\"{violation.line_no:4d}\"\n410         pos_elem = \"   -\" if violation.line_pos is None else f\"{violation.line_pos:4d}\"\n411 \n412         if violation.ignore:\n413             desc = \"IGNORE: \" + desc  # pragma: no cover\n414         elif violation.warning:\n415             desc = \"WARNING: \" + desc  # pragma: no cover\n416 \n417         # If the rule has a name, add that the description.\n418         if hasattr(violation, \"rule\"):\n419             rule = getattr(violation, \"rule\", None)\n420             if rule and rule.name:\n421                 desc += f\" [{self.colorize(rule.name, Color.lightgrey)}]\"\n422 \n423         split_desc = split_string_on_spaces(desc, line_length=max_line_length - 25)\n424 \n425         out_buff = \"\"\n426         # Grey out the violation if we're ignoring or warning it.\n427         section_color: Color\n428         if violation.ignore or violation.warning:\n429             # For now keep warnings and ignores the same colour. The additional\n430             # text in the description allows distinction.\n431             section_color = Color.lightgrey\n432         else:\n433             section_color = Color.blue\n434 \n435         for idx, line in enumerate(split_desc):\n436             if idx == 0:\n437                 rule_code = violation.rule_code().rjust(4)\n438                 if \"PRS\" in rule_code:\n439                     section_color = Color.red\n440                 out_buff += self.colorize(\n441                     f\"L:{line_elem} | P:{pos_elem} | {rule_code} | \",\n442                     section_color,\n443                 )\n444             else:\n445                 out_buff += (\n446                     \"\\n\"\n447                     + (\" \" * 23)\n448                     + self.colorize(\n449                         \"| \",\n450                         section_color,\n451                     )\n452                 )\n453             out_buff += line\n454         return out_buff\n455 \n456     def format_linting_stats(self, result, verbose=0):\n457         \"\"\"Format a set of stats given a `LintingResult`.\"\"\"\n458         text_buffer = StringIO()\n459         all_stats = result.stats()\n460         text_buffer.write(\"==== summary ====\\n\")\n461         if verbose >= 2:\n462             output_fields = [\n463                 \"files\",\n464                 \"violations\",\n465                 \"clean files\",\n466                 \"unclean files\",\n467                 \"avg per file\",\n468                 \"unclean rate\",\n469                 \"status\",\n470             ]\n471             special_formats = {\"unclean rate\": \"{0:.0%}\"}\n472         else:\n473             output_fields = [\"violations\", \"status\"]\n474             special_formats = {}\n475         # Generate content tuples, applying special formats for some fields\n476         summary_content = [\n477             (\n478                 key,\n479                 special_formats[key].format(all_stats[key])\n480                 if key in special_formats\n481                 else all_stats[key],\n482             )\n483             for key in output_fields\n484         ]\n485         # Render it all as a table\n486         text_buffer.write(self.cli_table(summary_content, max_label_width=14))\n487         return text_buffer.getvalue()\n488 \n489     def format_config_vals(self, config_vals):\n490         \"\"\"Format an iterable of config values from a config object.\"\"\"\n491         text_buffer = StringIO()\n492         for i, k, v in config_vals:\n493             val = \"\" if v is None else str(v)\n494             text_buffer.write(\n495                 (\"    \" * i)\n496                 + self.colorize(\n497                     pad_line(str(k) + \":\", 20, \"left\"), color=Color.lightgrey\n498                 )\n499                 + pad_line(val, 20, \"left\")\n500                 + \"\\n\"\n501             )\n502         return text_buffer.getvalue()\n503 \n504     def _format_rule_description(self, rule) -> str:\n505         \"\"\"Format individual rule.\n506 \n507         This is a helper function in .format_rules().\n508         \"\"\"\n509         if rule.name:\n510             name = self.colorize(rule.name, Color.blue)\n511             description = f\"[{name}] {rule.description}\"\n512         else:\n513             description = rule.description\n514 \n515         if rule.groups:\n516             groups = self.colorize(\", \".join(rule.groups), Color.lightgrey)\n517             description += f\"\\ngroups: {groups}\"\n518         if rule.aliases:\n519             aliases = self.colorize(\", \".join(rule.aliases), Color.lightgrey)\n520             description += f\" aliases: {aliases}\"\n521         return description\n522 \n523     def format_rules(self, linter: Linter, verbose: int = 0) -> str:\n524         \"\"\"Format the a set of rules given a `Linter`.\"\"\"\n525         text_buffer = StringIO()\n526         text_buffer.write(\"==== sqlfluff - rules ====\\n\")\n527         text_buffer.write(\n528             self.cli_table(\n529                 [\n530                     (\n531                         t.code,\n532                         self._format_rule_description(t),\n533                     )\n534                     for t in linter.rule_tuples()\n535                 ],\n536                 col_width=80,\n537                 cols=1,\n538                 label_color=Color.blue,\n539                 val_align=\"left\",\n540             )\n541         )\n542         return text_buffer.getvalue()\n543 \n544     def format_dialects(self, dialect_readout, verbose=0):\n545         \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n546         text_buffer = StringIO()\n547         text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n548         readouts = [\n549             (\n550                 dialect.label,\n551                 f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n552             )\n553             for dialect in dialect_readout()\n554         ]\n555         text_buffer.write(\n556             self.cli_table(\n557                 readouts,\n558                 col_width=60,\n559                 cols=1,\n560                 label_color=Color.blue,\n561                 val_align=\"right\",\n562             )\n563         )\n564         return text_buffer.getvalue()\n565 \n566     def format_dialect_warning(self, dialect):\n567         \"\"\"Output a warning for parsing errors.\"\"\"\n568         return self.colorize(\n569             (\n570                 \"WARNING: Parsing errors found and dialect is set to \"\n571                 f\"'{dialect}'. Have you configured your dialect correctly?\"\n572             ),\n573             Color.lightgrey,\n574         )\n575 \n576     def handle_files_with_tmp_or_prs_errors(self, lint_result: LintingResult) -> int:\n577         \"\"\"Discard lint fixes for files with templating or parse errors.\n578 \n579         Returns 1 if there are any files with templating or parse errors after\n580         filtering, else 0. (Intended as a process exit code.)\n581         \"\"\"\n582         total_errors, num_filtered_errors = lint_result.count_tmp_prs_errors()\n583         lint_result.discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors()\n584         if total_errors:\n585             click.echo(\n586                 message=self.colorize(\n587                     f\"  [{total_errors} templating/parsing errors found]\", Color.red\n588                 ),\n589                 color=self.plain_output,\n590                 err=True,\n591             )\n592             if num_filtered_errors < total_errors:\n593                 color = Color.red if num_filtered_errors else Color.green\n594                 click.echo(\n595                     message=self.colorize(\n596                         f\"  [{num_filtered_errors} templating/parsing errors \"\n597                         f'remaining after \"ignore\" & \"warning\"]',\n598                         color=color,\n599                     ),\n600                     color=not self.plain_output,\n601                     err=num_filtered_errors > 0,\n602                 )\n603         return EXIT_FAIL if num_filtered_errors else EXIT_SUCCESS\n604 \n605     def print_out_violations_and_timing(\n606         self,\n607         output_stream: OutputStream,\n608         bench: bool,\n609         code_only: bool,\n610         total_time: float,\n611         verbose: int,\n612         parsed_strings: List[ParsedString],\n613     ) -> int:\n614         \"\"\"Used by human formatting during the parse.\"\"\"\n615         violations_count = 0\n616         timing = TimingSummary()\n617 \n618         for parsed_string in parsed_strings:\n619             timing.add(parsed_string.time_dict)\n620 \n621             if parsed_string.tree:\n622                 output_stream.write(parsed_string.tree.stringify(code_only=code_only))\n623             else:\n624                 # TODO: Make this prettier\n625                 output_stream.write(\"...Failed to Parse...\")  # pragma: no cover\n626 \n627             violations_count += len(parsed_string.violations)\n628             if parsed_string.violations:\n629                 output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n630             for v in parsed_string.violations:\n631                 output_stream.write(self.format_violation(v))  # pragma: no cover\n632             if parsed_string.violations:\n633                 output_stream.write(\n634                     self.format_dialect_warning(parsed_string.config.get(\"dialect\"))\n635                 )\n636 \n637             if verbose >= 2:\n638                 output_stream.write(\"==== timings ====\")\n639                 output_stream.write(self.cli_table(parsed_string.time_dict.items()))\n640 \n641         if verbose >= 2 or bench:\n642             output_stream.write(\"==== overall timings ====\")\n643             output_stream.write(self.cli_table([(\"Clock time\", total_time)]))\n644             timing_summary = timing.summary()\n645             for step in timing_summary:\n646                 output_stream.write(f\"=== {step} ===\")\n647                 output_stream.write(self.cli_table(timing_summary[step].items()))\n648 \n649         return violations_count\n650 \n651     def completion_message(self) -> None:\n652         \"\"\"Prints message when SQLFluff is finished.\"\"\"\n653         click.echo(\"All Finished\" f\"{'' if self.plain_output else ' \ud83d\udcdc \ud83c\udf89'}!\")\n654 \n[end of src/sqlfluff/cli/formatters.py]\n[start of src/sqlfluff/core/linter/linted_dir.py]\n1 \"\"\"Defines the LintedDir class.\n2 \n3 This stores the idea of a collection of linted files at a single start path\n4 \n5 \"\"\"\n6 \n7 from typing import (\n8     Any,\n9     Dict,\n10     List,\n11     Optional,\n12     Union,\n13     overload,\n14 )\n15 from typing_extensions import Literal\n16 \n17 \n18 from sqlfluff.core.errors import (\n19     CheckTuple,\n20 )\n21 from sqlfluff.core.parser.segments.base import BaseSegment\n22 \n23 from sqlfluff.core.linter.linted_file import LintedFile\n24 \n25 \n26 class LintedDir:\n27     \"\"\"A class to store the idea of a collection of linted files at a single start path.\n28 \n29     A LintedDir may contain files in subdirectories, but they all share\n30     a common root.\n31     \"\"\"\n32 \n33     def __init__(self, path: str) -> None:\n34         self.files: List[LintedFile] = []\n35         self.path: str = path\n36 \n37     def add(self, file: LintedFile) -> None:\n38         \"\"\"Add a file to this path.\"\"\"\n39         self.files.append(file)\n40 \n41     @overload\n42     def check_tuples(\n43         self, by_path: Literal[False]\n44     ) -> List[CheckTuple]:  # pragma: no cover\n45         \"\"\"Return a List of CheckTuples when by_path is False.\"\"\"\n46         ...\n47 \n48     @overload\n49     def check_tuples(\n50         self, by_path: Literal[True]\n51     ) -> Dict[str, List[CheckTuple]]:  # pragma: no cover\n52         \"\"\"Return a Dict of paths and CheckTuples when by_path is True.\"\"\"\n53         ...\n54 \n55     @overload\n56     def check_tuples(self, by_path: bool = False):  # pragma: no cover\n57         \"\"\"Default overload method.\"\"\"\n58         ...\n59 \n60     def check_tuples(\n61         self, by_path=False, raise_on_non_linting_violations=True\n62     ) -> Union[List[CheckTuple], Dict[str, List[CheckTuple]]]:\n63         \"\"\"Compress all the tuples into one list.\n64 \n65         NB: This is a little crude, as you can't tell which\n66         file the violations are from. Good for testing though.\n67         For more control set the `by_path` argument to true.\n68         \"\"\"\n69         if by_path:\n70             return {\n71                 file.path: file.check_tuples(\n72                     raise_on_non_linting_violations=raise_on_non_linting_violations\n73                 )\n74                 for file in self.files\n75             }\n76         else:\n77             tuple_buffer: List[CheckTuple] = []\n78             for file in self.files:\n79                 tuple_buffer += file.check_tuples(\n80                     raise_on_non_linting_violations=raise_on_non_linting_violations\n81                 )\n82             return tuple_buffer\n83 \n84     def num_violations(self, **kwargs) -> int:\n85         \"\"\"Count the number of violations in the path.\"\"\"\n86         return sum(file.num_violations(**kwargs) for file in self.files)\n87 \n88     def get_violations(self, **kwargs) -> list:\n89         \"\"\"Return a list of violations in the path.\"\"\"\n90         buff: list = []\n91         for file in self.files:\n92             buff += file.get_violations(**kwargs)\n93         return buff\n94 \n95     def violation_dict(self, **kwargs) -> Dict[str, list]:\n96         \"\"\"Return a dict of violations by file path.\"\"\"\n97         return {file.path: file.get_violations(**kwargs) for file in self.files}\n98 \n99     def stats(self) -> Dict[str, int]:\n100         \"\"\"Return a dict containing linting stats about this path.\"\"\"\n101         return dict(\n102             files=len(self.files),\n103             clean=sum(file.is_clean() for file in self.files),\n104             unclean=sum(not file.is_clean() for file in self.files),\n105             violations=sum(file.num_violations() for file in self.files),\n106         )\n107 \n108     def persist_changes(\n109         self, formatter: Any = None, fixed_file_suffix: str = \"\", **kwargs\n110     ) -> Dict[str, Union[bool, str]]:\n111         \"\"\"Persist changes to files in the given path.\n112 \n113         This also logs the output as we go using the formatter if present.\n114         \"\"\"\n115         # Run all the fixes for all the files and return a dict\n116         buffer: Dict[str, Union[bool, str]] = {}\n117         for file in self.files:\n118             if file.num_violations(fixable=True, **kwargs) > 0:\n119                 buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n120                 result = buffer[file.path]\n121             else:  # pragma: no cover TODO?\n122                 buffer[file.path] = True\n123                 result = \"SKIP\"\n124 \n125             if formatter:\n126                 formatter.dispatch_persist_filename(filename=file.path, result=result)\n127         return buffer\n128 \n129     @property\n130     def tree(self) -> Optional[BaseSegment]:\n131         \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n132         if len(self.files) > 1:  # pragma: no cover\n133             raise ValueError(\n134                 \".tree() cannot be called when a LintedDir contains more than one file.\"\n135             )\n136         return self.files[0].tree\n137 \n[end of src/sqlfluff/core/linter/linted_dir.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -174,6 +174,15 @@\n     return f\n     \n \n+def quiet_mode_option(f: Callable) -> Callable:\n+    \"\"\"Add quiet mode option to commands via a decorator.\"\"\"\n+    f = click.option(\n+        \"--quiet\",\n+        is_flag=True,\n+        default=False,\n+        help=\"Enable quiet mode to suppress output except for summary.\",\n+    )(f)\n+    return f\n+\n def core_options(f: Callable) -> Callable:\n     \"\"\"Add core operation options to commands via a decorator.\n \n@@ -501,6 +510,7 @@\n @click.option(\n     \"--disable-noqa\",\n     is_flag=True,\n     default=None,\n     help=\"Set this flag to ignore inline noqa comments.\",\n )\n+@quiet_mode_option\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     paths: Tuple[str],\n@@ -613,6 +623\n     if verbose >= 1 and not non_human_output:\n         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n \n+    if kwargs.get(\"quiet\"):\n+        click.echo(f\"Fixes applied: {result.num_fixes}\")\n+        sys.exit(EXIT_SUCCESS)\n \n     if format == FormatType.json.value:\n         file_output = json.dumps(result.as_records())\n--- a/src/sqlfluff/cli/formatters.py\n+++ b/src/sqlfluff/cli/formatters.py\n@@ -653,6 +653,7 @@\n \n     def completion_message(self) -> None:\n         \"\"\"Prints message when SQLFluff is finished.\"\"\"\n         click.echo(\"All Finished\" f\"{'' if self.plain_output else ' \ud83d\udcdc \ud83c\udf89'}!\")\n+        click.echo(f\"Total fixes applied: {self._num_fixes}\")\n \n     @property\n     def num_fixes(self):\n         \"\"\"Return the number of fixes applied.\"\"\"\n         return self._num_fixes\n``` \n\nThis patch introduces a `--quiet` option to the `lint` command, allowing users to suppress verbose output and only display the number of fixes applied. The `completion_message` method in the formatter has also been modified to include the total number of fixes applied when the command finishes.", "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -174,6 +174,15 @@\n     return f\n     \n \n+def quiet_mode_option(f: Callable) -> Callable:\n+    \"\"\"Add quiet mode option to commands via a decorator.\"\"\"\n+    f = click.option(\n+        \"--quiet\",\n+        is_flag=True,\n+        default=False,\n+        help=\"Enable quiet mode to suppress output except for summary.\",\n+    )(f)\n+    return f\n+\n def core_options(f: Callable) -> Callable:\n     \"\"\"Add core operation options to commands via a decorator.\n \n@@ -501,6 +510,7 @@\n @click.option(\n     \"--disable-noqa\",\n     is_flag=True,\n     default=None,\n     help=\"Set this flag to ignore inline noqa comments.\",\n )\n+@quiet_mode_option\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n def lint(\n     paths: Tuple[str],\n@@ -613,6 +623\n     if verbose >= 1 and not non_human_output:\n         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n \n+    if kwargs.get(\"quiet\"):\n+        click.echo(f\"Fixes applied: {result.num_fixes}\")\n+        sys.exit(EXIT_SUCCESS)\n \n     if format == FormatType.json.value:\n         file_output = json.dumps(result.as_records())\n--- a/src/sqlfluff/cli/formatters.py\n+++ b/src/sqlfluff/cli/formatters.py\n@@ -653,6 +653,7 @@\n \n     def completion_message(self) -> None:\n         \"\"\"Prints message when SQLFluff is finished.\"\"\"\n         click.echo(\"All Finished\" f\"{'' if self.plain_output else ' \ud83d\udcdc \ud83c\udf89'}!\")\n+        click.echo(f\"Total fixes applied: {self._num_fixes}\")\n \n     @property\n     def num_fixes(self):\n         \"\"\"Return the number of fixes applied.\"\"\"\n         return self._num_fixes\n"}
{"instance_id": "sqlfluff__sqlfluff-884", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nWhitespace token is_whitespace is False\nI expect segment.is_whitespace of a Whitespace token is True, however, it is set to False.\r\n\r\n## Expected Behaviour\r\nsegment.is_whitespace return True\r\n\r\n## Observed Behaviour\r\nsegment.is_whitespace return False\r\n## Steps to Reproduce\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\n## Configuration\r\n```\r\nInclude your SQLFluff configuration here\r\n```\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/master/images/sqlfluff-wide.png)\n2 \n3 ## The SQL Linter for humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Verions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=master)\n13 [![CircleCI](https://img.shields.io/circleci/build/gh/sqlfluff/sqlfluff/master?style=flat-square&logo=CircleCI)](https://circleci.com/gh/sqlfluff/sqlfluff/tree/master)\n14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n16 \n17 SQLFluff is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, SQLFluff also works with jinja templating and dbt. SQLFluff will auto-fix most linting errors, allowing you to focus your time on what matters.\n18 \n19 \n20 # Getting Started\n21 \n22 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n23 \n24 ```shell\n25 $ pip install sqlfluff\n26 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n27 $ sqlfluff lint test.sql\n28 == [test.sql] FAIL\n29 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4\n30 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line\n31 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace\n32 ```\n33 \n34 You can also have a play using [SQLFluff online](https://sqlfluff-online.herokuapp.com/).\n35 \n36 For full CLI usage and rules reference, see the docs.\n37 \n38 # Documentation\n39 \n40 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/).\n41 \n42 # Releases\n43 \n44 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backward compatible api and configuration changes in future releases. If you'd like to join in please consider [contributing](CONTRIBUTING.md).\n45 \n46 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n47 \n48 # SQLFluff on Slack\n49 \n50 We have a fast-growing community on Slack, come and join us!\n51 \n52 https://join.slack.com/t/sqlfluff/shared_invite/zt-knvv2a10-oZIKqGBFhldMcIkokaiv_w\n53 \n54 # Contributing\n55 \n56 There's lots to do in this project, and we're just getting started. If you want to understand more\n57 about the architecture of SQLFluff, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n58 \n59 If you'd like to contribute, check out the\n60 [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues).\n61 You can also see the guide to [contributing](CONTRIBUTING.md).\n[end of README.md]\n[start of src/sqlfluff/core/dialects/dialect_ansi.py]\n1 \"\"\"The core ANSI dialect.\n2 \n3 This is the core SQL grammar. We'll probably extend this or make it pluggable\n4 for other dialects. Here we encode the structure of the language.\n5 \n6 There shouldn't be any underlying \"machinery\" here, that should all\n7 be defined elsewhere.\n8 \n9 A lot of the inspiration for this sql grammar is taken from the cockroach\n10 labs full sql grammar. In particular their way for dividing up the expression\n11 grammar. Check out their docs, they're awesome.\n12 https://www.cockroachlabs.com/docs/stable/sql-grammar.html#select_stmt\n13 \"\"\"\n14 \n15 from typing import Generator, List, Tuple, NamedTuple, Optional\n16 \n17 from sqlfluff.core.parser import (\n18     Matchable,\n19     BaseSegment,\n20     KeywordSegment,\n21     SymbolSegment,\n22     ReSegment,\n23     NamedSegment,\n24     Sequence,\n25     GreedyUntil,\n26     StartsWith,\n27     OneOf,\n28     Delimited,\n29     Bracketed,\n30     AnyNumberOf,\n31     Ref,\n32     SegmentGenerator,\n33     Anything,\n34     Indent,\n35     Dedent,\n36     Nothing,\n37     OptionallyBracketed,\n38 )\n39 \n40 from sqlfluff.core.dialects.base import Dialect\n41 from sqlfluff.core.dialects.ansi_keywords import (\n42     ansi_reserved_keywords,\n43     ansi_unreserved_keywords,\n44 )\n45 \n46 \n47 ansi_dialect = Dialect(\"ansi\", root_segment_name=\"FileSegment\")\n48 \n49 \n50 ansi_dialect.set_lexer_struct(\n51     [\n52         # name, type, pattern, kwargs\n53         (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n54         (\n55             \"inline_comment\",\n56             \"regex\",\n57             r\"(--|#)[^\\n]*\",\n58             dict(is_comment=True, type=\"comment\", trim_start=(\"--\", \"#\")),\n59         ),\n60         (\n61             \"block_comment\",\n62             \"regex\",\n63             r\"\\/\\*([^\\*]|\\*(?!\\/))*\\*\\/\",\n64             dict(\n65                 is_comment=True,\n66                 type=\"comment\",\n67                 subdivide=dict(type=\"newline\", name=\"newline\", regex=r\"\\r\\n|\\n\"),\n68                 trim_post_subdivide=dict(\n69                     type=\"whitespace\", name=\"whitespace\", regex=r\"[\\t ]+\"\n70                 ),\n71             ),\n72         ),\n73         # Matches 0 or more characters surrounded by quotes that (aren't a quote or backslash) or a sequence of backslash followed by any character, aka an escaped character.\n74         (\"single_quote\", \"regex\", r\"'([^'\\\\]|\\\\.)*'\", dict(is_code=True)),\n75         (\"double_quote\", \"regex\", r'\"([^\"\\\\]|\\\\.)*\"', dict(is_code=True)),\n76         (\"back_quote\", \"regex\", r\"`[^`]*`\", dict(is_code=True)),\n77         (\n78             \"numeric_literal\",\n79             \"regex\",\n80             r\"([0-9]+(\\.[0-9]+)?)|(\\.[0-9]+)\",\n81             dict(is_code=True),\n82         ),\n83         (\"not_equal\", \"regex\", r\"!=|<>\", dict(is_code=True)),\n84         (\"greater_than_or_equal\", \"regex\", r\">=\", dict(is_code=True)),\n85         (\"less_than_or_equal\", \"regex\", r\"<=\", dict(is_code=True)),\n86         (\"newline\", \"regex\", r\"\\r\\n|\\n\", dict(type=\"newline\")),\n87         (\"casting_operator\", \"regex\", r\"::\", dict(is_code=True)),\n88         (\"concat_operator\", \"regex\", r\"\\|\\|\", dict(is_code=True)),\n89         (\"equals\", \"singleton\", \"=\", dict(is_code=True)),\n90         (\"greater_than\", \"singleton\", \">\", dict(is_code=True)),\n91         (\"less_than\", \"singleton\", \"<\", dict(is_code=True)),\n92         (\"dot\", \"singleton\", \".\", dict(is_code=True)),\n93         (\"comma\", \"singleton\", \",\", dict(is_code=True, type=\"comma\")),\n94         (\"plus\", \"singleton\", \"+\", dict(is_code=True)),\n95         (\"tilde\", \"singleton\", \"~\", dict(is_code=True)),\n96         (\"minus\", \"singleton\", \"-\", dict(is_code=True)),\n97         (\"divide\", \"singleton\", \"/\", dict(is_code=True)),\n98         (\"percent\", \"singleton\", \"%\", dict(is_code=True)),\n99         (\"star\", \"singleton\", \"*\", dict(is_code=True)),\n100         (\"bracket_open\", \"singleton\", \"(\", dict(is_code=True)),\n101         (\"bracket_close\", \"singleton\", \")\", dict(is_code=True)),\n102         (\"sq_bracket_open\", \"singleton\", \"[\", dict(is_code=True)),\n103         (\"sq_bracket_close\", \"singleton\", \"]\", dict(is_code=True)),\n104         (\"crly_bracket_open\", \"singleton\", \"{\", dict(is_code=True)),\n105         (\"crly_bracket_close\", \"singleton\", \"}\", dict(is_code=True)),\n106         (\"colon\", \"singleton\", \":\", dict(is_code=True)),\n107         (\"semicolon\", \"singleton\", \";\", dict(is_code=True)),\n108         (\"code\", \"regex\", r\"[0-9a-zA-Z_]*\", dict(is_code=True)),\n109     ]\n110 )\n111 \n112 # Set the bare functions\n113 ansi_dialect.sets(\"bare_functions\").update(\n114     [\"current_timestamp\", \"current_time\", \"current_date\"]\n115 )\n116 \n117 \n118 # Set the datetime units\n119 ansi_dialect.sets(\"datetime_units\").update(\n120     [\n121         \"DAY\",\n122         \"DAYOFYEAR\",\n123         \"HOUR\",\n124         \"MILLISECOND\",\n125         \"MINUTE\",\n126         \"MONTH\",\n127         \"QUARTER\",\n128         \"SECOND\",\n129         \"WEEK\",\n130         \"WEEKDAY\",\n131         \"YEAR\",\n132     ]\n133 )\n134 \n135 # Set Keywords\n136 ansi_dialect.sets(\"unreserved_keywords\").update(\n137     [n.strip().upper() for n in ansi_unreserved_keywords.split(\"\\n\")]\n138 )\n139 \n140 ansi_dialect.sets(\"reserved_keywords\").update(\n141     [n.strip().upper() for n in ansi_reserved_keywords.split(\"\\n\")]\n142 )\n143 \n144 # Bracket pairs (a set of tuples).\n145 # (name, startref, endref, definitely_bracket)\n146 ansi_dialect.sets(\"bracket_pairs\").update(\n147     [\n148         (\"round\", \"StartBracketSegment\", \"EndBracketSegment\", True),\n149         (\"square\", \"StartSquareBracketSegment\", \"EndSquareBracketSegment\", True),\n150         (\"curly\", \"StartCurlyBracketSegment\", \"EndCurlyBracketSegment\", True),\n151     ]\n152 )\n153 \n154 # Set the value table functions. These are functions that, if they appear as\n155 # an item in \"FROM', are treated as returning a COLUMN, not a TABLE. Apparently,\n156 # among dialects supported by SQLFluff, only BigQuery has this concept, but this\n157 # set is defined in the ANSI dialect because:\n158 # - It impacts core linter rules (see L020 and several other rules that subclass\n159 #   from it) and how they interpret the contents of table_expressions\n160 # - At least one other database (DB2) has the same value table function,\n161 #   UNNEST(), as BigQuery. DB2 is not currently supported by SQLFluff.\n162 ansi_dialect.sets(\"value_table_functions\").update([])\n163 \n164 \n165 ansi_dialect.add(\n166     # Real segments\n167     SemicolonSegment=SymbolSegment.make(\n168         \";\", name=\"semicolon\", type=\"statement_terminator\"\n169     ),\n170     ColonSegment=SymbolSegment.make(\":\", name=\"colon\", type=\"colon\"),\n171     SliceSegment=SymbolSegment.make(\":\", name=\"slice\", type=\"slice\"),\n172     StartBracketSegment=SymbolSegment.make(\n173         \"(\", name=\"start_bracket\", type=\"start_bracket\"\n174     ),\n175     EndBracketSegment=SymbolSegment.make(\")\", name=\"end_bracket\", type=\"end_bracket\"),\n176     StartSquareBracketSegment=SymbolSegment.make(\n177         \"[\", name=\"start_square_bracket\", type=\"start_square_bracket\"\n178     ),\n179     EndSquareBracketSegment=SymbolSegment.make(\n180         \"]\", name=\"end_square_bracket\", type=\"end_square_bracket\"\n181     ),\n182     StartCurlyBracketSegment=SymbolSegment.make(\n183         \"{\", name=\"start_curly_bracket\", type=\"start_curly_bracket\"\n184     ),\n185     EndCurlyBracketSegment=SymbolSegment.make(\n186         \"}\", name=\"end_curly_bracket\", type=\"end_curly_bracket\"\n187     ),\n188     CommaSegment=SymbolSegment.make(\",\", name=\"comma\", type=\"comma\"),\n189     DotSegment=SymbolSegment.make(\".\", name=\"dot\", type=\"dot\"),\n190     StarSegment=SymbolSegment.make(\"*\", name=\"star\", type=\"star\"),\n191     TildeSegment=SymbolSegment.make(\"~\", name=\"tilde\", type=\"tilde\"),\n192     CastOperatorSegment=SymbolSegment.make(\n193         \"::\", name=\"casting_operator\", type=\"casting_operator\"\n194     ),\n195     PlusSegment=SymbolSegment.make(\"+\", name=\"plus\", type=\"binary_operator\"),\n196     MinusSegment=SymbolSegment.make(\"-\", name=\"minus\", type=\"binary_operator\"),\n197     PositiveSegment=SymbolSegment.make(\"+\", name=\"positive\", type=\"sign_indicator\"),\n198     NegativeSegment=SymbolSegment.make(\"-\", name=\"negative\", type=\"sign_indicator\"),\n199     DivideSegment=SymbolSegment.make(\"/\", name=\"divide\", type=\"binary_operator\"),\n200     MultiplySegment=SymbolSegment.make(\"*\", name=\"multiply\", type=\"binary_operator\"),\n201     ModuloSegment=SymbolSegment.make(\"%\", name=\"modulo\", type=\"binary_operator\"),\n202     ConcatSegment=SymbolSegment.make(\"||\", name=\"concatenate\", type=\"binary_operator\"),\n203     EqualsSegment=SymbolSegment.make(\"=\", name=\"equals\", type=\"comparison_operator\"),\n204     GreaterThanSegment=SymbolSegment.make(\n205         \">\", name=\"greater_than\", type=\"comparison_operator\"\n206     ),\n207     LessThanSegment=SymbolSegment.make(\n208         \"<\", name=\"less_than\", type=\"comparison_operator\"\n209     ),\n210     GreaterThanOrEqualToSegment=SymbolSegment.make(\n211         \">=\", name=\"greater_than_equal_to\", type=\"comparison_operator\"\n212     ),\n213     LessThanOrEqualToSegment=SymbolSegment.make(\n214         \"<=\", name=\"less_than_equal_to\", type=\"comparison_operator\"\n215     ),\n216     NotEqualToSegment_a=SymbolSegment.make(\n217         \"!=\", name=\"not_equal_to\", type=\"comparison_operator\"\n218     ),\n219     NotEqualToSegment_b=SymbolSegment.make(\n220         \"<>\", name=\"not_equal_to\", type=\"comparison_operator\"\n221     ),\n222     # The following functions can be called without parentheses per ANSI specification\n223     BareFunctionSegment=SegmentGenerator(\n224         lambda dialect: ReSegment.make(\n225             r\"^(\" + r\"|\".join(dialect.sets(\"bare_functions\")) + r\")$\",\n226             name=\"bare_function\",\n227             type=\"bare_function\",\n228         )\n229     ),\n230     # The strange regex here it to make sure we don't accidentally match numeric literals. We\n231     # also use a regex to explicitly exclude disallowed keywords.\n232     NakedIdentifierSegment=SegmentGenerator(\n233         # Generate the anti template from the set of reserved keywords\n234         lambda dialect: ReSegment.make(\n235             r\"[A-Z0-9_]*[A-Z][A-Z0-9_]*\",\n236             name=\"naked_identifier\",\n237             type=\"identifier\",\n238             _anti_template=r\"^(\" + r\"|\".join(dialect.sets(\"reserved_keywords\")) + r\")$\",\n239         )\n240     ),\n241     VersionIdentifierSegment=ReSegment.make(\n242         r\"[A-Z0-9_.]*\", name=\"version\", type=\"identifier\"\n243     ),\n244     ParameterNameSegment=ReSegment.make(\n245         r\"[A-Z][A-Z0-9_]*\", name=\"parameter\", type=\"parameter\"\n246     ),\n247     FunctionNameSegment=ReSegment.make(\n248         r\"[A-Z][A-Z0-9_]*\", name=\"function_name\", type=\"function_name\"\n249     ),\n250     # Maybe data types should be more restrictive?\n251     DatatypeIdentifierSegment=ReSegment.make(\n252         r\"[A-Z][A-Z0-9_]*\", name=\"data_type_identifier\", type=\"data_type_identifier\"\n253     ),\n254     # Ansi Intervals\n255     DatetimeUnitSegment=SegmentGenerator(\n256         lambda dialect: ReSegment.make(\n257             r\"^(\" + r\"|\".join(dialect.sets(\"datetime_units\")) + r\")$\",\n258             name=\"date_part\",\n259             type=\"date_part\",\n260         )\n261     ),\n262     QuotedIdentifierSegment=NamedSegment.make(\n263         \"double_quote\", name=\"quoted_identifier\", type=\"identifier\"\n264     ),\n265     QuotedLiteralSegment=NamedSegment.make(\n266         \"single_quote\", name=\"quoted_literal\", type=\"literal\"\n267     ),\n268     NumericLiteralSegment=NamedSegment.make(\n269         \"numeric_literal\", name=\"numeric_literal\", type=\"literal\"\n270     ),\n271     # NullSegment is defined seperately to the keyword so we can give it a different type\n272     NullLiteralSegment=KeywordSegment.make(\"null\", name=\"null_literal\", type=\"literal\"),\n273     TrueSegment=KeywordSegment.make(\"true\", name=\"boolean_literal\", type=\"literal\"),\n274     FalseSegment=KeywordSegment.make(\"false\", name=\"boolean_literal\", type=\"literal\"),\n275     # We use a GRAMMAR here not a Segment. Otherwise we get an unnecessary layer\n276     SingleIdentifierGrammar=OneOf(\n277         Ref(\"NakedIdentifierSegment\"), Ref(\"QuotedIdentifierSegment\")\n278     ),\n279     BooleanLiteralGrammar=OneOf(Ref(\"TrueSegment\"), Ref(\"FalseSegment\")),\n280     # We specifically define a group of arithmetic operators to make it easier to override this\n281     # if some dialects have different available operators\n282     ArithmeticBinaryOperatorGrammar=OneOf(\n283         Ref(\"PlusSegment\"),\n284         Ref(\"MinusSegment\"),\n285         Ref(\"DivideSegment\"),\n286         Ref(\"MultiplySegment\"),\n287         Ref(\"ModuloSegment\"),\n288     ),\n289     StringBinaryOperatorGrammar=OneOf(Ref(\"ConcatSegment\")),\n290     BooleanBinaryOperatorGrammar=OneOf(\n291         Ref(\"AndKeywordSegment\"), Ref(\"OrKeywordSegment\")\n292     ),\n293     ComparisonOperatorGrammar=OneOf(\n294         Ref(\"EqualsSegment\"),\n295         Ref(\"GreaterThanSegment\"),\n296         Ref(\"LessThanSegment\"),\n297         Ref(\"GreaterThanOrEqualToSegment\"),\n298         Ref(\"LessThanOrEqualToSegment\"),\n299         Ref(\"NotEqualToSegment_a\"),\n300         Ref(\"NotEqualToSegment_b\"),\n301     ),\n302     # hookpoint for other dialects\n303     # e.g. EXASOL str to date cast with DATE '2021-01-01'\n304     DateTimeLiteralGrammar=Nothing(),\n305     LiteralGrammar=OneOf(\n306         Ref(\"QuotedLiteralSegment\"),\n307         Ref(\"NumericLiteralSegment\"),\n308         Ref(\"BooleanLiteralGrammar\"),\n309         Ref(\"QualifiedNumericLiteralSegment\"),\n310         # NB: Null is included in the literals, because it is a keyword which\n311         # can otherwise be easily mistaken for an identifier.\n312         Ref(\"NullLiteralSegment\"),\n313         Ref(\"DateTimeLiteralGrammar\"),\n314     ),\n315     AndKeywordSegment=KeywordSegment.make(\"and\", type=\"binary_operator\"),\n316     OrKeywordSegment=KeywordSegment.make(\"or\", type=\"binary_operator\"),\n317     # This is a placeholder for other dialects.\n318     PreTableFunctionKeywordsGrammar=Nothing(),\n319     BinaryOperatorGrammar=OneOf(\n320         Ref(\"ArithmeticBinaryOperatorGrammar\"),\n321         Ref(\"StringBinaryOperatorGrammar\"),\n322         Ref(\"BooleanBinaryOperatorGrammar\"),\n323         Ref(\"ComparisonOperatorGrammar\"),\n324     ),\n325     # This pattern is used in a lot of places.\n326     # Defined here to avoid repetition.\n327     BracketedColumnReferenceListGrammar=Bracketed(\n328         Delimited(\n329             Ref(\"ColumnReferenceSegment\"),\n330             ephemeral_name=\"ColumnReferenceList\",\n331         )\n332     ),\n333     OrReplaceGrammar=Sequence(\"OR\", \"REPLACE\"),\n334     TemporaryTransientGrammar=OneOf(\"TRANSIENT\", Ref(\"TemporaryGrammar\")),\n335     TemporaryGrammar=OneOf(\"TEMP\", \"TEMPORARY\"),\n336     IfExistsGrammar=Sequence(\"IF\", \"EXISTS\"),\n337     IfNotExistsGrammar=Sequence(\"IF\", \"NOT\", \"EXISTS\"),\n338     LikeGrammar=OneOf(\"LIKE\", \"RLIKE\", \"ILIKE\"),\n339     IsClauseGrammar=OneOf(\n340         \"NULL\",\n341         \"NAN\",\n342         \"NOTNULL\",\n343         \"ISNULL\",\n344         Ref(\"BooleanLiteralGrammar\"),\n345     ),\n346     FromClauseTerminatorGrammar=OneOf(\n347         \"WHERE\",\n348         \"LIMIT\",\n349         \"GROUP\",\n350         \"ORDER\",\n351         \"HAVING\",\n352         \"QUALIFY\",\n353         \"WINDOW\",\n354         Ref(\"SetOperatorSegment\"),\n355         Ref(\"WithNoSchemaBindingClauseSegment\"),\n356     ),\n357     WhereClauseTerminatorGrammar=OneOf(\n358         \"LIMIT\", \"GROUP\", \"ORDER\", \"HAVING\", \"QUALIFY\", \"WINDOW\"\n359     ),\n360     PrimaryKeyGrammar=Sequence(\"PRIMARY\", \"KEY\"),\n361 )\n362 \n363 \n364 @ansi_dialect.segment()\n365 class FileSegment(BaseSegment):\n366     \"\"\"A segment representing a whole file or script.\n367 \n368     This is also the default \"root\" segment of the dialect,\n369     and so is usually instantiated directly. It therefore\n370     has no match_grammar.\n371     \"\"\"\n372 \n373     type = \"file\"\n374     # The file segment is the only one which can start or end with non-code\n375     can_start_end_non_code = True\n376     # A file can be empty!\n377     allow_empty = True\n378 \n379     # NB: We don't need a match_grammar here because we're\n380     # going straight into instantiating it directly usually.\n381     parse_grammar = Delimited(\n382         Ref(\"StatementSegment\"),\n383         delimiter=Ref(\"SemicolonSegment\"),\n384         allow_gaps=True,\n385         allow_trailing=True,\n386     )\n387 \n388     def get_table_references(self):\n389         \"\"\"Use parsed tree to extract table references.\"\"\"\n390         references = set()\n391         for stmt in self.get_children(\"statement\"):\n392             references |= stmt.get_table_references()\n393         return references\n394 \n395 \n396 @ansi_dialect.segment()\n397 class IntervalExpressionSegment(BaseSegment):\n398     \"\"\"An interval expression segment.\"\"\"\n399 \n400     type = \"interval_expression\"\n401     match_grammar = Sequence(\n402         \"INTERVAL\",\n403         OneOf(\n404             # The Numeric Version\n405             Sequence(\n406                 Ref(\"NumericLiteralSegment\"),\n407                 OneOf(Ref(\"QuotedLiteralSegment\"), Ref(\"DatetimeUnitSegment\")),\n408             ),\n409             # The String version\n410             Ref(\"QuotedLiteralSegment\"),\n411         ),\n412     )\n413 \n414 \n415 @ansi_dialect.segment()\n416 class ArrayLiteralSegment(BaseSegment):\n417     \"\"\"An array literal segment.\"\"\"\n418 \n419     type = \"array_literal_type\"\n420     match_grammar = Bracketed(\n421         Delimited(Ref(\"ExpressionSegment\")),\n422         bracket_type=\"square\",\n423     )\n424 \n425 \n426 @ansi_dialect.segment()\n427 class DatatypeSegment(BaseSegment):\n428     \"\"\"A data type segment.\"\"\"\n429 \n430     type = \"data_type\"\n431     match_grammar = Sequence(\n432         Sequence(\n433             # Some dialects allow optional qualification of data types with schemas\n434             Sequence(\n435                 Ref(\"SingleIdentifierGrammar\"),\n436                 Ref(\"DotSegment\"),\n437                 allow_gaps=False,\n438                 optional=True,\n439             ),\n440             Ref(\"DatatypeIdentifierSegment\"),\n441             allow_gaps=False,\n442         ),\n443         Bracketed(\n444             OneOf(\n445                 Delimited(Ref(\"ExpressionSegment\")),\n446                 # The brackets might be empty for some cases...\n447                 optional=True,\n448             ),\n449             # There may be no brackets for some data types\n450             optional=True,\n451         ),\n452         Ref(\"CharCharacterSetSegment\", optional=True),\n453     )\n454 \n455 \n456 # hookpoint\n457 ansi_dialect.add(CharCharacterSetSegment=Nothing())\n458 \n459 \n460 @ansi_dialect.segment()\n461 class ObjectReferenceSegment(BaseSegment):\n462     \"\"\"A reference to an object.\"\"\"\n463 \n464     type = \"object_reference\"\n465     # match grammar (don't allow whitespace)\n466     match_grammar: Matchable = Delimited(\n467         Ref(\"SingleIdentifierGrammar\"),\n468         delimiter=OneOf(Ref(\"DotSegment\"), Sequence(Ref(\"DotSegment\"))),\n469         terminator=OneOf(\n470             \"ON\",\n471             \"AS\",\n472             \"USING\",\n473             Ref(\"CommaSegment\"),\n474             Ref(\"CastOperatorSegment\"),\n475             Ref(\"StartSquareBracketSegment\"),\n476             Ref(\"StartBracketSegment\"),\n477             Ref(\"BinaryOperatorGrammar\"),\n478             Ref(\"ColonSegment\"),\n479             Ref(\"SemicolonSegment\"),\n480         ),\n481         allow_gaps=False,\n482     )\n483 \n484     class ObjectReferencePart(NamedTuple):\n485         \"\"\"Details about a table alias.\"\"\"\n486 \n487         part: str  # Name of the part\n488         segment: BaseSegment  # Segment containing the part\n489 \n490     @classmethod\n491     def _iter_reference_parts(cls, elem) -> Generator[ObjectReferencePart, None, None]:\n492         \"\"\"Extract the elements of a reference and yield.\"\"\"\n493         # trim on quotes and split out any dots.\n494         for part in elem.raw_trimmed().split(\".\"):\n495             yield cls.ObjectReferencePart(part, elem)\n496 \n497     def iter_raw_references(self) -> Generator[ObjectReferencePart, None, None]:\n498         \"\"\"Generate a list of reference strings and elements.\n499 \n500         Each element is a tuple of (str, segment). If some are\n501         split, then a segment may appear twice, but the substring\n502         will only appear once.\n503         \"\"\"\n504         # Extract the references from those identifiers (because some may be quoted)\n505         for elem in self.recursive_crawl(\"identifier\"):\n506             yield from self._iter_reference_parts(elem)\n507 \n508     def is_qualified(self):\n509         \"\"\"Return if there is more than one element to the reference.\"\"\"\n510         return len(list(self.iter_raw_references())) > 1\n511 \n512     def qualification(self):\n513         \"\"\"Return the qualification type of this reference.\"\"\"\n514         return \"qualified\" if self.is_qualified() else \"unqualified\"\n515 \n516     def extract_reference(self, level: int) -> Optional[ObjectReferencePart]:\n517         \"\"\"Extract a reference of a given level.\n518 \n519         e.g. level 1 = the object.\n520         level 2 = the table\n521         level 3 = the schema\n522         etc...\n523         \"\"\"\n524         refs = list(self.iter_raw_references())\n525         if len(refs) >= level:\n526             return refs[-level]\n527         return None\n528 \n529 \n530 @ansi_dialect.segment()\n531 class TableReferenceSegment(ObjectReferenceSegment):\n532     \"\"\"A reference to an table, CTE, subquery or alias.\"\"\"\n533 \n534     type = \"table_reference\"\n535 \n536 \n537 @ansi_dialect.segment()\n538 class SchemaReferenceSegment(ObjectReferenceSegment):\n539     \"\"\"A reference to a schema.\"\"\"\n540 \n541     type = \"schema_reference\"\n542 \n543 \n544 @ansi_dialect.segment()\n545 class DatabaseReferenceSegment(ObjectReferenceSegment):\n546     \"\"\"A reference to a database.\"\"\"\n547 \n548     type = \"database_reference\"\n549 \n550 \n551 @ansi_dialect.segment()\n552 class IndexReferenceSegment(ObjectReferenceSegment):\n553     \"\"\"A reference to an index.\"\"\"\n554 \n555     type = \"index_reference\"\n556 \n557 \n558 @ansi_dialect.segment()\n559 class ExtensionReferenceSegment(ObjectReferenceSegment):\n560     \"\"\"A reference to an extension.\"\"\"\n561 \n562     type = \"extension_reference\"\n563 \n564 \n565 @ansi_dialect.segment()\n566 class ColumnReferenceSegment(ObjectReferenceSegment):\n567     \"\"\"A reference to column, field or alias.\"\"\"\n568 \n569     type = \"column_reference\"\n570 \n571 \n572 @ansi_dialect.segment()\n573 class SingleIdentifierListSegment(BaseSegment):\n574     \"\"\"A comma delimited list of identifiers.\"\"\"\n575 \n576     type = \"identifier_list\"\n577     match_grammar = Delimited(Ref(\"SingleIdentifierGrammar\"))\n578 \n579 \n580 @ansi_dialect.segment()\n581 class ArrayAccessorSegment(BaseSegment):\n582     \"\"\"An array accessor e.g. [3:4].\"\"\"\n583 \n584     type = \"array_accessor\"\n585     match_grammar = Bracketed(\n586         Delimited(\n587             OneOf(Ref(\"NumericLiteralSegment\"), Ref(\"ExpressionSegment\")),\n588             delimiter=Ref(\"SliceSegment\"),\n589             ephemeral_name=\"ArrayAccessorContent\",\n590         ),\n591         bracket_type=\"square\",\n592     )\n593 \n594 \n595 @ansi_dialect.segment()\n596 class AliasedObjectReferenceSegment(BaseSegment):\n597     \"\"\"A reference to an object with an `AS` clause.\"\"\"\n598 \n599     type = \"object_reference\"\n600     match_grammar = Sequence(\n601         Ref(\"ObjectReferenceSegment\"), Ref(\"AliasExpressionSegment\")\n602     )\n603 \n604 \n605 @ansi_dialect.segment()\n606 class AliasedTableReferenceSegment(BaseSegment):\n607     \"\"\"A reference to a table with an `AS` clause.\"\"\"\n608 \n609     type = \"table_reference\"\n610     match_grammar = Sequence(\n611         Ref(\"TableReferenceSegment\"), Ref(\"AliasExpressionSegment\", optional=True)\n612     )\n613 \n614 \n615 @ansi_dialect.segment()\n616 class AliasExpressionSegment(BaseSegment):\n617     \"\"\"A reference to an object with an `AS` clause.\n618 \n619     The optional AS keyword allows both implicit and explicit aliasing.\n620     \"\"\"\n621 \n622     type = \"alias_expression\"\n623     match_grammar = Sequence(\n624         Ref.keyword(\"AS\", optional=True),\n625         OneOf(\n626             Sequence(\n627                 Ref(\"SingleIdentifierGrammar\"),\n628                 # Column alias in VALUES clause\n629                 Bracketed(Ref(\"SingleIdentifierListSegment\"), optional=True),\n630             ),\n631             Ref(\"QuotedLiteralSegment\"),\n632         ),\n633     )\n634 \n635 \n636 @ansi_dialect.segment()\n637 class ShorthandCastSegment(BaseSegment):\n638     \"\"\"A casting operation using '::'.\"\"\"\n639 \n640     type = \"cast_expression\"\n641     match_grammar = Sequence(\n642         Ref(\"CastOperatorSegment\"), Ref(\"DatatypeSegment\"), allow_gaps=False\n643     )\n644 \n645 \n646 @ansi_dialect.segment()\n647 class QualifiedNumericLiteralSegment(BaseSegment):\n648     \"\"\"A numeric literal with a + or - sign preceding.\n649 \n650     The qualified numeric literal is a compound of a raw\n651     literal and a plus/minus sign. We do it this way rather\n652     than at the lexing step because the lexer doesn't deal\n653     well with ambiguity.\n654     \"\"\"\n655 \n656     type = \"numeric_literal\"\n657     match_grammar = Sequence(\n658         OneOf(Ref(\"PlusSegment\"), Ref(\"MinusSegment\")),\n659         Ref(\"NumericLiteralSegment\"),\n660         allow_gaps=False,\n661     )\n662 \n663 \n664 ansi_dialect.add(\n665     # FunctionContentsExpressionGrammar intended as a hook to override\n666     # in other dialects.\n667     FunctionContentsExpressionGrammar=Ref(\"ExpressionSegment\"),\n668     FunctionContentsGrammar=AnyNumberOf(\n669         # A Cast-like function\n670         Sequence(Ref(\"ExpressionSegment\"), \"AS\", Ref(\"DatatypeSegment\")),\n671         # An extract-like or substring-like function\n672         Sequence(\n673             OneOf(Ref(\"DatetimeUnitSegment\"), Ref(\"ExpressionSegment\")),\n674             \"FROM\",\n675             Ref(\"ExpressionSegment\"),\n676         ),\n677         Sequence(\n678             # Allow an optional distinct keyword here.\n679             Ref.keyword(\"DISTINCT\", optional=True),\n680             OneOf(\n681                 # Most functions will be using the delimited route\n682                 # but for COUNT(*) or similar we allow the star segment\n683                 # here.\n684                 Ref(\"StarSegment\"),\n685                 Delimited(Ref(\"FunctionContentsExpressionGrammar\")),\n686             ),\n687         ),\n688         Ref(\n689             \"OrderByClauseSegment\"\n690         ),  # used by string_agg (postgres), group_concat (exasol), listagg (snowflake)...\n691         Sequence(Ref.keyword(\"SEPARATOR\"), Ref(\"LiteralGrammar\")),\n692         # like a function call: POSITION ( 'QL' IN 'SQL')\n693         Sequence(\n694             OneOf(Ref(\"QuotedLiteralSegment\"), Ref(\"SingleIdentifierGrammar\")),\n695             \"IN\",\n696             OneOf(Ref(\"QuotedLiteralSegment\"), Ref(\"SingleIdentifierGrammar\")),\n697         ),\n698     ),\n699     # Optional OVER suffix for window functions.\n700     # This is supported in biquery & postgres (and its derivatives)\n701     # and so is included here for now.\n702     PostFunctionGrammar=Sequence(\n703         Sequence(OneOf(\"IGNORE\", \"RESPECT\"), \"NULLS\", optional=True),\n704         Ref(\"OverClauseSegment\"),\n705     ),\n706 )\n707 \n708 \n709 @ansi_dialect.segment()\n710 class OverClauseSegment(BaseSegment):\n711     \"\"\"An OVER clause for window functions.\"\"\"\n712 \n713     type = \"over_clause\"\n714     match_grammar = Sequence(\n715         \"OVER\",\n716         OneOf(\n717             Ref(\"SingleIdentifierGrammar\"),  # Window name\n718             Bracketed(\n719                 Ref(\"WindowSpecificationSegment\"),\n720             ),\n721         ),\n722     )\n723 \n724 \n725 @ansi_dialect.segment()\n726 class WindowSpecificationSegment(BaseSegment):\n727     \"\"\"Window specification, e.g. OVER() or named window.\"\"\"\n728 \n729     type = \"window_specification\"\n730     match_grammar = Sequence(\n731         Ref(\"SingleIdentifierGrammar\", optional=True),  # \"Base\" window name\n732         Ref(\"PartitionClauseSegment\", optional=True),\n733         Ref(\"OrderByClauseSegment\", optional=True),\n734         Ref(\"FrameClauseSegment\", optional=True),\n735         optional=True,\n736         ephemeral_name=\"OverClauseContent\",\n737     )\n738 \n739 \n740 @ansi_dialect.segment()\n741 class FunctionSegment(BaseSegment):\n742     \"\"\"A scalar or aggregate function.\n743 \n744     Maybe in the future we should distinguish between\n745     aggregate functions and other functions. For now\n746     we treat them the same because they look the same\n747     for our purposes.\n748     \"\"\"\n749 \n750     type = \"function\"\n751     match_grammar = Sequence(\n752         Sequence(\n753             Sequence(\n754                 # a stored function could be accessed by schema identifier\n755                 Ref(\"SingleIdentifierGrammar\"),\n756                 Ref(\"DotSegment\"),\n757                 optional=True,\n758             ),\n759             Ref(\"FunctionNameSegment\"),\n760             Bracketed(\n761                 Ref(\n762                     \"FunctionContentsGrammar\",\n763                     # The brackets might be empty for some functions...\n764                     optional=True,\n765                     ephemeral_name=\"FunctionContentsGrammar\",\n766                 )\n767             ),\n768         ),\n769         Ref(\"PostFunctionGrammar\", optional=True),\n770     )\n771 \n772 \n773 @ansi_dialect.segment()\n774 class PartitionClauseSegment(BaseSegment):\n775     \"\"\"A `PARTITION BY` for window functions.\"\"\"\n776 \n777     type = \"partitionby_clause\"\n778     match_grammar = StartsWith(\n779         \"PARTITION\",\n780         terminator=OneOf(\"ORDER\", \"ROWS\"),\n781         enforce_whitespace_preceeding_terminator=True,\n782     )\n783     parse_grammar = Sequence(\n784         \"PARTITION\",\n785         \"BY\",\n786         Indent,\n787         OneOf(\n788             # Brackets are optional in a partition by statement\n789             Bracketed(Delimited(Ref(\"ExpressionSegment\"))),\n790             Delimited(Ref(\"ExpressionSegment\")),\n791         ),\n792         Dedent,\n793     )\n794 \n795 \n796 @ansi_dialect.segment()\n797 class FrameClauseSegment(BaseSegment):\n798     \"\"\"A frame clause for window functions.\"\"\"\n799 \n800     type = \"frame_clause\"\n801     match_grammar = StartsWith(\"ROWS\")\n802     # TODO: Expand a parse statement here properly to actually\n803     # parse rather than assuming that it's good.\n804     # parse_grammar = Sequence(\n805     #    'ROWS',\n806     #    ...\n807     # )\n808 \n809 \n810 ansi_dialect.add(\n811     # This is a hook point to allow subclassing for other dialects\n812     PostTableExpressionGrammar=Nothing()\n813 )\n814 \n815 \n816 class AliasInfo(NamedTuple):\n817     \"\"\"Details about a table alias.\"\"\"\n818 \n819     ref_str: str  # Name given to the alias\n820     segment: BaseSegment  # Identifier segment containing the name\n821     aliased: bool\n822     table_expression: BaseSegment\n823     alias_expression: Optional[BaseSegment]\n824     object_reference: Optional[BaseSegment]\n825 \n826 \n827 @ansi_dialect.segment()\n828 class TableExpressionSegment(BaseSegment):\n829     \"\"\"A table expression.\"\"\"\n830 \n831     type = \"table_expression\"\n832     match_grammar = Sequence(\n833         Indent,\n834         Ref(\"PreTableFunctionKeywordsGrammar\", optional=True),\n835         OneOf(\n836             Ref(\"MainTableExpressionSegment\"),\n837             Bracketed(Ref(\"MainTableExpressionSegment\")),\n838         ),\n839         Ref(\"AliasExpressionSegment\", optional=True),\n840         Ref(\"PostTableExpressionGrammar\", optional=True),\n841         Dedent.when(indented_joins=False),\n842         AnyNumberOf(\n843             Ref(\"JoinClauseSegment\"), Ref(\"JoinLikeClauseGrammar\"), optional=True\n844         ),\n845         Dedent.when(indented_joins=True),\n846     )\n847 \n848     def get_eventual_alias(self) -> Optional[AliasInfo]:\n849         \"\"\"Return the eventual table name referred to by this table expression.\n850 \n851         Returns:\n852             :obj:`tuple` of (:obj:`str`, :obj:`BaseSegment`, :obj:`bool`) containing\n853                 a string representation of the alias, a reference to the\n854                 segment containing it, and whether it's an alias.\n855 \n856         \"\"\"\n857         alias_expression = self.get_child(\"alias_expression\")\n858         ref = self.get_child(\"main_table_expression\").get_child(\"object_reference\")\n859         if alias_expression:\n860             # If it has an alias, return that\n861             segment = alias_expression.get_child(\"identifier\")\n862             return AliasInfo(segment.raw, segment, True, self, alias_expression, ref)\n863 \n864         # If not return the object name (or None if there isn't one)\n865         # ref = self.get_child(\"object_reference\")\n866         if ref:\n867             # Return the last element of the reference, which\n868             # will already be a tuple.\n869             penultimate_ref = list(ref.iter_raw_references())[-1]\n870             return AliasInfo(\n871                 penultimate_ref[0], penultimate_ref[1], False, self, None, ref\n872             )\n873         # No references or alias, return None\n874         return None\n875 \n876 \n877 @ansi_dialect.segment()\n878 class MainTableExpressionSegment(BaseSegment):\n879     \"\"\"The main table expression e.g. within a FROM clause.\"\"\"\n880 \n881     type = \"main_table_expression\"\n882     match_grammar = OneOf(\n883         Ref(\"BareFunctionSegment\"),\n884         Ref(\"FunctionSegment\"),\n885         Ref(\"TableReferenceSegment\"),\n886         # Nested Selects\n887         Bracketed(Ref(\"SelectableGrammar\")),\n888         # Values clause?\n889     )\n890 \n891 \n892 @ansi_dialect.segment()\n893 class WildcardIdentifierSegment(ObjectReferenceSegment):\n894     \"\"\"Any identifier of the form a.b.*.\n895 \n896     This inherits iter_raw_references from the\n897     ObjectReferenceSegment.\n898     \"\"\"\n899 \n900     type = \"wildcard_identifier\"\n901     match_grammar = Sequence(\n902         # *, blah.*, blah.blah.*, etc.\n903         AnyNumberOf(\n904             Sequence(Ref(\"SingleIdentifierGrammar\"), Ref(\"DotSegment\"), allow_gaps=True)\n905         ),\n906         Ref(\"StarSegment\"),\n907         allow_gaps=False,\n908     )\n909 \n910     def iter_raw_references(self):\n911         \"\"\"Generate a list of reference strings and elements.\n912 \n913         Each element is a tuple of (str, segment). If some are\n914         split, then a segment may appear twice, but the substring\n915         will only appear once.\n916         \"\"\"\n917         # Extract the references from those identifiers (because some may be quoted)\n918         for elem in self.recursive_crawl(\"identifier\", \"star\"):\n919             yield from self._iter_reference_parts(elem)\n920 \n921 \n922 @ansi_dialect.segment()\n923 class WildcardExpressionSegment(BaseSegment):\n924     \"\"\"A star (*) expression for a SELECT clause.\n925 \n926     This is separate from the identifier to allow for\n927     some dialects which extend this logic to allow\n928     REPLACE, EXCEPT or similar clauses e.g. BigQuery.\n929     \"\"\"\n930 \n931     type = \"wildcard_expression\"\n932     match_grammar = Sequence(\n933         # *, blah.*, blah.blah.*, etc.\n934         Ref(\"WildcardIdentifierSegment\")\n935     )\n936 \n937 \n938 @ansi_dialect.segment()\n939 class SelectTargetElementSegment(BaseSegment):\n940     \"\"\"An element in the targets of a select statement.\"\"\"\n941 \n942     type = \"select_target_element\"\n943     # Important to split elements before parsing, otherwise debugging is really hard.\n944     match_grammar = GreedyUntil(\n945         \"FROM\",\n946         \"LIMIT\",\n947         Ref(\"CommaSegment\"),\n948         Ref(\"SetOperatorSegment\"),\n949         enforce_whitespace_preceeding_terminator=True,\n950     )\n951 \n952     parse_grammar = OneOf(\n953         # *, blah.*, blah.blah.*, etc.\n954         Ref(\"WildcardExpressionSegment\"),\n955         Sequence(\n956             OneOf(\n957                 Ref(\"LiteralGrammar\"),\n958                 Ref(\"BareFunctionSegment\"),\n959                 Ref(\"FunctionSegment\"),\n960                 Ref(\"IntervalExpressionSegment\"),\n961                 Ref(\"ColumnReferenceSegment\"),\n962                 Ref(\"ExpressionSegment\"),\n963             ),\n964             Ref(\"AliasExpressionSegment\", optional=True),\n965         ),\n966     )\n967 \n968 \n969 @ansi_dialect.segment()\n970 class SelectClauseModifierSegment(BaseSegment):\n971     \"\"\"Things that come after SELECT but before the columns.\"\"\"\n972 \n973     type = \"select_clause_modifier\"\n974     match_grammar = OneOf(\n975         \"DISTINCT\",\n976         \"ALL\",\n977     )\n978 \n979 \n980 @ansi_dialect.segment()\n981 class SelectClauseSegment(BaseSegment):\n982     \"\"\"A group of elements in a select target statement.\"\"\"\n983 \n984     type = \"select_clause\"\n985     match_grammar = StartsWith(\n986         Sequence(\"SELECT\", Ref(\"WildcardExpressionSegment\", optional=True)),\n987         terminator=OneOf(\n988             \"FROM\",\n989             \"LIMIT\",\n990             Ref(\"SetOperatorSegment\"),\n991         ),\n992         enforce_whitespace_preceeding_terminator=True,\n993     )\n994 \n995     parse_grammar = Sequence(\n996         \"SELECT\",\n997         Ref(\"SelectClauseModifierSegment\", optional=True),\n998         Indent,\n999         Delimited(\n1000             Ref(\"SelectTargetElementSegment\"),\n1001             allow_trailing=True,\n1002         ),\n1003         # NB: The Dedent for the indent above lives in the\n1004         # SelectStatementSegment so that it sits in the right\n1005         # place corresponding to the whitespace.\n1006     )\n1007 \n1008 \n1009 @ansi_dialect.segment()\n1010 class JoinClauseSegment(BaseSegment):\n1011     \"\"\"Any number of join clauses, including the `JOIN` keyword.\"\"\"\n1012 \n1013     type = \"join_clause\"\n1014     match_grammar = Sequence(\n1015         # NB These qualifiers are optional\n1016         # TODO: Allow nested joins like:\n1017         # ....FROM S1.T1 t1 LEFT JOIN ( S2.T2 t2 JOIN S3.T3 t3 ON t2.col1=t3.col1) ON tab1.col1 = tab2.col1\n1018         OneOf(\n1019             \"CROSS\",\n1020             \"INNER\",\n1021             Sequence(\n1022                 OneOf(\n1023                     \"FULL\",\n1024                     \"LEFT\",\n1025                     \"RIGHT\",\n1026                 ),\n1027                 Ref.keyword(\"OUTER\", optional=True),\n1028             ),\n1029             optional=True,\n1030         ),\n1031         \"JOIN\",\n1032         Indent,\n1033         Sequence(\n1034             Ref(\"TableExpressionSegment\"),\n1035             # NB: this is optional\n1036             OneOf(\n1037                 # ON clause\n1038                 Ref(\"JoinOnConditionSegment\"),\n1039                 # USING clause\n1040                 Sequence(\n1041                     \"USING\",\n1042                     Indent,\n1043                     Bracketed(\n1044                         # NB: We don't use BracketedColumnReferenceListGrammar\n1045                         # here because we're just using SingleIdentifierGrammar,\n1046                         # rather than ObjectReferenceSegment or ColumnReferenceSegment.\n1047                         # This is a) so that we don't lint it as a reference and\n1048                         # b) because the column will probably be returned anyway\n1049                         # during parsing.\n1050                         Delimited(\n1051                             Ref(\"SingleIdentifierGrammar\"),\n1052                             ephemeral_name=\"UsingClauseContents\",\n1053                         )\n1054                     ),\n1055                     Dedent,\n1056                 ),\n1057                 # Unqualified joins *are* allowed. They just might not\n1058                 # be a good idea.\n1059                 optional=True,\n1060             ),\n1061         ),\n1062         Dedent,\n1063     )\n1064 \n1065     def get_eventual_alias(self) -> AliasInfo:\n1066         \"\"\"Return the eventual table name referred to by this join clause.\"\"\"\n1067         table_expression = self.get_child(\"table_expression\")\n1068         return table_expression.get_eventual_alias()\n1069 \n1070 \n1071 @ansi_dialect.segment()\n1072 class JoinOnConditionSegment(BaseSegment):\n1073     \"\"\"The `ON` condition within a `JOIN` clause.\"\"\"\n1074 \n1075     type = \"join_on_condition\"\n1076     match_grammar = Sequence(\n1077         \"ON\",\n1078         Indent,\n1079         OptionallyBracketed(Ref(\"ExpressionSegment\")),\n1080         Dedent,\n1081     )\n1082 \n1083 \n1084 ansi_dialect.add(\n1085     # This is a hook point to allow subclassing for other dialects\n1086     JoinLikeClauseGrammar=Nothing(),\n1087 )\n1088 \n1089 \n1090 @ansi_dialect.segment()\n1091 class FromClauseSegment(BaseSegment):\n1092     \"\"\"A `FROM` clause like in `SELECT`.\"\"\"\n1093 \n1094     type = \"from_clause\"\n1095     match_grammar = StartsWith(\n1096         \"FROM\",\n1097         terminator=Ref(\"FromClauseTerminatorGrammar\"),\n1098         enforce_whitespace_preceeding_terminator=True,\n1099     )\n1100     parse_grammar = Sequence(\n1101         \"FROM\",\n1102         Delimited(\n1103             OneOf(\n1104                 # check first for MLTableExpression, because of possible FunctionSegment in MainTableExpression\n1105                 Ref(\"MLTableExpressionSegment\"),\n1106                 Ref(\"TableExpressionSegment\"),\n1107             ),\n1108         ),\n1109     )\n1110 \n1111     def get_eventual_aliases(self) -> List[Tuple[BaseSegment, AliasInfo]]:\n1112         \"\"\"List the eventual aliases of this from clause.\n1113 \n1114         Comes as a list of tuples (table expr, tuple (string, segment, bool)).\n1115         \"\"\"\n1116         buff = []\n1117         direct_table_children = self.get_children(\"table_expression\")\n1118         join_clauses = [\n1119             join_clause\n1120             for child in direct_table_children\n1121             for join_clause in child.get_children(\"join_clause\")\n1122         ]\n1123         # Iterate through the potential sources of aliases\n1124         for clause in (*direct_table_children, *join_clauses):\n1125             ref: AliasInfo = clause.get_eventual_alias()\n1126             # Only append if non null. A None reference, may\n1127             # indicate a generator expression or similar.\n1128             table_expr = (\n1129                 clause\n1130                 if clause in direct_table_children\n1131                 else clause.get_child(\"table_expression\")\n1132             )\n1133             if ref:\n1134                 buff.append((table_expr, ref))\n1135         return buff\n1136 \n1137 \n1138 @ansi_dialect.segment()\n1139 class CaseExpressionSegment(BaseSegment):\n1140     \"\"\"A `CASE WHEN` clause.\"\"\"\n1141 \n1142     type = \"case_expression\"\n1143     match_grammar = OneOf(\n1144         Sequence(\n1145             \"CASE\",\n1146             Indent,\n1147             AnyNumberOf(\n1148                 Sequence(\n1149                     \"WHEN\",\n1150                     Indent,\n1151                     Ref(\"ExpressionSegment\"),\n1152                     \"THEN\",\n1153                     Ref(\"ExpressionSegment\"),\n1154                     Dedent,\n1155                 )\n1156             ),\n1157             Sequence(\"ELSE\", Indent, Ref(\"ExpressionSegment\"), Dedent, optional=True),\n1158             Dedent,\n1159             \"END\",\n1160         ),\n1161         Sequence(\n1162             \"CASE\",\n1163             OneOf(Ref(\"ExpressionSegment\")),\n1164             Indent,\n1165             AnyNumberOf(\n1166                 Sequence(\n1167                     \"WHEN\",\n1168                     Indent,\n1169                     Ref(\"ExpressionSegment\"),\n1170                     \"THEN\",\n1171                     Ref(\"ExpressionSegment\"),\n1172                     Dedent,\n1173                 )\n1174             ),\n1175             Sequence(\"ELSE\", Indent, Ref(\"ExpressionSegment\"), Dedent, optional=True),\n1176             Dedent,\n1177             \"END\",\n1178         ),\n1179     )\n1180 \n1181 \n1182 ansi_dialect.add(\n1183     # Expression_A_Grammar https://www.cockroachlabs.com/docs/v20.2/sql-grammar.html#a_expr\n1184     Expression_A_Grammar=Sequence(\n1185         OneOf(\n1186             Ref(\"Expression_C_Grammar\"),\n1187             Sequence(\n1188                 OneOf(\n1189                     Ref(\"PositiveSegment\"),\n1190                     Ref(\"NegativeSegment\"),\n1191                     # Ref('TildeSegment'),\n1192                     \"NOT\",\n1193                     \"PRIOR\",  # used in CONNECT BY clauses (EXASOL, Snowflake, Postgres...)\n1194                 ),\n1195                 Ref(\"Expression_C_Grammar\"),\n1196             ),\n1197         ),\n1198         AnyNumberOf(\n1199             OneOf(\n1200                 Sequence(\n1201                     OneOf(\n1202                         Sequence(\n1203                             Ref.keyword(\"NOT\", optional=True),\n1204                             Ref(\"LikeGrammar\"),\n1205                         ),\n1206                         Sequence(\n1207                             Ref(\"BinaryOperatorGrammar\"),\n1208                             Ref.keyword(\"NOT\", optional=True),\n1209                         ),\n1210                         # We need to add a lot more here...\n1211                     ),\n1212                     Ref(\"Expression_C_Grammar\"),\n1213                     Sequence(\n1214                         Ref.keyword(\"ESCAPE\"),\n1215                         Ref(\"Expression_C_Grammar\"),\n1216                         optional=True,\n1217                     ),\n1218                 ),\n1219                 Sequence(\n1220                     Ref.keyword(\"NOT\", optional=True),\n1221                     \"IN\",\n1222                     Bracketed(\n1223                         OneOf(\n1224                             Delimited(\n1225                                 Ref(\"LiteralGrammar\"),\n1226                                 Ref(\"IntervalExpressionSegment\"),\n1227                             ),\n1228                             Ref(\"SelectableGrammar\"),\n1229                             ephemeral_name=\"InExpression\",\n1230                         )\n1231                     ),\n1232                 ),\n1233                 Sequence(\n1234                     Ref.keyword(\"NOT\", optional=True),\n1235                     \"IN\",\n1236                     Ref(\"FunctionSegment\"),  # E.g. UNNEST()\n1237                 ),\n1238                 Sequence(\n1239                     \"IS\",\n1240                     Ref.keyword(\"NOT\", optional=True),\n1241                     Ref(\"IsClauseGrammar\"),\n1242                 ),\n1243                 Sequence(\n1244                     # e.g. NOT EXISTS, but other expressions could be met as\n1245                     # well by inverting the condition with the NOT operator\n1246                     \"NOT\",\n1247                     Ref(\"Expression_C_Grammar\"),\n1248                 ),\n1249                 Sequence(\n1250                     Ref.keyword(\"NOT\", optional=True),\n1251                     \"BETWEEN\",\n1252                     # In a between expression, we're restricted to arithmetic operations\n1253                     # because if we look for all binary operators then we would match AND\n1254                     # as both an operator and also as the delimiter within the BETWEEN\n1255                     # expression.\n1256                     Ref(\"Expression_C_Grammar\"),\n1257                     AnyNumberOf(\n1258                         Sequence(\n1259                             Ref(\"ArithmeticBinaryOperatorGrammar\"),\n1260                             Ref(\"Expression_C_Grammar\"),\n1261                         )\n1262                     ),\n1263                     \"AND\",\n1264                     Ref(\"Expression_C_Grammar\"),\n1265                     AnyNumberOf(\n1266                         Sequence(\n1267                             Ref(\"ArithmeticBinaryOperatorGrammar\"),\n1268                             Ref(\"Expression_C_Grammar\"),\n1269                         )\n1270                     ),\n1271                 ),\n1272             )\n1273         ),\n1274     ),\n1275     # Expression_B_Grammar https://www.cockroachlabs.com/docs/v20.2/sql-grammar.htm#b_expr\n1276     Expression_B_Grammar=None,  # TODO\n1277     # Expression_C_Grammar https://www.cockroachlabs.com/docs/v20.2/sql-grammar.htm#c_expr\n1278     Expression_C_Grammar=OneOf(\n1279         Sequence(\n1280             \"EXISTS\", Bracketed(Ref(\"SelectStatementSegment\"))\n1281         ),  # should be first priority, otherwise EXISTS() would be matched as a function\n1282         Ref(\"Expression_D_Grammar\"),\n1283         Ref(\"CaseExpressionSegment\"),\n1284     ),\n1285     # Expression_D_Grammar https://www.cockroachlabs.com/docs/v20.2/sql-grammar.htm#d_expr\n1286     Expression_D_Grammar=Sequence(\n1287         OneOf(\n1288             Ref(\"BareFunctionSegment\"),\n1289             Ref(\"FunctionSegment\"),\n1290             Bracketed(\n1291                 OneOf(\n1292                     # We're using the expression segment here rather than the grammar so\n1293                     # that in the parsed structure we get nested elements.\n1294                     Ref(\"ExpressionSegment\"),\n1295                     Ref(\"SelectableGrammar\"),\n1296                     Delimited(\n1297                         Ref(\n1298                             \"ColumnReferenceSegment\"\n1299                         ),  # WHERE (a,b,c) IN (select a,b,c FROM...)\n1300                         Ref(\n1301                             \"FunctionSegment\"\n1302                         ),  # WHERE (a, substr(b,1,3)) IN (select c,d FROM...)\n1303                         Ref(\"LiteralGrammar\"),  # WHERE (a, 2) IN (SELECT b, c FROM ...)\n1304                     ),\n1305                     ephemeral_name=\"BracketedExpression\",\n1306                 ),\n1307             ),\n1308             # Allow potential select statement without brackets\n1309             Ref(\"SelectStatementSegment\"),\n1310             Ref(\"LiteralGrammar\"),\n1311             Ref(\"IntervalExpressionSegment\"),\n1312             Ref(\"ColumnReferenceSegment\"),\n1313             Ref(\"ArrayLiteralSegment\"),\n1314         ),\n1315         Ref(\"Accessor_Grammar\", optional=True),\n1316         Ref(\"ShorthandCastSegment\", optional=True),\n1317         allow_gaps=True,\n1318     ),\n1319     Accessor_Grammar=AnyNumberOf(Ref(\"ArrayAccessorSegment\")),\n1320 )\n1321 \n1322 \n1323 @ansi_dialect.segment()\n1324 class ExpressionSegment(BaseSegment):\n1325     \"\"\"A expression, either arithmetic or boolean.\n1326 \n1327     NB: This is potentially VERY recursive and\n1328 \n1329     mostly uses the grammars above. This version\n1330     also doesn't bound itself first, and so is potentially\n1331     VERY SLOW. I don't really like this solution.\n1332 \n1333     We rely on elements of the expression to bound\n1334     themselves rather than bounding at the expression\n1335     level. Trying to bound the ExpressionSegment itself\n1336     has been too unstable and not resilient enough to\n1337     other bugs.\n1338     \"\"\"\n1339 \n1340     type = \"expression\"\n1341     match_grammar = Ref(\"Expression_A_Grammar\")\n1342 \n1343 \n1344 @ansi_dialect.segment()\n1345 class WhereClauseSegment(BaseSegment):\n1346     \"\"\"A `WHERE` clause like in `SELECT` or `INSERT`.\"\"\"\n1347 \n1348     type = \"where_clause\"\n1349     match_grammar = StartsWith(\n1350         \"WHERE\",\n1351         terminator=Ref(\"WhereClauseTerminatorGrammar\"),\n1352         enforce_whitespace_preceeding_terminator=True,\n1353     )\n1354     parse_grammar = Sequence(\n1355         \"WHERE\",\n1356         Indent,\n1357         OneOf(\n1358             Bracketed(\n1359                 # expression could be in brackets\n1360                 Ref(\"ExpressionSegment\"),\n1361             ),\n1362             Ref(\"ExpressionSegment\"),\n1363         ),\n1364         Dedent,\n1365     )\n1366 \n1367 \n1368 @ansi_dialect.segment()\n1369 class OrderByClauseSegment(BaseSegment):\n1370     \"\"\"A `ORDER BY` clause like in `SELECT`.\"\"\"\n1371 \n1372     type = \"orderby_clause\"\n1373     match_grammar = StartsWith(\n1374         \"ORDER\",\n1375         terminator=OneOf(\n1376             \"LIMIT\",\n1377             \"HAVING\",\n1378             \"QUALIFY\",\n1379             # For window functions\n1380             \"WINDOW\",\n1381             \"ROWS\",\n1382             \"SEPARATOR\",\n1383         ),\n1384     )\n1385     parse_grammar = Sequence(\n1386         \"ORDER\",\n1387         \"BY\",\n1388         Indent,\n1389         Delimited(\n1390             Sequence(\n1391                 OneOf(\n1392                     Ref(\"ColumnReferenceSegment\"),\n1393                     # Can `ORDER BY 1`\n1394                     Ref(\"NumericLiteralSegment\"),\n1395                     # Can order by an expression\n1396                     Ref(\"ExpressionSegment\"),\n1397                 ),\n1398                 OneOf(\"ASC\", \"DESC\", optional=True),\n1399                 # NB: This isn't really ANSI, and isn't supported in Mysql, but\n1400                 # is supported in enough other dialects for it to make sense here\n1401                 # for now.\n1402                 Sequence(\"NULLS\", OneOf(\"FIRST\", \"LAST\"), optional=True),\n1403             ),\n1404             terminator=Ref.keyword(\"LIMIT\"),\n1405         ),\n1406         Dedent,\n1407     )\n1408 \n1409 \n1410 @ansi_dialect.segment()\n1411 class GroupByClauseSegment(BaseSegment):\n1412     \"\"\"A `GROUP BY` clause like in `SELECT`.\"\"\"\n1413 \n1414     type = \"groupby_clause\"\n1415     match_grammar = StartsWith(\n1416         Sequence(\"GROUP\", \"BY\"),\n1417         terminator=OneOf(\"ORDER\", \"LIMIT\", \"HAVING\", \"QUALIFY\", \"WINDOW\"),\n1418         enforce_whitespace_preceeding_terminator=True,\n1419     )\n1420     parse_grammar = Sequence(\n1421         \"GROUP\",\n1422         \"BY\",\n1423         Indent,\n1424         Delimited(\n1425             OneOf(\n1426                 Ref(\"ColumnReferenceSegment\"),\n1427                 # Can `GROUP BY 1`\n1428                 Ref(\"NumericLiteralSegment\"),\n1429                 # Can `GROUP BY coalesce(col, 1)`\n1430                 Ref(\"ExpressionSegment\"),\n1431             ),\n1432             terminator=OneOf(\"ORDER\", \"LIMIT\", \"HAVING\", \"QUALIFY\", \"WINDOW\"),\n1433         ),\n1434         Dedent,\n1435     )\n1436 \n1437 \n1438 @ansi_dialect.segment()\n1439 class HavingClauseSegment(BaseSegment):\n1440     \"\"\"A `HAVING` clause like in `SELECT`.\"\"\"\n1441 \n1442     type = \"having_clause\"\n1443     match_grammar = StartsWith(\n1444         \"HAVING\",\n1445         terminator=OneOf(\"ORDER\", \"LIMIT\", \"QUALIFY\", \"WINDOW\"),\n1446         enforce_whitespace_preceeding_terminator=True,\n1447     )\n1448     parse_grammar = Sequence(\n1449         \"HAVING\",\n1450         Indent,\n1451         OneOf(\n1452             Bracketed(\n1453                 Ref(\"ExpressionSegment\"),\n1454             ),\n1455             Ref(\"ExpressionSegment\"),\n1456         ),\n1457         Dedent,\n1458     )\n1459 \n1460 \n1461 @ansi_dialect.segment()\n1462 class LimitClauseSegment(BaseSegment):\n1463     \"\"\"A `LIMIT` clause like in `SELECT`.\"\"\"\n1464 \n1465     type = \"limit_clause\"\n1466     match_grammar = Sequence(\n1467         \"LIMIT\",\n1468         OneOf(\n1469             Ref(\"NumericLiteralSegment\"),\n1470             Sequence(\n1471                 Ref(\"NumericLiteralSegment\"), \"OFFSET\", Ref(\"NumericLiteralSegment\")\n1472             ),\n1473             Sequence(\n1474                 Ref(\"NumericLiteralSegment\"),\n1475                 Ref(\"CommaSegment\"),\n1476                 Ref(\"NumericLiteralSegment\"),\n1477             ),\n1478         ),\n1479     )\n1480 \n1481 \n1482 @ansi_dialect.segment()\n1483 class NamedWindowSegment(BaseSegment):\n1484     \"\"\"A WINDOW clause.\"\"\"\n1485 \n1486     type = \"named_window\"\n1487     match_grammar = Sequence(\n1488         \"WINDOW\",\n1489         Delimited(\n1490             Ref(\"NamedWindowExpressionSegment\"),\n1491         ),\n1492     )\n1493 \n1494 \n1495 @ansi_dialect.segment()\n1496 class NamedWindowExpressionSegment(BaseSegment):\n1497     \"\"\"Named window expression.\"\"\"\n1498 \n1499     type = \"named_window_expression\"\n1500     match_grammar = Sequence(\n1501         Ref(\"SingleIdentifierGrammar\"),  # Window name\n1502         \"AS\",\n1503         Bracketed(\n1504             Ref(\"WindowSpecificationSegment\"),\n1505         ),\n1506     )\n1507 \n1508 \n1509 @ansi_dialect.segment()\n1510 class ValuesClauseSegment(BaseSegment):\n1511     \"\"\"A `VALUES` clause like in `INSERT`.\"\"\"\n1512 \n1513     type = \"values_clause\"\n1514     match_grammar = Sequence(\n1515         OneOf(\"VALUE\", \"VALUES\"),\n1516         Delimited(\n1517             Bracketed(\n1518                 Delimited(\n1519                     Ref(\"LiteralGrammar\"),\n1520                     Ref(\"IntervalExpressionSegment\"),\n1521                     Ref(\"FunctionSegment\"),\n1522                     \"DEFAULT\",  # not in `FROM` clause, rule?\n1523                     ephemeral_name=\"ValuesClauseElements\",\n1524                 )\n1525             ),\n1526         ),\n1527         Ref(\"AliasExpressionSegment\", optional=True),\n1528     )\n1529 \n1530 \n1531 @ansi_dialect.segment()\n1532 class SelectStatementSegment(BaseSegment):\n1533     \"\"\"A `SELECT` statement.\"\"\"\n1534 \n1535     type = \"select_statement\"\n1536     # match grammar. This one makes sense in the context of knowing that it's\n1537     # definitely a statement, we just don't know what type yet.\n1538     match_grammar = StartsWith(\n1539         # NB: In bigquery, the select clause may include an EXCEPT, which\n1540         # will also match the set operator, but by starting with the whole\n1541         # select clause rather than just the SELECT keyword, we mitigate that\n1542         # here.\n1543         Ref(\"SelectClauseSegment\"),\n1544         terminator=OneOf(\n1545             Ref(\"SetOperatorSegment\"), Ref(\"WithNoSchemaBindingClauseSegment\")\n1546         ),\n1547         enforce_whitespace_preceeding_terminator=True,\n1548     )\n1549 \n1550     parse_grammar = Sequence(\n1551         Ref(\"SelectClauseSegment\"),\n1552         # Dedent for the indent in the select clause.\n1553         # It's here so that it can come AFTER any whitespace.\n1554         Dedent,\n1555         Ref(\"FromClauseSegment\", optional=True),\n1556         Ref(\"WhereClauseSegment\", optional=True),\n1557         Ref(\"GroupByClauseSegment\", optional=True),\n1558         Ref(\"HavingClauseSegment\", optional=True),\n1559         Ref(\"OrderByClauseSegment\", optional=True),\n1560         Ref(\"LimitClauseSegment\", optional=True),\n1561         Ref(\"NamedWindowSegment\", optional=True),\n1562     )\n1563 \n1564 \n1565 ansi_dialect.add(\n1566     # Things that behave like select statements\n1567     SelectableGrammar=OneOf(\n1568         Ref(\"WithCompoundStatementSegment\"), Ref(\"NonWithSelectableGrammar\")\n1569     ),\n1570     # Things that behave like select statements, which can form part of with expressions.\n1571     NonWithSelectableGrammar=OneOf(\n1572         Ref(\"SetExpressionSegment\"), Ref(\"NonSetSelectableGrammar\")\n1573     ),\n1574     # Things that behave like select statements, which can form part of set expressions.\n1575     NonSetSelectableGrammar=OneOf(\n1576         Ref(\"SelectStatementSegment\"),\n1577         Ref(\"ValuesClauseSegment\"),\n1578         Bracketed(Ref(\"SelectStatementSegment\")),\n1579     ),\n1580 )\n1581 \n1582 \n1583 @ansi_dialect.segment()\n1584 class CTEDefinitionSegment(BaseSegment):\n1585     \"\"\"A CTE Definition from a WITH statement.\n1586 \n1587     `tab (col1,col2) AS (SELECT a,b FROM x)`\n1588     \"\"\"\n1589 \n1590     type = \"common_table_expression\"\n1591     match_grammar = Sequence(\n1592         Ref(\"SingleIdentifierGrammar\"),\n1593         Bracketed(\n1594             Ref(\"SingleIdentifierListSegment\"),\n1595             optional=True,\n1596         ),\n1597         \"AS\",\n1598         Bracketed(\n1599             # Ephemeral here to subdivide the query.\n1600             Ref(\"SelectableGrammar\", ephemeral_name=\"SelectableGrammar\")\n1601         ),\n1602     )\n1603 \n1604     def get_identifier(self) -> BaseSegment:\n1605         \"\"\"Gets the identifier of this CTE.\n1606 \n1607         Note: it blindly get the first identifier it finds\n1608         which given the structure of a CTE definition is\n1609         usually the right one.\n1610         \"\"\"\n1611         return self.get_child(\"identifier\")\n1612 \n1613 \n1614 @ansi_dialect.segment()\n1615 class WithCompoundStatementSegment(BaseSegment):\n1616     \"\"\"A `SELECT` statement preceded by a selection of `WITH` clauses.\n1617 \n1618     `WITH tab (col1,col2) AS (SELECT a,b FROM x)`\n1619     \"\"\"\n1620 \n1621     type = \"with_compound_statement\"\n1622     # match grammar\n1623     match_grammar = StartsWith(\"WITH\")\n1624     parse_grammar = Sequence(\n1625         \"WITH\",\n1626         Delimited(\n1627             Ref(\"CTEDefinitionSegment\"),\n1628             terminator=Ref.keyword(\"SELECT\"),\n1629         ),\n1630         Ref(\"NonWithSelectableGrammar\"),\n1631     )\n1632 \n1633 \n1634 @ansi_dialect.segment()\n1635 class SetOperatorSegment(BaseSegment):\n1636     \"\"\"A set operator such as Union, Minus, Except or Intersect.\"\"\"\n1637 \n1638     type = \"set_operator\"\n1639     match_grammar = OneOf(\n1640         Sequence(\"UNION\", OneOf(\"DISTINCT\", \"ALL\", optional=True)),\n1641         \"INTERSECT\",\n1642         \"EXCEPT\",\n1643         \"MINUS\",\n1644         exclude=Sequence(\"EXCEPT\", Bracketed(Anything())),\n1645     )\n1646 \n1647 \n1648 @ansi_dialect.segment()\n1649 class SetExpressionSegment(BaseSegment):\n1650     \"\"\"A set expression with either Union, Minus, Except or Intersect.\"\"\"\n1651 \n1652     type = \"set_expression\"\n1653     # match grammar\n1654     match_grammar = Sequence(\n1655         Ref(\"NonSetSelectableGrammar\"),\n1656         AnyNumberOf(\n1657             Sequence(\n1658                 Ref(\"SetOperatorSegment\"),\n1659                 Ref(\"NonSetSelectableGrammar\"),\n1660             ),\n1661             min_times=1,\n1662         ),\n1663     )\n1664 \n1665 \n1666 @ansi_dialect.segment()\n1667 class InsertStatementSegment(BaseSegment):\n1668     \"\"\"A `INSERT` statement.\"\"\"\n1669 \n1670     type = \"insert_statement\"\n1671     match_grammar = StartsWith(\"INSERT\")\n1672     parse_grammar = Sequence(\n1673         \"INSERT\",\n1674         Ref.keyword(\"OVERWRITE\", optional=True),  # Maybe this is just snowflake?\n1675         Ref.keyword(\"INTO\", optional=True),\n1676         Ref(\"TableReferenceSegment\"),\n1677         Ref(\"BracketedColumnReferenceListGrammar\", optional=True),\n1678         Ref(\"SelectableGrammar\"),\n1679     )\n1680 \n1681 \n1682 @ansi_dialect.segment()\n1683 class TransactionStatementSegment(BaseSegment):\n1684     \"\"\"A `COMMIT` or `ROLLBACK` statement.\"\"\"\n1685 \n1686     type = \"transaction_statement\"\n1687     match_grammar = OneOf(\n1688         # COMMIT [ WORK ] [ AND [ NO ] CHAIN ]\n1689         Sequence(\n1690             \"COMMIT\",\n1691             Ref.keyword(\"WORK\", optional=True),\n1692             Sequence(\"AND\", Ref.keyword(\"NO\", optional=True), \"CHAIN\", optional=True),\n1693         ),\n1694         # NOTE: \"TO SAVEPOINT\" is not yet supported\n1695         # ROLLBACK [ WORK ] [ AND [ NO ] CHAIN ]\n1696         Sequence(\n1697             \"ROLLBACK\",\n1698             Ref.keyword(\"WORK\", optional=True),\n1699             Sequence(\"AND\", Ref.keyword(\"NO\", optional=True), \"CHAIN\", optional=True),\n1700         ),\n1701     )\n1702 \n1703 \n1704 @ansi_dialect.segment()\n1705 class ColumnOptionSegment(BaseSegment):\n1706     \"\"\"A column option; each CREATE TABLE column can have 0 or more.\"\"\"\n1707 \n1708     type = \"column_constraint\"\n1709     # Column constraint from\n1710     # https://www.postgresql.org/docs/12/sql-createtable.html\n1711     match_grammar = Sequence(\n1712         Sequence(\n1713             \"CONSTRAINT\",\n1714             Ref(\"ObjectReferenceSegment\"),  # Constraint name\n1715             optional=True,\n1716         ),\n1717         OneOf(\n1718             Sequence(Ref.keyword(\"NOT\", optional=True), \"NULL\"),  # NOT NULL or NULL\n1719             Sequence(  # DEFAULT <value>\n1720                 \"DEFAULT\",\n1721                 Ref(\"LiteralGrammar\"),\n1722                 # ?? Ref('IntervalExpressionSegment')\n1723             ),\n1724             Ref(\"PrimaryKeyGrammar\"),\n1725             \"UNIQUE\",  # UNIQUE\n1726             \"AUTO_INCREMENT\",  # AUTO_INCREMENT (MySQL)\n1727             Sequence(  # REFERENCES reftable [ ( refcolumn) ]\n1728                 \"REFERENCES\",\n1729                 Ref(\"ColumnReferenceSegment\"),\n1730                 # Foreign columns making up FOREIGN KEY constraint\n1731                 Ref(\"BracketedColumnReferenceListGrammar\", optional=True),\n1732             ),\n1733             Sequence(  # [COMMENT 'string'] (MySQL)\n1734                 \"COMMENT\",\n1735                 Ref(\"QuotedLiteralSegment\"),\n1736             ),\n1737         ),\n1738     )\n1739 \n1740 \n1741 @ansi_dialect.segment()\n1742 class ColumnDefinitionSegment(BaseSegment):\n1743     \"\"\"A column definition, e.g. for CREATE TABLE or ALTER TABLE.\"\"\"\n1744 \n1745     type = \"column_definition\"\n1746     match_grammar = Sequence(\n1747         Ref(\"SingleIdentifierGrammar\"),  # Column name\n1748         Ref(\"DatatypeSegment\"),  # Column type\n1749         Bracketed(Anything(), optional=True),  # For types like VARCHAR(100)\n1750         AnyNumberOf(\n1751             Ref(\"ColumnOptionSegment\", optional=True),\n1752         ),\n1753     )\n1754 \n1755 \n1756 @ansi_dialect.segment()\n1757 class IndexColumnDefinitionSegment(BaseSegment):\n1758     \"\"\"A column definition for CREATE INDEX.\"\"\"\n1759 \n1760     type = \"index_column_definition\"\n1761     match_grammar = Sequence(\n1762         Ref(\"SingleIdentifierGrammar\"),  # Column name\n1763         OneOf(\"ASC\", \"DESC\", optional=True),\n1764     )\n1765 \n1766 \n1767 @ansi_dialect.segment()\n1768 class TableConstraintSegment(BaseSegment):\n1769     \"\"\"A table constraint, e.g. for CREATE TABLE.\"\"\"\n1770 \n1771     type = \"table_constraint_definition\"\n1772     # Later add support for CHECK constraint, others?\n1773     # e.g. CONSTRAINT constraint_1 PRIMARY KEY(column_1)\n1774     match_grammar = Sequence(\n1775         Sequence(  # [ CONSTRAINT <Constraint name> ]\n1776             \"CONSTRAINT\", Ref(\"ObjectReferenceSegment\"), optional=True\n1777         ),\n1778         OneOf(\n1779             Sequence(  # UNIQUE ( column_name [, ... ] )\n1780                 \"UNIQUE\",\n1781                 Ref(\"BracketedColumnReferenceListGrammar\"),\n1782                 # Later add support for index_parameters?\n1783             ),\n1784             Sequence(  # PRIMARY KEY ( column_name [, ... ] ) index_parameters\n1785                 Ref(\"PrimaryKeyGrammar\"),\n1786                 # Columns making up PRIMARY KEY constraint\n1787                 Ref(\"BracketedColumnReferenceListGrammar\"),\n1788                 # Later add support for index_parameters?\n1789             ),\n1790             Sequence(  # FOREIGN KEY ( column_name [, ... ] )\n1791                 # REFERENCES reftable [ ( refcolumn [, ... ] ) ]\n1792                 \"FOREIGN\",\n1793                 \"KEY\",\n1794                 # Local columns making up FOREIGN KEY constraint\n1795                 Ref(\"BracketedColumnReferenceListGrammar\"),\n1796                 \"REFERENCES\",\n1797                 Ref(\"ColumnReferenceSegment\"),\n1798                 # Foreign columns making up FOREIGN KEY constraint\n1799                 Ref(\"BracketedColumnReferenceListGrammar\"),\n1800                 # Later add support for [MATCH FULL/PARTIAL/SIMPLE] ?\n1801                 # Later add support for [ ON DELETE/UPDATE action ] ?\n1802             ),\n1803         ),\n1804     )\n1805 \n1806 \n1807 @ansi_dialect.segment()\n1808 class CreateTableStatementSegment(BaseSegment):\n1809     \"\"\"A `CREATE TABLE` statement.\"\"\"\n1810 \n1811     type = \"create_table_statement\"\n1812     # https://crate.io/docs/sql-99/en/latest/chapters/18.html\n1813     # https://www.postgresql.org/docs/12/sql-createtable.html\n1814     match_grammar = Sequence(\n1815         \"CREATE\",\n1816         Ref(\"OrReplaceGrammar\", optional=True),\n1817         Ref(\"TemporaryTransientGrammar\", optional=True),\n1818         \"TABLE\",\n1819         Ref(\"IfNotExistsGrammar\", optional=True),\n1820         Ref(\"TableReferenceSegment\"),\n1821         OneOf(\n1822             # Columns and comment syntax:\n1823             Sequence(\n1824                 Bracketed(\n1825                     Delimited(\n1826                         OneOf(\n1827                             Ref(\"TableConstraintSegment\"),\n1828                             Ref(\"ColumnDefinitionSegment\"),\n1829                         ),\n1830                     )\n1831                 ),\n1832                 Sequence(  # [COMMENT 'string'] (MySQL)\n1833                     \"COMMENT\", Ref(\"QuotedLiteralSegment\"), optional=True\n1834                 ),\n1835             ),\n1836             # Create AS syntax:\n1837             Sequence(\n1838                 \"AS\",\n1839                 OptionallyBracketed(Ref(\"SelectableGrammar\")),\n1840             ),\n1841             # Create like syntax\n1842             Sequence(\"LIKE\", Ref(\"TableReferenceSegment\")),\n1843         ),\n1844     )\n1845 \n1846 \n1847 @ansi_dialect.segment()\n1848 class CreateSchemaStatementSegment(BaseSegment):\n1849     \"\"\"A `CREATE SCHEMA` statement.\"\"\"\n1850 \n1851     type = \"create_schema_statement\"\n1852     match_grammar = Sequence(\n1853         \"CREATE\",\n1854         \"SCHEMA\",\n1855         Ref(\"IfNotExistsGrammar\", optional=True),\n1856         Ref(\"SchemaReferenceSegment\"),\n1857     )\n1858 \n1859 \n1860 @ansi_dialect.segment()\n1861 class CreateDatabaseStatementSegment(BaseSegment):\n1862     \"\"\"A `CREATE DATABASE` statement.\"\"\"\n1863 \n1864     type = \"create_database_statement\"\n1865     match_grammar = Sequence(\n1866         \"CREATE\",\n1867         \"DATABASE\",\n1868         Ref(\"IfNotExistsGrammar\", optional=True),\n1869         Ref(\"DatabaseReferenceSegment\"),\n1870     )\n1871 \n1872 \n1873 @ansi_dialect.segment()\n1874 class CreateExtensionStatementSegment(BaseSegment):\n1875     \"\"\"A `CREATE EXTENSION` statement.\n1876 \n1877     https://www.postgresql.org/docs/9.1/sql-createextension.html\n1878     \"\"\"\n1879 \n1880     type = \"create_extension_statement\"\n1881     match_grammar = Sequence(\n1882         \"CREATE\",\n1883         \"EXTENSION\",\n1884         Ref(\"IfNotExistsGrammar\", optional=True),\n1885         Ref(\"ExtensionReferenceSegment\"),\n1886         Ref.keyword(\"WITH\", optional=True),\n1887         Sequence(\"SCHEMA\", Ref(\"SchemaReferenceSegment\"), optional=True),\n1888         Sequence(\"VERSION\", Ref(\"VersionIdentifierSegment\"), optional=True),\n1889         Sequence(\"FROM\", Ref(\"VersionIdentifierSegment\"), optional=True),\n1890     )\n1891 \n1892 \n1893 @ansi_dialect.segment()\n1894 class CreateIndexStatementSegment(BaseSegment):\n1895     \"\"\"A `CREATE INDEX` statement.\"\"\"\n1896 \n1897     type = \"create_index_statement\"\n1898     match_grammar = Sequence(\n1899         \"CREATE\",\n1900         Ref(\"OrReplaceGrammar\", optional=True),\n1901         \"INDEX\",\n1902         Ref(\"IfNotExistsGrammar\", optional=True),\n1903         Ref(\"IndexReferenceSegment\"),\n1904         \"ON\",\n1905         Ref(\"TableReferenceSegment\"),\n1906         Sequence(\n1907             Bracketed(\n1908                 Delimited(\n1909                     Ref(\"IndexColumnDefinitionSegment\"),\n1910                 ),\n1911             )\n1912         ),\n1913     )\n1914 \n1915 \n1916 @ansi_dialect.segment()\n1917 class AlterTableStatementSegment(BaseSegment):\n1918     \"\"\"An `ALTER TABLE` statement.\"\"\"\n1919 \n1920     type = \"alter_table_statement\"\n1921     # Based loosely on:\n1922     # https://dev.mysql.com/doc/refman/8.0/en/alter-table.html\n1923     # TODO: Flesh this out with more detail.\n1924     match_grammar = Sequence(\n1925         \"ALTER\",\n1926         \"TABLE\",\n1927         Ref(\"TableReferenceSegment\"),\n1928         Delimited(\n1929             OneOf(\n1930                 # Table options\n1931                 Sequence(\n1932                     Ref(\"ParameterNameSegment\"),\n1933                     Ref(\"EqualsSegment\", optional=True),\n1934                     OneOf(Ref(\"LiteralGrammar\"), Ref(\"NakedIdentifierSegment\")),\n1935                 ),\n1936                 # Add things\n1937                 Sequence(\n1938                     OneOf(\"ADD\", \"MODIFY\"),\n1939                     Ref.keyword(\"COLUMN\", optional=True),\n1940                     Ref(\"ColumnDefinitionSegment\"),\n1941                     OneOf(\n1942                         Sequence(\n1943                             OneOf(\"FIRST\", \"AFTER\"), Ref(\"ColumnReferenceSegment\")\n1944                         ),\n1945                         # Bracketed Version of the same\n1946                         Ref(\"BracketedColumnReferenceListGrammar\"),\n1947                         optional=True,\n1948                     ),\n1949                 ),\n1950             ),\n1951         ),\n1952     )\n1953 \n1954 \n1955 @ansi_dialect.segment()\n1956 class CreateViewStatementSegment(BaseSegment):\n1957     \"\"\"A `CREATE VIEW` statement.\"\"\"\n1958 \n1959     type = \"create_view_statement\"\n1960     # https://crate.io/docs/sql-99/en/latest/chapters/18.html#create-view-statement\n1961     # https://dev.mysql.com/doc/refman/8.0/en/create-view.html\n1962     # https://www.postgresql.org/docs/12/sql-createview.html\n1963     match_grammar = Sequence(\n1964         \"CREATE\",\n1965         Ref(\"OrReplaceGrammar\", optional=True),\n1966         \"VIEW\",\n1967         Ref(\"TableReferenceSegment\"),\n1968         # Optional list of column names\n1969         Ref(\"BracketedColumnReferenceListGrammar\", optional=True),\n1970         \"AS\",\n1971         Ref(\"SelectableGrammar\"),\n1972         Ref(\"WithNoSchemaBindingClauseSegment\", optional=True),\n1973     )\n1974 \n1975 \n1976 @ansi_dialect.segment()\n1977 class DropStatementSegment(BaseSegment):\n1978     \"\"\"A `DROP` statement.\"\"\"\n1979 \n1980     type = \"drop_statement\"\n1981     # DROP {TABLE | VIEW} <Table name> [IF EXISTS} {RESTRICT | CASCADE}\n1982     match_grammar = Sequence(\n1983         \"DROP\",\n1984         OneOf(\n1985             \"TABLE\",\n1986             \"VIEW\",\n1987             \"USER\",\n1988         ),\n1989         Ref(\"IfExistsGrammar\", optional=True),\n1990         Ref(\"TableReferenceSegment\"),\n1991         OneOf(\"RESTRICT\", Ref.keyword(\"CASCADE\", optional=True), optional=True),\n1992     )\n1993 \n1994 \n1995 @ansi_dialect.segment()\n1996 class DropIndexStatementSegment(BaseSegment):\n1997     \"\"\"A `DROP INDEX` statement.\"\"\"\n1998 \n1999     type = \"drop_statement\"\n2000     # DROP INDEX <Index name> [CONCURRENTLY] [IF EXISTS] {RESTRICT | CASCADE}\n2001     match_grammar = Sequence(\n2002         \"DROP\",\n2003         \"INDEX\",\n2004         Ref.keyword(\"CONCURRENTLY\", optional=True),\n2005         Ref(\"IfExistsGrammar\", optional=True),\n2006         Ref(\"IndexReferenceSegment\"),\n2007         OneOf(\"RESTRICT\", Ref.keyword(\"CASCADE\", optional=True), optional=True),\n2008     )\n2009 \n2010 \n2011 @ansi_dialect.segment()\n2012 class AccessStatementSegment(BaseSegment):\n2013     \"\"\"A `GRANT` or `REVOKE` statement.\n2014 \n2015     In order to help reduce code duplication we decided to implement other dialect specific grants (like snowflake)\n2016     here too which will help with maintainability. We also note that this causes the grammar to be less \"correct\",\n2017     but the benefits outweigh the con in our opinion.\n2018 \n2019 \n2020     Grant specific information:\n2021      * https://www.postgresql.org/docs/9.0/sql-grant.html\n2022      * https://docs.snowflake.com/en/sql-reference/sql/grant-privilege.html\n2023 \n2024     Revoke specific information:\n2025      * https://www.postgresql.org/docs/9.0/sql-revoke.html\n2026      * https://docs.snowflake.com/en/sql-reference/sql/revoke-role.html\n2027      * https://docs.snowflake.com/en/sql-reference/sql/revoke-privilege.html\n2028      * https://docs.snowflake.com/en/sql-reference/sql/revoke-privilege-share.html\n2029     \"\"\"\n2030 \n2031     type = \"access_statement\"\n2032 \n2033     # Privileges that can be set on the account (specific to snowflake)\n2034     _global_permissions = OneOf(\n2035         Sequence(\n2036             \"CREATE\",\n2037             OneOf(\n2038                 \"ROLE\",\n2039                 \"USER\",\n2040                 \"WAREHOUSE\",\n2041                 \"DATABASE\",\n2042                 \"INTEGRATION\",\n2043             ),\n2044         ),\n2045         Sequence(\"APPLY\", \"MASKING\", \"POLICY\"),\n2046         Sequence(\"EXECUTE\", \"TASK\"),\n2047         Sequence(\"MANAGE\", \"GRANTS\"),\n2048         Sequence(\"MONITOR\", OneOf(\"EXECUTION\", \"USAGE\")),\n2049     )\n2050 \n2051     _schema_object_names = [\n2052         \"TABLE\",\n2053         \"VIEW\",\n2054         \"STAGE\",\n2055         \"FUNCTION\",\n2056         \"PROCEDURE\",\n2057         \"SEQUENCE\",\n2058         \"STREAM\",\n2059         \"TASK\",\n2060     ]\n2061 \n2062     _schema_object_types = OneOf(\n2063         *_schema_object_names,\n2064         Sequence(\"MATERIALIZED\", \"VIEW\"),\n2065         Sequence(\"EXTERNAL\", \"TABLE\"),\n2066         Sequence(\"FILE\", \"FORMAT\"),\n2067     )\n2068 \n2069     # We reuse the object names above and simply append an `S` to the end of them to get plurals\n2070     _schema_object_types_plural = OneOf(\n2071         *[f\"{object_name}S\" for object_name in _schema_object_names]\n2072     )\n2073 \n2074     _permissions = Sequence(\n2075         OneOf(\n2076             Sequence(\n2077                 \"CREATE\",\n2078                 OneOf(\n2079                     \"SCHEMA\",\n2080                     Sequence(\"MASKING\", \"POLICY\"),\n2081                     \"PIPE\",\n2082                     _schema_object_types,\n2083                 ),\n2084             ),\n2085             Sequence(\"IMPORTED\", \"PRIVILEGES\"),\n2086             \"MODIFY\",\n2087             \"USE_ANY_ROLE\",\n2088             \"USAGE\",\n2089             \"SELECT\",\n2090             \"INSERT\",\n2091             \"UPDATE\",\n2092             \"DELETE\",\n2093             \"TRUNCATE\",\n2094             \"REFERENCES\",\n2095             \"READ\",\n2096             \"WRITE\",\n2097             \"MONITOR\",\n2098             \"OPERATE\",\n2099             \"APPLY\",\n2100             \"OWNERSHIP\",\n2101             \"REFERENCE_USAGE\",\n2102             Sequence(\"ALL\", Ref.keyword(\"PRIVILEGES\", optional=True)),\n2103         ),\n2104         Ref(\"BracketedColumnReferenceListGrammar\", optional=True),\n2105     )\n2106 \n2107     # All of the object types that we can grant permissions on.\n2108     # This list will contain ansi sql objects as well as dialect specific ones.\n2109     _objects = OneOf(\n2110         \"ACCOUNT\",\n2111         Sequence(\n2112             OneOf(\n2113                 Sequence(\"RESOURCE\", \"MONITOR\"),\n2114                 \"WAREHOUSE\",\n2115                 \"DATABASE\",\n2116                 \"INTEGRATION\",\n2117                 \"SCHEMA\",\n2118                 \"ROLE\",\n2119                 Sequence(\"ALL\", \"SCHEMAS\", \"IN\", \"DATABASE\"),\n2120                 Sequence(\"FUTURE\", \"SCHEMAS\", \"IN\", \"DATABASE\"),\n2121                 _schema_object_types,\n2122                 Sequence(\"ALL\", _schema_object_types_plural, \"IN\", \"SCHEMA\"),\n2123                 Sequence(\n2124                     \"FUTURE\",\n2125                     _schema_object_types_plural,\n2126                     \"IN\",\n2127                     OneOf(\"DATABASE\", \"SCHEMA\"),\n2128                 ),\n2129                 optional=True,\n2130             ),\n2131             Ref(\"ObjectReferenceSegment\"),\n2132             Ref(\"FunctionParameterGrammar\", optional=True),\n2133         ),\n2134     )\n2135 \n2136     match_grammar = OneOf(\n2137         # Based on https://www.postgresql.org/docs/12/sql-grant.html\n2138         # and https://docs.snowflake.com/en/sql-reference/sql/grant-privilege.html\n2139         Sequence(\n2140             \"GRANT\",\n2141             OneOf(\n2142                 Sequence(\n2143                     Delimited(\n2144                         OneOf(_global_permissions, _permissions),\n2145                         delimiter=Ref(\"CommaSegment\"),\n2146                     ),\n2147                     \"ON\",\n2148                     _objects,\n2149                 ),\n2150                 Sequence(\"ROLE\", Ref(\"ObjectReferenceSegment\")),\n2151             ),\n2152             \"TO\",\n2153             OneOf(\"GROUP\", \"USER\", \"ROLE\", \"SHARE\", optional=True),\n2154             OneOf(\n2155                 Ref(\"ObjectReferenceSegment\"),\n2156                 \"PUBLIC\",\n2157             ),\n2158             OneOf(\n2159                 Sequence(\"WITH\", \"GRANT\", \"OPTION\"),\n2160                 Sequence(\"COPY\", \"CURRENT\", \"GRANTS\"),\n2161                 optional=True,\n2162             ),\n2163         ),\n2164         # Based on https://www.postgresql.org/docs/12/sql-revoke.html\n2165         Sequence(\n2166             \"REVOKE\",\n2167             Sequence(\"GRANT\", \"OPTION\", \"FOR\", optional=True),\n2168             OneOf(\n2169                 Sequence(\n2170                     Delimited(\n2171                         OneOf(_global_permissions, _permissions),\n2172                         delimiter=Ref(\"CommaSegment\"),\n2173                     ),\n2174                     \"ON\",\n2175                     _objects,\n2176                 ),\n2177                 Sequence(\"ROLE\", Ref(\"ObjectReferenceSegment\")),\n2178             ),\n2179             \"FROM\",\n2180             OneOf(\"GROUP\", \"USER\", \"ROLE\", \"SHARE\", optional=True),\n2181             Ref(\"ObjectReferenceSegment\"),\n2182             OneOf(\"RESTRICT\", Ref.keyword(\"CASCADE\", optional=True), optional=True),\n2183         ),\n2184     )\n2185 \n2186 \n2187 @ansi_dialect.segment()\n2188 class DeleteStatementSegment(BaseSegment):\n2189     \"\"\"A `DELETE` statement.\n2190 \n2191     DELETE FROM <table name> [ WHERE <search condition> ]\n2192     \"\"\"\n2193 \n2194     type = \"delete_statement\"\n2195     # match grammar. This one makes sense in the context of knowing that it's\n2196     # definitely a statement, we just don't know what type yet.\n2197     match_grammar = StartsWith(\"DELETE\")\n2198     parse_grammar = Sequence(\n2199         \"DELETE\",\n2200         Ref(\"FromClauseSegment\"),\n2201         Ref(\"WhereClauseSegment\", optional=True),\n2202     )\n2203 \n2204 \n2205 @ansi_dialect.segment()\n2206 class UpdateStatementSegment(BaseSegment):\n2207     \"\"\"A `Update` statement.\n2208 \n2209     UPDATE <table name> SET <set clause list> [ WHERE <search condition> ]\n2210     \"\"\"\n2211 \n2212     type = \"update_statement\"\n2213     match_grammar = StartsWith(\"UPDATE\")\n2214     parse_grammar = Sequence(\n2215         \"UPDATE\",\n2216         OneOf(Ref(\"TableReferenceSegment\"), Ref(\"AliasedTableReferenceSegment\")),\n2217         Ref(\"SetClauseListSegment\"),\n2218         Ref(\"FromClauseSegment\", optional=True),\n2219         Ref(\"WhereClauseSegment\", optional=True),\n2220     )\n2221 \n2222 \n2223 @ansi_dialect.segment()\n2224 class SetClauseListSegment(BaseSegment):\n2225     \"\"\"SQL 1992 set clause list.\n2226 \n2227     <set clause list> ::=\n2228               <set clause> [ { <comma> <set clause> }... ]\n2229 \n2230          <set clause> ::=\n2231               <object column> <equals operator> <update source>\n2232 \n2233          <update source> ::=\n2234                 <value expression>\n2235               | <null specification>\n2236               | DEFAULT\n2237 \n2238          <object column> ::= <column name>\n2239     \"\"\"\n2240 \n2241     type = \"set_clause_list\"\n2242     match_grammar = Sequence(\n2243         \"SET\",\n2244         Indent,\n2245         OneOf(\n2246             Ref(\"SetClauseSegment\"),\n2247             # set clause\n2248             AnyNumberOf(\n2249                 Delimited(Ref(\"SetClauseSegment\")),\n2250             ),\n2251         ),\n2252         Dedent,\n2253     )\n2254 \n2255 \n2256 @ansi_dialect.segment()\n2257 class SetClauseSegment(BaseSegment):\n2258     \"\"\"SQL 1992 set clause.\n2259 \n2260     <set clause> ::=\n2261               <object column> <equals operator> <update source>\n2262 \n2263          <update source> ::=\n2264                 <value expression>\n2265               | <null specification>\n2266               | DEFAULT\n2267 \n2268          <object column> ::= <column name>\n2269     \"\"\"\n2270 \n2271     type = \"set_clause\"\n2272 \n2273     match_grammar = Sequence(\n2274         Ref(\"ColumnReferenceSegment\"),\n2275         Ref(\"EqualsSegment\"),\n2276         OneOf(\n2277             Ref(\"LiteralGrammar\"),\n2278             Ref(\"BareFunctionSegment\"),\n2279             Ref(\"FunctionSegment\"),\n2280             Ref(\"ColumnReferenceSegment\"),\n2281             \"DEFAULT\",\n2282         ),\n2283     )\n2284 \n2285 \n2286 @ansi_dialect.segment()\n2287 class FunctionDefinitionGrammar(BaseSegment):\n2288     \"\"\"This is the body of a `CREATE FUNCTION AS` statement.\"\"\"\n2289 \n2290     match_grammar = Sequence(\n2291         \"AS\",\n2292         Ref(\"QuotedLiteralSegment\"),\n2293         Sequence(\n2294             \"LANGUAGE\",\n2295             # Not really a parameter, but best fit for now.\n2296             Ref(\"ParameterNameSegment\"),\n2297             optional=True,\n2298         ),\n2299     )\n2300 \n2301 \n2302 @ansi_dialect.segment()\n2303 class CreateFunctionStatementSegment(BaseSegment):\n2304     \"\"\"A `CREATE FUNCTION` statement.\n2305 \n2306     This version in the ANSI dialect should be a \"common subset\" of the\n2307     structure of the code for those dialects.\n2308     postgres: https://www.postgresql.org/docs/9.1/sql-createfunction.html\n2309     snowflake: https://docs.snowflake.com/en/sql-reference/sql/create-function.html\n2310     bigquery: https://cloud.google.com/bigquery/docs/reference/standard-sql/user-defined-functions\n2311     \"\"\"\n2312 \n2313     type = \"create_function_statement\"\n2314 \n2315     match_grammar = Sequence(\n2316         \"CREATE\",\n2317         Sequence(\"OR\", \"REPLACE\", optional=True),\n2318         Ref(\"TemporaryGrammar\", optional=True),\n2319         \"FUNCTION\",\n2320         Anything(),\n2321     )\n2322 \n2323     parse_grammar = Sequence(\n2324         \"CREATE\",\n2325         Sequence(\"OR\", \"REPLACE\", optional=True),\n2326         Ref(\"TemporaryGrammar\", optional=True),\n2327         \"FUNCTION\",\n2328         Sequence(\"IF\", \"NOT\", \"EXISTS\", optional=True),\n2329         Ref(\"FunctionNameSegment\"),\n2330         Ref(\"FunctionParameterGrammar\"),\n2331         Sequence(  # Optional function return type\n2332             \"RETURNS\",\n2333             Ref(\"DatatypeSegment\"),\n2334             optional=True,\n2335         ),\n2336         Ref(\"FunctionDefinitionGrammar\"),\n2337     )\n2338 \n2339 \n2340 @ansi_dialect.segment()\n2341 class FunctionParameterGrammar(BaseSegment):\n2342     \"\"\"The parameters for a function ie. `(string, number)`.\"\"\"\n2343 \n2344     # Function parameter list\n2345     match_grammar = Bracketed(\n2346         Delimited(\n2347             # Odd syntax, but prevents eager parameters being confused for data types\n2348             OneOf(\n2349                 Sequence(\n2350                     Ref(\"ParameterNameSegment\", optional=True),\n2351                     OneOf(Sequence(\"ANY\", \"TYPE\"), Ref(\"DatatypeSegment\")),\n2352                 ),\n2353                 OneOf(Sequence(\"ANY\", \"TYPE\"), Ref(\"DatatypeSegment\")),\n2354             ),\n2355             delimiter=Ref(\"CommaSegment\"),\n2356         )\n2357     )\n2358 \n2359 \n2360 @ansi_dialect.segment()\n2361 class CreateModelStatementSegment(BaseSegment):\n2362     \"\"\"A BigQuery `CREATE MODEL` statement.\"\"\"\n2363 \n2364     type = \"create_model_statement\"\n2365     # https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create\n2366     match_grammar = Sequence(\n2367         \"CREATE\",\n2368         Ref(\"OrReplaceGrammar\", optional=True),\n2369         \"MODEL\",\n2370         Ref(\"IfNotExistsGrammar\", optional=True),\n2371         Ref(\"ObjectReferenceSegment\"),\n2372         Sequence(\n2373             \"OPTIONS\",\n2374             Bracketed(\n2375                 Delimited(\n2376                     Sequence(\n2377                         Ref(\"ParameterNameSegment\"),\n2378                         Ref(\"EqualsSegment\"),\n2379                         OneOf(\n2380                             # This covers many but not all the extensive list of\n2381                             # possible 'CREATE MODEL' options.\n2382                             Ref(\"LiteralGrammar\"),  # Single value\n2383                             Bracketed(\n2384                                 # E.g. input_label_cols: list of column names\n2385                                 Delimited(Ref(\"QuotedLiteralSegment\")),\n2386                                 bracket_type=\"square\",\n2387                                 optional=True,\n2388                             ),\n2389                         ),\n2390                     ),\n2391                 )\n2392             ),\n2393             optional=True,\n2394         ),\n2395         \"AS\",\n2396         Ref(\"SelectableGrammar\"),\n2397     )\n2398 \n2399 \n2400 @ansi_dialect.segment()\n2401 class CreateTypeStatementSegment(BaseSegment):\n2402     \"\"\"A `CREATE TYPE` statement.\n2403 \n2404     This is based around the Postgres syntax.\n2405     https://www.postgresql.org/docs/current/sql-createtype.html\n2406 \n2407     Note: This is relatively permissive currently\n2408     and does not lint the syntax strictly, to allow\n2409     for some deviation between dialects.\n2410     \"\"\"\n2411 \n2412     type = \"create_type_statement\"\n2413     match_grammar = Sequence(\n2414         \"CREATE\",\n2415         \"TYPE\",\n2416         Ref(\"ObjectReferenceSegment\"),\n2417         Sequence(\"AS\", OneOf(\"ENUM\", \"RANGE\", optional=True), optional=True),\n2418         Bracketed(Delimited(Anything()), optional=True),\n2419     )\n2420 \n2421 \n2422 @ansi_dialect.segment()\n2423 class DropModelStatementSegment(BaseSegment):\n2424     \"\"\"A `DROP MODEL` statement.\"\"\"\n2425 \n2426     type = \"drop_MODELstatement\"\n2427     # DROP MODEL <Model name> [IF EXISTS}\n2428     # https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-drop-model\n2429     match_grammar = Sequence(\n2430         \"DROP\",\n2431         \"MODEL\",\n2432         Ref(\"IfExistsGrammar\", optional=True),\n2433         Ref(\"ObjectReferenceSegment\"),\n2434     )\n2435 \n2436 \n2437 @ansi_dialect.segment()\n2438 class MLTableExpressionSegment(BaseSegment):\n2439     \"\"\"An ML table expression.\"\"\"\n2440 \n2441     type = \"ml_table_expression\"\n2442     # E.g. ML.WEIGHTS(MODEL `project.dataset.model`)\n2443     match_grammar = Sequence(\n2444         \"ML\",\n2445         Ref(\"DotSegment\"),\n2446         Ref(\"SingleIdentifierGrammar\"),\n2447         Bracketed(\n2448             Sequence(\"MODEL\", Ref(\"ObjectReferenceSegment\")),\n2449             OneOf(\n2450                 Sequence(\n2451                     Ref(\"CommaSegment\"),\n2452                     Bracketed(\n2453                         Ref(\"SelectableGrammar\"),\n2454                     ),\n2455                 ),\n2456                 optional=True,\n2457             ),\n2458         ),\n2459     )\n2460 \n2461 \n2462 @ansi_dialect.segment()\n2463 class StatementSegment(BaseSegment):\n2464     \"\"\"A generic segment, to any of its child subsegments.\"\"\"\n2465 \n2466     type = \"statement\"\n2467     match_grammar = GreedyUntil(Ref(\"SemicolonSegment\"))\n2468 \n2469     parse_grammar = OneOf(\n2470         Ref(\"SelectableGrammar\"),\n2471         Ref(\"InsertStatementSegment\"),\n2472         Ref(\"TransactionStatementSegment\"),\n2473         Ref(\"DropStatementSegment\"),\n2474         Ref(\"AccessStatementSegment\"),\n2475         Ref(\"CreateTableStatementSegment\"),\n2476         Ref(\"CreateTypeStatementSegment\"),\n2477         Ref(\"AlterTableStatementSegment\"),\n2478         Ref(\"CreateSchemaStatementSegment\"),\n2479         Ref(\"CreateDatabaseStatementSegment\"),\n2480         Ref(\"CreateExtensionStatementSegment\"),\n2481         Ref(\"CreateIndexStatementSegment\"),\n2482         Ref(\"DropIndexStatementSegment\"),\n2483         Ref(\"CreateViewStatementSegment\"),\n2484         Ref(\"DeleteStatementSegment\"),\n2485         Ref(\"UpdateStatementSegment\"),\n2486         Ref(\"CreateFunctionStatementSegment\"),\n2487         Ref(\"CreateModelStatementSegment\"),\n2488         Ref(\"DropModelStatementSegment\"),\n2489         Ref(\"DescribeStatementSegment\"),\n2490     )\n2491 \n2492     def get_table_references(self):\n2493         \"\"\"Use parsed tree to extract table references.\"\"\"\n2494         table_refs = set(\n2495             tbl_ref.raw for tbl_ref in self.recursive_crawl(\"table_reference\")\n2496         )\n2497         cte_refs = set(\n2498             cte_def.get_identifier().raw\n2499             for cte_def in self.recursive_crawl(\"common_table_expression\")\n2500         )\n2501         # External references are any table references which aren't\n2502         # also cte aliases.\n2503         return table_refs - cte_refs\n2504 \n2505 \n2506 @ansi_dialect.segment()\n2507 class WithNoSchemaBindingClauseSegment(BaseSegment):\n2508     \"\"\"WITH NO SCHEMA BINDING clause for Redshift's Late Binding Views.\n2509 \n2510     https://docs.aws.amazon.com/redshift/latest/dg/r_CREATE_VIEW.html\n2511     \"\"\"\n2512 \n2513     type = \"with_no_schema_binding_clause\"\n2514     match_grammar = Sequence(\n2515         \"WITH\",\n2516         \"NO\",\n2517         \"SCHEMA\",\n2518         \"BINDING\",\n2519     )\n2520 \n2521 \n2522 @ansi_dialect.segment()\n2523 class DescribeStatementSegment(BaseSegment):\n2524     \"\"\"A `Describe` statement.\n2525 \n2526     DESCRIBE <object type> <object name>\n2527     \"\"\"\n2528 \n2529     type = \"describe_statement\"\n2530     match_grammar = StartsWith(\"DESCRIBE\")\n2531 \n2532     parse_grammar = Sequence(\n2533         \"DESCRIBE\",\n2534         Ref(\"NakedIdentifierSegment\"),\n2535         Ref(\"ObjectReferenceSegment\"),\n2536     )\n2537 \n[end of src/sqlfluff/core/dialects/dialect_ansi.py]\n[start of src/sqlfluff/core/parser/lexer.py]\n1 \"\"\"The code for the Lexer.\"\"\"\n2 \n3 import logging\n4 from typing import Optional, List, Tuple, Union\n5 from collections import namedtuple\n6 import re\n7 \n8 from sqlfluff.core.parser.markers import FilePositionMarker, EnrichedFilePositionMarker\n9 from sqlfluff.core.parser.segments import (\n10     BaseSegment,\n11     RawSegment,\n12     Indent,\n13     Dedent,\n14     TemplateSegment,\n15 )\n16 from sqlfluff.core.errors import SQLLexError\n17 from sqlfluff.core.templaters import TemplatedFile\n18 from sqlfluff.core.config import FluffConfig\n19 \n20 # Instantiate the lexer logger\n21 lexer_logger = logging.getLogger(\"sqlfluff.lexer\")\n22 \n23 \n24 class LexMatch(namedtuple(\"LexMatch\", [\"new_string\", \"new_pos\", \"segments\"])):\n25     \"\"\"A class to hold matches from the Lexer.\"\"\"\n26 \n27     def __bool__(self):\n28         \"\"\"A LexMatch is truthy if it contains a non-zero number of matched segments.\"\"\"\n29         return len(self.segments) > 0\n30 \n31 \n32 class SingletonMatcher:\n33     \"\"\"This singleton matcher matches single characters.\n34 \n35     This is the simplest usable matcher, but it also defines some of the\n36     mechanisms for more complicated matchers, which may simply override the\n37     `_match` function rather than the public `match` function.  This acts as\n38     the base class for matchers.\n39     \"\"\"\n40 \n41     def __init__(\n42         self,\n43         name,\n44         template,\n45         target_seg_class,\n46         subdivide=None,\n47         trim_post_subdivide=None,\n48         *args,\n49         **kwargs\n50     ):\n51         self.name = name\n52         self.template = template\n53         self.target_seg_class = target_seg_class\n54         self.subdivide = subdivide\n55         self.trim_post_subdivide = trim_post_subdivide\n56 \n57     def _match(self, forward_string):\n58         \"\"\"The private match function. Just look for a single character match.\"\"\"\n59         if forward_string[0] == self.template:\n60             return forward_string[0]\n61         else:\n62             return None\n63 \n64     def _trim(self, matched, start_pos):\n65         \"\"\"Given a string, trim if we are allowed to.\n66 \n67         Returns:\n68             :obj:`tuple` of segments\n69 \n70         \"\"\"\n71         seg_buff = ()\n72         cont_buff = matched\n73         cont_pos_buff = start_pos\n74         idx = 0\n75 \n76         if self.trim_post_subdivide:\n77             trimmer = re.compile(self.trim_post_subdivide[\"regex\"], re.DOTALL)\n78             TrimClass = RawSegment.make(\n79                 self.trim_post_subdivide[\"regex\"],\n80                 name=self.trim_post_subdivide[\"name\"],\n81                 type=self.trim_post_subdivide[\"type\"],\n82             )\n83 \n84             for trim_mat in trimmer.finditer(matched):\n85                 trim_span = trim_mat.span()\n86                 # Is it at the start?\n87                 if trim_span[0] == 0:\n88                     seg_buff += (\n89                         TrimClass(\n90                             raw=matched[: trim_span[1]], pos_marker=cont_pos_buff\n91                         ),\n92                     )\n93                     idx = trim_span[1]\n94                     cont_pos_buff = cont_pos_buff.advance_by(matched[: trim_span[1]])\n95                     # Have we consumed the whole string? This avoids us having\n96                     # an empty string on the end.\n97                     if idx == len(matched):\n98                         break\n99                 # Is it at the end?\n100                 if trim_span[1] == len(matched):\n101                     seg_buff += (\n102                         self.target_seg_class(\n103                             raw=matched[idx : trim_span[0]], pos_marker=cont_pos_buff\n104                         ),\n105                         TrimClass(\n106                             raw=matched[trim_span[0] : trim_span[1]],\n107                             pos_marker=cont_pos_buff.advance_by(\n108                                 cont_buff[idx : trim_span[0]]\n109                             ),\n110                         ),\n111                     )\n112                     idx = len(matched)\n113 \n114         # Do we have anything left? (or did nothing happen)\n115         if idx < len(matched):\n116             seg_buff += (\n117                 self.target_seg_class(raw=matched[idx:], pos_marker=cont_pos_buff),\n118             )\n119 \n120         return seg_buff\n121 \n122     def _subdivide(self, matched, start_pos):\n123         \"\"\"Given a string, subdivide if we area allowed to.\n124 \n125         Returns:\n126             :obj:`tuple` of segments\n127 \n128         \"\"\"\n129         # Can we have to subdivide?\n130         if self.subdivide:\n131             # Yes subdivision\n132             seg_buff = ()\n133             str_buff = matched\n134             pos_buff = start_pos\n135             divider = re.compile(self.subdivide[\"regex\"], re.DOTALL)\n136             DividerClass = RawSegment.make(\n137                 self.subdivide[\"regex\"],\n138                 name=self.subdivide[\"name\"],\n139                 type=self.subdivide[\"type\"],\n140             )\n141 \n142             while True:\n143                 # Iterate through subdividing as appropriate\n144                 mat = divider.search(str_buff)\n145                 if mat:\n146                     # Found a division\n147                     span = mat.span()\n148                     trimmed_segments = self._trim(str_buff[: span[0]], pos_buff)\n149                     div_seg = DividerClass(\n150                         raw=str_buff[span[0] : span[1]],\n151                         pos_marker=pos_buff.advance_by(str_buff[: span[0]]),\n152                     )\n153                     seg_buff += trimmed_segments + (div_seg,)\n154                     pos_buff = pos_buff.advance_by(str_buff[: span[1]])\n155                     str_buff = str_buff[span[1] :]\n156                 else:\n157                     # No more division matches. Trim?\n158                     trimmed_segments = self._trim(str_buff, pos_buff)\n159                     seg_buff += trimmed_segments\n160                     pos_buff = pos_buff.advance_by(str_buff)\n161                     break\n162             return seg_buff\n163         else:\n164             # NB: Tuple literal\n165             return (self.target_seg_class(raw=matched, pos_marker=start_pos),)\n166 \n167     def match(self, forward_string, start_pos):\n168         \"\"\"Given a string, match what we can and return the rest.\n169 \n170         Returns:\n171             :obj:`LexMatch`\n172 \n173         \"\"\"\n174         if len(forward_string) == 0:\n175             raise ValueError(\"Unexpected empty string!\")\n176         matched = self._match(forward_string)\n177 \n178         if matched:\n179             # Handle potential subdivision elsewhere.\n180             new_segments = self._subdivide(matched, start_pos)\n181             return LexMatch(\n182                 forward_string[len(matched) :],\n183                 new_segments[-1].get_end_pos_marker(),\n184                 new_segments,\n185             )\n186         else:\n187             return LexMatch(forward_string, start_pos, ())\n188 \n189     @classmethod\n190     def from_shorthand(cls, name, template, **kwargs):\n191         \"\"\"A shorthand was of making new instances of this class.\n192 \n193         This is the primary way of defining matchers. It is convenient\n194         because several parameters of the matcher and the class of segment\n195         to be returned are shared, and here we define both together.\n196         \"\"\"\n197         # Some kwargs get consumed by the class, the rest\n198         # are passed to the raw segment.\n199         class_kwargs = {}\n200         possible_class_kwargs = [\"subdivide\", \"trim_post_subdivide\"]\n201         for k in possible_class_kwargs:\n202             if k in kwargs:\n203                 class_kwargs[k] = kwargs.pop(k)\n204 \n205         return cls(\n206             name,\n207             template,\n208             RawSegment.make(template, name=name, **kwargs),\n209             **class_kwargs\n210         )\n211 \n212 \n213 class RegexMatcher(SingletonMatcher):\n214     \"\"\"This RegexMatcher matches based on regular expressions.\"\"\"\n215 \n216     def __init__(self, *args, **kwargs):\n217         super().__init__(*args, **kwargs)\n218         # We might want to configure this at some point, but for now, newlines\n219         # do get matched by .\n220         flags = re.DOTALL\n221         self._compiled_regex = re.compile(self.template, flags)\n222 \n223     def _match(self, forward_string):\n224         \"\"\"Use regexes to match chunks.\"\"\"\n225         match = self._compiled_regex.match(forward_string)\n226         if match:\n227             return match.group(0)\n228         else:\n229             return None\n230 \n231 \n232 class RepeatedMultiMatcher(SingletonMatcher):\n233     \"\"\"Uses other matchers in priority order.\n234 \n235     Args:\n236         *submatchers: An iterable of other matchers which can be tried\n237             in turn. If none match a given forward looking string we simply\n238             return the unmatched part as per any other matcher.\n239 \n240     \"\"\"\n241 \n242     def __init__(self, *submatchers):\n243         self.submatchers = submatchers\n244 \n245     def match(self, forward_string, start_pos):\n246         \"\"\"Iteratively match strings using the selection of submatchers.\"\"\"\n247         seg_buff = ()\n248         while True:\n249             if len(forward_string) == 0:\n250                 return LexMatch(forward_string, start_pos, seg_buff)\n251             for matcher in self.submatchers:\n252                 res = matcher.match(forward_string, start_pos)\n253                 if res.segments:\n254                     # If we have new segments then whoop!\n255                     seg_buff += res.segments\n256                     forward_string = res.new_string\n257                     start_pos = res.new_pos\n258                     # Cycle back around again and start with the top\n259                     # matcher again.\n260                     break\n261                 else:\n262                     continue\n263             else:\n264                 # We've got so far, but now can't match. Return\n265                 return LexMatch(forward_string, start_pos, seg_buff)\n266 \n267     @classmethod\n268     def from_struct(cls, s):\n269         \"\"\"Creates a matcher from a lexer_struct.\n270 \n271         Expects an iterable of :obj:`tuple`. Each tuple should be:\n272         (name, type, pattern, kwargs).\n273 \n274         \"\"\"\n275         matchers = []\n276         for elem in s:\n277             if elem[1] == \"regex\":\n278                 m_cls = RegexMatcher\n279             elif elem[1] == \"singleton\":\n280                 m_cls = SingletonMatcher\n281             else:\n282                 raise ValueError(\n283                     \"Unexpected matcher type in lexer struct: {0!r}\".format(elem[1])\n284                 )\n285             k = elem[3] or {}\n286             m = m_cls.from_shorthand(elem[0], elem[2], **k)\n287             matchers.append(m)\n288         return cls(*matchers)\n289 \n290 \n291 class Lexer:\n292     \"\"\"The Lexer class actually does the lexing step.\"\"\"\n293 \n294     def __init__(\n295         self,\n296         config: Optional[FluffConfig] = None,\n297         last_resort_lexer: Optional[SingletonMatcher] = None,\n298         dialect: Optional[str] = None,\n299     ):\n300         # Allow optional config and dialect\n301         self.config = FluffConfig.from_kwargs(config=config, dialect=dialect)\n302         lexer_struct = self.config.get(\"dialect_obj\").get_lexer_struct()\n303         self.matcher = RepeatedMultiMatcher.from_struct(lexer_struct)\n304         self.last_resort_lexer = last_resort_lexer or RegexMatcher.from_shorthand(\n305             \"<unlexable>\", r\"[^\\t\\n\\,\\.\\ \\-\\+\\*\\\\\\/\\'\\\"\\;\\:\\[\\]\\(\\)\\|]*\", is_code=True\n306         )\n307 \n308     def lex(\n309         self, raw: Union[str, TemplatedFile]\n310     ) -> Tuple[Tuple[BaseSegment, ...], List[SQLLexError]]:\n311         \"\"\"Take a string or TemplatedFile and return segments.\n312 \n313         If we fail to match the *whole* string, then we must have\n314         found something that we cannot lex. If that happens we should\n315         package it up as unlexable and keep track of the exceptions.\n316         \"\"\"\n317         start_pos = FilePositionMarker()\n318         segment_buff = ()\n319         violations = []\n320 \n321         # Handle potential TemplatedFile for now\n322         str_buff = str(raw)\n323 \n324         while True:\n325             res = self.matcher.match(str_buff, start_pos)\n326             segment_buff += res.segments\n327             if len(res.new_string) > 0:\n328                 violations.append(\n329                     SQLLexError(\n330                         \"Unable to lex characters: '{0!r}...'\".format(\n331                             res.new_string[:10]\n332                         ),\n333                         pos=res.new_pos,\n334                     )\n335                 )\n336                 resort_res = self.last_resort_lexer.match(res.new_string, res.new_pos)\n337                 if not resort_res:\n338                     # If we STILL can't match, then just panic out.\n339                     raise violations[-1]\n340 \n341                 str_buff = resort_res.new_string\n342                 start_pos = resort_res.new_pos\n343                 segment_buff += resort_res.segments\n344             else:\n345                 break\n346 \n347         # Enrich the segments if we can using the templated file\n348         if isinstance(raw, TemplatedFile):\n349             return self.enrich_segments(segment_buff, raw), violations\n350         else:\n351             return segment_buff, violations\n352 \n353     @staticmethod\n354     def enrich_segments(\n355         segment_buff: Tuple[BaseSegment, ...], templated_file: TemplatedFile\n356     ) -> Tuple[BaseSegment, ...]:\n357         \"\"\"Enrich the segments using the templated file.\n358 \n359         We use the mapping in the template to provide positions\n360         in the source file.\n361         \"\"\"\n362         # Make a new buffer to hold the enriched segments.\n363         # We need a new buffer to hold the new meta segments\n364         # introduced.\n365         new_segment_buff = []\n366         # Get the templated slices to re-insert tokens for them\n367         source_only_slices = templated_file.source_only_slices()\n368 \n369         lexer_logger.info(\n370             \"Enriching Segments. Source-only slices: %s\", source_only_slices\n371         )\n372 \n373         for segment in segment_buff:\n374             templated_slice = slice(\n375                 segment.pos_marker.char_pos,\n376                 segment.pos_marker.char_pos + len(segment.raw),\n377             )\n378             source_slice = templated_file.templated_slice_to_source_slice(\n379                 templated_slice\n380             )\n381 \n382             # At this stage, templated slices will be INCLUDED in the source slice,\n383             # so we should consider whether we've captured any. If we have then\n384             # we need to re-evaluate whether it's a literal or not.\n385 \n386             for source_only_slice in source_only_slices:\n387                 if source_only_slice.source_idx > source_slice.start:\n388                     break\n389                 elif source_only_slice.source_idx == source_slice.start:\n390                     lexer_logger.debug(\n391                         \"Found templated section! %s, %s, %s\",\n392                         source_only_slice.source_slice(),\n393                         source_only_slice.slice_type,\n394                         templated_slice.start,\n395                     )\n396                     # Adjust the source slice accordingly.\n397                     source_slice = slice(\n398                         source_only_slice.end_source_idx(), source_slice.stop\n399                     )\n400 \n401                     # Add segments as appropriate.\n402                     # If it's a block end, add a dedent.\n403                     if source_only_slice.slice_type in (\"block_end\", \"block_mid\"):\n404                         new_segment_buff.append(\n405                             Dedent.when(template_blocks_indent=True)(\n406                                 pos_marker=segment.pos_marker\n407                             )\n408                         )\n409                     # Always add a placeholder\n410                     new_segment_buff.append(\n411                         TemplateSegment(\n412                             pos_marker=segment.pos_marker,\n413                             source_str=source_only_slice.raw,\n414                             block_type=source_only_slice.slice_type,\n415                         )\n416                     )\n417                     # If it's a block end, add a dedent.\n418                     if source_only_slice.slice_type in (\"block_start\", \"block_mid\"):\n419                         new_segment_buff.append(\n420                             Indent.when(template_blocks_indent=True)(\n421                                 pos_marker=segment.pos_marker\n422                             )\n423                         )\n424 \n425             source_line, source_pos = templated_file.get_line_pos_of_char_pos(\n426                 source_slice.start\n427             )\n428 \n429             # Recalculate is_literal\n430             is_literal = templated_file.is_source_slice_literal(source_slice)\n431 \n432             segment.pos_marker = EnrichedFilePositionMarker(\n433                 statement_index=segment.pos_marker.statement_index,\n434                 line_no=segment.pos_marker.line_no,\n435                 line_pos=segment.pos_marker.line_pos,\n436                 char_pos=segment.pos_marker.char_pos,\n437                 templated_slice=templated_slice,\n438                 source_slice=source_slice,\n439                 is_literal=is_literal,\n440                 source_pos_marker=FilePositionMarker(\n441                     segment.pos_marker.statement_index,\n442                     source_line,\n443                     source_pos,\n444                     source_slice.start,\n445                 ),\n446             )\n447             new_segment_buff.append(segment)\n448 \n449         lexer_logger.debug(\"Enriched Segments:\")\n450         for seg in new_segment_buff:\n451             lexer_logger.debug(\n452                 \"\\tTmp: %s\\tSrc: %s\\tSeg: %s\",\n453                 getattr(seg.pos_marker, \"templated_slice\", None),\n454                 getattr(seg.pos_marker, \"source_slice\", None),\n455                 seg,\n456             )\n457 \n458         return tuple(new_segment_buff)\n459 \n[end of src/sqlfluff/core/parser/lexer.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/core/dialects/dialect_ansi.py\n+++ b/src/sqlfluff/core/dialects/dialect_ansi.py\n@@ -53,7 +53,7 @@\n         (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n         (\n             \"inline_comment\",\n             \"regex\",\n-            r\"(--|#)[^\\n]*\",\n+            r\"(--|#)[^\\n]*\", \n             dict(is_comment=True, type=\"comment\", trim_start=(\"--\", \"#\")),\n         ),\n         (\n@@ -86,6 +86,7 @@\n         (\"newline\", \"regex\", r\"\\r\\n|\\n\", dict(type=\"newline\")),\n         (\"casting_operator\", \"regex\", r\"::\", dict(is_code=True)),\n         (\"concat_operator\", \"regex\", r\"\\|\\|\", dict(is_code=True)),\n+        (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n         (\"equals\", \"singleton\", \"=\", dict(is_code=True)),\n         (\"greater_than\", \"singleton\", \">\", dict(is_code=True)),\n         (\"less_than\", \"singleton\", \"<\", dict(is_code=True)),\n@@ -364,6 +365,7 @@\n         \"whitespace\",\n         \"regex\",\n         r\"[\\t ]+\",\n         dict(type=\"whitespace\"),\n     ),\n+    (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n     # Other segments...\n )\n \n```\n", "model_patch": "--- a/src/sqlfluff/core/dialects/dialect_ansi.py\n+++ b/src/sqlfluff/core/dialects/dialect_ansi.py\n@@ -53,7 +53,7 @@\n         (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n         (\n             \"inline_comment\",\n             \"regex\",\n-            r\"(--|#)[^\\n]*\",\n+            r\"(--|#)[^\\n]*\", \n             dict(is_comment=True, type=\"comment\", trim_start=(\"--\", \"#\")),\n         ),\n         (\n@@ -86,6 +86,7 @@\n         (\"newline\", \"regex\", r\"\\r\\n|\\n\", dict(type=\"newline\")),\n         (\"casting_operator\", \"regex\", r\"::\", dict(is_code=True)),\n         (\"concat_operator\", \"regex\", r\"\\|\\|\", dict(is_code=True)),\n+        (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n         (\"equals\", \"singleton\", \"=\", dict(is_code=True)),\n         (\"greater_than\", \"singleton\", \">\", dict(is_code=True)),\n         (\"less_than\", \"singleton\", \"<\", dict(is_code=True)),\n@@ -364,6 +365,7 @@\n         \"whitespace\",\n         \"regex\",\n         r\"[\\t ]+\",\n         dict(type=\"whitespace\"),\n     ),\n+    (\"whitespace\", \"regex\", r\"[\\t ]+\", dict(type=\"whitespace\")),\n     # Other segments...\n )\n \n"}
{"instance_id": "sqlfluff__sqlfluff-3904", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nStandardise `--disable_progress_bar` naming\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nAs noted in https://github.com/sqlfluff/sqlfluff/pull/3610#discussion_r926014745 `--disable_progress_bar` is the only command line option using underscores instead of dashes.\r\n\r\nShould we change this?\r\n\r\nThis would be a breaking change, so do we leave until next major release?\r\nOr do we accept both options?\n\n### Expected Behaviour\n\nWe should be standard in out command line option format\n\n### Observed Behaviour\n\n`--disable_progress_bar` is the only non-standard one\n\n### How to reproduce\n\nN/A\n\n### Dialect\n\nN/A\n\n### Version\n\n1.2.1\n\n### Configuration\n\nN/A\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [X] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![GitHub Workflow Status](https://img.shields.io/github/workflow/status/sqlfluff/sqlfluff/CI%20Tests?logo=github&style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 [![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)\n16 \n17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n18 \n19 ## Dialects Supported\n20 \n21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n22 \n23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n24 - [Athena](https://aws.amazon.com/athena/)\n25 - [BigQuery](https://cloud.google.com/bigquery/)\n26 - [ClickHouse](https://clickhouse.com/)\n27 - [Databricks](https://databricks.com/) (note: currently this is just an alias for the `sparksql` dialect).\n28 - [Db2](https://www.ibm.com/analytics/db2)\n29 - [Exasol](https://www.exasol.com/)\n30 - [Hive](https://hive.apache.org/)\n31 - [MySQL](https://www.mysql.com/)\n32 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n33 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n34 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n35 - [Snowflake](https://www.snowflake.com/)\n36 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n37 - [SparkSQL](https://spark.apache.org/docs/latest/)\n38 - [SQLite](https://www.sqlite.org/)\n39 - [Teradata](https://www.teradata.com/)\n40 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n41 \n42 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n43 \n44 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n45 \n46 ## Templates Supported\n47 \n48 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n49 \n50 **SQLFluff** supports the following templates:\n51 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n52 - [dbt](https://www.getdbt.com/)\n53 \n54 Again, please raise issues if you wish to support more templating languages/syntaxes.\n55 \n56 # Getting Started\n57 \n58 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n59 \n60 ```shell\n61 $ pip install sqlfluff\n62 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n63 $ sqlfluff lint test.sql --dialect ansi\n64 == [test.sql] FAIL\n65 L:   1 | P:   1 | L050 | Files must not begin with newlines or whitespace.\n66 L:   1 | P:   3 | L003 | First line has unexpected indent\n67 L:   1 | P:  11 | L039 | Unnecessary whitespace found.\n68 L:   1 | P:  14 | L039 | Unnecessary whitespace found.\n69 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace.\n70 ```\n71 \n72 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n73 \n74 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n75 \n76 # Documentation\n77 \n78 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n79 \n80 # Releases\n81 \n82 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html), so breaking changes\n83 should be restricted to major versions releases. Some elements (such as the python API) are in a less\n84 stable state and may see more significant changes more often. See the [changelog](CHANGELOG.md) for more details.\n85 If you would like to join in please consider [contributing](CONTRIBUTING.md).\n86 \n87 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n88 \n89 # SQLFluff on Slack\n90 \n91 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n92 \n93 # SQLFluff on Twitter\n94 \n95 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n96 \n97 # Contributing\n98 \n99 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n100 \n101 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n102 \n103 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n104 \n[end of README.md]\n[start of src/sqlfluff/cli/commands.py]\n1 \"\"\"Contains the CLI.\"\"\"\n2 \n3 from itertools import chain\n4 import os\n5 import sys\n6 import json\n7 import logging\n8 import time\n9 from logging import LogRecord\n10 from typing import Callable, Tuple, Optional, cast\n11 \n12 import yaml\n13 \n14 import click\n15 \n16 # For the profiler\n17 import pstats\n18 from io import StringIO\n19 \n20 # To enable colour cross platform\n21 import colorama\n22 from tqdm import tqdm\n23 from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n24 \n25 from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n26 from sqlfluff.cli.formatters import (\n27     format_linting_result_header,\n28     OutputStreamFormatter,\n29 )\n30 from sqlfluff.cli.helpers import get_package_version\n31 from sqlfluff.cli.outputstream import make_output_stream, OutputStream\n32 \n33 # Import from sqlfluff core.\n34 from sqlfluff.core import (\n35     Linter,\n36     FluffConfig,\n37     SQLLintError,\n38     SQLTemplaterError,\n39     SQLFluffUserError,\n40     dialect_selector,\n41     dialect_readout,\n42 )\n43 from sqlfluff.core.config import progress_bar_configuration\n44 \n45 from sqlfluff.core.enums import FormatType, Color\n46 from sqlfluff.core.plugin.host import get_plugin_manager\n47 \n48 \n49 class RedWarningsFilter(logging.Filter):\n50     \"\"\"This filter makes all warnings or above red.\"\"\"\n51 \n52     def __init__(self, formatter: OutputStreamFormatter):\n53         super().__init__()\n54         self.formatter = formatter\n55 \n56     def filter(self, record: logging.LogRecord) -> bool:\n57         \"\"\"Filter any warnings (or above) to turn them red.\"\"\"\n58         if record.levelno >= logging.WARNING:\n59             record.msg = f\"{self.formatter.colorize(record.msg, Color.red)} \"\n60         return True\n61 \n62 \n63 class StreamHandlerTqdm(logging.StreamHandler):\n64     \"\"\"Modified StreamHandler which takes care of writing within `tqdm` context.\n65 \n66     It uses `tqdm` write which takes care of conflicting prints with progressbar.\n67     Without it, there were left artifacts in DEBUG mode (not sure about another ones,\n68     but probably would happen somewhere).\n69     \"\"\"\n70 \n71     def emit(self, record: LogRecord) -> None:\n72         \"\"\"Behaves like original one except uses `tqdm` to write.\"\"\"\n73         try:\n74             msg = self.format(record)\n75             tqdm.write(msg, file=self.stream)\n76             self.flush()\n77         except Exception:  # pragma: no cover\n78             self.handleError(record)\n79 \n80 \n81 def set_logging_level(\n82     verbosity: int,\n83     formatter: OutputStreamFormatter,\n84     logger: Optional[logging.Logger] = None,\n85     stderr_output: bool = False,\n86 ) -> None:\n87     \"\"\"Set up logging for the CLI.\n88 \n89     We either set up global logging based on the verbosity\n90     or, if `logger` is specified, we only limit to a single\n91     sqlfluff logger. Verbosity is applied in the same way.\n92 \n93     Implementation: If `logger` is not specified, the handler\n94     is attached to the `sqlfluff` logger. If it is specified\n95     then it attaches the the logger in question. In addition\n96     if `logger` is specified, then that logger will also\n97     not propagate.\n98     \"\"\"\n99     fluff_logger = logging.getLogger(\"sqlfluff\")\n100     # Don't propagate logging\n101     fluff_logger.propagate = False\n102 \n103     # Enable colorama\n104     colorama.init()\n105 \n106     # Set up the log handler which is able to print messages without overlapping\n107     # with progressbars.\n108     handler = StreamHandlerTqdm(stream=sys.stderr if stderr_output else sys.stdout)\n109     # NB: the unicode character at the beginning is to squash any badly\n110     # tamed ANSI colour statements, and return us to normality.\n111     handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n112     # Set up a handler to colour warnings red.\n113     handler.addFilter(RedWarningsFilter(formatter))\n114     if logger:\n115         focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n116         focus_logger.addHandler(handler)\n117     else:\n118         fluff_logger.addHandler(handler)\n119 \n120     # NB: We treat the parser logger slightly differently because it's noisier.\n121     # It's important that we set levels for all each time so\n122     # that we don't break tests by changing the granularity\n123     # between tests.\n124     parser_logger = logging.getLogger(\"sqlfluff.parser\")\n125     if verbosity < 3:\n126         fluff_logger.setLevel(logging.WARNING)\n127         parser_logger.setLevel(logging.NOTSET)\n128     elif verbosity == 3:\n129         fluff_logger.setLevel(logging.INFO)\n130         parser_logger.setLevel(logging.WARNING)\n131     elif verbosity == 4:\n132         fluff_logger.setLevel(logging.DEBUG)\n133         parser_logger.setLevel(logging.INFO)\n134     elif verbosity > 4:\n135         fluff_logger.setLevel(logging.DEBUG)\n136         parser_logger.setLevel(logging.DEBUG)\n137 \n138 \n139 class PathAndUserErrorHandler:\n140     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n141 \n142     def __init__(self, formatter, paths):\n143         self.formatter = formatter\n144         self.paths = paths\n145 \n146     def __enter__(self):\n147         return self\n148 \n149     def __exit__(self, exc_type, exc_val, exc_tb):\n150         if exc_type is OSError:\n151             click.echo(\n152                 self.formatter.colorize(\n153                     f\"The path(s) { self.paths } could not be \"\n154                     \"accessed. Check it/they exist(s).\",\n155                     Color.red,\n156                 )\n157             )\n158             sys.exit(EXIT_ERROR)\n159         elif exc_type is SQLFluffUserError:\n160             click.echo(\n161                 \"\\nUser Error: \"\n162                 + self.formatter.colorize(\n163                     str(exc_val),\n164                     Color.red,\n165                 )\n166             )\n167             sys.exit(EXIT_ERROR)\n168 \n169 \n170 def common_options(f: Callable) -> Callable:\n171     \"\"\"Add common options to commands via a decorator.\n172 \n173     These are applied to all of the cli commands.\n174     \"\"\"\n175     f = click.version_option()(f)\n176     f = click.option(\n177         \"-v\",\n178         \"--verbose\",\n179         count=True,\n180         default=None,\n181         help=(\n182             \"Verbosity, how detailed should the output be. This is *stackable*, so \"\n183             \"`-vv` is more verbose than `-v`. For the most verbose option try `-vvvv` \"\n184             \"or `-vvvvv`.\"\n185         ),\n186     )(f)\n187     f = click.option(\n188         \"-n\",\n189         \"--nocolor\",\n190         is_flag=True,\n191         default=None,\n192         help=\"No color - output will be without ANSI color codes.\",\n193     )(f)\n194 \n195     return f\n196 \n197 \n198 def core_options(f: Callable) -> Callable:\n199     \"\"\"Add core operation options to commands via a decorator.\n200 \n201     These are applied to the main (but not all) cli commands like\n202     `parse`, `lint` and `fix`.\n203     \"\"\"\n204     # Only enable dialect completion if on version of click\n205     # that supports it\n206     if shell_completion_enabled:\n207         f = click.option(\n208             \"-d\",\n209             \"--dialect\",\n210             default=None,\n211             help=\"The dialect of SQL to lint\",\n212             shell_complete=dialect_shell_complete,\n213         )(f)\n214     else:  # pragma: no cover\n215         f = click.option(\n216             \"-d\",\n217             \"--dialect\",\n218             default=None,\n219             help=\"The dialect of SQL to lint\",\n220         )(f)\n221     f = click.option(\n222         \"-t\",\n223         \"--templater\",\n224         default=None,\n225         help=\"The templater to use (default=jinja)\",\n226         type=click.Choice(\n227             [\n228                 templater.name\n229                 for templater in chain.from_iterable(\n230                     get_plugin_manager().hook.get_templaters()\n231                 )\n232             ]\n233         ),\n234     )(f)\n235     f = click.option(\n236         \"-r\",\n237         \"--rules\",\n238         default=None,\n239         help=(\n240             \"Narrow the search to only specific rules. For example \"\n241             \"specifying `--rules L001` will only search for rule `L001` (Unnecessary \"\n242             \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n243             \"`--rules L001,L002` will specify only looking for violations of rule \"\n244             \"`L001` and rule `L002`.\"\n245         ),\n246     )(f)\n247     f = click.option(\n248         \"-e\",\n249         \"--exclude-rules\",\n250         default=None,\n251         help=(\n252             \"Exclude specific rules. For example \"\n253             \"specifying `--exclude-rules L001` will remove rule `L001` (Unnecessary \"\n254             \"trailing whitespace) from the set of considered rules. This could either \"\n255             \"be the allowlist, or the general set if there is no specific allowlist. \"\n256             \"Multiple rules can be specified with commas e.g. \"\n257             \"`--exclude-rules L001,L002` will exclude violations of rule \"\n258             \"`L001` and rule `L002`.\"\n259         ),\n260     )(f)\n261     f = click.option(\n262         \"--config\",\n263         \"extra_config_path\",\n264         default=None,\n265         help=(\n266             \"Include additional config file. By default the config is generated \"\n267             \"from the standard configuration files described in the documentation. \"\n268             \"This argument allows you to specify an additional configuration file that \"\n269             \"overrides the standard configuration files. N.B. cfg format is required.\"\n270         ),\n271         type=click.Path(),\n272     )(f)\n273     f = click.option(\n274         \"--ignore-local-config\",\n275         is_flag=True,\n276         help=(\n277             \"Ignore config files in default search path locations. \"\n278             \"This option allows the user to lint with the default config \"\n279             \"or can be used in conjunction with --config to only \"\n280             \"reference the custom config file.\"\n281         ),\n282     )(f)\n283     f = click.option(\n284         \"--encoding\",\n285         default=None,\n286         help=(\n287             \"Specify encoding to use when reading and writing files. Defaults to \"\n288             \"autodetect.\"\n289         ),\n290     )(f)\n291     f = click.option(\n292         \"-i\",\n293         \"--ignore\",\n294         default=None,\n295         help=(\n296             \"Ignore particular families of errors so that they don't cause a failed \"\n297             \"run. For example `--ignore parsing` would mean that any parsing errors \"\n298             \"are ignored and don't influence the success or fail of a run. \"\n299             \"`--ignore` behaves somewhat like `noqa` comments, except it \"\n300             \"applies globally. Multiple options are possible if comma separated: \"\n301             \"e.g. `--ignore parsing,templating`.\"\n302         ),\n303     )(f)\n304     f = click.option(\n305         \"--bench\",\n306         is_flag=True,\n307         help=\"Set this flag to engage the benchmarking tool output.\",\n308     )(f)\n309     f = click.option(\n310         \"--logger\",\n311         type=click.Choice(\n312             [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\"], case_sensitive=False\n313         ),\n314         help=\"Choose to limit the logging to one of the loggers.\",\n315     )(f)\n316     f = click.option(\n317         \"--disable-noqa\",\n318         is_flag=True,\n319         default=None,\n320         help=\"Set this flag to ignore inline noqa comments.\",\n321     )(f)\n322     return f\n323 \n324 \n325 def get_config(\n326     extra_config_path: Optional[str] = None,\n327     ignore_local_config: bool = False,\n328     **kwargs,\n329 ) -> FluffConfig:\n330     \"\"\"Get a config object from kwargs.\"\"\"\n331     plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n332     if kwargs.get(\"dialect\"):\n333         try:\n334             # We're just making sure it exists at this stage.\n335             # It will be fetched properly in the linter.\n336             dialect_selector(kwargs[\"dialect\"])\n337         except SQLFluffUserError as err:\n338             click.echo(\n339                 OutputStreamFormatter.colorize_helper(\n340                     plain_output,\n341                     f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n342                     color=Color.red,\n343                 )\n344             )\n345             sys.exit(EXIT_ERROR)\n346         except KeyError:\n347             click.echo(\n348                 OutputStreamFormatter.colorize_helper(\n349                     plain_output,\n350                     f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n351                     color=Color.red,\n352                 )\n353             )\n354             sys.exit(EXIT_ERROR)\n355     from_root_kwargs = {}\n356     if \"require_dialect\" in kwargs:\n357         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n358     # Instantiate a config object (filtering out the nulls)\n359     overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n360     try:\n361         return FluffConfig.from_root(\n362             extra_config_path=extra_config_path,\n363             ignore_local_config=ignore_local_config,\n364             overrides=overrides,\n365             **from_root_kwargs,\n366         )\n367     except SQLFluffUserError as err:  # pragma: no cover\n368         click.echo(\n369             OutputStreamFormatter.colorize_helper(\n370                 plain_output,\n371                 f\"Error loading config: {str(err)}\",\n372                 color=Color.red,\n373             )\n374         )\n375         sys.exit(EXIT_ERROR)\n376 \n377 \n378 def get_linter_and_formatter(\n379     cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n380 ) -> Tuple[Linter, OutputStreamFormatter]:\n381     \"\"\"Get a linter object given a config.\"\"\"\n382     try:\n383         # We're just making sure it exists at this stage.\n384         # It will be fetched properly in the linter.\n385         dialect = cfg.get(\"dialect\")\n386         if dialect:\n387             dialect_selector(dialect)\n388     except KeyError:  # pragma: no cover\n389         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n390         sys.exit(EXIT_ERROR)\n391     formatter = OutputStreamFormatter(\n392         output_stream=output_stream or make_output_stream(cfg),\n393         nocolor=cfg.get(\"nocolor\"),\n394         verbosity=cfg.get(\"verbose\"),\n395         output_line_length=cfg.get(\"output_line_length\"),\n396     )\n397     return Linter(config=cfg, formatter=formatter), formatter\n398 \n399 \n400 @click.group(\n401     context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n402     epilog=\"\"\"\\b\\bExamples:\\n\n403   sqlfluff lint --dialect postgres .\\n\n404   sqlfluff lint --dialect postgres --rules L042 .\\n\n405   sqlfluff fix --dialect sqlite --rules L041,L042 src/queries\\n\n406   sqlfluff parse --dialect sqlite --templater jinja src/queries/common.sql\n407 \"\"\",\n408 )\n409 @click.version_option()\n410 def cli():\n411     \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n412 \n413 \n414 @cli.command()\n415 @common_options\n416 def version(**kwargs) -> None:\n417     \"\"\"Show the version of sqlfluff.\"\"\"\n418     c = get_config(**kwargs, require_dialect=False)\n419     if c.get(\"verbose\") > 0:\n420         # Instantiate the linter\n421         lnt, formatter = get_linter_and_formatter(c)\n422         # Dispatch the detailed config from the linter.\n423         formatter.dispatch_config(lnt)\n424     else:\n425         # Otherwise just output the package version.\n426         click.echo(get_package_version(), color=c.get(\"color\"))\n427 \n428 \n429 @cli.command()\n430 @common_options\n431 def rules(**kwargs) -> None:\n432     \"\"\"Show the current rules in use.\"\"\"\n433     c = get_config(**kwargs, dialect=\"ansi\")\n434     lnt, formatter = get_linter_and_formatter(c)\n435     click.echo(formatter.format_rules(lnt), color=c.get(\"color\"))\n436 \n437 \n438 @cli.command()\n439 @common_options\n440 def dialects(**kwargs) -> None:\n441     \"\"\"Show the current dialects available.\"\"\"\n442     c = get_config(**kwargs, require_dialect=False)\n443     _, formatter = get_linter_and_formatter(c)\n444     click.echo(formatter.format_dialects(dialect_readout), color=c.get(\"color\"))\n445 \n446 \n447 def dump_file_payload(filename: Optional[str], payload: str):\n448     \"\"\"Write the output file content to stdout or file.\"\"\"\n449     # If there's a file specified to write to, write to it.\n450     if filename:\n451         with open(filename, \"w\") as out_file:\n452             out_file.write(payload)\n453     # Otherwise write to stdout\n454     else:\n455         click.echo(payload)\n456 \n457 \n458 @cli.command()\n459 @common_options\n460 @core_options\n461 @click.option(\n462     \"-f\",\n463     \"--format\",\n464     \"format\",\n465     default=\"human\",\n466     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n467     help=\"What format to return the lint result in (default=human).\",\n468 )\n469 @click.option(\n470     \"--write-output\",\n471     help=(\n472         \"Optionally provide a filename to write the results to, mostly used in \"\n473         \"tandem with --format. NB: Setting an output file re-enables normal \"\n474         \"stdout logging.\"\n475     ),\n476 )\n477 @click.option(\n478     \"--annotation-level\",\n479     default=\"notice\",\n480     type=click.Choice([\"notice\", \"warning\", \"failure\", \"error\"], case_sensitive=False),\n481     help=(\n482         \"When format is set to github-annotation or github-annotation-native, \"\n483         \"default annotation level (default=notice). failure and error are equivalent.\"\n484     ),\n485 )\n486 @click.option(\n487     \"--nofail\",\n488     is_flag=True,\n489     help=(\n490         \"If set, the exit code will always be zero, regardless of violations \"\n491         \"found. This is potentially useful during rollout.\"\n492     ),\n493 )\n494 @click.option(\n495     \"--disregard-sqlfluffignores\",\n496     is_flag=True,\n497     help=\"Perform the operation regardless of .sqlfluffignore configurations\",\n498 )\n499 @click.option(\n500     \"-p\",\n501     \"--processes\",\n502     type=int,\n503     default=None,\n504     help=(\n505         \"The number of parallel processes to run. Positive numbers work as \"\n506         \"expected. Zero and negative numbers will work as number_of_cpus - \"\n507         \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n508     ),\n509 )\n510 @click.option(\n511     \"--disable_progress_bar\",\n512     is_flag=True,\n513     help=\"Disables progress bars.\",\n514 )\n515 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n516 def lint(\n517     paths: Tuple[str],\n518     format: str,\n519     write_output: Optional[str],\n520     annotation_level: str,\n521     nofail: bool,\n522     disregard_sqlfluffignores: bool,\n523     logger: Optional[logging.Logger] = None,\n524     bench: bool = False,\n525     processes: Optional[int] = None,\n526     disable_progress_bar: Optional[bool] = False,\n527     extra_config_path: Optional[str] = None,\n528     ignore_local_config: bool = False,\n529     **kwargs,\n530 ) -> None:\n531     \"\"\"Lint SQL files via passing a list of files or using stdin.\n532 \n533     PATH is the path to a sql file or directory to lint. This can be either a\n534     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n535     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n536     be interpreted like passing the current working directory as a path argument.\n537 \n538     Linting SQL files:\n539 \n540         sqlfluff lint path/to/file.sql\n541         sqlfluff lint directory/of/sql/files\n542 \n543     Linting a file via stdin (note the lone '-' character):\n544 \n545         cat path/to/file.sql | sqlfluff lint -\n546         echo 'select col from tbl' | sqlfluff lint -\n547 \n548     \"\"\"\n549     config = get_config(\n550         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n551     )\n552     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n553     file_output = None\n554     output_stream = make_output_stream(config, format, write_output)\n555     lnt, formatter = get_linter_and_formatter(config, output_stream)\n556 \n557     verbose = config.get(\"verbose\")\n558     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n559 \n560     formatter.dispatch_config(lnt)\n561 \n562     # Set up logging.\n563     set_logging_level(\n564         verbosity=verbose,\n565         formatter=formatter,\n566         logger=logger,\n567         stderr_output=non_human_output,\n568     )\n569 \n570     # Output the results as we go\n571     if verbose >= 1:\n572         click.echo(format_linting_result_header())\n573 \n574     with PathAndUserErrorHandler(formatter, paths):\n575         # add stdin if specified via lone '-'\n576         if (\"-\",) == paths:\n577             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n578         else:\n579             result = lnt.lint_paths(\n580                 paths,\n581                 ignore_non_existent_files=False,\n582                 ignore_files=not disregard_sqlfluffignores,\n583                 processes=processes,\n584             )\n585 \n586     # Output the final stats\n587     if verbose >= 1:\n588         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n589 \n590     if format == FormatType.json.value:\n591         file_output = json.dumps(result.as_records())\n592     elif format == FormatType.yaml.value:\n593         file_output = yaml.dump(result.as_records(), sort_keys=False)\n594     elif format == FormatType.github_annotation.value:\n595         if annotation_level == \"error\":\n596             annotation_level = \"failure\"\n597 \n598         github_result = []\n599         for record in result.as_records():\n600             filepath = record[\"filepath\"]\n601             for violation in record[\"violations\"]:\n602                 # NOTE: The output format is designed for this GitHub action:\n603                 # https://github.com/yuzutech/annotations-action\n604                 # It is similar, but not identical, to the native GitHub format:\n605                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n606                 github_result.append(\n607                     {\n608                         \"file\": filepath,\n609                         \"line\": violation[\"line_no\"],\n610                         \"start_column\": violation[\"line_pos\"],\n611                         \"end_column\": violation[\"line_pos\"],\n612                         \"title\": \"SQLFluff\",\n613                         \"message\": f\"{violation['code']}: {violation['description']}\",\n614                         \"annotation_level\": annotation_level,\n615                     }\n616                 )\n617         file_output = json.dumps(github_result)\n618     elif format == FormatType.github_annotation_native.value:\n619         if annotation_level == \"failure\":\n620             annotation_level = \"error\"\n621 \n622         github_result_native = []\n623         for record in result.as_records():\n624             filepath = record[\"filepath\"]\n625             for violation in record[\"violations\"]:\n626                 # NOTE: The output format is designed for GitHub action:\n627                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n628                 line = f\"::{annotation_level} \"\n629                 line += \"title=SQLFluff,\"\n630                 line += f\"file={filepath},\"\n631                 line += f\"line={violation['line_no']},\"\n632                 line += f\"col={violation['line_pos']}\"\n633                 line += \"::\"\n634                 line += f\"{violation['code']}: {violation['description']}\"\n635 \n636                 github_result_native.append(line)\n637 \n638         file_output = \"\\n\".join(github_result_native)\n639 \n640     if file_output:\n641         dump_file_payload(write_output, cast(str, file_output))\n642 \n643     output_stream.close()\n644     if bench:\n645         click.echo(\"==== overall timings ====\")\n646         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n647         timing_summary = result.timing_summary()\n648         for step in timing_summary:\n649             click.echo(f\"=== {step} ===\")\n650             click.echo(formatter.cli_table(timing_summary[step].items()))\n651 \n652     if not nofail:\n653         if not non_human_output:\n654             formatter.completion_message()\n655         sys.exit(result.stats()[\"exit code\"])\n656     else:\n657         sys.exit(EXIT_SUCCESS)\n658 \n659 \n660 def do_fixes(lnt, result, formatter=None, **kwargs):\n661     \"\"\"Actually do the fixes.\"\"\"\n662     click.echo(\"Persisting Changes...\")\n663     res = result.persist_changes(formatter=formatter, **kwargs)\n664     if all(res.values()):\n665         click.echo(\"Done. Please check your files to confirm.\")\n666         return True\n667     # If some failed then return false\n668     click.echo(\n669         \"Done. Some operations failed. Please check your files to confirm.\"\n670     )  # pragma: no cover\n671     click.echo(\n672         \"Some errors cannot be fixed or there is another error blocking it.\"\n673     )  # pragma: no cover\n674     return False  # pragma: no cover\n675 \n676 \n677 @cli.command()\n678 @common_options\n679 @core_options\n680 @click.option(\n681     \"-f\",\n682     \"--force\",\n683     is_flag=True,\n684     help=(\n685         \"skip the confirmation prompt and go straight to applying \"\n686         \"fixes. **Use this with caution.**\"\n687     ),\n688 )\n689 @click.option(\n690     \"-x\",\n691     \"--fixed-suffix\",\n692     default=None,\n693     help=\"An optional suffix to add to fixed files.\",\n694 )\n695 @click.option(\n696     \"-p\",\n697     \"--processes\",\n698     type=int,\n699     default=None,\n700     help=(\n701         \"The number of parallel processes to run. Positive numbers work as \"\n702         \"expected. Zero and negative numbers will work as number_of_cpus - \"\n703         \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n704     ),\n705 )\n706 @click.option(\n707     \"--disable_progress_bar\",\n708     is_flag=True,\n709     help=\"Disables progress bars.\",\n710 )\n711 @click.option(\n712     \"--FIX-EVEN-UNPARSABLE\",\n713     is_flag=True,\n714     default=None,\n715     help=(\n716         \"Enables fixing of files that have templating or parse errors. \"\n717         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n718         \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n719         \"command will not make any fixes in files that have templating or parse \"\n720         \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n721         \"or in the .sqlfluff config file.\"\n722     ),\n723 )\n724 @click.option(\n725     \"--show-lint-violations\",\n726     is_flag=True,\n727     help=\"Show lint violations\",\n728 )\n729 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n730 def fix(\n731     force: bool,\n732     paths: Tuple[str],\n733     bench: bool = False,\n734     fixed_suffix: str = \"\",\n735     logger: Optional[logging.Logger] = None,\n736     processes: Optional[int] = None,\n737     disable_progress_bar: Optional[bool] = False,\n738     extra_config_path: Optional[str] = None,\n739     ignore_local_config: bool = False,\n740     show_lint_violations: bool = False,\n741     **kwargs,\n742 ) -> None:\n743     \"\"\"Fix SQL files.\n744 \n745     PATH is the path to a sql file or directory to lint. This can be either a\n746     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n747     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n748     be interpreted like passing the current working directory as a path argument.\n749     \"\"\"\n750     # some quick checks\n751     fixing_stdin = (\"-\",) == paths\n752 \n753     config = get_config(\n754         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n755     )\n756     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n757     output_stream = make_output_stream(\n758         config, None, os.devnull if fixing_stdin else None\n759     )\n760     lnt, formatter = get_linter_and_formatter(config, output_stream)\n761 \n762     verbose = config.get(\"verbose\")\n763     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n764 \n765     exit_code = EXIT_SUCCESS\n766 \n767     formatter.dispatch_config(lnt)\n768 \n769     # Set up logging.\n770     set_logging_level(\n771         verbosity=verbose,\n772         formatter=formatter,\n773         logger=logger,\n774         stderr_output=fixing_stdin,\n775     )\n776 \n777     # handle stdin case. should output formatted sql to stdout and nothing else.\n778     if fixing_stdin:\n779         stdin = sys.stdin.read()\n780 \n781         result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n782         templater_error = result.num_violations(types=SQLTemplaterError) > 0\n783         unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n784         if not fix_even_unparsable:\n785             exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n786 \n787         if result.num_violations(types=SQLLintError, fixable=True) > 0:\n788             stdout = result.paths[0].files[0].fix_string()[0]\n789         else:\n790             stdout = stdin\n791 \n792         if templater_error:\n793             click.echo(\n794                 formatter.colorize(\n795                     \"Fix aborted due to unparseable template variables.\",\n796                     Color.red,\n797                 ),\n798                 err=True,\n799             )\n800             click.echo(\n801                 formatter.colorize(\n802                     \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n803                     Color.red,\n804                 ),\n805                 err=True,\n806             )\n807 \n808         if unfixable_error:\n809             click.echo(\n810                 formatter.colorize(\"Unfixable violations detected.\", Color.red),\n811                 err=True,\n812             )\n813 \n814         click.echo(stdout, nl=False)\n815         sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n816 \n817     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n818     click.echo(\"==== finding fixable violations ====\")\n819 \n820     with PathAndUserErrorHandler(formatter, paths):\n821         result = lnt.lint_paths(\n822             paths,\n823             fix=True,\n824             ignore_non_existent_files=False,\n825             processes=processes,\n826         )\n827 \n828     if not fix_even_unparsable:\n829         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n830 \n831     # NB: We filter to linting violations here, because they're\n832     # the only ones which can be potentially fixed.\n833     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n834         click.echo(\"==== fixing violations ====\")\n835         click.echo(\n836             f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\n837             \"linting violations found\"\n838         )\n839         if force:\n840             click.echo(\n841                 f\"{formatter.colorize('FORCE MODE', Color.red)}: Attempting fixes...\"\n842             )\n843             success = do_fixes(\n844                 lnt,\n845                 result,\n846                 formatter,\n847                 types=SQLLintError,\n848                 fixed_file_suffix=fixed_suffix,\n849             )\n850             if not success:\n851                 sys.exit(EXIT_FAIL)  # pragma: no cover\n852         else:\n853             click.echo(\n854                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n855             )\n856             c = click.getchar().lower()\n857             click.echo(\"...\")\n858             if c in (\"y\", \"\\r\", \"\\n\"):\n859                 click.echo(\"Attempting fixes...\")\n860                 success = do_fixes(\n861                     lnt,\n862                     result,\n863                     formatter,\n864                     types=SQLLintError,\n865                     fixed_file_suffix=fixed_suffix,\n866                 )\n867                 if not success:\n868                     sys.exit(EXIT_FAIL)  # pragma: no cover\n869                 else:\n870                     formatter.completion_message()\n871             elif c == \"n\":\n872                 click.echo(\"Aborting...\")\n873                 exit_code = EXIT_FAIL\n874             else:  # pragma: no cover\n875                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n876                 click.echo(\"Aborting...\")\n877                 exit_code = EXIT_FAIL\n878     else:\n879         click.echo(\"==== no fixable linting violations found ====\")\n880         formatter.completion_message()\n881 \n882     error_types = [\n883         (\n884             dict(types=SQLLintError, fixable=False),\n885             \"  [{} unfixable linting violations found]\",\n886             EXIT_FAIL,\n887         ),\n888     ]\n889     for num_violations_kwargs, message_format, error_level in error_types:\n890         num_violations = result.num_violations(**num_violations_kwargs)\n891         if num_violations > 0:\n892             click.echo(message_format.format(num_violations))\n893             exit_code = max(exit_code, error_level)\n894 \n895     if bench:\n896         click.echo(\"==== overall timings ====\")\n897         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n898         timing_summary = result.timing_summary()\n899         for step in timing_summary:\n900             click.echo(f\"=== {step} ===\")\n901             click.echo(formatter.cli_table(timing_summary[step].items()))\n902 \n903     if show_lint_violations:\n904         click.echo(\"==== lint for unfixable violations ====\")\n905         all_results = result.violation_dict(**num_violations_kwargs)\n906         sorted_files = sorted(all_results.keys())\n907         for file in sorted_files:\n908             violations = all_results.get(file, [])\n909             click.echo(formatter.format_filename(file, success=(not violations)))\n910             for violation in violations:\n911                 click.echo(formatter.format_violation(violation))\n912 \n913     sys.exit(exit_code)\n914 \n915 \n916 def quoted_presenter(dumper, data):\n917     \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n918     if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n919         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n920     else:\n921         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n922 \n923 \n924 @cli.command()\n925 @common_options\n926 @core_options\n927 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n928 @click.option(\n929     \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n930 )\n931 @click.option(\n932     \"-c\",\n933     \"--code-only\",\n934     is_flag=True,\n935     help=\"Output only the code elements of the parse tree.\",\n936 )\n937 @click.option(\n938     \"-m\",\n939     \"--include-meta\",\n940     is_flag=True,\n941     help=(\n942         \"Include meta segments (indents, dedents and placeholders) in the output. \"\n943         \"This only applies when outputting json or yaml.\"\n944     ),\n945 )\n946 @click.option(\n947     \"-f\",\n948     \"--format\",\n949     default=FormatType.human.value,\n950     type=click.Choice(\n951         [\n952             FormatType.human.value,\n953             FormatType.json.value,\n954             FormatType.yaml.value,\n955         ],\n956         case_sensitive=False,\n957     ),\n958     help=\"What format to return the parse result in.\",\n959 )\n960 @click.option(\n961     \"--write-output\",\n962     help=(\n963         \"Optionally provide a filename to write the results to, mostly used in \"\n964         \"tandem with --format. NB: Setting an output file re-enables normal \"\n965         \"stdout logging.\"\n966     ),\n967 )\n968 @click.option(\n969     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n970 )\n971 @click.option(\n972     \"--nofail\",\n973     is_flag=True,\n974     help=(\n975         \"If set, the exit code will always be zero, regardless of violations \"\n976         \"found. This is potentially useful during rollout.\"\n977     ),\n978 )\n979 def parse(\n980     path: str,\n981     code_only: bool,\n982     include_meta: bool,\n983     format: str,\n984     write_output: Optional[str],\n985     profiler: bool,\n986     bench: bool,\n987     nofail: bool,\n988     logger: Optional[logging.Logger] = None,\n989     extra_config_path: Optional[str] = None,\n990     ignore_local_config: bool = False,\n991     **kwargs,\n992 ) -> None:\n993     \"\"\"Parse SQL files and just spit out the result.\n994 \n995     PATH is the path to a sql file or directory to lint. This can be either a\n996     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n997     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n998     be interpreted like passing the current working directory as a path argument.\n999     \"\"\"\n1000     c = get_config(\n1001         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1002     )\n1003     # We don't want anything else to be logged if we want json or yaml output\n1004     # unless we're writing to a file.\n1005     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n1006     output_stream = make_output_stream(c, format, write_output)\n1007     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1008     verbose = c.get(\"verbose\")\n1009     recurse = c.get(\"recurse\")\n1010 \n1011     progress_bar_configuration.disable_progress_bar = True\n1012 \n1013     formatter.dispatch_config(lnt)\n1014 \n1015     # Set up logging.\n1016     set_logging_level(\n1017         verbosity=verbose,\n1018         formatter=formatter,\n1019         logger=logger,\n1020         stderr_output=non_human_output,\n1021     )\n1022 \n1023     # TODO: do this better\n1024 \n1025     if profiler:\n1026         # Set up the profiler if required\n1027         try:\n1028             import cProfile\n1029         except ImportError:  # pragma: no cover\n1030             click.echo(\"The cProfiler is not available on your platform.\")\n1031             sys.exit(EXIT_ERROR)\n1032         pr = cProfile.Profile()\n1033         pr.enable()\n1034 \n1035     t0 = time.monotonic()\n1036 \n1037     # handle stdin if specified via lone '-'\n1038     with PathAndUserErrorHandler(formatter, path):\n1039         if \"-\" == path:\n1040             parsed_strings = [\n1041                 lnt.parse_string(\n1042                     sys.stdin.read(),\n1043                     \"stdin\",\n1044                     recurse=recurse,\n1045                     config=lnt.config,\n1046                 ),\n1047             ]\n1048         else:\n1049             # A single path must be specified for this command\n1050             parsed_strings = list(\n1051                 lnt.parse_path(\n1052                     path=path,\n1053                     recurse=recurse,\n1054                 )\n1055             )\n1056 \n1057     total_time = time.monotonic() - t0\n1058     violations_count = 0\n1059 \n1060     # iterative print for human readout\n1061     if format == FormatType.human.value:\n1062         violations_count = formatter.print_out_violations_and_timing(\n1063             output_stream, bench, code_only, total_time, verbose, parsed_strings\n1064         )\n1065     else:\n1066         parsed_strings_dict = [\n1067             dict(\n1068                 filepath=linted_result.fname,\n1069                 segments=linted_result.tree.as_record(\n1070                     code_only=code_only, show_raw=True, include_meta=include_meta\n1071                 )\n1072                 if linted_result.tree\n1073                 else None,\n1074             )\n1075             for linted_result in parsed_strings\n1076         ]\n1077 \n1078         if format == FormatType.yaml.value:\n1079             # For yaml dumping always dump double quoted strings if they contain\n1080             # tabs or newlines.\n1081             yaml.add_representer(str, quoted_presenter)\n1082             file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n1083         elif format == FormatType.json.value:\n1084             file_output = json.dumps(parsed_strings_dict)\n1085 \n1086         # Dump the output to stdout or to file as appropriate.\n1087         dump_file_payload(write_output, file_output)\n1088     if profiler:\n1089         pr.disable()\n1090         profiler_buffer = StringIO()\n1091         ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n1092         ps.print_stats()\n1093         click.echo(\"==== profiler stats ====\")\n1094         # Only print the first 50 lines of it\n1095         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n1096 \n1097     if violations_count > 0 and not nofail:\n1098         sys.exit(EXIT_FAIL)  # pragma: no cover\n1099     else:\n1100         sys.exit(EXIT_SUCCESS)\n1101 \n1102 \n1103 # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n1104 # simplifies the use of cProfile, e.g.:\n1105 # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\n1106 if __name__ == \"__main__\":\n1107     cli.main(sys.argv[1:])  # pragma: no cover\n1108 \n[end of src/sqlfluff/cli/commands.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -511,7 +511,7 @@\n \n @click.option(\n     \"--disable_progress_bar\",\n-    is_flag=True,\n+    \"disable_progress_bar\",\n     help=\"Disables progress bars.\",\n )\n \n@@ -707,7 +707\n     \"--disable_progress_bar\",\n-    is_flag=True,\n+    \"disable_progress_bar\",\n     help=\"Disables progress bars.\",\n )\n```", "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -511,7 +511,7 @@\n \n @click.option(\n     \"--disable_progress_bar\",\n-    is_flag=True,\n+    \"disable_progress_bar\",\n     help=\"Disables progress bars.\",\n )\n \n@@ -707,7 +707\n     \"--disable_progress_bar\",\n-    is_flag=True,\n+    \"disable_progress_bar\",\n     help=\"Disables progress bars.\",\n )\n"}
{"instance_id": "sqlfluff__sqlfluff-1625", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nTSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![Requirements Status](https://img.shields.io/requires/github/sqlfluff/sqlfluff.svg?style=flat-square)](https://requires.io/github/sqlfluff/sqlfluff/requirements/?branch=main)\n13 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)\n14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n16 \n17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n18 \n19 ## Dialects Supported\n20 \n21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n22 \n23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n24 - [BigQuery](https://cloud.google.com/bigquery/)\n25 - [Exasol](https://www.exasol.com/)\n26 - [Hive](https://hive.apache.org/)\n27 - [MySQL](https://www.mysql.com/)\n28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n29 - [Snowflake](https://www.snowflake.com/)\n30 - [SQLite](https://www.sqlite.org/)\n31 - [Teradata](https://www.teradata.com/)\n32 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n33 \n34 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n35 \n36 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n37 \n38 ## Templates Supported\n39 \n40 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n41 \n42 **SQLFluff** supports the following templates:\n43 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n44 - [dbt](https://www.getdbt.com/)\n45 \n46 Again, please raise issues if you wish to support more templating languages/syntaxes.\n47 \n48 # Getting Started\n49 \n50 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n51 \n52 ```shell\n53 $ pip install sqlfluff\n54 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n55 $ sqlfluff lint test.sql\n56 == [test.sql] FAIL\n57 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4\n58 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line\n59 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace\n60 ```\n61 \n62 You can also have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n63 \n64 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n65 \n66 # Documentation\n67 \n68 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n69 \n70 # Releases\n71 \n72 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).\n73 \n74 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n75 \n76 # SQLFluff on Slack\n77 \n78 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n79 \n80 # SQLFluff on Twitter\n81 \n82 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n83 \n84 # Contributing\n85 \n86 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n87 \n88 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n89 \n90 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n91 \n[end of README.md]\n[start of src/sqlfluff/rules/L031.py]\n1 \"\"\"Implementation of Rule L031.\"\"\"\n2 \n3 from collections import Counter, defaultdict\n4 from typing import Generator, NamedTuple\n5 \n6 from sqlfluff.core.parser import BaseSegment\n7 from sqlfluff.core.rules.base import BaseRule, LintFix, LintResult\n8 from sqlfluff.core.rules.doc_decorators import document_fix_compatible\n9 \n10 \n11 @document_fix_compatible\n12 class Rule_L031(BaseRule):\n13     \"\"\"Avoid table aliases in from clauses and join conditions.\n14 \n15     | **Anti-pattern**\n16     | In this example, alias 'o' is used for the orders table, and 'c' is used for 'customers' table.\n17 \n18     .. code-block:: sql\n19 \n20         SELECT\n21             COUNT(o.customer_id) as order_amount,\n22             c.name\n23         FROM orders as o\n24         JOIN customers as c on o.id = c.user_id\n25 \n26 \n27     | **Best practice**\n28     |  Avoid aliases.\n29 \n30     .. code-block:: sql\n31 \n32         SELECT\n33             COUNT(orders.customer_id) as order_amount,\n34             customers.name\n35         FROM orders\n36         JOIN customers on orders.id = customers.user_id\n37 \n38         -- Self-join will not raise issue\n39 \n40         SELECT\n41             table.a,\n42             table_alias.b,\n43         FROM\n44             table\n45             LEFT JOIN table AS table_alias ON table.foreign_key = table_alias.foreign_key\n46 \n47     \"\"\"\n48 \n49     def _eval(self, segment, **kwargs):\n50         \"\"\"Identify aliases in from clause and join conditions.\n51 \n52         Find base table, table expressions in join, and other expressions in select clause\n53         and decide if it's needed to report them.\n54         \"\"\"\n55         if segment.is_type(\"select_statement\"):\n56             # A buffer for all table expressions in join conditions\n57             from_expression_elements = []\n58             column_reference_segments = []\n59 \n60             from_clause_segment = segment.get_child(\"from_clause\")\n61 \n62             if not from_clause_segment:\n63                 return None\n64 \n65             from_expression = from_clause_segment.get_child(\"from_expression\")\n66             from_expression_element = None\n67             if from_expression:\n68                 from_expression_element = from_expression.get_child(\n69                     \"from_expression_element\"\n70                 )\n71 \n72             if not from_expression_element:\n73                 return None\n74             from_expression_element = from_expression_element.get_child(\n75                 \"table_expression\"\n76             )\n77 \n78             # Find base table\n79             base_table = None\n80             if from_expression_element:\n81                 base_table = from_expression_element.get_child(\"object_reference\")\n82 \n83             from_clause_index = segment.segments.index(from_clause_segment)\n84             from_clause_and_after = segment.segments[from_clause_index:]\n85 \n86             for clause in from_clause_and_after:\n87                 for from_expression_element in clause.recursive_crawl(\n88                     \"from_expression_element\"\n89                 ):\n90                     from_expression_elements.append(from_expression_element)\n91                 for column_reference in clause.recursive_crawl(\"column_reference\"):\n92                     column_reference_segments.append(column_reference)\n93 \n94             return (\n95                 self._lint_aliases_in_join(\n96                     base_table,\n97                     from_expression_elements,\n98                     column_reference_segments,\n99                     segment,\n100                 )\n101                 or None\n102             )\n103         return None\n104 \n105     class TableAliasInfo(NamedTuple):\n106         \"\"\"Structure yielded by_filter_table_expressions().\"\"\"\n107 \n108         table_ref: BaseSegment\n109         whitespace_ref: BaseSegment\n110         alias_exp_ref: BaseSegment\n111         alias_identifier_ref: BaseSegment\n112 \n113     @classmethod\n114     def _filter_table_expressions(\n115         cls, base_table, from_expression_elements\n116     ) -> Generator[TableAliasInfo, None, None]:\n117         for from_expression in from_expression_elements:\n118             table_expression = from_expression.get_child(\"table_expression\")\n119             if not table_expression:\n120                 continue\n121             table_ref = table_expression.get_child(\"object_reference\")\n122 \n123             # If the from_expression_element has no object_references - skip it\n124             # An example case is a lateral flatten, where we have a function segment\n125             # instead of a table_reference segment.\n126             if not table_ref:\n127                 continue\n128 \n129             # If this is self-join - skip it\n130             if (\n131                 base_table\n132                 and base_table.raw == table_ref.raw\n133                 and base_table != table_ref\n134             ):\n135                 continue\n136 \n137             whitespace_ref = from_expression.get_child(\"whitespace\")\n138 \n139             # If there's no alias expression - skip it\n140             alias_exp_ref = from_expression.get_child(\"alias_expression\")\n141             if alias_exp_ref is None:\n142                 continue\n143 \n144             alias_identifier_ref = alias_exp_ref.get_child(\"identifier\")\n145             yield cls.TableAliasInfo(\n146                 table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref\n147             )\n148 \n149     def _lint_aliases_in_join(\n150         self, base_table, from_expression_elements, column_reference_segments, segment\n151     ):\n152         \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n153         # A buffer to keep any violations.\n154         violation_buff = []\n155 \n156         to_check = list(\n157             self._filter_table_expressions(base_table, from_expression_elements)\n158         )\n159 \n160         # How many times does each table appear in the FROM clause?\n161         table_counts = Counter(ai.table_ref.raw for ai in to_check)\n162 \n163         # What is the set of aliases used for each table? (We are mainly\n164         # interested in the NUMBER of different aliases used.)\n165         table_aliases = defaultdict(set)\n166         for ai in to_check:\n167             table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n168 \n169         # For each aliased table, check whether to keep or remove it.\n170         for alias_info in to_check:\n171             # If the same table appears more than once in the FROM clause with\n172             # different alias names, do not consider removing its aliases.\n173             # The aliases may have been introduced simply to make each\n174             # occurrence of the table independent within the query.\n175             if (\n176                 table_counts[alias_info.table_ref.raw] > 1\n177                 and len(table_aliases[alias_info.table_ref.raw]) > 1\n178             ):\n179                 continue\n180 \n181             select_clause = segment.get_child(\"select_clause\")\n182 \n183             ids_refs = []\n184 \n185             # Find all references to alias in select clause\n186             alias_name = alias_info.alias_identifier_ref.raw\n187             for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n188                 used_alias_ref = alias_with_column.get_child(\"identifier\")\n189                 if used_alias_ref and used_alias_ref.raw == alias_name:\n190                     ids_refs.append(used_alias_ref)\n191 \n192             # Find all references to alias in column references\n193             for exp_ref in column_reference_segments:\n194                 used_alias_ref = exp_ref.get_child(\"identifier\")\n195                 # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n196                 if used_alias_ref.raw == alias_name and exp_ref.get_child(\"dot\"):\n197                     ids_refs.append(used_alias_ref)\n198 \n199             # Fixes for deleting ` as sth` and for editing references to aliased tables\n200             fixes = [\n201                 *[\n202                     LintFix(\"delete\", d)\n203                     for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n204                 ],\n205                 *[\n206                     LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n207                     for alias in [alias_info.alias_identifier_ref, *ids_refs]\n208                 ],\n209             ]\n210 \n211             violation_buff.append(\n212                 LintResult(\n213                     anchor=alias_info.alias_identifier_ref,\n214                     description=\"Avoid using aliases in join condition\",\n215                     fixes=fixes,\n216                 )\n217             )\n218 \n219         return violation_buff or None\n220 \n[end of src/sqlfluff/rules/L031.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "<patch>\n--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -49,6 +49,7 @@\n \n             from_clause_segment = segment.get_child(\"from_clause\")\n \n+            # Check if there is a join condition\n             if not from_clause_segment:\n                 return None\n \n@@ -60,6 +61,10 @@\n             from_expression_element = None\n             if from_expression:\n                 from_expression_element = from_expression.get_child(\n-                    \"from_expression_element\"\n+                    \"from_expression_element\"\n                 )\n \n+            # If there is no join condition, skip the alias check\n+            if not from_expression_element and not from_expression.get_child(\"join\"):\n+                return None\n             \n             if not from_expression_element:\n                 return None\n@@ -100,7 +105\n                 self._lint_aliases_in_join(\n-                    base_table,\n+                    base_table,  # Pass base_table to lint_aliases_in_join\n                     from_expression_elements,\n                     column_reference_segments,\n                     segment,\n@@ -219,6 +226,7 @@\n \n         return violation_buff or None\n+        # Return None if there are no violations\n </patch>", "model_patch": "\n--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -49,6 +49,7 @@\n \n             from_clause_segment = segment.get_child(\"from_clause\")\n \n+            # Check if there is a join condition\n             if not from_clause_segment:\n                 return None\n \n@@ -60,6 +61,10 @@\n             from_expression_element = None\n             if from_expression:\n                 from_expression_element = from_expression.get_child(\n-                    \"from_expression_element\"\n+                    \"from_expression_element\"\n                 )\n \n+            # If there is no join condition, skip the alias check\n+            if not from_expression_element and not from_expression.get_child(\"join\"):\n+                return None\n             \n             if not from_expression_element:\n                 return None\n@@ -100,7 +105\n                 self._lint_aliases_in_join(\n-                    base_table,\n+                    base_table,  # Pass base_table to lint_aliases_in_join\n                     from_expression_elements,\n                     column_reference_segments,\n                     segment,\n@@ -219,6 +226,7 @@\n \n         return violation_buff or None\n+        # Return None if there are no violations\n "}
{"instance_id": "sqlfluff__sqlfluff-3662", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nNumber of processes configurable in .sqlfluff\nBeing able to set the number of processes to run with in .sqlfluff might be useful to avoid having to pass it in the CLI every time.\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 \n16 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n17 \n18 ## Dialects Supported\n19 \n20 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n21 \n22 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n23 - [Athena](https://aws.amazon.com/athena/)\n24 - [BigQuery](https://cloud.google.com/bigquery/)\n25 - [ClickHouse](https://clickhouse.com/)\n26 - [Databricks](https://databricks.com/) (note: currently this is just an alias for the `sparksql` dialect).\n27 - [Db2](https://www.ibm.com/analytics/db2)\n28 - [Exasol](https://www.exasol.com/)\n29 - [Hive](https://hive.apache.org/)\n30 - [MySQL](https://www.mysql.com/)\n31 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n32 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n33 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n34 - [Snowflake](https://www.snowflake.com/)\n35 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n36 - [SparkSQL](https://spark.apache.org/docs/latest/)\n37 - [SQLite](https://www.sqlite.org/)\n38 - [Teradata](https://www.teradata.com/)\n39 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n40 \n41 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n42 \n43 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n44 \n45 ## Templates Supported\n46 \n47 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n48 \n49 **SQLFluff** supports the following templates:\n50 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n51 - [dbt](https://www.getdbt.com/)\n52 \n53 Again, please raise issues if you wish to support more templating languages/syntaxes.\n54 \n55 # Getting Started\n56 \n57 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n58 \n59 ```shell\n60 $ pip install sqlfluff\n61 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n62 $ sqlfluff lint test.sql --dialect ansi\n63 == [test.sql] FAIL\n64 L:   1 | P:   1 | L050 | Files must not begin with newlines or whitespace.\n65 L:   1 | P:   3 | L003 | First line has unexpected indent\n66 L:   1 | P:  11 | L039 | Unnecessary whitespace found.\n67 L:   1 | P:  14 | L039 | Unnecessary whitespace found.\n68 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace.\n69 ```\n70 \n71 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n72 \n73 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n74 \n75 # Documentation\n76 \n77 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n78 \n79 # Releases\n80 \n81 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html), so breaking changes\n82 should be restricted to major versions releases. Some elements (such as the python API) are in a less\n83 stable state and may see more significant changes more often. See the [changelog](CHANGELOG.md) for more details.\n84 If you would like to join in please consider [contributing](CONTRIBUTING.md).\n85 \n86 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n87 \n88 # SQLFluff on Slack\n89 \n90 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n91 \n92 # SQLFluff on Twitter\n93 \n94 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n95 \n96 # Contributing\n97 \n98 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n99 \n100 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n101 \n102 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n103 \n[end of README.md]\n[start of src/sqlfluff/cli/commands.py]\n1 \"\"\"Contains the CLI.\"\"\"\n2 \n3 from itertools import chain\n4 import os\n5 import sys\n6 import json\n7 import logging\n8 import time\n9 from logging import LogRecord\n10 from typing import Callable, Tuple, Optional, cast\n11 \n12 import yaml\n13 \n14 import click\n15 \n16 # For the profiler\n17 import pstats\n18 from io import StringIO\n19 \n20 # To enable colour cross platform\n21 import colorama\n22 from tqdm import tqdm\n23 from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n24 \n25 from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n26 from sqlfluff.cli.formatters import (\n27     format_linting_result_header,\n28     OutputStreamFormatter,\n29 )\n30 from sqlfluff.cli.helpers import get_package_version\n31 from sqlfluff.cli.outputstream import make_output_stream, OutputStream\n32 \n33 # Import from sqlfluff core.\n34 from sqlfluff.core import (\n35     Linter,\n36     FluffConfig,\n37     SQLLintError,\n38     SQLTemplaterError,\n39     SQLFluffUserError,\n40     dialect_selector,\n41     dialect_readout,\n42 )\n43 from sqlfluff.core.config import progress_bar_configuration\n44 \n45 from sqlfluff.core.enums import FormatType, Color\n46 from sqlfluff.core.plugin.host import get_plugin_manager\n47 \n48 \n49 class RedWarningsFilter(logging.Filter):\n50     \"\"\"This filter makes all warnings or above red.\"\"\"\n51 \n52     def __init__(self, formatter: OutputStreamFormatter):\n53         super().__init__()\n54         self.formatter = formatter\n55 \n56     def filter(self, record: logging.LogRecord) -> bool:\n57         \"\"\"Filter any warnings (or above) to turn them red.\"\"\"\n58         if record.levelno >= logging.WARNING:\n59             record.msg = f\"{self.formatter.colorize(record.msg, Color.red)} \"\n60         return True\n61 \n62 \n63 class StreamHandlerTqdm(logging.StreamHandler):\n64     \"\"\"Modified StreamHandler which takes care of writing within `tqdm` context.\n65 \n66     It uses `tqdm` write which takes care of conflicting prints with progressbar.\n67     Without it, there were left artifacts in DEBUG mode (not sure about another ones,\n68     but probably would happen somewhere).\n69     \"\"\"\n70 \n71     def emit(self, record: LogRecord) -> None:\n72         \"\"\"Behaves like original one except uses `tqdm` to write.\"\"\"\n73         try:\n74             msg = self.format(record)\n75             tqdm.write(msg, file=self.stream)\n76             self.flush()\n77         except Exception:  # pragma: no cover\n78             self.handleError(record)\n79 \n80 \n81 def set_logging_level(\n82     verbosity: int,\n83     formatter: OutputStreamFormatter,\n84     logger: Optional[logging.Logger] = None,\n85     stderr_output: bool = False,\n86 ) -> None:\n87     \"\"\"Set up logging for the CLI.\n88 \n89     We either set up global logging based on the verbosity\n90     or, if `logger` is specified, we only limit to a single\n91     sqlfluff logger. Verbosity is applied in the same way.\n92 \n93     Implementation: If `logger` is not specified, the handler\n94     is attached to the `sqlfluff` logger. If it is specified\n95     then it attaches the the logger in question. In addition\n96     if `logger` is specified, then that logger will also\n97     not propagate.\n98     \"\"\"\n99     fluff_logger = logging.getLogger(\"sqlfluff\")\n100     # Don't propagate logging\n101     fluff_logger.propagate = False\n102 \n103     # Enable colorama\n104     colorama.init()\n105 \n106     # Set up the log handler which is able to print messages without overlapping\n107     # with progressbars.\n108     handler = StreamHandlerTqdm(stream=sys.stderr if stderr_output else sys.stdout)\n109     # NB: the unicode character at the beginning is to squash any badly\n110     # tamed ANSI colour statements, and return us to normality.\n111     handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n112     # Set up a handler to colour warnings red.\n113     handler.addFilter(RedWarningsFilter(formatter))\n114     if logger:\n115         focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n116         focus_logger.addHandler(handler)\n117     else:\n118         fluff_logger.addHandler(handler)\n119 \n120     # NB: We treat the parser logger slightly differently because it's noisier.\n121     # It's important that we set levels for all each time so\n122     # that we don't break tests by changing the granularity\n123     # between tests.\n124     parser_logger = logging.getLogger(\"sqlfluff.parser\")\n125     if verbosity < 3:\n126         fluff_logger.setLevel(logging.WARNING)\n127         parser_logger.setLevel(logging.NOTSET)\n128     elif verbosity == 3:\n129         fluff_logger.setLevel(logging.INFO)\n130         parser_logger.setLevel(logging.WARNING)\n131     elif verbosity == 4:\n132         fluff_logger.setLevel(logging.DEBUG)\n133         parser_logger.setLevel(logging.INFO)\n134     elif verbosity > 4:\n135         fluff_logger.setLevel(logging.DEBUG)\n136         parser_logger.setLevel(logging.DEBUG)\n137 \n138 \n139 class PathAndUserErrorHandler:\n140     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n141 \n142     def __init__(self, formatter, paths):\n143         self.formatter = formatter\n144         self.paths = paths\n145 \n146     def __enter__(self):\n147         return self\n148 \n149     def __exit__(self, exc_type, exc_val, exc_tb):\n150         if exc_type is OSError:\n151             click.echo(\n152                 self.formatter.colorize(\n153                     f\"The path(s) { self.paths } could not be \"\n154                     \"accessed. Check it/they exist(s).\",\n155                     Color.red,\n156                 )\n157             )\n158             sys.exit(EXIT_ERROR)\n159         elif exc_type is SQLFluffUserError:\n160             click.echo(\n161                 \"\\nUser Error: \"\n162                 + self.formatter.colorize(\n163                     str(exc_val),\n164                     Color.red,\n165                 )\n166             )\n167             sys.exit(EXIT_ERROR)\n168 \n169 \n170 def common_options(f: Callable) -> Callable:\n171     \"\"\"Add common options to commands via a decorator.\n172 \n173     These are applied to all of the cli commands.\n174     \"\"\"\n175     f = click.version_option()(f)\n176     f = click.option(\n177         \"-v\",\n178         \"--verbose\",\n179         count=True,\n180         default=None,\n181         help=(\n182             \"Verbosity, how detailed should the output be. This is *stackable*, so \"\n183             \"`-vv` is more verbose than `-v`. For the most verbose option try `-vvvv` \"\n184             \"or `-vvvvv`.\"\n185         ),\n186     )(f)\n187     f = click.option(\n188         \"-n\",\n189         \"--nocolor\",\n190         is_flag=True,\n191         default=None,\n192         help=\"No color - output will be without ANSI color codes.\",\n193     )(f)\n194 \n195     return f\n196 \n197 \n198 def core_options(f: Callable) -> Callable:\n199     \"\"\"Add core operation options to commands via a decorator.\n200 \n201     These are applied to the main (but not all) cli commands like\n202     `parse`, `lint` and `fix`.\n203     \"\"\"\n204     # Only enable dialect completion if on version of click\n205     # that supports it\n206     if shell_completion_enabled:\n207         f = click.option(\n208             \"--dialect\",\n209             default=None,\n210             help=\"The dialect of SQL to lint\",\n211             shell_complete=dialect_shell_complete,\n212         )(f)\n213     else:  # pragma: no cover\n214         f = click.option(\n215             \"--dialect\",\n216             default=None,\n217             help=\"The dialect of SQL to lint\",\n218         )(f)\n219     f = click.option(\n220         \"--templater\",\n221         default=None,\n222         help=\"The templater to use (default=jinja)\",\n223         type=click.Choice(\n224             [\n225                 templater.name\n226                 for templater in chain.from_iterable(\n227                     get_plugin_manager().hook.get_templaters()\n228                 )\n229             ]\n230         ),\n231     )(f)\n232     f = click.option(\n233         \"--rules\",\n234         default=None,\n235         help=(\n236             \"Narrow the search to only specific rules. For example \"\n237             \"specifying `--rules L001` will only search for rule `L001` (Unnecessary \"\n238             \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n239             \"`--rules L001,L002` will specify only looking for violations of rule \"\n240             \"`L001` and rule `L002`.\"\n241         ),\n242     )(f)\n243     f = click.option(\n244         \"--exclude-rules\",\n245         default=None,\n246         help=(\n247             \"Exclude specific rules. For example \"\n248             \"specifying `--exclude-rules L001` will remove rule `L001` (Unnecessary \"\n249             \"trailing whitespace) from the set of considered rules. This could either \"\n250             \"be the allowlist, or the general set if there is no specific allowlist. \"\n251             \"Multiple rules can be specified with commas e.g. \"\n252             \"`--exclude-rules L001,L002` will exclude violations of rule \"\n253             \"`L001` and rule `L002`.\"\n254         ),\n255     )(f)\n256     f = click.option(\n257         \"--config\",\n258         \"extra_config_path\",\n259         default=None,\n260         help=(\n261             \"Include additional config file. By default the config is generated \"\n262             \"from the standard configuration files described in the documentation. \"\n263             \"This argument allows you to specify an additional configuration file that \"\n264             \"overrides the standard configuration files. N.B. cfg format is required.\"\n265         ),\n266         type=click.Path(),\n267     )(f)\n268     f = click.option(\n269         \"--ignore-local-config\",\n270         is_flag=True,\n271         help=(\n272             \"Ignore config files in default search path locations. \"\n273             \"This option allows the user to lint with the default config \"\n274             \"or can be used in conjunction with --config to only \"\n275             \"reference the custom config file.\"\n276         ),\n277     )(f)\n278     f = click.option(\n279         \"--encoding\",\n280         default=None,\n281         help=(\n282             \"Specify encoding to use when reading and writing files. Defaults to \"\n283             \"autodetect.\"\n284         ),\n285     )(f)\n286     f = click.option(\n287         \"--ignore\",\n288         default=None,\n289         help=(\n290             \"Ignore particular families of errors so that they don't cause a failed \"\n291             \"run. For example `--ignore parsing` would mean that any parsing errors \"\n292             \"are ignored and don't influence the success or fail of a run. \"\n293             \"`--ignore` behaves somewhat like `noqa` comments, except it \"\n294             \"applies globally. Multiple options are possible if comma separated: \"\n295             \"e.g. `--ignore parsing,templating`.\"\n296         ),\n297     )(f)\n298     f = click.option(\n299         \"--bench\",\n300         is_flag=True,\n301         help=\"Set this flag to engage the benchmarking tool output.\",\n302     )(f)\n303     f = click.option(\n304         \"--logger\",\n305         type=click.Choice(\n306             [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\"], case_sensitive=False\n307         ),\n308         help=\"Choose to limit the logging to one of the loggers.\",\n309     )(f)\n310     f = click.option(\n311         \"--disable-noqa\",\n312         is_flag=True,\n313         default=None,\n314         help=\"Set this flag to ignore inline noqa comments.\",\n315     )(f)\n316     return f\n317 \n318 \n319 def get_config(\n320     extra_config_path: Optional[str] = None,\n321     ignore_local_config: bool = False,\n322     **kwargs,\n323 ) -> FluffConfig:\n324     \"\"\"Get a config object from kwargs.\"\"\"\n325     plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n326     if kwargs.get(\"dialect\"):\n327         try:\n328             # We're just making sure it exists at this stage.\n329             # It will be fetched properly in the linter.\n330             dialect_selector(kwargs[\"dialect\"])\n331         except SQLFluffUserError as err:\n332             click.echo(\n333                 OutputStreamFormatter.colorize_helper(\n334                     plain_output,\n335                     f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n336                     color=Color.red,\n337                 )\n338             )\n339             sys.exit(EXIT_ERROR)\n340         except KeyError:\n341             click.echo(\n342                 OutputStreamFormatter.colorize_helper(\n343                     plain_output,\n344                     f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n345                     color=Color.red,\n346                 )\n347             )\n348             sys.exit(EXIT_ERROR)\n349     from_root_kwargs = {}\n350     if \"require_dialect\" in kwargs:\n351         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n352     # Instantiate a config object (filtering out the nulls)\n353     overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n354     try:\n355         return FluffConfig.from_root(\n356             extra_config_path=extra_config_path,\n357             ignore_local_config=ignore_local_config,\n358             overrides=overrides,\n359             **from_root_kwargs,\n360         )\n361     except SQLFluffUserError as err:  # pragma: no cover\n362         click.echo(\n363             OutputStreamFormatter.colorize_helper(\n364                 plain_output,\n365                 f\"Error loading config: {str(err)}\",\n366                 color=Color.red,\n367             )\n368         )\n369         sys.exit(EXIT_ERROR)\n370 \n371 \n372 def get_linter_and_formatter(\n373     cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n374 ) -> Tuple[Linter, OutputStreamFormatter]:\n375     \"\"\"Get a linter object given a config.\"\"\"\n376     try:\n377         # We're just making sure it exists at this stage.\n378         # It will be fetched properly in the linter.\n379         dialect = cfg.get(\"dialect\")\n380         if dialect:\n381             dialect_selector(dialect)\n382     except KeyError:  # pragma: no cover\n383         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n384         sys.exit(EXIT_ERROR)\n385     formatter = OutputStreamFormatter(\n386         output_stream=output_stream or make_output_stream(cfg),\n387         nocolor=cfg.get(\"nocolor\"),\n388         verbosity=cfg.get(\"verbose\"),\n389         output_line_length=cfg.get(\"output_line_length\"),\n390     )\n391     return Linter(config=cfg, formatter=formatter), formatter\n392 \n393 \n394 @click.group(context_settings={\"help_option_names\": [\"-h\", \"--help\"]})\n395 @click.version_option()\n396 def cli():\n397     \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n398 \n399 \n400 @cli.command()\n401 @common_options\n402 def version(**kwargs) -> None:\n403     \"\"\"Show the version of sqlfluff.\"\"\"\n404     c = get_config(**kwargs, require_dialect=False)\n405     if c.get(\"verbose\") > 0:\n406         # Instantiate the linter\n407         lnt, formatter = get_linter_and_formatter(c)\n408         # Dispatch the detailed config from the linter.\n409         formatter.dispatch_config(lnt)\n410     else:\n411         # Otherwise just output the package version.\n412         click.echo(get_package_version(), color=c.get(\"color\"))\n413 \n414 \n415 @cli.command()\n416 @common_options\n417 def rules(**kwargs) -> None:\n418     \"\"\"Show the current rules in use.\"\"\"\n419     c = get_config(**kwargs, dialect=\"ansi\")\n420     lnt, formatter = get_linter_and_formatter(c)\n421     click.echo(formatter.format_rules(lnt), color=c.get(\"color\"))\n422 \n423 \n424 @cli.command()\n425 @common_options\n426 def dialects(**kwargs) -> None:\n427     \"\"\"Show the current dialects available.\"\"\"\n428     c = get_config(**kwargs, require_dialect=False)\n429     _, formatter = get_linter_and_formatter(c)\n430     click.echo(formatter.format_dialects(dialect_readout), color=c.get(\"color\"))\n431 \n432 \n433 def dump_file_payload(filename: Optional[str], payload: str):\n434     \"\"\"Write the output file content to stdout or file.\"\"\"\n435     # If there's a file specified to write to, write to it.\n436     if filename:\n437         with open(filename, \"w\") as out_file:\n438             out_file.write(payload)\n439     # Otherwise write to stdout\n440     else:\n441         click.echo(payload)\n442 \n443 \n444 @cli.command()\n445 @common_options\n446 @core_options\n447 @click.option(\n448     \"-f\",\n449     \"--format\",\n450     \"format\",\n451     default=\"human\",\n452     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n453     help=\"What format to return the lint result in (default=human).\",\n454 )\n455 @click.option(\n456     \"--write-output\",\n457     help=(\n458         \"Optionally provide a filename to write the results to, mostly used in \"\n459         \"tandem with --format. NB: Setting an output file re-enables normal \"\n460         \"stdout logging.\"\n461     ),\n462 )\n463 @click.option(\n464     \"--annotation-level\",\n465     default=\"notice\",\n466     type=click.Choice([\"notice\", \"warning\", \"failure\", \"error\"], case_sensitive=False),\n467     help=(\n468         \"When format is set to github-annotation or github-annotation-native, \"\n469         \"default annotation level (default=notice). failure and error are equivalent.\"\n470     ),\n471 )\n472 @click.option(\n473     \"--nofail\",\n474     is_flag=True,\n475     help=(\n476         \"If set, the exit code will always be zero, regardless of violations \"\n477         \"found. This is potentially useful during rollout.\"\n478     ),\n479 )\n480 @click.option(\n481     \"--disregard-sqlfluffignores\",\n482     is_flag=True,\n483     help=\"Perform the operation regardless of .sqlfluffignore configurations\",\n484 )\n485 @click.option(\n486     \"-p\",\n487     \"--processes\",\n488     type=int,\n489     default=1,\n490     help=\"The number of parallel processes to run.\",\n491 )\n492 @click.option(\n493     \"--disable_progress_bar\",\n494     is_flag=True,\n495     help=\"Disables progress bars.\",\n496 )\n497 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n498 def lint(\n499     paths: Tuple[str],\n500     processes: int,\n501     format: str,\n502     write_output: Optional[str],\n503     annotation_level: str,\n504     nofail: bool,\n505     disregard_sqlfluffignores: bool,\n506     logger: Optional[logging.Logger] = None,\n507     bench: bool = False,\n508     disable_progress_bar: Optional[bool] = False,\n509     extra_config_path: Optional[str] = None,\n510     ignore_local_config: bool = False,\n511     **kwargs,\n512 ) -> None:\n513     \"\"\"Lint SQL files via passing a list of files or using stdin.\n514 \n515     PATH is the path to a sql file or directory to lint. This can be either a\n516     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n517     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n518     be interpreted like passing the current working directory as a path argument.\n519 \n520     Linting SQL files:\n521 \n522         sqlfluff lint path/to/file.sql\n523         sqlfluff lint directory/of/sql/files\n524 \n525     Linting a file via stdin (note the lone '-' character):\n526 \n527         cat path/to/file.sql | sqlfluff lint -\n528         echo 'select col from tbl' | sqlfluff lint -\n529 \n530     \"\"\"\n531     config = get_config(\n532         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n533     )\n534     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n535     file_output = None\n536     output_stream = make_output_stream(config, format, write_output)\n537     lnt, formatter = get_linter_and_formatter(config, output_stream)\n538 \n539     verbose = config.get(\"verbose\")\n540     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n541 \n542     formatter.dispatch_config(lnt)\n543 \n544     # Set up logging.\n545     set_logging_level(\n546         verbosity=verbose,\n547         formatter=formatter,\n548         logger=logger,\n549         stderr_output=non_human_output,\n550     )\n551 \n552     # Output the results as we go\n553     if verbose >= 1:\n554         click.echo(format_linting_result_header())\n555 \n556     with PathAndUserErrorHandler(formatter, paths):\n557         # add stdin if specified via lone '-'\n558         if (\"-\",) == paths:\n559             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n560         else:\n561             result = lnt.lint_paths(\n562                 paths,\n563                 ignore_non_existent_files=False,\n564                 ignore_files=not disregard_sqlfluffignores,\n565                 processes=processes,\n566             )\n567 \n568     # Output the final stats\n569     if verbose >= 1:\n570         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n571 \n572     if format == FormatType.json.value:\n573         file_output = json.dumps(result.as_records())\n574     elif format == FormatType.yaml.value:\n575         file_output = yaml.dump(result.as_records(), sort_keys=False)\n576     elif format == FormatType.github_annotation.value:\n577         if annotation_level == \"error\":\n578             annotation_level = \"failure\"\n579 \n580         github_result = []\n581         for record in result.as_records():\n582             filepath = record[\"filepath\"]\n583             for violation in record[\"violations\"]:\n584                 # NOTE: The output format is designed for this GitHub action:\n585                 # https://github.com/yuzutech/annotations-action\n586                 # It is similar, but not identical, to the native GitHub format:\n587                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n588                 github_result.append(\n589                     {\n590                         \"file\": filepath,\n591                         \"line\": violation[\"line_no\"],\n592                         \"start_column\": violation[\"line_pos\"],\n593                         \"end_column\": violation[\"line_pos\"],\n594                         \"title\": \"SQLFluff\",\n595                         \"message\": f\"{violation['code']}: {violation['description']}\",\n596                         \"annotation_level\": annotation_level,\n597                     }\n598                 )\n599         file_output = json.dumps(github_result)\n600     elif format == FormatType.github_annotation_native.value:\n601         if annotation_level == \"failure\":\n602             annotation_level = \"error\"\n603 \n604         github_result_native = []\n605         for record in result.as_records():\n606             filepath = record[\"filepath\"]\n607             for violation in record[\"violations\"]:\n608                 # NOTE: The output format is designed for GitHub action:\n609                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n610                 line = f\"::{annotation_level} \"\n611                 line += \"title=SQLFluff,\"\n612                 line += f\"file={filepath},\"\n613                 line += f\"line={violation['line_no']},\"\n614                 line += f\"col={violation['line_pos']}\"\n615                 line += \"::\"\n616                 line += f\"{violation['code']}: {violation['description']}\"\n617 \n618                 github_result_native.append(line)\n619 \n620         file_output = \"\\n\".join(github_result_native)\n621 \n622     if file_output:\n623         dump_file_payload(write_output, cast(str, file_output))\n624 \n625     output_stream.close()\n626     if bench:\n627         click.echo(\"==== overall timings ====\")\n628         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n629         timing_summary = result.timing_summary()\n630         for step in timing_summary:\n631             click.echo(f\"=== {step} ===\")\n632             click.echo(formatter.cli_table(timing_summary[step].items()))\n633 \n634     if not nofail:\n635         if not non_human_output:\n636             formatter.completion_message()\n637         sys.exit(result.stats()[\"exit code\"])\n638     else:\n639         sys.exit(EXIT_SUCCESS)\n640 \n641 \n642 def do_fixes(lnt, result, formatter=None, **kwargs):\n643     \"\"\"Actually do the fixes.\"\"\"\n644     click.echo(\"Persisting Changes...\")\n645     res = result.persist_changes(formatter=formatter, **kwargs)\n646     if all(res.values()):\n647         click.echo(\"Done. Please check your files to confirm.\")\n648         return True\n649     # If some failed then return false\n650     click.echo(\n651         \"Done. Some operations failed. Please check your files to confirm.\"\n652     )  # pragma: no cover\n653     click.echo(\n654         \"Some errors cannot be fixed or there is another error blocking it.\"\n655     )  # pragma: no cover\n656     return False  # pragma: no cover\n657 \n658 \n659 @cli.command()\n660 @common_options\n661 @core_options\n662 @click.option(\n663     \"-f\",\n664     \"--force\",\n665     is_flag=True,\n666     help=(\n667         \"skip the confirmation prompt and go straight to applying \"\n668         \"fixes. **Use this with caution.**\"\n669     ),\n670 )\n671 @click.option(\n672     \"--fixed-suffix\", default=None, help=\"An optional suffix to add to fixed files.\"\n673 )\n674 @click.option(\n675     \"-p\",\n676     \"--processes\",\n677     type=int,\n678     default=1,\n679     help=\"The number of parallel processes to run.\",\n680 )\n681 @click.option(\n682     \"--disable_progress_bar\",\n683     is_flag=True,\n684     help=\"Disables progress bars.\",\n685 )\n686 @click.option(\n687     \"--FIX-EVEN-UNPARSABLE\",\n688     is_flag=True,\n689     default=None,\n690     help=(\n691         \"Enables fixing of files that have templating or parse errors. \"\n692         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n693         \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n694         \"command will not make any fixes in files that have templating or parse \"\n695         \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n696         \"or in the .sqlfluff config file.\"\n697     ),\n698 )\n699 @click.option(\n700     \"--show-lint-violations\",\n701     is_flag=True,\n702     help=\"Show lint violations\",\n703 )\n704 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n705 def fix(\n706     force: bool,\n707     paths: Tuple[str],\n708     processes: int,\n709     bench: bool = False,\n710     fixed_suffix: str = \"\",\n711     logger: Optional[logging.Logger] = None,\n712     disable_progress_bar: Optional[bool] = False,\n713     extra_config_path: Optional[str] = None,\n714     ignore_local_config: bool = False,\n715     show_lint_violations: bool = False,\n716     **kwargs,\n717 ) -> None:\n718     \"\"\"Fix SQL files.\n719 \n720     PATH is the path to a sql file or directory to lint. This can be either a\n721     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n722     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n723     be interpreted like passing the current working directory as a path argument.\n724     \"\"\"\n725     # some quick checks\n726     fixing_stdin = (\"-\",) == paths\n727 \n728     config = get_config(\n729         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n730     )\n731     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n732     output_stream = make_output_stream(\n733         config, None, os.devnull if fixing_stdin else None\n734     )\n735     lnt, formatter = get_linter_and_formatter(config, output_stream)\n736 \n737     verbose = config.get(\"verbose\")\n738     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n739 \n740     exit_code = EXIT_SUCCESS\n741 \n742     formatter.dispatch_config(lnt)\n743 \n744     # Set up logging.\n745     set_logging_level(\n746         verbosity=verbose,\n747         formatter=formatter,\n748         logger=logger,\n749         stderr_output=fixing_stdin,\n750     )\n751 \n752     # handle stdin case. should output formatted sql to stdout and nothing else.\n753     if fixing_stdin:\n754         stdin = sys.stdin.read()\n755 \n756         result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n757         templater_error = result.num_violations(types=SQLTemplaterError) > 0\n758         unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n759         if not fix_even_unparsable:\n760             exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n761 \n762         if result.num_violations(types=SQLLintError, fixable=True) > 0:\n763             stdout = result.paths[0].files[0].fix_string()[0]\n764         else:\n765             stdout = stdin\n766 \n767         if templater_error:\n768             click.echo(\n769                 formatter.colorize(\n770                     \"Fix aborted due to unparseable template variables.\",\n771                     Color.red,\n772                 ),\n773                 err=True,\n774             )\n775             click.echo(\n776                 formatter.colorize(\n777                     \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n778                     Color.red,\n779                 ),\n780                 err=True,\n781             )\n782 \n783         if unfixable_error:\n784             click.echo(\n785                 formatter.colorize(\"Unfixable violations detected.\", Color.red),\n786                 err=True,\n787             )\n788 \n789         click.echo(stdout, nl=False)\n790         sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n791 \n792     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n793     click.echo(\"==== finding fixable violations ====\")\n794 \n795     with PathAndUserErrorHandler(formatter, paths):\n796         result = lnt.lint_paths(\n797             paths,\n798             fix=True,\n799             ignore_non_existent_files=False,\n800             processes=processes,\n801         )\n802 \n803     if not fix_even_unparsable:\n804         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n805 \n806     # NB: We filter to linting violations here, because they're\n807     # the only ones which can be potentially fixed.\n808     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n809         click.echo(\"==== fixing violations ====\")\n810         click.echo(\n811             f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable \"\n812             \"linting violations found\"\n813         )\n814         if force:\n815             click.echo(\n816                 f\"{formatter.colorize('FORCE MODE', Color.red)}: Attempting fixes...\"\n817             )\n818             success = do_fixes(\n819                 lnt,\n820                 result,\n821                 formatter,\n822                 types=SQLLintError,\n823                 fixed_file_suffix=fixed_suffix,\n824             )\n825             if not success:\n826                 sys.exit(EXIT_FAIL)  # pragma: no cover\n827         else:\n828             click.echo(\n829                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n830             )\n831             c = click.getchar().lower()\n832             click.echo(\"...\")\n833             if c in (\"y\", \"\\r\", \"\\n\"):\n834                 click.echo(\"Attempting fixes...\")\n835                 success = do_fixes(\n836                     lnt,\n837                     result,\n838                     formatter,\n839                     types=SQLLintError,\n840                     fixed_file_suffix=fixed_suffix,\n841                 )\n842                 if not success:\n843                     sys.exit(EXIT_FAIL)  # pragma: no cover\n844                 else:\n845                     formatter.completion_message()\n846             elif c == \"n\":\n847                 click.echo(\"Aborting...\")\n848                 exit_code = EXIT_FAIL\n849             else:  # pragma: no cover\n850                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n851                 click.echo(\"Aborting...\")\n852                 exit_code = EXIT_FAIL\n853     else:\n854         click.echo(\"==== no fixable linting violations found ====\")\n855         formatter.completion_message()\n856 \n857     error_types = [\n858         (\n859             dict(types=SQLLintError, fixable=False),\n860             \"  [{} unfixable linting violations found]\",\n861             EXIT_FAIL,\n862         ),\n863     ]\n864     for num_violations_kwargs, message_format, error_level in error_types:\n865         num_violations = result.num_violations(**num_violations_kwargs)\n866         if num_violations > 0:\n867             click.echo(message_format.format(num_violations))\n868             exit_code = max(exit_code, error_level)\n869 \n870     if bench:\n871         click.echo(\"==== overall timings ====\")\n872         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n873         timing_summary = result.timing_summary()\n874         for step in timing_summary:\n875             click.echo(f\"=== {step} ===\")\n876             click.echo(formatter.cli_table(timing_summary[step].items()))\n877 \n878     if show_lint_violations:\n879         click.echo(\"==== lint for unfixable violations ====\")\n880         for violation in result.get_violations(**num_violations_kwargs):\n881             click.echo(formatter.format_violation(violation))\n882 \n883     sys.exit(exit_code)\n884 \n885 \n886 def quoted_presenter(dumper, data):\n887     \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n888     if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n889         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n890     else:\n891         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n892 \n893 \n894 @cli.command()\n895 @common_options\n896 @core_options\n897 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n898 @click.option(\n899     \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n900 )\n901 @click.option(\n902     \"-c\",\n903     \"--code-only\",\n904     is_flag=True,\n905     help=\"Output only the code elements of the parse tree.\",\n906 )\n907 @click.option(\n908     \"-m\",\n909     \"--include-meta\",\n910     is_flag=True,\n911     help=(\n912         \"Include meta segments (indents, dedents and placeholders) in the output. \"\n913         \"This only applies when outputting json or yaml.\"\n914     ),\n915 )\n916 @click.option(\n917     \"-f\",\n918     \"--format\",\n919     default=FormatType.human.value,\n920     type=click.Choice(\n921         [\n922             FormatType.human.value,\n923             FormatType.json.value,\n924             FormatType.yaml.value,\n925         ],\n926         case_sensitive=False,\n927     ),\n928     help=\"What format to return the parse result in.\",\n929 )\n930 @click.option(\n931     \"--write-output\",\n932     help=(\n933         \"Optionally provide a filename to write the results to, mostly used in \"\n934         \"tandem with --format. NB: Setting an output file re-enables normal \"\n935         \"stdout logging.\"\n936     ),\n937 )\n938 @click.option(\n939     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n940 )\n941 @click.option(\n942     \"--nofail\",\n943     is_flag=True,\n944     help=(\n945         \"If set, the exit code will always be zero, regardless of violations \"\n946         \"found. This is potentially useful during rollout.\"\n947     ),\n948 )\n949 def parse(\n950     path: str,\n951     code_only: bool,\n952     include_meta: bool,\n953     format: str,\n954     write_output: Optional[str],\n955     profiler: bool,\n956     bench: bool,\n957     nofail: bool,\n958     logger: Optional[logging.Logger] = None,\n959     extra_config_path: Optional[str] = None,\n960     ignore_local_config: bool = False,\n961     **kwargs,\n962 ) -> None:\n963     \"\"\"Parse SQL files and just spit out the result.\n964 \n965     PATH is the path to a sql file or directory to lint. This can be either a\n966     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n967     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n968     be interpreted like passing the current working directory as a path argument.\n969     \"\"\"\n970     c = get_config(\n971         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n972     )\n973     # We don't want anything else to be logged if we want json or yaml output\n974     # unless we're writing to a file.\n975     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n976     output_stream = make_output_stream(c, format, write_output)\n977     lnt, formatter = get_linter_and_formatter(c, output_stream)\n978     verbose = c.get(\"verbose\")\n979     recurse = c.get(\"recurse\")\n980 \n981     progress_bar_configuration.disable_progress_bar = True\n982 \n983     formatter.dispatch_config(lnt)\n984 \n985     # Set up logging.\n986     set_logging_level(\n987         verbosity=verbose,\n988         formatter=formatter,\n989         logger=logger,\n990         stderr_output=non_human_output,\n991     )\n992 \n993     # TODO: do this better\n994 \n995     if profiler:\n996         # Set up the profiler if required\n997         try:\n998             import cProfile\n999         except ImportError:  # pragma: no cover\n1000             click.echo(\"The cProfiler is not available on your platform.\")\n1001             sys.exit(EXIT_ERROR)\n1002         pr = cProfile.Profile()\n1003         pr.enable()\n1004 \n1005     t0 = time.monotonic()\n1006 \n1007     # handle stdin if specified via lone '-'\n1008     with PathAndUserErrorHandler(formatter, path):\n1009         if \"-\" == path:\n1010             parsed_strings = [\n1011                 lnt.parse_string(\n1012                     sys.stdin.read(),\n1013                     \"stdin\",\n1014                     recurse=recurse,\n1015                     config=lnt.config,\n1016                 ),\n1017             ]\n1018         else:\n1019             # A single path must be specified for this command\n1020             parsed_strings = list(\n1021                 lnt.parse_path(\n1022                     path=path,\n1023                     recurse=recurse,\n1024                 )\n1025             )\n1026 \n1027     total_time = time.monotonic() - t0\n1028     violations_count = 0\n1029 \n1030     # iterative print for human readout\n1031     if format == FormatType.human.value:\n1032         violations_count = formatter.print_out_violations_and_timing(\n1033             output_stream, bench, code_only, total_time, verbose, parsed_strings\n1034         )\n1035     else:\n1036         parsed_strings_dict = [\n1037             dict(\n1038                 filepath=linted_result.fname,\n1039                 segments=linted_result.tree.as_record(\n1040                     code_only=code_only, show_raw=True, include_meta=include_meta\n1041                 )\n1042                 if linted_result.tree\n1043                 else None,\n1044             )\n1045             for linted_result in parsed_strings\n1046         ]\n1047 \n1048         if format == FormatType.yaml.value:\n1049             # For yaml dumping always dump double quoted strings if they contain\n1050             # tabs or newlines.\n1051             yaml.add_representer(str, quoted_presenter)\n1052             file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n1053         elif format == FormatType.json.value:\n1054             file_output = json.dumps(parsed_strings_dict)\n1055 \n1056         # Dump the output to stdout or to file as appropriate.\n1057         dump_file_payload(write_output, file_output)\n1058     if profiler:\n1059         pr.disable()\n1060         profiler_buffer = StringIO()\n1061         ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n1062         ps.print_stats()\n1063         click.echo(\"==== profiler stats ====\")\n1064         # Only print the first 50 lines of it\n1065         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n1066 \n1067     if violations_count > 0 and not nofail:\n1068         sys.exit(EXIT_FAIL)  # pragma: no cover\n1069     else:\n1070         sys.exit(EXIT_SUCCESS)\n1071 \n1072 \n1073 # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n1074 # simplifies the use of cProfile, e.g.:\n1075 # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\n1076 if __name__ == \"__main__\":\n1077     cli.main(sys.argv[1:])  # pragma: no cover\n1078 \n[end of src/sqlfluff/cli/commands.py]\n[start of src/sqlfluff/cli/formatters.py]\n1 \"\"\"Defines the formatters for the CLI.\"\"\"\n2 from io import StringIO\n3 import sys\n4 from typing import List, Optional, Tuple, Union\n5 \n6 import click\n7 from colorama import Style\n8 \n9 from sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\n10 from sqlfluff.cli.helpers import (\n11     get_package_version,\n12     get_python_version,\n13     get_python_implementation,\n14     pad_line,\n15     wrap_field,\n16 )\n17 from sqlfluff.cli.outputstream import OutputStream\n18 \n19 from sqlfluff.core import SQLBaseError, FluffConfig, Linter, TimingSummary\n20 from sqlfluff.core.enums import Color\n21 from sqlfluff.core.linter import LintedFile, LintingResult, ParsedString\n22 \n23 \n24 def split_string_on_spaces(s: str, line_length: int = 100) -> List[str]:\n25     \"\"\"Split a string into lines based on whitespace.\"\"\"\n26     line_buff = []\n27     str_buff = \"\"\n28     for token in s.split():\n29         # Can we put this token on this line without going over?\n30         if str_buff:\n31             if len(str_buff) + len(token) > line_length:\n32                 line_buff.append(str_buff)\n33                 str_buff = token\n34             else:\n35                 str_buff += \" \" + token\n36         else:\n37             # In the case that the buffer is already empty, add it without checking,\n38             # otherwise there might be things that we might never.\n39             str_buff = token\n40     # If we have left over buff, add it in\n41     if str_buff:\n42         line_buff.append(str_buff)\n43     return line_buff\n44 \n45 \n46 def format_linting_result_header():\n47     \"\"\"Format the header of a linting result output.\"\"\"\n48     text_buffer = StringIO()\n49     text_buffer.write(\"==== readout ====\\n\")\n50     return text_buffer.getvalue()\n51 \n52 \n53 class OutputStreamFormatter:\n54     \"\"\"Formatter which writes to an OutputStream.\n55 \n56     On instantiation, this formatter accepts a function to\n57     dispatch messages. Each public method accepts an object\n58     or data in a common format, with this class handling the\n59     formatting and output.\n60 \n61     This class is designed to be subclassed if we eventually\n62     want to provide other methods of surfacing output.\n63 \n64 \n65     Args:\n66         output_stream: Output is sent here\n67         verbosity: Specifies how verbose output should be\n68         filter_empty: If True, empty messages will not be dispatched\n69         output_line_length: Maximum line length\n70     \"\"\"\n71 \n72     def __init__(\n73         self,\n74         output_stream: OutputStream,\n75         nocolor: bool,\n76         verbosity: int = 0,\n77         filter_empty: bool = True,\n78         output_line_length: int = 80,\n79     ):\n80         self._output_stream = output_stream\n81         self.plain_output = self.should_produce_plain_output(nocolor)\n82         self._verbosity = verbosity\n83         self._filter_empty = filter_empty\n84         self.output_line_length = output_line_length\n85 \n86     @staticmethod\n87     def should_produce_plain_output(nocolor: bool) -> bool:\n88         \"\"\"Returns True if text output should be plain (not colored).\"\"\"\n89         return nocolor or not sys.stdout.isatty()\n90 \n91     def _dispatch(self, s: str) -> None:\n92         \"\"\"Dispatch a string to the callback.\n93 \n94         This method is designed as a point for subclassing.\n95         \"\"\"\n96         # The strip here is to filter out any empty messages\n97         if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n98             self._output_stream.write(s)\n99 \n100     def _format_config(self, linter: Linter) -> str:\n101         \"\"\"Format the config of a `Linter`.\"\"\"\n102         text_buffer = StringIO()\n103         # Only show version information if verbosity is high enough\n104         if self._verbosity > 0:\n105             text_buffer.write(\"==== sqlfluff ====\\n\")\n106             config_content = [\n107                 (\"sqlfluff\", get_package_version()),\n108                 (\"python\", get_python_version()),\n109                 (\"implementation\", get_python_implementation()),\n110                 (\"verbosity\", self._verbosity),\n111             ]\n112             if linter.dialect:\n113                 config_content.append((\"dialect\", linter.dialect.name))\n114             config_content += linter.templater.config_pairs()\n115             text_buffer.write(\n116                 self.cli_table(config_content, col_width=30, max_label_width=15)\n117             )\n118             text_buffer.write(\"\\n\")\n119             if linter.config.get(\"rule_allowlist\"):\n120                 text_buffer.write(\n121                     self.cli_table(\n122                         [(\"rules\", \", \".join(linter.config.get(\"rule_allowlist\")))],\n123                         col_width=41,\n124                     )\n125                 )\n126             if self._verbosity > 1:\n127                 text_buffer.write(\"\\n== Raw Config:\\n\")\n128                 text_buffer.write(self.format_config_vals(linter.config.iter_vals()))\n129         return text_buffer.getvalue()\n130 \n131     def dispatch_config(self, linter: Linter) -> None:\n132         \"\"\"Dispatch configuration output appropriately.\"\"\"\n133         self._dispatch(self._format_config(linter))\n134 \n135     def dispatch_persist_filename(self, filename, result):\n136         \"\"\"Dispatch filenames during a persist operation.\"\"\"\n137         # Only show the skip records at higher levels of verbosity\n138         if self._verbosity >= 2 or result != \"SKIP\":\n139             self._dispatch(self.format_filename(filename=filename, success=result))\n140 \n141     def _format_path(self, path: str) -> str:\n142         \"\"\"Format paths.\"\"\"\n143         return f\"=== [ path: {self.colorize(path, Color.lightgrey)} ] ===\\n\"\n144 \n145     def dispatch_path(self, path: str) -> None:\n146         \"\"\"Dispatch paths for display.\"\"\"\n147         if self._verbosity > 0:\n148             self._dispatch(self._format_path(path))\n149 \n150     def dispatch_template_header(\n151         self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\n152     ) -> None:\n153         \"\"\"Dispatch the header displayed before templating.\"\"\"\n154         if self._verbosity > 1:\n155             self._dispatch(self.format_filename(filename=fname, success=\"TEMPLATING\"))\n156             # This is where we output config diffs if they exist.\n157             if file_config:\n158                 # Only output config diffs if there is a config to diff to.\n159                 config_diff = file_config.diff_to(linter_config)\n160                 if config_diff:  # pragma: no cover\n161                     self._dispatch(\"   Config Diff:\")\n162                     self._dispatch(\n163                         self.format_config_vals(\n164                             linter_config.iter_vals(cfg=config_diff)\n165                         )\n166                     )\n167 \n168     def dispatch_parse_header(self, fname: str) -> None:\n169         \"\"\"Dispatch the header displayed before parsing.\"\"\"\n170         if self._verbosity > 1:\n171             self._dispatch(self.format_filename(filename=fname, success=\"PARSING\"))\n172 \n173     def dispatch_lint_header(self, fname: str) -> None:\n174         \"\"\"Dispatch the header displayed before linting.\"\"\"\n175         if self._verbosity > 1:\n176             self._dispatch(self.format_filename(filename=fname, success=\"LINTING\"))\n177 \n178     def dispatch_compilation_header(self, templater, message):\n179         \"\"\"Dispatch the header displayed before linting.\"\"\"\n180         self._dispatch(\n181             f\"=== [{self.colorize(templater, Color.lightgrey)}] {message}\"\n182         )  # pragma: no cover\n183 \n184     def dispatch_dialect_warning(self, dialect) -> None:\n185         \"\"\"Dispatch a warning for dialects.\"\"\"\n186         self._dispatch(self.format_dialect_warning(dialect))  # pragma: no cover\n187 \n188     def _format_file_violations(\n189         self, fname: str, violations: List[SQLBaseError]\n190     ) -> str:\n191         \"\"\"Format a set of violations in a `LintingResult`.\"\"\"\n192         text_buffer = StringIO()\n193         # Success is having no violations (which aren't ignored)\n194         success = sum(int(not violation.ignore) for violation in violations) == 0\n195 \n196         # Only print the filename if it's either a failure or verbosity > 1\n197         if self._verbosity > 0 or not success:\n198             text_buffer.write(self.format_filename(fname, success=success))\n199             text_buffer.write(\"\\n\")\n200 \n201         # If we have violations, print them\n202         if not success:\n203             # sort by position in file (using line number and position)\n204             s = sorted(violations, key=lambda v: (v.line_no, v.line_pos))\n205             for violation in s:\n206                 text_buffer.write(\n207                     self.format_violation(\n208                         violation, max_line_length=self.output_line_length\n209                     )\n210                 )\n211                 text_buffer.write(\"\\n\")\n212         str_buffer = text_buffer.getvalue()\n213         # Remove the trailing newline if there is one\n214         if len(str_buffer) > 0 and str_buffer[-1] == \"\\n\":\n215             str_buffer = str_buffer[:-1]\n216         return str_buffer\n217 \n218     def dispatch_file_violations(\n219         self, fname: str, linted_file: LintedFile, only_fixable: bool\n220     ) -> None:\n221         \"\"\"Dispatch any violations found in a file.\"\"\"\n222         s = self._format_file_violations(\n223             fname, linted_file.get_violations(fixable=True if only_fixable else None)\n224         )\n225         self._dispatch(s)\n226 \n227     def colorize(self, s: str, color: Optional[Color] = None) -> str:\n228         \"\"\"Optionally use ANSI colour codes to colour a string.\"\"\"\n229         return self.colorize_helper(self.plain_output, s, color)\n230 \n231     @staticmethod\n232     def colorize_helper(\n233         plain_output: bool, s: str, color: Optional[Color] = None\n234     ) -> str:\n235         \"\"\"Static version of colorize() method.\"\"\"\n236         if not color or plain_output:\n237             return s\n238         else:\n239             return f\"{color.value}{s}{Style.RESET_ALL}\"\n240 \n241     def cli_table_row(\n242         self,\n243         fields: List[Tuple[str, str]],\n244         col_width,\n245         max_label_width=10,\n246         sep_char=\": \",\n247         divider_char=\" \",\n248         label_color=Color.lightgrey,\n249         val_align=\"right\",\n250     ) -> str:\n251         \"\"\"Make a row of a CLI table, using wrapped values.\"\"\"\n252         # Do some intel first\n253         cols = len(fields)\n254         last_col_idx = cols - 1\n255         wrapped_fields = [\n256             wrap_field(\n257                 field[0],\n258                 field[1],\n259                 width=col_width,\n260                 max_label_width=max_label_width,\n261                 sep_char=sep_char,\n262             )\n263             for field in fields\n264         ]\n265         max_lines = max(fld[\"lines\"] for fld in wrapped_fields)\n266         last_line_idx = max_lines - 1\n267         # Make some text\n268         buff = StringIO()\n269         for line_idx in range(max_lines):\n270             for col_idx in range(cols):\n271                 # Assume we pad labels left and values right\n272                 fld = wrapped_fields[col_idx]\n273                 ll = fld[\"label_list\"]\n274                 vl = fld[\"val_list\"]\n275                 buff.write(\n276                     self.colorize(\n277                         pad_line(\n278                             ll[line_idx] if line_idx < len(ll) else \"\",\n279                             width=fld[\"label_width\"],\n280                         ),\n281                         color=label_color,\n282                     )\n283                 )\n284                 if line_idx == 0:\n285                     buff.write(sep_char)\n286                 else:\n287                     buff.write(\" \" * len(sep_char))\n288                 buff.write(\n289                     pad_line(\n290                         vl[line_idx] if line_idx < len(vl) else \"\",\n291                         width=fld[\"val_width\"],\n292                         align=val_align,\n293                     )\n294                 )\n295                 if col_idx != last_col_idx:\n296                     buff.write(divider_char)\n297                 elif line_idx != last_line_idx:\n298                     buff.write(\"\\n\")\n299         return buff.getvalue()\n300 \n301     def cli_table(\n302         self,\n303         fields,\n304         col_width=20,\n305         cols=2,\n306         divider_char=\" \",\n307         sep_char=\": \",\n308         label_color=Color.lightgrey,\n309         float_format=\"{0:.2f}\",\n310         max_label_width=10,\n311         val_align=\"right\",\n312     ) -> str:\n313         \"\"\"Make a crude ascii table.\n314 \n315         Assume that `fields` is an iterable of (label, value) pairs.\n316         \"\"\"\n317         # First format all the values into strings\n318         formatted_fields = []\n319         for label, value in fields:\n320             label = str(label)\n321             if isinstance(value, float):\n322                 value = float_format.format(value)\n323             else:\n324                 value = str(value)\n325             formatted_fields.append((label, value))\n326 \n327         # Set up a buffer to hold the whole table\n328         buff = StringIO()\n329         while len(formatted_fields) > 0:\n330             row_buff: List[Tuple[str, str]] = []\n331             while len(row_buff) < cols and len(formatted_fields) > 0:\n332                 row_buff.append(formatted_fields.pop(0))\n333             buff.write(\n334                 self.cli_table_row(\n335                     row_buff,\n336                     col_width=col_width,\n337                     max_label_width=max_label_width,\n338                     sep_char=sep_char,\n339                     divider_char=divider_char,\n340                     label_color=label_color,\n341                     val_align=val_align,\n342                 )\n343             )\n344             if len(formatted_fields) > 0:\n345                 buff.write(\"\\n\")\n346         return buff.getvalue()\n347 \n348     def format_filename(\n349         self,\n350         filename: str,\n351         success: Union[str, bool] = False,\n352         success_text: str = \"PASS\",\n353     ) -> str:\n354         \"\"\"Format filenames.\"\"\"\n355         if isinstance(success, str):\n356             status_string = success\n357         else:\n358             status_string = self.colorize(\n359                 success_text if success else \"FAIL\",\n360                 Color.green if success else Color.red,\n361             )\n362         return f\"== [{self.colorize(filename, Color.lightgrey)}] {status_string}\"\n363 \n364     def format_violation(\n365         self, violation: SQLBaseError, max_line_length: int = 90\n366     ) -> str:\n367         \"\"\"Format a violation.\"\"\"\n368         if not isinstance(violation, SQLBaseError):  # pragma: no cover\n369             raise ValueError(f\"Unexpected violation format: {violation}\")\n370 \n371         desc: str = violation.desc()\n372         line_elem = \"   -\" if violation.line_no is None else f\"{violation.line_no:4d}\"\n373         pos_elem = \"   -\" if violation.line_pos is None else f\"{violation.line_pos:4d}\"\n374 \n375         if violation.ignore:\n376             desc = \"IGNORE: \" + desc  # pragma: no cover\n377 \n378         split_desc = split_string_on_spaces(desc, line_length=max_line_length - 25)\n379 \n380         out_buff = \"\"\n381         # Grey out the violation if we're ignoring it.\n382         section_color: Color = Color.lightgrey if violation.ignore else Color.blue\n383         for idx, line in enumerate(split_desc):\n384             if idx == 0:\n385                 rule_code = violation.rule_code().rjust(4)\n386                 if \"PRS\" in rule_code:\n387                     section_color = Color.red\n388                 out_buff += self.colorize(\n389                     f\"L:{line_elem} | P:{pos_elem} | {rule_code} | \",\n390                     section_color,\n391                 )\n392             else:\n393                 out_buff += (\n394                     \"\\n\"\n395                     + (\" \" * 23)\n396                     + self.colorize(\n397                         \"| \",\n398                         section_color,\n399                     )\n400                 )\n401             out_buff += line\n402         return out_buff\n403 \n404     def format_linting_stats(self, result, verbose=0):\n405         \"\"\"Format a set of stats given a `LintingResult`.\"\"\"\n406         text_buffer = StringIO()\n407         all_stats = result.stats()\n408         text_buffer.write(\"==== summary ====\\n\")\n409         if verbose >= 2:\n410             output_fields = [\n411                 \"files\",\n412                 \"violations\",\n413                 \"clean files\",\n414                 \"unclean files\",\n415                 \"avg per file\",\n416                 \"unclean rate\",\n417                 \"status\",\n418             ]\n419             special_formats = {\"unclean rate\": \"{0:.0%}\"}\n420         else:\n421             output_fields = [\"violations\", \"status\"]\n422             special_formats = {}\n423         # Generate content tuples, applying special formats for some fields\n424         summary_content = [\n425             (\n426                 key,\n427                 special_formats[key].format(all_stats[key])\n428                 if key in special_formats\n429                 else all_stats[key],\n430             )\n431             for key in output_fields\n432         ]\n433         # Render it all as a table\n434         text_buffer.write(self.cli_table(summary_content, max_label_width=14))\n435         return text_buffer.getvalue()\n436 \n437     def format_config_vals(self, config_vals):\n438         \"\"\"Format an iterable of config values from a config object.\"\"\"\n439         text_buffer = StringIO()\n440         for i, k, v in config_vals:\n441             val = \"\" if v is None else str(v)\n442             text_buffer.write(\n443                 (\"    \" * i)\n444                 + self.colorize(\n445                     pad_line(str(k) + \":\", 20, \"left\"), color=Color.lightgrey\n446                 )\n447                 + pad_line(val, 20, \"left\")\n448                 + \"\\n\"\n449             )\n450         return text_buffer.getvalue()\n451 \n452     def format_rules(self, linter: Linter, verbose: int = 0) -> str:\n453         \"\"\"Format the a set of rules given a `Linter`.\"\"\"\n454         text_buffer = StringIO()\n455         text_buffer.write(\"==== sqlfluff - rules ====\\n\")\n456         text_buffer.write(\n457             self.cli_table(\n458                 linter.rule_tuples(),\n459                 col_width=80,\n460                 cols=1,\n461                 label_color=Color.blue,\n462                 val_align=\"left\",\n463             )\n464         )\n465         return text_buffer.getvalue()\n466 \n467     def format_dialects(self, dialect_readout, verbose=0):\n468         \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n469         text_buffer = StringIO()\n470         text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n471         readouts = [\n472             (\n473                 dialect.label,\n474                 f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n475             )\n476             for dialect in dialect_readout()\n477         ]\n478         text_buffer.write(\n479             self.cli_table(\n480                 readouts,\n481                 col_width=60,\n482                 cols=1,\n483                 label_color=Color.blue,\n484                 val_align=\"right\",\n485             )\n486         )\n487         return text_buffer.getvalue()\n488 \n489     def format_dialect_warning(self, dialect):\n490         \"\"\"Output a warning for parsing errors.\"\"\"\n491         return self.colorize(\n492             (\n493                 \"WARNING: Parsing errors found and dialect is set to \"\n494                 f\"'{dialect}'. Have you configured your dialect correctly?\"\n495             ),\n496             Color.lightgrey,\n497         )\n498 \n499     def handle_files_with_tmp_or_prs_errors(self, lint_result: LintingResult) -> int:\n500         \"\"\"Discard lint fixes for files with templating or parse errors.\n501 \n502         Returns 1 if there are any files with templating or parse errors after\n503         filtering, else 0. (Intended as a process exit code.)\n504         \"\"\"\n505         total_errors, num_filtered_errors = lint_result.count_tmp_prs_errors()\n506         lint_result.discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors()\n507         if total_errors:\n508             click.echo(\n509                 self.colorize(\n510                     f\"  [{total_errors} templating/parsing errors found]\", Color.red\n511                 )\n512             )\n513             if num_filtered_errors < total_errors:\n514                 color = Color.red if num_filtered_errors else Color.green\n515                 click.echo(\n516                     self.colorize(\n517                         f\"  [{num_filtered_errors} templating/parsing errors \"\n518                         f'remaining after \"ignore\"]',\n519                         color,\n520                     )\n521                 )\n522         return EXIT_FAIL if num_filtered_errors else EXIT_SUCCESS\n523 \n524     def print_out_violations_and_timing(\n525         self,\n526         output_stream: OutputStream,\n527         bench: bool,\n528         code_only: bool,\n529         total_time: float,\n530         verbose: int,\n531         parsed_strings: List[ParsedString],\n532     ) -> int:\n533         \"\"\"Used by human formatting during the parse.\"\"\"\n534         violations_count = 0\n535         timing = TimingSummary()\n536 \n537         for parsed_string in parsed_strings:\n538             timing.add(parsed_string.time_dict)\n539 \n540             if parsed_string.tree:\n541                 output_stream.write(parsed_string.tree.stringify(code_only=code_only))\n542             else:\n543                 # TODO: Make this prettier\n544                 output_stream.write(\"...Failed to Parse...\")  # pragma: no cover\n545 \n546             violations_count += len(parsed_string.violations)\n547             if parsed_string.violations:\n548                 output_stream.write(\"==== parsing violations ====\")  # pragma: no cover\n549             for v in parsed_string.violations:\n550                 output_stream.write(self.format_violation(v))  # pragma: no cover\n551             if parsed_string.violations:\n552                 output_stream.write(\n553                     self.format_dialect_warning(parsed_string.config.get(\"dialect\"))\n554                 )\n555 \n556             if verbose >= 2:\n557                 output_stream.write(\"==== timings ====\")\n558                 output_stream.write(self.cli_table(parsed_string.time_dict.items()))\n559 \n560         if verbose >= 2 or bench:\n561             output_stream.write(\"==== overall timings ====\")\n562             output_stream.write(self.cli_table([(\"Clock time\", total_time)]))\n563             timing_summary = timing.summary()\n564             for step in timing_summary:\n565                 output_stream.write(f\"=== {step} ===\")\n566                 output_stream.write(self.cli_table(timing_summary[step].items()))\n567 \n568         return violations_count\n569 \n570     def completion_message(self) -> None:\n571         \"\"\"Prints message when SQLFluff is finished.\"\"\"\n572         click.echo(\"All Finished\" f\"{'' if self.plain_output else ' \ud83d\udcdc \ud83c\udf89'}!\")\n573 \n[end of src/sqlfluff/cli/formatters.py]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import Any, List, Sequence, Optional, Tuple, cast, Iterable, Iterator, Set\n8 \n9 import pathspec\n10 import regex\n11 from tqdm import tqdm\n12 \n13 from sqlfluff.core.errors import (\n14     SQLBaseError,\n15     SQLLexError,\n16     SQLLintError,\n17     SQLParseError,\n18     SQLTemplaterSkipFile,\n19 )\n20 from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n21 from sqlfluff.core.file_helpers import get_encoding\n22 from sqlfluff.core.templaters import TemplatedFile\n23 from sqlfluff.core.rules import get_ruleset\n24 from sqlfluff.core.rules.doc_decorators import is_fix_compatible\n25 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n26 \n27 # Classes needed only for type checking\n28 from sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n29 from sqlfluff.core.parser.segments.meta import MetaSegment\n30 from sqlfluff.core.parser.segments.raw import RawSegment\n31 from sqlfluff.core.rules import BaseRule\n32 \n33 from sqlfluff.core.linter.common import (\n34     RuleTuple,\n35     ParsedString,\n36     NoQaDirective,\n37     RenderedFile,\n38 )\n39 from sqlfluff.core.linter.linted_file import LintedFile\n40 from sqlfluff.core.linter.linted_dir import LintedDir\n41 from sqlfluff.core.linter.linting_result import LintingResult\n42 \n43 \n44 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n45 \n46 # Instantiate the linter logger\n47 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n48 \n49 \n50 class Linter:\n51     \"\"\"The interface class to interact with the linter.\"\"\"\n52 \n53     # Default to allowing process parallelism\n54     allow_process_parallelism = True\n55 \n56     def __init__(\n57         self,\n58         config: Optional[FluffConfig] = None,\n59         formatter: Any = None,\n60         dialect: Optional[str] = None,\n61         rules: Optional[List[str]] = None,\n62         user_rules: Optional[List[BaseRule]] = None,\n63         exclude_rules: Optional[List[str]] = None,\n64     ) -> None:\n65         # Store the config object\n66         self.config = FluffConfig.from_kwargs(\n67             config=config,\n68             dialect=dialect,\n69             rules=rules,\n70             exclude_rules=exclude_rules,\n71             # Don't require a dialect to be provided yet. Defer this until we\n72             # are actually linting something, since the directory we are linting\n73             # from may provide additional configuration, including a dialect.\n74             require_dialect=False,\n75         )\n76         # Get the dialect and templater\n77         self.dialect = self.config.get(\"dialect_obj\")\n78         self.templater = self.config.get(\"templater_obj\")\n79         # Store the formatter for output\n80         self.formatter = formatter\n81         # Store references to user rule classes\n82         self.user_rules = user_rules or []\n83 \n84     def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n85         \"\"\"Get hold of a set of rules.\"\"\"\n86         rs = get_ruleset()\n87         # Register any user rules\n88         for rule in self.user_rules:\n89             rs.register(rule)\n90         cfg = config or self.config\n91         return rs.get_rulelist(config=cfg)\n92 \n93     def rule_tuples(self) -> List[RuleTuple]:\n94         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n95         rs = self.get_ruleset()\n96         return [RuleTuple(rule.code, rule.description) for rule in rs]\n97 \n98     # #### Static methods\n99     # These are the building blocks of the linting process.\n100 \n101     @staticmethod\n102     def _load_raw_file_and_config(\n103         fname: str, root_config: FluffConfig\n104     ) -> Tuple[str, FluffConfig, str]:\n105         \"\"\"Load a raw file and the associated config.\"\"\"\n106         file_config = root_config.make_child_from_path(fname)\n107         encoding = get_encoding(fname=fname, config=file_config)\n108         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n109             raw_file = target_file.read()\n110         # Scan the raw file for config commands.\n111         file_config.process_raw_file_for_config(raw_file)\n112         # Return the raw file and config\n113         return raw_file, file_config, encoding\n114 \n115     @staticmethod\n116     def _normalise_newlines(string: str) -> str:\n117         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n118         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n119 \n120     @staticmethod\n121     def _lex_templated_file(\n122         templated_file: TemplatedFile, config: FluffConfig\n123     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n124         \"\"\"Lex a templated file.\n125 \n126         NOTE: This potentially mutates the config, so make sure to\n127         use the returned one.\n128         \"\"\"\n129         violations = []\n130         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n131         # Get the lexer\n132         lexer = Lexer(config=config)\n133         # Lex the file and log any problems\n134         try:\n135             tokens, lex_vs = lexer.lex(templated_file)\n136             # We might just get the violations as a list\n137             violations += lex_vs\n138             linter_logger.info(\n139                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n140             )\n141         except SQLLexError as err:\n142             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n143             violations.append(err)\n144             return None, violations, config\n145 \n146         if not tokens:  # pragma: no cover TODO?\n147             return None, violations, config\n148 \n149         # Check that we've got sensible indentation from the lexer.\n150         # We might need to suppress if it's a complicated file.\n151         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n152         if isinstance(templating_blocks_indent, str):\n153             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n154         else:\n155             force_block_indent = False\n156         templating_blocks_indent = bool(templating_blocks_indent)\n157         # If we're forcing it through we don't check.\n158         if templating_blocks_indent and not force_block_indent:\n159             indent_balance = sum(\n160                 getattr(elem, \"indent_val\", 0)\n161                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n162             )\n163             if indent_balance != 0:\n164                 linter_logger.debug(\n165                     \"Indent balance test failed for %r. Template indents will not be \"\n166                     \"linted for this file.\",\n167                     templated_file.fname,\n168                 )\n169                 # Don't enable the templating blocks.\n170                 templating_blocks_indent = False\n171 \n172         # The file will have been lexed without config, so check all indents\n173         # are enabled.\n174         new_tokens = []\n175         for token in cast(Tuple[BaseSegment, ...], tokens):\n176             if token.is_meta:\n177                 token = cast(MetaSegment, token)\n178                 if token.indent_val != 0:\n179                     # Don't allow it if we're not linting templating block indents.\n180                     if not templating_blocks_indent:\n181                         continue\n182             new_tokens.append(token)\n183 \n184         # Return new buffer\n185         return new_tokens, violations, config\n186 \n187     @staticmethod\n188     def _parse_tokens(\n189         tokens: Sequence[BaseSegment],\n190         config: FluffConfig,\n191         recurse: bool = True,\n192         fname: Optional[str] = None,\n193     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n194         parser = Parser(config=config)\n195         violations = []\n196         # Parse the file and log any problems\n197         try:\n198             parsed: Optional[BaseSegment] = parser.parse(\n199                 tokens,\n200                 recurse=recurse,\n201                 fname=fname,\n202             )\n203         except SQLParseError as err:\n204             linter_logger.info(\"PARSING FAILED! : %s\", err)\n205             violations.append(err)\n206             return None, violations\n207 \n208         if parsed:\n209             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n210             linter_logger.info(\"\\n\" + parsed.stringify())\n211             # We may succeed parsing, but still have unparsable segments. Extract them\n212             # here.\n213             for unparsable in parsed.iter_unparsables():\n214                 # No exception has been raised explicitly, but we still create one here\n215                 # so that we can use the common interface\n216                 violations.append(\n217                     SQLParseError(\n218                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n219                         \"{1!r}\".format(\n220                             unparsable.pos_marker.working_loc,\n221                             unparsable.raw\n222                             if len(unparsable.raw) < 40\n223                             else unparsable.raw[:40] + \"...\",\n224                         ),\n225                         segment=unparsable,\n226                     )\n227                 )\n228                 linter_logger.info(\"Found unparsable segment...\")\n229                 linter_logger.info(unparsable.stringify())\n230         return parsed, violations\n231 \n232     @staticmethod\n233     def parse_noqa(\n234         comment: str,\n235         line_no: int,\n236         rule_codes: List[str],\n237     ):\n238         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n239         # Also trim any whitespace afterward\n240 \n241         # Comment lines can also have noqa e.g.\n242         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: L016\n243         # Therefore extract last possible inline ignore.\n244         comment = [c.strip() for c in comment.split(\"--\")][-1]\n245 \n246         if comment.startswith(\"noqa\"):\n247             # This is an ignore identifier\n248             comment_remainder = comment[4:]\n249             if comment_remainder:\n250                 if not comment_remainder.startswith(\":\"):\n251                     return SQLParseError(\n252                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n253                         line_no=line_no,\n254                     )\n255                 comment_remainder = comment_remainder[1:].strip()\n256                 if comment_remainder:\n257                     action: Optional[str]\n258                     if \"=\" in comment_remainder:\n259                         action, rule_part = comment_remainder.split(\"=\", 1)\n260                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n261                             return SQLParseError(\n262                                 \"Malformed 'noqa' section. \"\n263                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n264                                 \"or 'noqa: disable=<rule>[,...] | all\",\n265                                 line_no=line_no,\n266                             )\n267                     else:\n268                         action = None\n269                         rule_part = comment_remainder\n270                         if rule_part in {\"disable\", \"enable\"}:\n271                             return SQLParseError(\n272                                 \"Malformed 'noqa' section. \"\n273                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n274                                 \"or 'noqa: disable=<rule>[,...] | all\",\n275                                 line_no=line_no,\n276                             )\n277                     rules: Optional[Tuple[str, ...]]\n278                     if rule_part != \"all\":\n279                         # Rules can be globs therefore we compare to the rule_set to\n280                         # expand the globs.\n281                         unexpanded_rules = tuple(\n282                             r.strip() for r in rule_part.split(\",\")\n283                         )\n284                         expanded_rules = []\n285                         for r in unexpanded_rules:\n286                             expanded_rule = [\n287                                 x\n288                                 for x in fnmatch.filter(rule_codes, r)\n289                                 if x not in expanded_rules\n290                             ]\n291                             if expanded_rule:\n292                                 expanded_rules.extend(expanded_rule)\n293                             elif r not in expanded_rules:\n294                                 # We were unable to expand the glob.\n295                                 # Therefore assume the user is referencing\n296                                 # a special error type (e.g. PRS, LXR, or TMP)\n297                                 # and add this to the list of rules to ignore.\n298                                 expanded_rules.append(r)\n299                         rules = tuple(expanded_rules)\n300                     else:\n301                         rules = None\n302                     return NoQaDirective(line_no, rules, action)\n303             return NoQaDirective(line_no, None, None)\n304         return None\n305 \n306     @staticmethod\n307     def remove_templated_errors(\n308         linting_errors: List[SQLBaseError],\n309     ) -> List[SQLBaseError]:\n310         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n311         # Filter out any linting errors in templated sections if relevant.\n312         result: List[SQLBaseError] = []\n313         for e in linting_errors:\n314             if isinstance(e, SQLLintError):\n315                 if (\n316                     # Is it in a literal section?\n317                     e.segment.pos_marker.is_literal()\n318                     # Is it a rule that is designed to work on templated sections?\n319                     or e.rule.targets_templated\n320                 ):\n321                     result.append(e)\n322             else:\n323                 # If it's another type, just keep it. (E.g. SQLParseError from\n324                 # malformed \"noqa\" comment).\n325                 result.append(e)\n326         return result\n327 \n328     @staticmethod\n329     def _report_conflicting_fixes_same_anchor(message: str):  # pragma: no cover\n330         # This function exists primarily in order to let us monkeypatch it at\n331         # runtime (replacing it with a function that raises an exception).\n332         linter_logger.critical(message)\n333 \n334     @staticmethod\n335     def _warn_unfixable(code: str):\n336         linter_logger.warning(\n337             f\"One fix for {code} not applied, it would re-cause the same error.\"\n338         )\n339 \n340     # ### Class Methods\n341     # These compose the base static methods into useful recipes.\n342 \n343     @classmethod\n344     def parse_rendered(\n345         cls,\n346         rendered: RenderedFile,\n347         recurse: bool = True,\n348     ) -> ParsedString:\n349         \"\"\"Parse a rendered file.\"\"\"\n350         t0 = time.monotonic()\n351         violations = cast(List[SQLBaseError], rendered.templater_violations)\n352         tokens: Optional[Sequence[BaseSegment]]\n353         if rendered.templated_file:\n354             tokens, lvs, config = cls._lex_templated_file(\n355                 rendered.templated_file, rendered.config\n356             )\n357             violations += lvs\n358         else:\n359             tokens = None\n360 \n361         t1 = time.monotonic()\n362         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n363 \n364         if tokens:\n365             parsed, pvs = cls._parse_tokens(\n366                 tokens,\n367                 rendered.config,\n368                 recurse=recurse,\n369                 fname=rendered.fname,\n370             )\n371             violations += pvs\n372         else:\n373             parsed = None\n374 \n375         time_dict = {\n376             **rendered.time_dict,\n377             \"lexing\": t1 - t0,\n378             \"parsing\": time.monotonic() - t1,\n379         }\n380         return ParsedString(\n381             parsed,\n382             violations,\n383             time_dict,\n384             rendered.templated_file,\n385             rendered.config,\n386             rendered.fname,\n387             rendered.source_str,\n388         )\n389 \n390     @classmethod\n391     def extract_ignore_from_comment(\n392         cls,\n393         comment: RawSegment,\n394         rule_codes: List[str],\n395     ):\n396         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n397         # Also trim any whitespace afterward\n398         comment_content = comment.raw_trimmed().strip()\n399         comment_line, _ = comment.pos_marker.source_position()\n400         result = cls.parse_noqa(comment_content, comment_line, rule_codes)\n401         if isinstance(result, SQLParseError):\n402             result.segment = comment\n403         return result\n404 \n405     @classmethod\n406     def extract_ignore_mask_tree(\n407         cls,\n408         tree: BaseSegment,\n409         rule_codes: List[str],\n410     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n411         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n412         ignore_buff: List[NoQaDirective] = []\n413         violations: List[SQLBaseError] = []\n414         for comment in tree.recursive_crawl(\"comment\"):\n415             if comment.name == \"inline_comment\":\n416                 ignore_entry = cls.extract_ignore_from_comment(comment, rule_codes)\n417                 if isinstance(ignore_entry, SQLParseError):\n418                     violations.append(ignore_entry)\n419                 elif ignore_entry:\n420                     ignore_buff.append(ignore_entry)\n421         if ignore_buff:\n422             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n423         return ignore_buff, violations\n424 \n425     @classmethod\n426     def extract_ignore_mask_source(\n427         cls,\n428         source: str,\n429         inline_comment_regex: RegexLexer,\n430         rule_codes: List[str],\n431     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n432         \"\"\"Look for inline ignore comments and return NoQaDirectives.\n433 \n434         Very similar to extract_ignore_mask_tree(), but can be run on raw source\n435         (i.e. does not require the code to have parsed successfully).\n436         \"\"\"\n437         ignore_buff: List[NoQaDirective] = []\n438         violations: List[SQLBaseError] = []\n439         for idx, line in enumerate(source.split(\"\\n\")):\n440             match = inline_comment_regex.search(line) if line else None\n441             if match:\n442                 ignore_entry = cls.parse_noqa(\n443                     line[match[0] : match[1]], idx + 1, rule_codes\n444                 )\n445                 if isinstance(ignore_entry, SQLParseError):\n446                     violations.append(ignore_entry)  # pragma: no cover\n447                 elif ignore_entry:\n448                     ignore_buff.append(ignore_entry)\n449         if ignore_buff:\n450             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n451         return ignore_buff, violations\n452 \n453     @classmethod\n454     def lint_fix_parsed(\n455         cls,\n456         tree: BaseSegment,\n457         config: FluffConfig,\n458         rule_set: List[BaseRule],\n459         fix: bool = False,\n460         fname: Optional[str] = None,\n461         templated_file: Optional[TemplatedFile] = None,\n462         formatter: Any = None,\n463     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n464         \"\"\"Lint and optionally fix a tree object.\"\"\"\n465         # Keep track of the linting errors on the very first linter pass. The\n466         # list of issues output by \"lint\" and \"fix\" only includes issues present\n467         # in the initial SQL code, EXCLUDING any issues that may be created by\n468         # the fixes themselves.\n469         initial_linting_errors = []\n470         # A placeholder for the fixes we had on the previous loop\n471         last_fixes = None\n472         # Keep a set of previous versions to catch infinite loops.\n473         previous_versions: Set[Tuple[str, Tuple[SourceFix, ...]]] = {(tree.raw, ())}\n474 \n475         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n476         # once for linting.\n477         loop_limit = config.get(\"runaway_limit\") if fix else 1\n478 \n479         # Dispatch the output for the lint header\n480         if formatter:\n481             formatter.dispatch_lint_header(fname)\n482 \n483         # Look for comment segments which might indicate lines to ignore.\n484         if not config.get(\"disable_noqa\"):\n485             rule_codes = [r.code for r in rule_set]\n486             ignore_buff, ivs = cls.extract_ignore_mask_tree(tree, rule_codes)\n487             initial_linting_errors += ivs\n488         else:\n489             ignore_buff = []\n490 \n491         save_tree = tree\n492         # There are two phases of rule running.\n493         # 1. The main loop is for most rules. These rules are assumed to\n494         # interact and cause a cascade of fixes requiring multiple passes.\n495         # These are run the `runaway_limit` number of times (default 10).\n496         # 2. The post loop is for post-processing rules, not expected to trigger\n497         # any downstream rules, e.g. capitalization fixes. They are run on the\n498         # first loop and then twice at the end (once to fix, and once again to\n499         # check result of fixes), but not in the intervening loops.\n500         phases = [\"main\"]\n501         if fix:\n502             phases.append(\"post\")\n503         for phase in phases:\n504             if len(phases) > 1:\n505                 rules_this_phase = [\n506                     rule for rule in rule_set if rule.lint_phase == phase\n507                 ]\n508             else:\n509                 rules_this_phase = rule_set\n510             for loop in range(loop_limit if phase == \"main\" else 2):\n511 \n512                 def is_first_linter_pass():\n513                     return phase == phases[0] and loop == 0\n514 \n515                 linter_logger.info(f\"Linter phase {phase}, loop {loop+1}/{loop_limit}\")\n516                 changed = False\n517 \n518                 if is_first_linter_pass():\n519                     # In order to compute initial_linting_errors correctly, need\n520                     # to run all rules on the first loop of the main phase.\n521                     rules_this_phase = rule_set\n522                 progress_bar_crawler = tqdm(\n523                     rules_this_phase,\n524                     desc=\"lint by rules\",\n525                     leave=False,\n526                     disable=progress_bar_configuration.disable_progress_bar,\n527                 )\n528 \n529                 for crawler in progress_bar_crawler:\n530                     # Performance: After first loop pass, skip rules that don't\n531                     # do fixes. Any results returned won't be seen by the user\n532                     # anyway (linting errors ADDED by rules changing SQL, are\n533                     # not reported back to the user - only initial linting errors),\n534                     # so there's absolutely no reason to run them.\n535                     if (\n536                         fix\n537                         and not is_first_linter_pass()\n538                         and not is_fix_compatible(crawler)\n539                     ):\n540                         continue\n541 \n542                     progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n543 \n544                     # fixes should be a dict {} with keys edit, delete, create\n545                     # delete is just a list of segments to delete\n546                     # edit and create are list of tuples. The first element is\n547                     # the \"anchor\", the segment to look for either to edit or to\n548                     # insert BEFORE. The second is the element to insert or create.\n549                     linting_errors, _, fixes, _ = crawler.crawl(\n550                         tree,\n551                         dialect=config.get(\"dialect_obj\"),\n552                         fix=fix,\n553                         templated_file=templated_file,\n554                         ignore_mask=ignore_buff,\n555                         fname=fname,\n556                     )\n557                     if is_first_linter_pass():\n558                         initial_linting_errors += linting_errors\n559 \n560                     if fix and fixes:\n561                         linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n562                         # Do some sanity checks on the fixes before applying.\n563                         anchor_info = BaseSegment.compute_anchor_edit_info(fixes)\n564                         if any(\n565                             not info.is_valid for info in anchor_info.values()\n566                         ):  # pragma: no cover\n567                             message = (\n568                                 f\"Rule {crawler.code} returned conflicting \"\n569                                 \"fixes with the same anchor. This is only \"\n570                                 \"supported for create_before+create_after, so \"\n571                                 \"the fixes will not be applied. {fixes!r}\"\n572                             )\n573                             cls._report_conflicting_fixes_same_anchor(message)\n574                             for lint_result in linting_errors:\n575                                 lint_result.fixes = []\n576                         elif fixes == last_fixes:  # pragma: no cover\n577                             # If we generate the same fixes two times in a row,\n578                             # that means we're in a loop, and we want to stop.\n579                             # (Fixes should address issues, hence different\n580                             # and/or fewer fixes next time.)\n581                             cls._warn_unfixable(crawler.code)\n582                         else:\n583                             # This is the happy path. We have fixes, now we want to\n584                             # apply them.\n585                             last_fixes = fixes\n586                             new_tree, _, _ = tree.apply_fixes(\n587                                 config.get(\"dialect_obj\"), crawler.code, anchor_info\n588                             )\n589                             # Check for infinite loops. We use a combination of the\n590                             # fixed templated file and the list of source fixes to\n591                             # apply.\n592                             loop_check_tuple = (\n593                                 new_tree.raw,\n594                                 tuple(new_tree.source_fixes),\n595                             )\n596                             if loop_check_tuple not in previous_versions:\n597                                 # We've not seen this version of the file so\n598                                 # far. Continue.\n599                                 tree = new_tree\n600                                 previous_versions.add(loop_check_tuple)\n601                                 changed = True\n602                                 continue\n603                             else:\n604                                 # Applying these fixes took us back to a state\n605                                 # which we've seen before. We're in a loop, so\n606                                 # we want to stop.\n607                                 cls._warn_unfixable(crawler.code)\n608 \n609                 if fix and not changed:\n610                     # We did not change the file. Either the file is clean (no\n611                     # fixes), or any fixes which are present will take us back\n612                     # to a previous state.\n613                     linter_logger.info(\n614                         f\"Fix loop complete for {phase} phase. Stability \"\n615                         f\"achieved after {loop}/{loop_limit} loops.\"\n616                     )\n617                     break\n618             else:\n619                 if fix:\n620                     # The linter loop hit the limit before reaching a stable point\n621                     # (i.e. free of lint errors). If this happens, it's usually\n622                     # because one or more rules produced fixes which did not address\n623                     # the original issue **or** created new issues.\n624                     linter_logger.warning(\n625                         f\"Loop limit on fixes reached [{loop_limit}].\"\n626                     )\n627 \n628                     # Discard any fixes for the linting errors, since they caused a\n629                     # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n630                     # these linting errors are \"unfixable\". This is important,\n631                     # because when \"sqlfluff fix\" encounters unfixable lint errors,\n632                     # it exits with a \"failure\" exit code, which is exactly what we\n633                     # want in this situation. (Reason: Although this is more of an\n634                     # internal SQLFluff issue, users deserve to know about it,\n635                     # because it means their file(s) weren't fixed.\n636                     for violation in initial_linting_errors:\n637                         if isinstance(violation, SQLLintError):\n638                             violation.fixes = []\n639 \n640                     # Return the original parse tree, before any fixes were applied.\n641                     # Reason: When the linter hits the loop limit, the file is often\n642                     # messy, e.g. some of the fixes were applied repeatedly, possibly\n643                     # other weird things. We don't want the user to see this junk!\n644                     return save_tree, initial_linting_errors, ignore_buff\n645 \n646         if config.get(\"ignore_templated_areas\", default=True):\n647             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n648 \n649         return tree, initial_linting_errors, ignore_buff\n650 \n651     @classmethod\n652     def lint_parsed(\n653         cls,\n654         parsed: ParsedString,\n655         rule_set: List[BaseRule],\n656         fix: bool = False,\n657         formatter: Any = None,\n658         encoding: str = \"utf8\",\n659     ):\n660         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n661         violations = parsed.violations\n662         time_dict = parsed.time_dict\n663         tree: Optional[BaseSegment]\n664         if parsed.tree:\n665             t0 = time.monotonic()\n666             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n667             tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n668                 parsed.tree,\n669                 config=parsed.config,\n670                 rule_set=rule_set,\n671                 fix=fix,\n672                 fname=parsed.fname,\n673                 templated_file=parsed.templated_file,\n674                 formatter=formatter,\n675             )\n676             # Update the timing dict\n677             time_dict[\"linting\"] = time.monotonic() - t0\n678 \n679             # We're only going to return the *initial* errors, rather\n680             # than any generated during the fixing cycle.\n681             violations += initial_linting_errors\n682         else:\n683             # If no parsed tree, set to None\n684             tree = None\n685             ignore_buff = []\n686             if not parsed.config.get(\"disable_noqa\"):\n687                 # Templating and/or parsing have failed. Look for \"noqa\"\n688                 # comments (the normal path for identifying these comments\n689                 # requires access to the parse tree, and because of the failure,\n690                 # we don't have a parse tree).\n691                 ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n692                     parsed.source_str,\n693                     [\n694                         lm\n695                         for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n696                         if lm.name == \"inline_comment\"\n697                     ][0],\n698                     [r.code for r in rule_set],\n699                 )\n700                 violations += ignore_violations\n701 \n702         # We process the ignore config here if appropriate\n703         for violation in violations:\n704             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n705 \n706         linted_file = LintedFile(\n707             parsed.fname,\n708             violations,\n709             time_dict,\n710             tree,\n711             ignore_mask=ignore_buff,\n712             templated_file=parsed.templated_file,\n713             encoding=encoding,\n714         )\n715 \n716         # This is the main command line output from linting.\n717         if formatter:\n718             formatter.dispatch_file_violations(\n719                 parsed.fname, linted_file, only_fixable=fix\n720             )\n721 \n722         # Safety flag for unset dialects\n723         if linted_file.get_violations(\n724             fixable=True if fix else None, types=SQLParseError\n725         ):\n726             if formatter:  # pragma: no cover TODO?\n727                 formatter.dispatch_dialect_warning(parsed.config.get(\"dialect\"))\n728 \n729         return linted_file\n730 \n731     @classmethod\n732     def lint_rendered(\n733         cls,\n734         rendered: RenderedFile,\n735         rule_set: List[BaseRule],\n736         fix: bool = False,\n737         formatter: Any = None,\n738     ) -> LintedFile:\n739         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n740         parsed = cls.parse_rendered(rendered)\n741         return cls.lint_parsed(\n742             parsed,\n743             rule_set=rule_set,\n744             fix=fix,\n745             formatter=formatter,\n746             encoding=rendered.encoding,\n747         )\n748 \n749     # ### Instance Methods\n750     # These are tied to a specific instance and so are not necessarily\n751     # safe to use in parallel operations.\n752 \n753     def render_string(\n754         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n755     ) -> RenderedFile:\n756         \"\"\"Template the file.\"\"\"\n757         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n758 \n759         # Start the templating timer\n760         t0 = time.monotonic()\n761 \n762         # Newlines are normalised to unix-style line endings (\\n).\n763         # The motivation is that Jinja normalises newlines during templating and\n764         # we want consistent mapping between the raw and templated slices.\n765         in_str = self._normalise_newlines(in_str)\n766 \n767         # Since Linter.__init__() does not require a dialect to be specified,\n768         # check for one now. (We're processing a string, not a file, so we're\n769         # not going to pick up a .sqlfluff or other config file to provide a\n770         # missing dialect at this point.)\n771         config.verify_dialect_specified()\n772         if not config.get(\"templater_obj\") == self.templater:\n773             linter_logger.warning(\n774                 (\n775                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n776                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n777                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n778                     \"working directory. It can be set in a .sqlfluff in the current \"\n779                     \"working directory. See Nesting section of the docs for more \"\n780                     \"details.\"\n781                 )\n782             )\n783         try:\n784             templated_file, templater_violations = self.templater.process(\n785                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n786             )\n787         except SQLTemplaterSkipFile as s:  # pragma: no cover\n788             linter_logger.warning(str(s))\n789             templated_file = None\n790             templater_violations = []\n791 \n792         if not templated_file:\n793             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n794 \n795         # Record time\n796         time_dict = {\"templating\": time.monotonic() - t0}\n797 \n798         return RenderedFile(\n799             templated_file,\n800             templater_violations,\n801             config,\n802             time_dict,\n803             fname,\n804             encoding,\n805             in_str,\n806         )\n807 \n808     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n809         \"\"\"Load and render a file with relevant config.\"\"\"\n810         # Load the raw file.\n811         raw_file, config, encoding = self._load_raw_file_and_config(fname, root_config)\n812         # Render the file\n813         return self.render_string(raw_file, fname, config, encoding)\n814 \n815     def parse_string(\n816         self,\n817         in_str: str,\n818         fname: str = \"<string>\",\n819         recurse: bool = True,\n820         config: Optional[FluffConfig] = None,\n821         encoding: str = \"utf-8\",\n822     ) -> ParsedString:\n823         \"\"\"Parse a string.\"\"\"\n824         violations: List[SQLBaseError] = []\n825 \n826         # Dispatch the output for the template header (including the config diff)\n827         if self.formatter:\n828             self.formatter.dispatch_template_header(fname, self.config, config)\n829 \n830         # Just use the local config from here:\n831         config = config or self.config\n832 \n833         # Scan the raw file for config commands.\n834         config.process_raw_file_for_config(in_str)\n835         rendered = self.render_string(in_str, fname, config, encoding)\n836         violations += rendered.templater_violations\n837 \n838         # Dispatch the output for the parse header\n839         if self.formatter:\n840             self.formatter.dispatch_parse_header(fname)\n841 \n842         return self.parse_rendered(rendered, recurse=recurse)\n843 \n844     def fix(\n845         self,\n846         tree: BaseSegment,\n847         config: Optional[FluffConfig] = None,\n848         fname: Optional[str] = None,\n849         templated_file: Optional[TemplatedFile] = None,\n850     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n851         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n852         config = config or self.config\n853         rule_set = self.get_ruleset(config=config)\n854         fixed_tree, violations, _ = self.lint_fix_parsed(\n855             tree,\n856             config,\n857             rule_set,\n858             fix=True,\n859             fname=fname,\n860             templated_file=templated_file,\n861             formatter=self.formatter,\n862         )\n863         return fixed_tree, violations\n864 \n865     def lint(\n866         self,\n867         tree: BaseSegment,\n868         config: Optional[FluffConfig] = None,\n869         fname: Optional[str] = None,\n870         templated_file: Optional[TemplatedFile] = None,\n871     ) -> List[SQLBaseError]:\n872         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n873         config = config or self.config\n874         rule_set = self.get_ruleset(config=config)\n875         _, violations, _ = self.lint_fix_parsed(\n876             tree,\n877             config,\n878             rule_set,\n879             fix=False,\n880             fname=fname,\n881             templated_file=templated_file,\n882             formatter=self.formatter,\n883         )\n884         return violations\n885 \n886     def lint_string(\n887         self,\n888         in_str: str = \"\",\n889         fname: str = \"<string input>\",\n890         fix: bool = False,\n891         config: Optional[FluffConfig] = None,\n892         encoding: str = \"utf8\",\n893     ) -> LintedFile:\n894         \"\"\"Lint a string.\n895 \n896         Returns:\n897             :obj:`LintedFile`: an object representing that linted file.\n898 \n899         \"\"\"\n900         # Sort out config, defaulting to the built in config if no override\n901         config = config or self.config\n902         # Parse the string.\n903         parsed = self.parse_string(\n904             in_str=in_str,\n905             fname=fname,\n906             config=config,\n907         )\n908         # Get rules as appropriate\n909         rule_set = self.get_ruleset(config=config)\n910         # Lint the file and return the LintedFile\n911         return self.lint_parsed(\n912             parsed,\n913             rule_set,\n914             fix=fix,\n915             formatter=self.formatter,\n916             encoding=encoding,\n917         )\n918 \n919     def paths_from_path(\n920         self,\n921         path: str,\n922         ignore_file_name: str = \".sqlfluffignore\",\n923         ignore_non_existent_files: bool = False,\n924         ignore_files: bool = True,\n925         working_path: str = os.getcwd(),\n926     ) -> List[str]:\n927         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n928 \n929         Here we also deal with the .sqlfluffignore file if present.\n930 \n931         When a path to a file to be linted is explicitly passed\n932         we look for ignore files in all directories that are parents of the file,\n933         up to the current directory.\n934 \n935         If the current directory is not a parent of the file we only\n936         look for an ignore file in the direct parent of the file.\n937 \n938         \"\"\"\n939         if not os.path.exists(path):\n940             if ignore_non_existent_files:\n941                 return []\n942             else:\n943                 raise OSError(\"Specified path does not exist\")\n944 \n945         # Files referred to exactly are also ignored if\n946         # matched, but we warn the users when that happens\n947         is_exact_file = os.path.isfile(path)\n948 \n949         path_walk: WalkableType\n950         if is_exact_file:\n951             # When the exact file to lint is passed, we\n952             # fill path_walk with an input that follows\n953             # the structure of `os.walk`:\n954             #   (root, directories, files)\n955             dirpath = os.path.dirname(path)\n956             files = [os.path.basename(path)]\n957             path_walk = [(dirpath, None, files)]\n958         else:\n959             path_walk = list(os.walk(path))\n960 \n961         ignore_file_paths = ConfigLoader.find_ignore_config_files(\n962             path=path, working_path=working_path, ignore_file_name=ignore_file_name\n963         )\n964         # Add paths that could contain \"ignore files\"\n965         # to the path_walk list\n966         path_walk_ignore_file = [\n967             (\n968                 os.path.dirname(ignore_file_path),\n969                 None,\n970                 # Only one possible file, since we only\n971                 # have one \"ignore file name\"\n972                 [os.path.basename(ignore_file_path)],\n973             )\n974             for ignore_file_path in ignore_file_paths\n975         ]\n976         path_walk += path_walk_ignore_file\n977 \n978         # If it's a directory then expand the path!\n979         buffer = []\n980         ignores = {}\n981         for dirpath, _, filenames in path_walk:\n982             for fname in filenames:\n983                 fpath = os.path.join(dirpath, fname)\n984                 # Handle potential .sqlfluffignore files\n985                 if ignore_files and fname == ignore_file_name:\n986                     with open(fpath) as fh:\n987                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n988                         ignores[dirpath] = spec\n989                     # We don't need to process the ignore file any futher\n990                     continue\n991 \n992                 # We won't purge files *here* because there's an edge case\n993                 # that the ignore file is processed after the sql file.\n994 \n995                 # Scan for remaining files\n996                 for ext in (\n997                     self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n998                 ):\n999                     # is it a sql file?\n1000                     if fname.lower().endswith(ext):\n1001                         buffer.append(fpath)\n1002 \n1003         if not ignore_files:\n1004             return sorted(buffer)\n1005 \n1006         # Check the buffer for ignore items and normalise the rest.\n1007         # It's a set, so we can do natural deduplication.\n1008         filtered_buffer = set()\n1009 \n1010         for fpath in buffer:\n1011             abs_fpath = os.path.abspath(fpath)\n1012             for ignore_base, ignore_spec in ignores.items():\n1013                 abs_ignore_base = os.path.abspath(ignore_base)\n1014                 if abs_fpath.startswith(\n1015                     abs_ignore_base + os.sep\n1016                 ) and ignore_spec.match_file(\n1017                     os.path.relpath(abs_fpath, abs_ignore_base)\n1018                 ):\n1019                     # This file is ignored, skip it.\n1020                     if is_exact_file:\n1021                         linter_logger.warning(\n1022                             \"Exact file path %s was given but \"\n1023                             \"it was ignored by a %s pattern in %s, \"\n1024                             \"re-run with `--disregard-sqlfluffignores` to \"\n1025                             \"skip %s\"\n1026                             % (\n1027                                 path,\n1028                                 ignore_file_name,\n1029                                 ignore_base,\n1030                                 ignore_file_name,\n1031                             )\n1032                         )\n1033                     break\n1034             else:\n1035                 npath = os.path.normpath(fpath)\n1036                 # For debugging, log if we already have the file.\n1037                 if npath in filtered_buffer:\n1038                     linter_logger.debug(  # pragma: no cover\n1039                         \"Developer Warning: Path crawler attempted to \"\n1040                         \"requeue the same file twice. %s is already in \"\n1041                         \"filtered buffer.\",\n1042                         npath,\n1043                     )\n1044                 filtered_buffer.add(npath)\n1045 \n1046         # Return a sorted list\n1047         return sorted(filtered_buffer)\n1048 \n1049     def lint_string_wrapped(\n1050         self,\n1051         string: str,\n1052         fname: str = \"<string input>\",\n1053         fix: bool = False,\n1054     ) -> LintingResult:\n1055         \"\"\"Lint strings directly.\"\"\"\n1056         result = LintingResult()\n1057         linted_path = LintedDir(fname)\n1058         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n1059         result.add(linted_path)\n1060         result.stop_timer()\n1061         return result\n1062 \n1063     def lint_path(\n1064         self,\n1065         path: str,\n1066         fix: bool = False,\n1067         ignore_non_existent_files: bool = False,\n1068         ignore_files: bool = True,\n1069         processes: int = 1,\n1070     ) -> LintedDir:\n1071         \"\"\"Lint a path.\"\"\"\n1072         linted_path = LintedDir(path)\n1073         if self.formatter:\n1074             self.formatter.dispatch_path(path)\n1075         fnames = list(\n1076             self.paths_from_path(\n1077                 path,\n1078                 ignore_non_existent_files=ignore_non_existent_files,\n1079                 ignore_files=ignore_files,\n1080             )\n1081         )\n1082 \n1083         # to avoid circular import\n1084         from sqlfluff.core.linter.runner import get_runner\n1085 \n1086         runner = get_runner(\n1087             self,\n1088             self.config,\n1089             processes=processes,\n1090             allow_process_parallelism=self.allow_process_parallelism,\n1091         )\n1092 \n1093         # Show files progress bar only when there is more than one.\n1094         files_count = len(fnames)\n1095         progress_bar_files = tqdm(\n1096             total=files_count,\n1097             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n1098             leave=False,\n1099             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1100         )\n1101 \n1102         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n1103             linted_path.add(linted_file)\n1104             # If any fatal errors, then stop iteration.\n1105             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n1106                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n1107                 break\n1108 \n1109             # Progress bar for files is rendered only when there is more than one file.\n1110             # Additionally as it's updated after each loop, we need to get file name\n1111             # from the next loop. This is why `enumerate` starts with `1` and there\n1112             # is `i < len` to not exceed files list length.\n1113             progress_bar_files.update(n=1)\n1114             if i < len(fnames):\n1115                 progress_bar_files.set_description(\n1116                     f\"file {os.path.basename(fnames[i])}\"\n1117                 )\n1118 \n1119         return linted_path\n1120 \n1121     def lint_paths(\n1122         self,\n1123         paths: Tuple[str, ...],\n1124         fix: bool = False,\n1125         ignore_non_existent_files: bool = False,\n1126         ignore_files: bool = True,\n1127         processes: int = 1,\n1128     ) -> LintingResult:\n1129         \"\"\"Lint an iterable of paths.\"\"\"\n1130         paths_count = len(paths)\n1131 \n1132         # If no paths specified - assume local\n1133         if not paths_count:  # pragma: no cover\n1134             paths = (os.getcwd(),)\n1135         # Set up the result to hold what we get back\n1136         result = LintingResult()\n1137 \n1138         progress_bar_paths = tqdm(\n1139             total=paths_count,\n1140             desc=\"path\",\n1141             leave=False,\n1142             disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1143         )\n1144         for path in paths:\n1145             progress_bar_paths.set_description(f\"path {path}\")\n1146 \n1147             # Iterate through files recursively in the specified directory (if it's a\n1148             # directory) or read the file directly if it's not\n1149             result.add(\n1150                 self.lint_path(\n1151                     path,\n1152                     fix=fix,\n1153                     ignore_non_existent_files=ignore_non_existent_files,\n1154                     ignore_files=ignore_files,\n1155                     processes=processes,\n1156                 )\n1157             )\n1158 \n1159             progress_bar_paths.update(1)\n1160 \n1161         result.stop_timer()\n1162         return result\n1163 \n1164     def parse_path(\n1165         self,\n1166         path: str,\n1167         recurse: bool = True,\n1168     ) -> Iterator[ParsedString]:\n1169         \"\"\"Parse a path of sql files.\n1170 \n1171         NB: This a generator which will yield the result of each file\n1172         within the path iteratively.\n1173         \"\"\"\n1174         for fname in self.paths_from_path(path):\n1175             if self.formatter:\n1176                 self.formatter.dispatch_path(path)\n1177             # Load the file with the config and yield the result.\n1178             raw_file, config, encoding = self._load_raw_file_and_config(\n1179                 fname, self.config\n1180             )\n1181             yield self.parse_string(\n1182                 raw_file,\n1183                 fname=fname,\n1184                 recurse=recurse,\n1185                 config=config,\n1186                 encoding=encoding,\n1187             )\n1188 \n[end of src/sqlfluff/core/linter/linter.py]\n[start of src/sqlfluff/core/linter/runner.py]\n1 \"\"\"Implements runner classes used internally by the Linter class.\n2 \n3 Implements various runner types for SQLFluff:\n4 - Serial\n5 - Parallel\n6   - Multiprocess\n7   - Multithread (used only by automated tests)\n8 \"\"\"\n9 from abc import ABC\n10 import bdb\n11 import functools\n12 import logging\n13 import multiprocessing.dummy\n14 import signal\n15 import sys\n16 import traceback\n17 from typing import Callable, List, Tuple, Iterator\n18 \n19 from sqlfluff.core import FluffConfig, Linter\n20 from sqlfluff.core.linter import LintedFile\n21 \n22 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n23 \n24 \n25 class BaseRunner(ABC):\n26     \"\"\"Base runner class.\"\"\"\n27 \n28     def __init__(\n29         self,\n30         linter,\n31         config,\n32     ):\n33         self.linter = linter\n34         self.config = config\n35 \n36     pass_formatter = True\n37 \n38     def iter_rendered(self, fnames: List[str]) -> Iterator[Tuple]:\n39         \"\"\"Iterate through rendered files ready for linting.\"\"\"\n40         for fname in self.linter.templater.sequence_files(\n41             fnames, config=self.config, formatter=self.linter.formatter\n42         ):\n43             yield fname, self.linter.render_file(fname, self.config)\n44 \n45     def iter_partials(\n46         self,\n47         fnames: List[str],\n48         fix: bool = False,\n49     ) -> Iterator[Tuple[str, Callable]]:\n50         \"\"\"Iterate through partials for linted files.\n51 \n52         Generates filenames and objects which return LintedFiles.\n53         \"\"\"\n54         for fname, rendered in self.iter_rendered(fnames):\n55             # Generate a fresh ruleset\n56             rule_set = self.linter.get_ruleset(config=rendered.config)\n57             yield (\n58                 fname,\n59                 functools.partial(\n60                     self.linter.lint_rendered,\n61                     rendered,\n62                     rule_set,\n63                     fix,\n64                     # Formatters may or may not be passed. They don't pickle\n65                     # nicely so aren't appropriate in a multiprocessing world.\n66                     self.linter.formatter if self.pass_formatter else None,\n67                 ),\n68             )\n69 \n70     def run(self, fnames: List[str], fix: bool):\n71         \"\"\"Run linting on the specified list of files.\"\"\"\n72         raise NotImplementedError  # pragma: no cover\n73 \n74     @classmethod\n75     def _init_global(cls, config):\n76         \"\"\"Initializes any global state.\n77 \n78         May be overridden by subclasses to apply global configuration, initialize\n79         logger state in child processes, etc.\n80         \"\"\"\n81         pass\n82 \n83     @staticmethod\n84     def _handle_lint_path_exception(fname, e):\n85         if isinstance(e, IOError):\n86             # IOErrors are caught in commands.py, so propagate it\n87             raise (e)  # pragma: no cover\n88         linter_logger.warning(\n89             f\"\"\"Unable to lint {fname} due to an internal error. \\\n90 Please report this as an issue with your query's contents and stacktrace below!\n91 To hide this warning, add the failing file to .sqlfluffignore\n92 {traceback.format_exc()}\"\"\",\n93         )\n94 \n95 \n96 class SequentialRunner(BaseRunner):\n97     \"\"\"Simple runner that does sequential processing.\"\"\"\n98 \n99     def run(self, fnames: List[str], fix: bool) -> Iterator[LintedFile]:\n100         \"\"\"Sequential implementation.\"\"\"\n101         for fname, partial in self.iter_partials(fnames, fix=fix):\n102             try:\n103                 yield partial()\n104             except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n105                 raise\n106             except Exception as e:\n107                 self._handle_lint_path_exception(fname, e)\n108 \n109 \n110 class ParallelRunner(BaseRunner):\n111     \"\"\"Base class for parallel runner implementations (process or thread).\"\"\"\n112 \n113     POOL_TYPE: Callable\n114     MAP_FUNCTION_NAME: str\n115     # Don't pass the formatter in a parallel world, they\n116     # don't pickle well.\n117     pass_formatter = False\n118 \n119     def __init__(self, linter, config, processes):\n120         super().__init__(linter, config)\n121         self.processes = processes\n122 \n123     def run(self, fnames: List[str], fix: bool):\n124         \"\"\"Parallel implementation.\n125 \n126         Note that the partials are generated one at a time then\n127         passed directly into the pool as they're ready. This means\n128         the main thread can do the IO work while passing the parsing\n129         and linting work out to the threads.\n130         \"\"\"\n131         with self._create_pool(\n132             self.processes,\n133             self._init_global,\n134             (self.config,),\n135         ) as pool:\n136             try:\n137                 for lint_result in self._map(\n138                     pool,\n139                     self._apply,\n140                     self.iter_partials(fnames, fix=fix),\n141                 ):\n142                     if isinstance(lint_result, DelayedException):\n143                         try:\n144                             lint_result.reraise()\n145                         except Exception as e:\n146                             self._handle_lint_path_exception(lint_result.fname, e)\n147                     else:\n148                         # It's a LintedDir.\n149                         if self.linter.formatter:\n150                             self.linter.formatter.dispatch_file_violations(\n151                                 lint_result.path, lint_result, only_fixable=fix\n152                             )\n153                         yield lint_result\n154             except KeyboardInterrupt:  # pragma: no cover\n155                 # On keyboard interrupt (Ctrl-C), terminate the workers.\n156                 # Notify the user we've received the signal and are cleaning up,\n157                 # in case it takes awhile.\n158                 print(\"Received keyboard interrupt. Cleaning up and shutting down...\")\n159                 pool.terminate()\n160 \n161     @staticmethod\n162     def _apply(partial_tuple):\n163         \"\"\"Shim function used in parallel mode.\"\"\"\n164         # Unpack the tuple and ditch the filename in this case.\n165         fname, partial = partial_tuple\n166         try:\n167             return partial()\n168         # Capture any exceptions and return as delayed exception to handle\n169         # in the main thread.\n170         except Exception as e:\n171             return DelayedException(e, fname=fname)\n172 \n173     @classmethod\n174     def _create_pool(cls, *args, **kwargs):\n175         return cls.POOL_TYPE(*args, **kwargs)\n176 \n177     @classmethod\n178     def _map(cls, pool, *args, **kwargs):\n179         \"\"\"Runs a class-appropriate version of the general map() function.\"\"\"\n180         return getattr(pool, cls.MAP_FUNCTION_NAME)(*args, **kwargs)\n181 \n182 \n183 class MultiProcessRunner(ParallelRunner):\n184     \"\"\"Runner that does parallel processing using multiple processes.\"\"\"\n185 \n186     POOL_TYPE = multiprocessing.Pool\n187     MAP_FUNCTION_NAME = \"imap_unordered\"\n188 \n189     @classmethod\n190     def _init_global(cls, config):  # pragma: no cover\n191         super()._init_global(config)\n192 \n193         # Disable signal handling in the child processes to let the parent\n194         # control all KeyboardInterrupt handling (Control C). This is\n195         # necessary in order for keyboard interrupts to exit quickly and\n196         # cleanly. Adapted from this post:\n197         # https://stackoverflow.com/questions/11312525/catch-ctrlc-sigint-and-exit-multiprocesses-gracefully-in-python\n198         signal.signal(signal.SIGINT, signal.SIG_IGN)\n199 \n200 \n201 class MultiThreadRunner(ParallelRunner):\n202     \"\"\"Runner that does parallel processing using multiple threads.\n203 \n204     Used only by automated tests.\n205     \"\"\"\n206 \n207     POOL_TYPE = multiprocessing.dummy.Pool\n208     MAP_FUNCTION_NAME = \"imap\"\n209 \n210 \n211 class DelayedException(Exception):\n212     \"\"\"Multiprocessing process pool uses this to propagate exceptions.\"\"\"\n213 \n214     def __init__(self, ee, fname=None):\n215         self.ee = ee\n216         __, __, self.tb = sys.exc_info()\n217         self.fname = None\n218         super().__init__(str(ee))\n219 \n220     def reraise(self):\n221         \"\"\"Reraise the encapsulated exception.\"\"\"\n222         raise self.ee.with_traceback(self.tb)\n223 \n224 \n225 def get_runner(\n226     linter: Linter,\n227     config: FluffConfig,\n228     processes: int,\n229     allow_process_parallelism: bool = True,\n230 ) -> BaseRunner:\n231     \"\"\"Generate a runner instance based on parallel and system configuration.\"\"\"\n232     if processes > 1:\n233         # Process parallelism isn't really supported during testing\n234         # so this flag allows us to fall back to a threaded runner\n235         # in those cases.\n236         if allow_process_parallelism:\n237             return MultiProcessRunner(linter, config, processes=processes)\n238         else:\n239             return MultiThreadRunner(linter, config, processes=processes)\n240     else:\n241         return SequentialRunner(linter, config)\n242 \n[end of src/sqlfluff/core/linter/runner.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -1,6 +1,7 @@\n import os\n import sys\n import json\n+import multiprocessing\n import logging\n import time\n from logging import LogRecord\n@@ -488,6 +489,7 @@\n     help=\"The number of parallel processes to run.\",\n )\n @click.option(\n+    \"--processes\",\n     type=int,\n     default=1,\n     help=\"The number of parallel processes to run.\",\n@@ -566,7 +568\n             processes=processes,\n         )\n \n+    # Set the number of processes in the config\n     config.set(\"processes\", processes)\n \n     # Output the results as we go\n@@ -1120,6 +1121,7 @@\n     )\n     @click.option(\n         \"-p\",\n         \"--processes\",\n         type=int,\n         default=1,\n         help=\"The number of parallel processes to run.\",\n     )\n     def lint(\n         paths: Tuple[str],\n         processes: int,\n         ...\n     ) -> None:\n         \"\"\"Lint SQL files via passing a list of files or using stdin.\n \n         PATH is the path to a sql file or directory to lint. This can be either a\n         file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n         character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n         be interpreted like passing the current working directory as a path argument.\n \n         Linting SQL files:\n \n             sqlfluff lint path/to/file.sql\n             sqlfluff lint directory/of/sql/files\n \n         Linting a file via stdin (note the lone '-' character):\n \n             cat path/to/file.sql | sqlfluff lint -\n             echo 'select col from tbl' | sqlfluff lint -\n         \"\"\"\n         config = get_config(\n             extra_config_path,\n             ignore_local_config,\n             require_dialect=False,\n             processes=processes,\n             **kwargs\n         )\n         ...\n \n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1129,6 +1129,7 @@\n     def lint_paths(\n         self,\n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n-        processes: int = 1,\n+        processes: Optional[int] = None,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         ...\n         # If processes is set, use it for parallel linting\n         if processes:\n             runner = get_runner(\n                 self,\n                 self.config,\n                 processes=processes,\n                 allow_process_parallelism=self.allow_process_parallelism,\n             )\n         else:\n             runner = SequentialRunner(self, self.config)\n         ...\n```\n", "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -1,6 +1,7 @@\n import os\n import sys\n import json\n+import multiprocessing\n import logging\n import time\n from logging import LogRecord\n@@ -488,6 +489,7 @@\n     help=\"The number of parallel processes to run.\",\n )\n @click.option(\n+    \"--processes\",\n     type=int,\n     default=1,\n     help=\"The number of parallel processes to run.\",\n@@ -566,7 +568\n             processes=processes,\n         )\n \n+    # Set the number of processes in the config\n     config.set(\"processes\", processes)\n \n     # Output the results as we go\n@@ -1120,6 +1121,7 @@\n     )\n     @click.option(\n         \"-p\",\n         \"--processes\",\n         type=int,\n         default=1,\n         help=\"The number of parallel processes to run.\",\n     )\n     def lint(\n         paths: Tuple[str],\n         processes: int,\n         ...\n     ) -> None:\n         \"\"\"Lint SQL files via passing a list of files or using stdin.\n \n         PATH is the path to a sql file or directory to lint. This can be either a\n         file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n         character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n         be interpreted like passing the current working directory as a path argument.\n \n         Linting SQL files:\n \n             sqlfluff lint path/to/file.sql\n             sqlfluff lint directory/of/sql/files\n \n         Linting a file via stdin (note the lone '-' character):\n \n             cat path/to/file.sql | sqlfluff lint -\n             echo 'select col from tbl' | sqlfluff lint -\n         \"\"\"\n         config = get_config(\n             extra_config_path,\n             ignore_local_config,\n             require_dialect=False,\n             processes=processes,\n             **kwargs\n         )\n         ...\n \n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1129,6 +1129,7 @@\n     def lint_paths(\n         self,\n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n-        processes: int = 1,\n+        processes: Optional[int] = None,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         ...\n         # If processes is set, use it for parallel linting\n         if processes:\n             runner = get_runner(\n                 self,\n                 self.config,\n                 processes=processes,\n                 allow_process_parallelism=self.allow_process_parallelism,\n             )\n         else:\n             runner = SequentialRunner(self, self.config)\n         ...\n"}
{"instance_id": "sqlfluff__sqlfluff-2509", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nnoqa is ignored for jinja templated lines\n## Expected Behaviour\r\nLine with `noqa: TMP` should be ignored (despite of evaluation error)\r\n\r\n## Observed Behaviour\r\ntrying to lint airflow sql-template for AWS Athena query\r\nsetting up inline `-- noqa` or `--noqa: TMP` for jinja templated line not silenting templating error (typecasting error due to unable to pass datetime object while linting into template context):\r\n```\r\n== [transform/airflow/dags/queries/sfmc/player_balance.sql] FAIL\r\nL:   0 | P:   0 |  TMP | Unrecoverable failure in Jinja templating: unsupported operand type(s) for -: 'int' and 'datetime.timedelta'. Have you configured your variables?\r\n                       | https://docs.sqlfluff.com/en/latest/configuration.html\r\n```\r\n\r\n## Steps to Reproduce\r\ntemplated file:\r\n```sql\r\nselect *, row_number() over (partition by player_id order by balance_change_date desc)  as rnk\r\nfrom raw\r\nwhere\r\n    balance_change_date >= cast(from_iso8601_timestamp('{{ execution_date - macros.timedelta(hours=2, minutes=10) }}') as timestamp)  and  --noqa: TMP\r\n    balance_change_date < cast(from_iso8601_timestamp('{{ next_execution_date - macros.timedelta(minutes=10) }}') as timestamp) --noqa: TMP\r\n```\r\nrun:\r\n```bash\r\nsqlfluff lint transform/airflow/dags/queries/sfmc/player_balance.sql\r\n```\r\n\r\n## Dialect\r\npostgres (used for AWS Athena)\r\n\r\n## Version\r\ndatalake % sqlfluff --version\r\nsqlfluff, version 0.8.1\r\ndatalake % python3 --version\r\nPython 3.9.8\r\n\r\n## Configuration\r\n```ini\r\n# tox.ini\r\n[sqlfluff]\r\ntemplater = jinja\r\noutput_line_length = 180\r\nexclude_rules = L011,L012,L022,L031,L034\r\ndialect = postgres\r\n\r\n[sqlfluff:rules]\r\nmax_line_length = 120\r\n\r\n[sqlfluff:templater:jinja]\r\nlibrary_path = operation/deploy/lint\r\napply_dbt_builtins = false\r\n\r\n[sqlfluff:templater:jinja:context]\r\nds = 2021-11-11\r\nds_nodash = 20211111\r\nstart_date = 2021-11-11\r\nend_date = 2021-11-11\r\ninterval = 1\r\n# passed as int due to inabliity to pass datetime obkject \r\ndata_interval_start = 1636588800\r\ndata_interval_end = 1636588800\r\n```\r\n\r\n```python\r\n# operation/deploy/lint/macro.py\r\nfrom datetime import datetime, timedelta  # noqa: F401\r\n\r\nimport dateutil  # noqa: F401\r\n```\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 \n16 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n17 \n18 ## Dialects Supported\n19 \n20 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n21 \n22 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n23 - [BigQuery](https://cloud.google.com/bigquery/)\n24 - [Exasol](https://www.exasol.com/)\n25 - [Hive](https://hive.apache.org/)\n26 - [MySQL](https://www.mysql.com/)\n27 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n30 - [Snowflake](https://www.snowflake.com/)\n31 - [Spark3](https://spark.apache.org/docs/latest/)\n32 - [SQLite](https://www.sqlite.org/)\n33 - [Teradata](https://www.teradata.com/)\n34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n35 \n36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n37 \n38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n39 \n40 ## Templates Supported\n41 \n42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n43 \n44 **SQLFluff** supports the following templates:\n45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n46 - [dbt](https://www.getdbt.com/)\n47 \n48 Again, please raise issues if you wish to support more templating languages/syntaxes.\n49 \n50 # Getting Started\n51 \n52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n53 \n54 ```shell\n55 $ pip install sqlfluff\n56 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n57 $ sqlfluff lint test.sql\n58 == [test.sql] FAIL\n59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4\n60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line\n61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace\n62 ```\n63 \n64 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n65 \n66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n67 \n68 # Documentation\n69 \n70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n71 \n72 # Releases\n73 \n74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).\n75 \n76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n77 \n78 # SQLFluff on Slack\n79 \n80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n81 \n82 # SQLFluff on Twitter\n83 \n84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n85 \n86 # Contributing\n87 \n88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n89 \n90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n91 \n92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n93 \n[end of README.md]\n[start of src/sqlfluff/core/linter/common.py]\n1 \"\"\"Defines small container classes to hold intermediate results during linting.\"\"\"\n2 \n3 from typing import (\n4     List,\n5     NamedTuple,\n6     Optional,\n7     Tuple,\n8     Dict,\n9 )\n10 \n11 from sqlfluff.core.errors import SQLBaseError, SQLTemplaterError\n12 from sqlfluff.core.templaters import TemplatedFile\n13 from sqlfluff.core.config import FluffConfig\n14 from sqlfluff.core.parser.segments.base import BaseSegment\n15 \n16 \n17 class RuleTuple(NamedTuple):\n18     \"\"\"Rule Tuple object for describing rules.\"\"\"\n19 \n20     code: str\n21     description: str\n22 \n23 \n24 class NoQaDirective(NamedTuple):\n25     \"\"\"Parsed version of a 'noqa' comment.\"\"\"\n26 \n27     line_no: int  # Source line number\n28     rules: Optional[Tuple[str, ...]]  # Affected rule names\n29     action: Optional[str]  # \"enable\", \"disable\", or \"None\"\n30 \n31 \n32 class RenderedFile(NamedTuple):\n33     \"\"\"An object to store the result of a templated file/string.\n34 \n35     This is notable as it's the intermediate state between what happens\n36     in the main process and the child processes when running in parallel mode.\n37     \"\"\"\n38 \n39     templated_file: TemplatedFile\n40     templater_violations: List[SQLTemplaterError]\n41     config: FluffConfig\n42     time_dict: Dict[str, float]\n43     fname: str\n44     encoding: str\n45 \n46 \n47 class ParsedString(NamedTuple):\n48     \"\"\"An object to store the result of parsing a string.\n49 \n50     Args:\n51         `parsed` is a segment structure representing the parsed file. If\n52             parsing fails due to an unrecoverable violation then we will\n53             return None.\n54         `violations` is a :obj:`list` of violations so far, which will either be\n55             templating, lexing or parsing violations at this stage.\n56         `time_dict` is a :obj:`dict` containing timings for how long each step\n57             took in the process.\n58         `templated_file` is a :obj:`TemplatedFile` containing the details\n59             of the templated file.\n60     \"\"\"\n61 \n62     tree: Optional[BaseSegment]\n63     violations: List[SQLBaseError]\n64     time_dict: dict\n65     templated_file: TemplatedFile\n66     config: FluffConfig\n67     fname: str\n68 \n69 \n70 class EnrichedFixPatch(NamedTuple):\n71     \"\"\"An edit patch for a source file.\"\"\"\n72 \n73     source_slice: slice\n74     templated_slice: slice\n75     fixed_raw: str\n76     # The patch category, functions mostly for debugging and explanation\n77     # than for function. It allows traceability of *why* this patch was\n78     # generated.\n79     patch_category: str\n80     templated_str: str\n81     source_str: str\n82 \n83     def dedupe_tuple(self):\n84         \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n85         return (self.source_slice, self.fixed_raw)\n86 \n[end of src/sqlfluff/core/linter/common.py]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import (\n8     Any,\n9     List,\n10     Sequence,\n11     Optional,\n12     Tuple,\n13     cast,\n14     Iterable,\n15     Iterator,\n16 )\n17 \n18 import pathspec\n19 import regex\n20 from tqdm import tqdm\n21 \n22 from sqlfluff.core.errors import (\n23     SQLBaseError,\n24     SQLLexError,\n25     SQLLintError,\n26     SQLParseError,\n27     SQLTemplaterSkipFile,\n28 )\n29 from sqlfluff.core.parser import Lexer, Parser\n30 from sqlfluff.core.file_helpers import get_encoding\n31 from sqlfluff.core.templaters import TemplatedFile\n32 from sqlfluff.core.rules import get_ruleset\n33 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n34 \n35 # Classes needed only for type checking\n36 from sqlfluff.core.parser.segments.base import BaseSegment\n37 from sqlfluff.core.parser.segments.meta import MetaSegment\n38 from sqlfluff.core.parser.segments.raw import RawSegment\n39 from sqlfluff.core.rules.base import BaseRule\n40 \n41 from sqlfluff.core.linter.common import (\n42     RuleTuple,\n43     ParsedString,\n44     NoQaDirective,\n45     RenderedFile,\n46 )\n47 from sqlfluff.core.linter.linted_file import LintedFile\n48 from sqlfluff.core.linter.linted_dir import LintedDir\n49 from sqlfluff.core.linter.linting_result import LintingResult\n50 \n51 \n52 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n53 \n54 # Instantiate the linter logger\n55 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n56 \n57 \n58 class Linter:\n59     \"\"\"The interface class to interact with the linter.\"\"\"\n60 \n61     # Default to allowing process parallelism\n62     allow_process_parallelism = True\n63 \n64     def __init__(\n65         self,\n66         config: Optional[FluffConfig] = None,\n67         formatter: Any = None,\n68         dialect: Optional[str] = None,\n69         rules: Optional[List[str]] = None,\n70         user_rules: Optional[List[BaseRule]] = None,\n71         exclude_rules: Optional[List[str]] = None,\n72     ) -> None:\n73         # Store the config object\n74         self.config = FluffConfig.from_kwargs(\n75             config=config,\n76             dialect=dialect,\n77             rules=rules,\n78             exclude_rules=exclude_rules,\n79         )\n80         # Get the dialect and templater\n81         self.dialect = self.config.get(\"dialect_obj\")\n82         self.templater = self.config.get(\"templater_obj\")\n83         # Store the formatter for output\n84         self.formatter = formatter\n85         # Store references to user rule classes\n86         self.user_rules = user_rules or []\n87 \n88     def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n89         \"\"\"Get hold of a set of rules.\"\"\"\n90         rs = get_ruleset()\n91         # Register any user rules\n92         for rule in self.user_rules:\n93             rs.register(rule)\n94         cfg = config or self.config\n95         return rs.get_rulelist(config=cfg)\n96 \n97     def rule_tuples(self) -> List[RuleTuple]:\n98         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n99         rs = self.get_ruleset()\n100         return [RuleTuple(rule.code, rule.description) for rule in rs]\n101 \n102     # #### Static methods\n103     # These are the building blocks of the linting process.\n104 \n105     @staticmethod\n106     def _load_raw_file_and_config(\n107         fname: str, root_config: FluffConfig\n108     ) -> Tuple[str, FluffConfig, str]:\n109         \"\"\"Load a raw file and the associated config.\"\"\"\n110         file_config = root_config.make_child_from_path(fname)\n111         encoding = get_encoding(fname=fname, config=file_config)\n112         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n113             raw_file = target_file.read()\n114         # Scan the raw file for config commands.\n115         file_config.process_raw_file_for_config(raw_file)\n116         # Return the raw file and config\n117         return raw_file, file_config, encoding\n118 \n119     @staticmethod\n120     def _normalise_newlines(string: str) -> str:\n121         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n122         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n123 \n124     @staticmethod\n125     def _lex_templated_file(\n126         templated_file: TemplatedFile, config: FluffConfig\n127     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n128         \"\"\"Lex a templated file.\n129 \n130         NOTE: This potentially mutates the config, so make sure to\n131         use the returned one.\n132         \"\"\"\n133         violations = []\n134         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n135         # Get the lexer\n136         lexer = Lexer(config=config)\n137         # Lex the file and log any problems\n138         try:\n139             tokens, lex_vs = lexer.lex(templated_file)\n140             # We might just get the violations as a list\n141             violations += lex_vs\n142             linter_logger.info(\n143                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n144             )\n145         except SQLLexError as err:\n146             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n147             violations.append(err)\n148             return None, violations, config\n149 \n150         if not tokens:  # pragma: no cover TODO?\n151             return None, violations, config\n152 \n153         # Check that we've got sensible indentation from the lexer.\n154         # We might need to suppress if it's a complicated file.\n155         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n156         if isinstance(templating_blocks_indent, str):\n157             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n158         else:\n159             force_block_indent = False\n160         templating_blocks_indent = bool(templating_blocks_indent)\n161         # If we're forcing it through we don't check.\n162         if templating_blocks_indent and not force_block_indent:\n163             indent_balance = sum(\n164                 getattr(elem, \"indent_val\", 0)\n165                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n166             )\n167             if indent_balance != 0:\n168                 linter_logger.debug(\n169                     \"Indent balance test failed for %r. Template indents will not be \"\n170                     \"linted for this file.\",\n171                     templated_file.fname,\n172                 )\n173                 # Don't enable the templating blocks.\n174                 templating_blocks_indent = False\n175 \n176         # The file will have been lexed without config, so check all indents\n177         # are enabled.\n178         new_tokens = []\n179         for token in cast(Tuple[BaseSegment, ...], tokens):\n180             if token.is_meta:\n181                 token = cast(MetaSegment, token)\n182                 if token.indent_val != 0:\n183                     # Don't allow it if we're not linting templating block indents.\n184                     if not templating_blocks_indent:\n185                         continue\n186             new_tokens.append(token)\n187 \n188         # Return new buffer\n189         return new_tokens, violations, config\n190 \n191     @staticmethod\n192     def _parse_tokens(\n193         tokens: Sequence[BaseSegment],\n194         config: FluffConfig,\n195         recurse: bool = True,\n196         fname: Optional[str] = None,\n197     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n198         parser = Parser(config=config)\n199         violations = []\n200         # Parse the file and log any problems\n201         try:\n202             parsed: Optional[BaseSegment] = parser.parse(\n203                 tokens,\n204                 recurse=recurse,\n205                 fname=fname,\n206             )\n207         except SQLParseError as err:\n208             linter_logger.info(\"PARSING FAILED! : %s\", err)\n209             violations.append(err)\n210             return None, violations\n211 \n212         if parsed:\n213             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n214             linter_logger.info(\"\\n\" + parsed.stringify())\n215             # We may succeed parsing, but still have unparsable segments. Extract them\n216             # here.\n217             for unparsable in parsed.iter_unparsables():\n218                 # No exception has been raised explicitly, but we still create one here\n219                 # so that we can use the common interface\n220                 violations.append(\n221                     SQLParseError(\n222                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n223                         \"{1!r}\".format(\n224                             unparsable.pos_marker.working_loc,\n225                             unparsable.raw\n226                             if len(unparsable.raw) < 40\n227                             else unparsable.raw[:40] + \"...\",\n228                         ),\n229                         segment=unparsable,\n230                     )\n231                 )\n232                 linter_logger.info(\"Found unparsable segment...\")\n233                 linter_logger.info(unparsable.stringify())\n234         return parsed, violations\n235 \n236     @staticmethod\n237     def parse_noqa(\n238         comment: str,\n239         line_no: int,\n240         rule_codes: List[str],\n241     ):\n242         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n243         # Also trim any whitespace afterward\n244 \n245         # Comment lines can also have noqa e.g.\n246         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: L016\n247         # Therefore extract last possible inline ignore.\n248         comment = [c.strip() for c in comment.split(\"--\")][-1]\n249 \n250         if comment.startswith(\"noqa\"):\n251             # This is an ignore identifier\n252             comment_remainder = comment[4:]\n253             if comment_remainder:\n254                 if not comment_remainder.startswith(\":\"):\n255                     return SQLParseError(\n256                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n257                         line_no=line_no,\n258                     )\n259                 comment_remainder = comment_remainder[1:].strip()\n260                 if comment_remainder:\n261                     action: Optional[str]\n262                     if \"=\" in comment_remainder:\n263                         action, rule_part = comment_remainder.split(\"=\", 1)\n264                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n265                             return SQLParseError(\n266                                 \"Malformed 'noqa' section. \"\n267                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n268                                 \"or 'noqa: disable=<rule>[,...] | all\",\n269                                 line_no=line_no,\n270                             )\n271                     else:\n272                         action = None\n273                         rule_part = comment_remainder\n274                         if rule_part in {\"disable\", \"enable\"}:\n275                             return SQLParseError(\n276                                 \"Malformed 'noqa' section. \"\n277                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n278                                 \"or 'noqa: disable=<rule>[,...] | all\",\n279                                 line_no=line_no,\n280                             )\n281                     rules: Optional[Tuple[str, ...]]\n282                     if rule_part != \"all\":\n283                         # Rules can be globs therefore we compare to the rule_set to\n284                         # expand the globs.\n285                         unexpanded_rules = tuple(\n286                             r.strip() for r in rule_part.split(\",\")\n287                         )\n288                         expanded_rules = []\n289                         for r in unexpanded_rules:\n290                             expanded_rule = [\n291                                 x\n292                                 for x in fnmatch.filter(rule_codes, r)\n293                                 if x not in expanded_rules\n294                             ]\n295                             if expanded_rule:\n296                                 expanded_rules.extend(expanded_rule)\n297                             elif r not in expanded_rules:\n298                                 # We were unable to expand the glob.\n299                                 # Therefore assume the user is referencing\n300                                 # a special error type (e.g. PRS, LXR, or TMP)\n301                                 # and add this to the list of rules to ignore.\n302                                 expanded_rules.append(r)\n303                         rules = tuple(expanded_rules)\n304                     else:\n305                         rules = None\n306                     return NoQaDirective(line_no, rules, action)\n307             return NoQaDirective(line_no, None, None)\n308         return None\n309 \n310     @staticmethod\n311     def remove_templated_errors(\n312         linting_errors: List[SQLBaseError],\n313     ) -> List[SQLBaseError]:\n314         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n315         # Filter out any linting errors in templated sections if relevant.\n316         result: List[SQLBaseError] = []\n317         for e in linting_errors:\n318             if isinstance(e, SQLLintError):\n319                 if (\n320                     # Is it in a literal section?\n321                     e.segment.pos_marker.is_literal()\n322                     # Is it a rule that is designed to work on templated sections?\n323                     or e.rule.targets_templated\n324                 ):\n325                     result.append(e)\n326             else:\n327                 # If it's another type, just keep it. (E.g. SQLParseError from\n328                 # malformed \"noqa\" comment).\n329                 result.append(e)\n330         return result\n331 \n332     @staticmethod\n333     def _warn_unfixable(code: str):\n334         linter_logger.warning(\n335             f\"One fix for {code} not applied, it would re-cause the same error.\"\n336         )\n337 \n338     # ### Class Methods\n339     # These compose the base static methods into useful recipes.\n340 \n341     @classmethod\n342     def parse_rendered(\n343         cls,\n344         rendered: RenderedFile,\n345         recurse: bool = True,\n346     ):\n347         \"\"\"Parse a rendered file.\"\"\"\n348         t0 = time.monotonic()\n349         violations = cast(List[SQLBaseError], rendered.templater_violations)\n350         tokens: Optional[Sequence[BaseSegment]]\n351         if rendered.templated_file:\n352             tokens, lvs, config = cls._lex_templated_file(\n353                 rendered.templated_file, rendered.config\n354             )\n355             violations += lvs\n356         else:\n357             tokens = None\n358 \n359         t1 = time.monotonic()\n360         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n361 \n362         if tokens:\n363             parsed, pvs = cls._parse_tokens(\n364                 tokens,\n365                 rendered.config,\n366                 recurse=recurse,\n367                 fname=rendered.fname,\n368             )\n369             violations += pvs\n370         else:\n371             parsed = None\n372 \n373         time_dict = {\n374             **rendered.time_dict,\n375             \"lexing\": t1 - t0,\n376             \"parsing\": time.monotonic() - t1,\n377         }\n378         return ParsedString(\n379             parsed,\n380             violations,\n381             time_dict,\n382             rendered.templated_file,\n383             rendered.config,\n384             rendered.fname,\n385         )\n386 \n387     @classmethod\n388     def extract_ignore_from_comment(\n389         cls,\n390         comment: RawSegment,\n391         rule_codes: List[str],\n392     ):\n393         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n394         # Also trim any whitespace afterward\n395         comment_content = comment.raw_trimmed().strip()\n396         comment_line, _ = comment.pos_marker.source_position()\n397         result = cls.parse_noqa(comment_content, comment_line, rule_codes)\n398         if isinstance(result, SQLParseError):\n399             result.segment = comment\n400         return result\n401 \n402     @classmethod\n403     def extract_ignore_mask(\n404         cls,\n405         tree: BaseSegment,\n406         rule_codes: List[str],\n407     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n408         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n409         ignore_buff: List[NoQaDirective] = []\n410         violations: List[SQLBaseError] = []\n411         for comment in tree.recursive_crawl(\"comment\"):\n412             if comment.name == \"inline_comment\":\n413                 ignore_entry = cls.extract_ignore_from_comment(comment, rule_codes)\n414                 if isinstance(ignore_entry, SQLParseError):\n415                     violations.append(ignore_entry)\n416                 elif ignore_entry:\n417                     ignore_buff.append(ignore_entry)\n418         if ignore_buff:\n419             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n420         return ignore_buff, violations\n421 \n422     @classmethod\n423     def lint_fix_parsed(\n424         cls,\n425         tree: BaseSegment,\n426         config: FluffConfig,\n427         rule_set: List[BaseRule],\n428         fix: bool = False,\n429         fname: Optional[str] = None,\n430         templated_file: Optional[TemplatedFile] = None,\n431         formatter: Any = None,\n432     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n433         \"\"\"Lint and optionally fix a tree object.\"\"\"\n434         # Keep track of the linting errors\n435         all_linting_errors = []\n436         # A placeholder for the fixes we had on the previous loop\n437         last_fixes = None\n438         # Keep a set of previous versions to catch infinite loops.\n439         previous_versions = {tree.raw}\n440 \n441         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n442         # once for linting.\n443         loop_limit = config.get(\"runaway_limit\") if fix else 1\n444 \n445         # Dispatch the output for the lint header\n446         if formatter:\n447             formatter.dispatch_lint_header(fname)\n448 \n449         # Look for comment segments which might indicate lines to ignore.\n450         if not config.get(\"disable_noqa\"):\n451             rule_codes = [r.code for r in rule_set]\n452             ignore_buff, ivs = cls.extract_ignore_mask(tree, rule_codes)\n453             all_linting_errors += ivs\n454         else:\n455             ignore_buff = []\n456 \n457         for loop in range(loop_limit):\n458             changed = False\n459 \n460             progress_bar_crawler = tqdm(\n461                 rule_set,\n462                 desc=\"lint by rules\",\n463                 leave=False,\n464                 disable=progress_bar_configuration.disable_progress_bar,\n465             )\n466 \n467             for crawler in progress_bar_crawler:\n468                 progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n469 \n470                 # fixes should be a dict {} with keys edit, delete, create\n471                 # delete is just a list of segments to delete\n472                 # edit and create are list of tuples. The first element is the\n473                 # \"anchor\", the segment to look for either to edit or to insert BEFORE.\n474                 # The second is the element to insert or create.\n475                 linting_errors, _, fixes, _ = crawler.crawl(\n476                     tree,\n477                     ignore_mask=ignore_buff,\n478                     dialect=config.get(\"dialect_obj\"),\n479                     fname=fname,\n480                     templated_file=templated_file,\n481                 )\n482                 all_linting_errors += linting_errors\n483 \n484                 if fix and fixes:\n485                     linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n486                     # Do some sanity checks on the fixes before applying.\n487                     if fixes == last_fixes:  # pragma: no cover\n488                         cls._warn_unfixable(crawler.code)\n489                     else:\n490                         last_fixes = fixes\n491                         new_tree, _ = tree.apply_fixes(fixes)\n492                         # Check for infinite loops\n493                         if new_tree.raw not in previous_versions:\n494                             # We've not seen this version of the file so far. Continue.\n495                             tree = new_tree\n496                             previous_versions.add(tree.raw)\n497                             changed = True\n498                             continue\n499                         else:\n500                             # Applying these fixes took us back to a state which we've\n501                             # seen before. Abort.\n502                             cls._warn_unfixable(crawler.code)\n503 \n504             if loop == 0:\n505                 # Keep track of initial errors for reporting.\n506                 initial_linting_errors = all_linting_errors.copy()\n507 \n508             if fix and not changed:\n509                 # We did not change the file. Either the file is clean (no fixes), or\n510                 # any fixes which are present will take us back to a previous state.\n511                 linter_logger.info(\n512                     f\"Fix loop complete. Stability achieved after {loop}/{loop_limit} \"\n513                     \"loops.\"\n514                 )\n515                 break\n516         if fix and loop + 1 == loop_limit:\n517             linter_logger.warning(f\"Loop limit on fixes reached [{loop_limit}].\")\n518 \n519         if config.get(\"ignore_templated_areas\", default=True):\n520             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n521 \n522         return tree, initial_linting_errors, ignore_buff\n523 \n524     @classmethod\n525     def lint_parsed(\n526         cls,\n527         parsed: ParsedString,\n528         rule_set: List[BaseRule],\n529         fix: bool = False,\n530         formatter: Any = None,\n531         encoding: str = \"utf8\",\n532     ):\n533         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n534         violations = parsed.violations\n535         time_dict = parsed.time_dict\n536         tree: Optional[BaseSegment]\n537         if parsed.tree:\n538             t0 = time.monotonic()\n539             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n540             tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n541                 parsed.tree,\n542                 config=parsed.config,\n543                 rule_set=rule_set,\n544                 fix=fix,\n545                 fname=parsed.fname,\n546                 templated_file=parsed.templated_file,\n547                 formatter=formatter,\n548             )\n549             # Update the timing dict\n550             time_dict[\"linting\"] = time.monotonic() - t0\n551 \n552             # We're only going to return the *initial* errors, rather\n553             # than any generated during the fixing cycle.\n554             violations += initial_linting_errors\n555         else:\n556             # If no parsed tree, set to None\n557             tree = None\n558             ignore_buff = []\n559 \n560         # We process the ignore config here if appropriate\n561         for violation in violations:\n562             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n563 \n564         linted_file = LintedFile(\n565             parsed.fname,\n566             violations,\n567             time_dict,\n568             tree,\n569             ignore_mask=ignore_buff,\n570             templated_file=parsed.templated_file,\n571             encoding=encoding,\n572         )\n573 \n574         # This is the main command line output from linting.\n575         if formatter:\n576             formatter.dispatch_file_violations(\n577                 parsed.fname, linted_file, only_fixable=fix\n578             )\n579 \n580         # Safety flag for unset dialects\n581         if parsed.config.get(\"dialect\") == \"ansi\" and linted_file.get_violations(\n582             fixable=True if fix else None, types=SQLParseError\n583         ):\n584             if formatter:  # pragma: no cover TODO?\n585                 formatter.dispatch_dialect_warning()\n586 \n587         return linted_file\n588 \n589     @classmethod\n590     def lint_rendered(\n591         cls,\n592         rendered: RenderedFile,\n593         rule_set: List[BaseRule],\n594         fix: bool = False,\n595         formatter: Any = None,\n596     ) -> LintedFile:\n597         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n598         parsed = cls.parse_rendered(rendered)\n599         return cls.lint_parsed(\n600             parsed,\n601             rule_set=rule_set,\n602             fix=fix,\n603             formatter=formatter,\n604             encoding=rendered.encoding,\n605         )\n606 \n607     # ### Instance Methods\n608     # These are tied to a specific instance and so are not necessarily\n609     # safe to use in parallel operations.\n610 \n611     def render_string(\n612         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n613     ) -> RenderedFile:\n614         \"\"\"Template the file.\"\"\"\n615         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n616 \n617         # Start the templating timer\n618         t0 = time.monotonic()\n619 \n620         # Newlines are normalised to unix-style line endings (\\n).\n621         # The motivation is that Jinja normalises newlines during templating and\n622         # we want consistent mapping between the raw and templated slices.\n623         in_str = self._normalise_newlines(in_str)\n624 \n625         if not config.get(\"templater_obj\") == self.templater:\n626             linter_logger.warning(\n627                 (\n628                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n629                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n630                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n631                     \"working directory. It can be set in a .sqlfluff in the current \"\n632                     \"working directory. See Nesting section of the docs for more \"\n633                     \"details.\"\n634                 )\n635             )\n636         try:\n637             templated_file, templater_violations = self.templater.process(\n638                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n639             )\n640         except SQLTemplaterSkipFile as s:  # pragma: no cover\n641             linter_logger.warning(str(s))\n642             templated_file = None\n643             templater_violations = []\n644 \n645         if not templated_file:\n646             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n647 \n648         # Record time\n649         time_dict = {\"templating\": time.monotonic() - t0}\n650 \n651         return RenderedFile(\n652             templated_file, templater_violations, config, time_dict, fname, encoding\n653         )\n654 \n655     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n656         \"\"\"Load and render a file with relevant config.\"\"\"\n657         # Load the raw file.\n658         raw_file, config, encoding = self._load_raw_file_and_config(fname, root_config)\n659         # Render the file\n660         return self.render_string(raw_file, fname, config, encoding)\n661 \n662     def parse_string(\n663         self,\n664         in_str: str,\n665         fname: str = \"<string>\",\n666         recurse: bool = True,\n667         config: Optional[FluffConfig] = None,\n668         encoding: str = \"utf-8\",\n669     ) -> ParsedString:\n670         \"\"\"Parse a string.\"\"\"\n671         violations: List[SQLBaseError] = []\n672 \n673         # Dispatch the output for the template header (including the config diff)\n674         if self.formatter:\n675             self.formatter.dispatch_template_header(fname, self.config, config)\n676 \n677         # Just use the local config from here:\n678         config = config or self.config\n679 \n680         # Scan the raw file for config commands.\n681         config.process_raw_file_for_config(in_str)\n682         rendered = self.render_string(in_str, fname, config, encoding)\n683         violations += rendered.templater_violations\n684 \n685         # Dispatch the output for the parse header\n686         if self.formatter:\n687             self.formatter.dispatch_parse_header(fname)\n688 \n689         return self.parse_rendered(rendered, recurse=recurse)\n690 \n691     def fix(\n692         self,\n693         tree: BaseSegment,\n694         config: Optional[FluffConfig] = None,\n695         fname: Optional[str] = None,\n696         templated_file: Optional[TemplatedFile] = None,\n697     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n698         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n699         config = config or self.config\n700         rule_set = self.get_ruleset(config=config)\n701         fixed_tree, violations, _ = self.lint_fix_parsed(\n702             tree,\n703             config,\n704             rule_set,\n705             fix=True,\n706             fname=fname,\n707             templated_file=templated_file,\n708             formatter=self.formatter,\n709         )\n710         return fixed_tree, violations\n711 \n712     def lint(\n713         self,\n714         tree: BaseSegment,\n715         config: Optional[FluffConfig] = None,\n716         fname: Optional[str] = None,\n717         templated_file: Optional[TemplatedFile] = None,\n718     ) -> List[SQLBaseError]:\n719         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n720         config = config or self.config\n721         rule_set = self.get_ruleset(config=config)\n722         _, violations, _ = self.lint_fix_parsed(\n723             tree,\n724             config,\n725             rule_set,\n726             fix=False,\n727             fname=fname,\n728             templated_file=templated_file,\n729             formatter=self.formatter,\n730         )\n731         return violations\n732 \n733     def lint_string(\n734         self,\n735         in_str: str = \"\",\n736         fname: str = \"<string input>\",\n737         fix: bool = False,\n738         config: Optional[FluffConfig] = None,\n739         encoding: str = \"utf8\",\n740     ) -> LintedFile:\n741         \"\"\"Lint a string.\n742 \n743         Returns:\n744             :obj:`LintedFile`: an object representing that linted file.\n745 \n746         \"\"\"\n747         # Sort out config, defaulting to the built in config if no override\n748         config = config or self.config\n749         # Parse the string.\n750         parsed = self.parse_string(\n751             in_str=in_str,\n752             fname=fname,\n753             config=config,\n754         )\n755         # Get rules as appropriate\n756         rule_set = self.get_ruleset(config=config)\n757         # Lint the file and return the LintedFile\n758         return self.lint_parsed(\n759             parsed,\n760             rule_set,\n761             fix=fix,\n762             formatter=self.formatter,\n763             encoding=encoding,\n764         )\n765 \n766     def paths_from_path(\n767         self,\n768         path: str,\n769         ignore_file_name: str = \".sqlfluffignore\",\n770         ignore_non_existent_files: bool = False,\n771         ignore_files: bool = True,\n772         working_path: str = os.getcwd(),\n773     ) -> List[str]:\n774         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n775 \n776         Here we also deal with the .sqlfluffignore file if present.\n777 \n778         When a path to a file to be linted is explicitly passed\n779         we look for ignore files in all directories that are parents of the file,\n780         up to the current directory.\n781 \n782         If the current directory is not a parent of the file we only\n783         look for an ignore file in the direct parent of the file.\n784 \n785         \"\"\"\n786         if not os.path.exists(path):\n787             if ignore_non_existent_files:\n788                 return []\n789             else:\n790                 raise OSError(\"Specified path does not exist\")\n791 \n792         # Files referred to exactly are also ignored if\n793         # matched, but we warn the users when that happens\n794         is_exact_file = os.path.isfile(path)\n795 \n796         if is_exact_file:\n797             # When the exact file to lint is passed, we\n798             # fill path_walk with an input that follows\n799             # the structure of `os.walk`:\n800             #   (root, directories, files)\n801             dirpath = os.path.dirname(path)\n802             files = [os.path.basename(path)]\n803             ignore_file_paths = ConfigLoader.find_ignore_config_files(\n804                 path=path, working_path=working_path, ignore_file_name=ignore_file_name\n805             )\n806             # Add paths that could contain \"ignore files\"\n807             # to the path_walk list\n808             path_walk_ignore_file = [\n809                 (\n810                     os.path.dirname(ignore_file_path),\n811                     None,\n812                     # Only one possible file, since we only\n813                     # have one \"ignore file name\"\n814                     [os.path.basename(ignore_file_path)],\n815                 )\n816                 for ignore_file_path in ignore_file_paths\n817             ]\n818             path_walk: WalkableType = [(dirpath, None, files)] + path_walk_ignore_file\n819         else:\n820             path_walk = os.walk(path)\n821 \n822         # If it's a directory then expand the path!\n823         buffer = []\n824         ignores = {}\n825         for dirpath, _, filenames in path_walk:\n826             for fname in filenames:\n827                 fpath = os.path.join(dirpath, fname)\n828                 # Handle potential .sqlfluffignore files\n829                 if ignore_files and fname == ignore_file_name:\n830                     with open(fpath) as fh:\n831                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n832                         ignores[dirpath] = spec\n833                     # We don't need to process the ignore file any futher\n834                     continue\n835 \n836                 # We won't purge files *here* because there's an edge case\n837                 # that the ignore file is processed after the sql file.\n838 \n839                 # Scan for remaining files\n840                 for ext in self.config.get(\"sql_file_exts\", default=\".sql\").split(\",\"):\n841                     # is it a sql file?\n842                     if fname.endswith(ext):\n843                         buffer.append(fpath)\n844 \n845         if not ignore_files:\n846             return sorted(buffer)\n847 \n848         # Check the buffer for ignore items and normalise the rest.\n849         filtered_buffer = []\n850 \n851         for fpath in buffer:\n852             abs_fpath = os.path.abspath(fpath)\n853             for ignore_base, ignore_spec in ignores.items():\n854                 abs_ignore_base = os.path.abspath(ignore_base)\n855                 if abs_fpath.startswith(\n856                     abs_ignore_base + os.sep\n857                 ) and ignore_spec.match_file(\n858                     os.path.relpath(abs_fpath, abs_ignore_base)\n859                 ):\n860                     # This file is ignored, skip it.\n861                     if is_exact_file:\n862                         linter_logger.warning(\n863                             \"Exact file path %s was given but \"\n864                             \"it was ignored by a %s pattern in %s, \"\n865                             \"re-run with `--disregard-sqlfluffignores` to \"\n866                             \"skip %s\"\n867                             % (\n868                                 path,\n869                                 ignore_file_name,\n870                                 ignore_base,\n871                                 ignore_file_name,\n872                             )\n873                         )\n874                     break\n875             else:\n876                 filtered_buffer.append(os.path.normpath(fpath))\n877 \n878         # Return\n879         return sorted(filtered_buffer)\n880 \n881     def lint_string_wrapped(\n882         self,\n883         string: str,\n884         fname: str = \"<string input>\",\n885         fix: bool = False,\n886     ) -> LintingResult:\n887         \"\"\"Lint strings directly.\"\"\"\n888         result = LintingResult()\n889         linted_path = LintedDir(fname)\n890         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n891         result.add(linted_path)\n892         result.stop_timer()\n893         return result\n894 \n895     def lint_path(\n896         self,\n897         path: str,\n898         fix: bool = False,\n899         ignore_non_existent_files: bool = False,\n900         ignore_files: bool = True,\n901         processes: int = 1,\n902     ) -> LintedDir:\n903         \"\"\"Lint a path.\"\"\"\n904         linted_path = LintedDir(path)\n905         if self.formatter:\n906             self.formatter.dispatch_path(path)\n907         fnames = list(\n908             self.paths_from_path(\n909                 path,\n910                 ignore_non_existent_files=ignore_non_existent_files,\n911                 ignore_files=ignore_files,\n912             )\n913         )\n914 \n915         # to avoid circular import\n916         from sqlfluff.core.linter.runner import get_runner\n917 \n918         runner = get_runner(\n919             self,\n920             self.config,\n921             processes=processes,\n922             allow_process_parallelism=self.allow_process_parallelism,\n923         )\n924 \n925         # Show files progress bar only when there is more than one.\n926         files_count = len(fnames)\n927         progress_bar_files = tqdm(\n928             total=files_count,\n929             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n930             leave=False,\n931             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n932         )\n933 \n934         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n935             linted_path.add(linted_file)\n936             # If any fatal errors, then stop iteration.\n937             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n938                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n939                 break\n940 \n941             # Progress bar for files is rendered only when there is more than one file.\n942             # Additionally as it's updated after each loop, we need to get file name\n943             # from the next loop. This is why `enumerate` starts with `1` and there\n944             # is `i < len` to not exceed files list length.\n945             progress_bar_files.update(n=1)\n946             if i < len(fnames):\n947                 progress_bar_files.set_description(\n948                     f\"file {os.path.basename(fnames[i])}\"\n949                 )\n950 \n951         return linted_path\n952 \n953     def lint_paths(\n954         self,\n955         paths: Tuple[str, ...],\n956         fix: bool = False,\n957         ignore_non_existent_files: bool = False,\n958         ignore_files: bool = True,\n959         processes: int = 1,\n960     ) -> LintingResult:\n961         \"\"\"Lint an iterable of paths.\"\"\"\n962         paths_count = len(paths)\n963 \n964         # If no paths specified - assume local\n965         if not paths_count:  # pragma: no cover\n966             paths = (os.getcwd(),)\n967         # Set up the result to hold what we get back\n968         result = LintingResult()\n969 \n970         progress_bar_paths = tqdm(\n971             total=paths_count,\n972             desc=\"path\",\n973             leave=False,\n974             disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n975         )\n976         for path in paths:\n977             progress_bar_paths.set_description(f\"path {path}\")\n978 \n979             # Iterate through files recursively in the specified directory (if it's a\n980             # directory) or read the file directly if it's not\n981             result.add(\n982                 self.lint_path(\n983                     path,\n984                     fix=fix,\n985                     ignore_non_existent_files=ignore_non_existent_files,\n986                     ignore_files=ignore_files,\n987                     processes=processes,\n988                 )\n989             )\n990 \n991             progress_bar_paths.update(1)\n992 \n993         result.stop_timer()\n994         return result\n995 \n996     def parse_path(\n997         self,\n998         path: str,\n999         recurse: bool = True,\n1000     ) -> Iterator[ParsedString]:\n1001         \"\"\"Parse a path of sql files.\n1002 \n1003         NB: This a generator which will yield the result of each file\n1004         within the path iteratively.\n1005         \"\"\"\n1006         for fname in self.paths_from_path(path):\n1007             if self.formatter:\n1008                 self.formatter.dispatch_path(path)\n1009             # Load the file with the config and yield the result.\n1010             raw_file, config, encoding = self._load_raw_file_and_config(\n1011                 fname, self.config\n1012             )\n1013             yield self.parse_string(\n1014                 raw_file,\n1015                 fname=fname,\n1016                 recurse=recurse,\n1017                 config=config,\n1018                 encoding=encoding,\n1019             )\n1020 \n[end of src/sqlfluff/core/linter/linter.py]\n[start of src/sqlfluff/core/templaters/jinja.py]\n1 \"\"\"Defines the templaters.\"\"\"\n2 import logging\n3 import os.path\n4 import pkgutil\n5 from functools import reduce\n6 from typing import Callable, Dict, List, Optional, Tuple\n7 \n8 import jinja2.nodes\n9 from jinja2 import (\n10     Environment,\n11     FileSystemLoader,\n12     TemplateError,\n13     TemplateSyntaxError,\n14     meta,\n15 )\n16 from jinja2.environment import Template\n17 from jinja2.sandbox import SandboxedEnvironment\n18 \n19 from sqlfluff.core.config import FluffConfig\n20 from sqlfluff.core.errors import SQLTemplaterError\n21 from sqlfluff.core.templaters.base import (\n22     RawFileSlice,\n23     TemplatedFile,\n24     TemplatedFileSlice,\n25 )\n26 from sqlfluff.core.templaters.python import PythonTemplater\n27 from sqlfluff.core.templaters.slicers.tracer import JinjaTracer\n28 \n29 \n30 # Instantiate the templater logger\n31 templater_logger = logging.getLogger(\"sqlfluff.templater\")\n32 \n33 \n34 class JinjaTemplater(PythonTemplater):\n35     \"\"\"A templater using the jinja2 library.\n36 \n37     See: https://jinja.palletsprojects.com/\n38     \"\"\"\n39 \n40     name = \"jinja\"\n41 \n42     class Libraries:\n43         \"\"\"Mock namespace for user-defined Jinja library.\"\"\"\n44 \n45         pass\n46 \n47     @staticmethod\n48     def _extract_macros_from_template(template, env, ctx):\n49         \"\"\"Take a template string and extract any macros from it.\n50 \n51         Lovingly inspired by http://codyaray.com/2015/05/auto-load-jinja2-macros\n52         \"\"\"\n53         from jinja2.runtime import Macro  # noqa\n54 \n55         # Iterate through keys exported from the loaded template string\n56         context = {}\n57         macro_template = env.from_string(template, globals=ctx)\n58         # This is kind of low level and hacky but it works\n59         for k in macro_template.module.__dict__:\n60             attr = getattr(macro_template.module, k)\n61             # Is it a macro? If so install it at the name of the macro\n62             if isinstance(attr, Macro):\n63                 context[k] = attr\n64         # Return the context\n65         return context\n66 \n67     @classmethod\n68     def _extract_macros_from_path(cls, path: List[str], env: Environment, ctx: Dict):\n69         \"\"\"Take a path and extract macros from it.\"\"\"\n70         for path_entry in path:\n71             # Does it exist? It should as this check was done on config load.\n72             if not os.path.exists(path_entry):\n73                 raise ValueError(f\"Path does not exist: {path_entry}\")\n74 \n75             macro_ctx = {}\n76             if os.path.isfile(path_entry):\n77                 # It's a file. Extract macros from it.\n78                 with open(path_entry) as opened_file:\n79                     template = opened_file.read()\n80                 # Update the context with macros from the file.\n81                 macro_ctx.update(\n82                     cls._extract_macros_from_template(template, env=env, ctx=ctx)\n83                 )\n84             else:\n85                 # It's a directory. Iterate through files in it and extract from them.\n86                 for dirpath, _, files in os.walk(path_entry):\n87                     for fname in files:\n88                         if fname.endswith(\".sql\"):\n89                             macro_ctx.update(\n90                                 cls._extract_macros_from_path(\n91                                     [os.path.join(dirpath, fname)], env=env, ctx=ctx\n92                                 )\n93                             )\n94         return macro_ctx\n95 \n96     def _extract_macros_from_config(self, config, env, ctx):\n97         \"\"\"Take a config and load any macros from it.\"\"\"\n98         if config:\n99             # This is now a nested section\n100             loaded_context = (\n101                 config.get_section((self.templater_selector, self.name, \"macros\")) or {}\n102             )\n103         else:  # pragma: no cover TODO?\n104             loaded_context = {}\n105 \n106         # Iterate to load macros\n107         macro_ctx = {}\n108         for value in loaded_context.values():\n109             macro_ctx.update(\n110                 self._extract_macros_from_template(value, env=env, ctx=ctx)\n111             )\n112         return macro_ctx\n113 \n114     def _extract_libraries_from_config(self, config):\n115         library_path = config.get_section(\n116             (self.templater_selector, self.name, \"library_path\")\n117         )\n118         if not library_path:\n119             return {}\n120 \n121         libraries = JinjaTemplater.Libraries()\n122 \n123         # If library_path has __init__.py we parse it as one module, else we parse it\n124         # a set of modules\n125         is_library_module = os.path.exists(os.path.join(library_path, \"__init__.py\"))\n126         library_module_name = os.path.basename(library_path)\n127 \n128         # Need to go one level up to parse as a module correctly\n129         walk_path = (\n130             os.path.join(library_path, \"..\") if is_library_module else library_path\n131         )\n132 \n133         for loader, module_name, is_pkg in pkgutil.walk_packages([walk_path]):\n134             # skip other modules that can be near module_dir\n135             if is_library_module and not module_name.startswith(library_module_name):\n136                 continue\n137 \n138             module = loader.find_module(module_name).load_module(module_name)\n139 \n140             if \".\" in module_name:  # nested modules have `.` in module_name\n141                 *module_path, last_module_name = module_name.split(\".\")\n142                 # find parent module recursively\n143                 parent_module = reduce(\n144                     lambda res, path_part: getattr(res, path_part),\n145                     module_path,\n146                     libraries,\n147                 )\n148 \n149                 # set attribute on module object to make jinja working correctly\n150                 setattr(parent_module, last_module_name, module)\n151             else:\n152                 # set attr on `libraries` obj to make it work in jinja nicely\n153                 setattr(libraries, module_name, module)\n154 \n155         if is_library_module:\n156             # when library is module we have one more root module in hierarchy and we\n157             # remove it\n158             libraries = getattr(libraries, library_module_name)\n159 \n160         # remove magic methods from result\n161         return {k: v for k, v in libraries.__dict__.items() if not k.startswith(\"__\")}\n162 \n163     @staticmethod\n164     def _generate_dbt_builtins():\n165         \"\"\"Generate the dbt builtins which are injected in the context.\"\"\"\n166         # This feels a bit wrong defining these here, they should probably\n167         # be configurable somewhere sensible. But for now they're not.\n168         # TODO: Come up with a better solution.\n169 \n170         class ThisEmulator:\n171             \"\"\"A class which emulates the `this` class from dbt.\"\"\"\n172 \n173             name = \"this_model\"\n174             schema = \"this_schema\"\n175             database = \"this_database\"\n176 \n177             def __str__(self):  # pragma: no cover TODO?\n178                 return self.name\n179 \n180         dbt_builtins = {\n181             # `is_incremental()` renders as False, always in this case.\n182             # TODO: This means we'll never parse the other part of the query,\n183             # so we should find a solution to that. Perhaps forcing the file\n184             # to be parsed TWICE if it uses this variable.\n185             \"is_incremental\": lambda: False,\n186             \"this\": ThisEmulator(),\n187         }\n188         return dbt_builtins\n189 \n190     @classmethod\n191     def _crawl_tree(cls, tree, variable_names, raw):\n192         \"\"\"Crawl the tree looking for occurrences of the undeclared values.\"\"\"\n193         # First iterate through children\n194         for elem in tree.iter_child_nodes():\n195             yield from cls._crawl_tree(elem, variable_names, raw)\n196         # Then assess self\n197         if isinstance(tree, jinja2.nodes.Name) and tree.name in variable_names:\n198             line_no = tree.lineno\n199             line = raw.split(\"\\n\")[line_no - 1]\n200             pos = line.index(tree.name) + 1\n201             yield SQLTemplaterError(\n202                 f\"Undefined jinja template variable: {tree.name!r}\",\n203                 line_no=line_no,\n204                 line_pos=pos,\n205             )\n206 \n207     def _get_jinja_env(self, config=None):\n208         \"\"\"Get a properly configured jinja environment.\"\"\"\n209         # We explicitly want to preserve newlines.\n210         macros_path = self._get_macros_path(config)\n211         return SandboxedEnvironment(\n212             keep_trailing_newline=True,\n213             # The do extension allows the \"do\" directive\n214             autoescape=False,\n215             extensions=[\"jinja2.ext.do\"],\n216             loader=FileSystemLoader(macros_path) if macros_path else None,\n217         )\n218 \n219     def _get_macros_path(self, config: FluffConfig) -> Optional[List[str]]:\n220         if config:\n221             macros_path = config.get_section(\n222                 (self.templater_selector, self.name, \"load_macros_from_path\")\n223             )\n224             if macros_path:\n225                 result = [s.strip() for s in macros_path.split(\",\") if s.strip()]\n226                 if result:\n227                     return result\n228         return None\n229 \n230     def get_context(self, fname=None, config=None, **kw) -> Dict:\n231         \"\"\"Get the templating context from the config.\"\"\"\n232         # Load the context\n233         env = kw.pop(\"env\")\n234         live_context = super().get_context(fname=fname, config=config)\n235         # Apply dbt builtin functions if we're allowed.\n236         if config:\n237             apply_dbt_builtins = config.get_section(\n238                 (self.templater_selector, self.name, \"apply_dbt_builtins\")\n239             )\n240             if apply_dbt_builtins:\n241                 # This feels a bit wrong defining these here, they should probably\n242                 # be configurable somewhere sensible. But for now they're not.\n243                 # TODO: Come up with a better solution.\n244                 dbt_builtins = self._generate_dbt_builtins()\n245                 for name in dbt_builtins:\n246                     # Only apply if it hasn't already been set at this stage.\n247                     if name not in live_context:\n248                         live_context[name] = dbt_builtins[name]\n249 \n250         # Load macros from path (if applicable)\n251         if config:\n252             macros_path = self._get_macros_path(config)\n253             if macros_path:\n254                 live_context.update(\n255                     self._extract_macros_from_path(\n256                         macros_path, env=env, ctx=live_context\n257                     )\n258                 )\n259 \n260             # Load config macros, these will take precedence over macros from the path\n261             live_context.update(\n262                 self._extract_macros_from_config(\n263                     config=config, env=env, ctx=live_context\n264                 )\n265             )\n266 \n267             live_context.update(self._extract_libraries_from_config(config=config))\n268         return live_context\n269 \n270     def template_builder(\n271         self, fname=None, config=None\n272     ) -> Tuple[Environment, dict, Callable[[str], Template]]:\n273         \"\"\"Builds and returns objects needed to create and run templates.\"\"\"\n274         # Load the context\n275         env = self._get_jinja_env(config)\n276         live_context = self.get_context(fname=fname, config=config, env=env)\n277 \n278         def make_template(in_str):\n279             \"\"\"Used by JinjaTracer to instantiate templates.\n280 \n281             This function is a closure capturing internal state from process().\n282             Note that creating templates involves quite a bit of state known to\n283             _this_ function but not to JinjaTracer.\n284 \n285             https://www.programiz.com/python-programming/closure\n286             \"\"\"\n287             return env.from_string(in_str, globals=live_context)\n288 \n289         return env, live_context, make_template\n290 \n291     def process(\n292         self, *, in_str: str, fname: str, config=None, formatter=None\n293     ) -> Tuple[Optional[TemplatedFile], list]:\n294         \"\"\"Process a string and return the new string.\n295 \n296         Note that the arguments are enforced as keywords\n297         because Templaters can have differences in their\n298         `process` method signature.\n299         A Templater that only supports reading from a file\n300         would need the following signature:\n301             process(*, fname, in_str=None, config=None)\n302         (arguments are swapped)\n303 \n304         Args:\n305             in_str (:obj:`str`): The input string.\n306             fname (:obj:`str`, optional): The filename of this string. This is\n307                 mostly for loading config files at runtime.\n308             config (:obj:`FluffConfig`): A specific config to use for this\n309                 templating operation. Only necessary for some templaters.\n310             formatter (:obj:`CallbackFormatter`): Optional object for output.\n311 \n312         \"\"\"\n313         if not config:  # pragma: no cover\n314             raise ValueError(\n315                 \"For the jinja templater, the `process()` method requires a config \"\n316                 \"object.\"\n317             )\n318 \n319         env, live_context, make_template = self.template_builder(\n320             fname=fname, config=config\n321         )\n322 \n323         # Load the template, passing the global context.\n324         try:\n325             template = make_template(in_str)\n326         except TemplateSyntaxError as err:\n327             # Something in the template didn't parse, return the original\n328             # and a violation around what happened.\n329             return (\n330                 TemplatedFile(source_str=in_str, fname=fname),\n331                 [\n332                     SQLTemplaterError(\n333                         f\"Failure to parse jinja template: {err}.\",\n334                         line_no=err.lineno,\n335                     )\n336                 ],\n337             )\n338 \n339         violations = []\n340 \n341         # Attempt to identify any undeclared variables. The majority\n342         # will be found during the _crawl_tree step rather than this\n343         # first Exception which serves only to catch catastrophic errors.\n344         try:\n345             syntax_tree = env.parse(in_str)\n346             undefined_variables = meta.find_undeclared_variables(syntax_tree)\n347         except Exception as err:  # pragma: no cover\n348             # TODO: Add a url here so people can get more help.\n349             raise SQLTemplaterError(f\"Failure in identifying Jinja variables: {err}.\")\n350 \n351         # Get rid of any that *are* actually defined.\n352         for val in live_context:\n353             if val in undefined_variables:\n354                 undefined_variables.remove(val)\n355 \n356         if undefined_variables:\n357             # Lets go through and find out where they are:\n358             for val in self._crawl_tree(syntax_tree, undefined_variables, in_str):\n359                 violations.append(val)\n360 \n361         try:\n362             # NB: Passing no context. Everything is loaded when the template is loaded.\n363             out_str = template.render()\n364             # Slice the file once rendered.\n365             raw_sliced, sliced_file, out_str = self.slice_file(\n366                 in_str,\n367                 out_str,\n368                 config=config,\n369                 make_template=make_template,\n370             )\n371             return (\n372                 TemplatedFile(\n373                     source_str=in_str,\n374                     templated_str=out_str,\n375                     fname=fname,\n376                     sliced_file=sliced_file,\n377                     raw_sliced=raw_sliced,\n378                 ),\n379                 violations,\n380             )\n381         except (TemplateError, TypeError) as err:\n382             templater_logger.info(\"Unrecoverable Jinja Error: %s\", err)\n383             violations.append(\n384                 SQLTemplaterError(\n385                     (\n386                         \"Unrecoverable failure in Jinja templating: {}. Have you \"\n387                         \"configured your variables? \"\n388                         \"https://docs.sqlfluff.com/en/latest/configuration.html\"\n389                     ).format(err)\n390                 )\n391             )\n392             return None, violations\n393 \n394     def slice_file(\n395         self, raw_str: str, templated_str: str, config=None, **kwargs\n396     ) -> Tuple[List[RawFileSlice], List[TemplatedFileSlice], str]:\n397         \"\"\"Slice the file to determine regions where we can fix.\"\"\"\n398         # The JinjaTracer slicing algorithm is more robust, but it requires\n399         # us to create and render a second template (not raw_str) and is only\n400         # enabled if the caller passes a make_template() function.\n401         make_template = kwargs.pop(\"make_template\", None)\n402         if make_template is None:\n403             # make_template() was not provided. Use the base class\n404             # implementation instead.\n405             return super().slice_file(raw_str, templated_str, config, **kwargs)\n406 \n407         templater_logger.info(\"Slicing File Template\")\n408         templater_logger.debug(\"    Raw String: %r\", raw_str)\n409         templater_logger.debug(\"    Templated String: %r\", templated_str)\n410         tracer = JinjaTracer(raw_str, self._get_jinja_env(), make_template)\n411         trace = tracer.trace()\n412         return trace.raw_sliced, trace.sliced_file, trace.templated_str\n413 \n[end of src/sqlfluff/core/templaters/jinja.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -382,6 +382,9 @@\n             # Attempt to identify any undeclared variables. The majority\n             # will be found during the _crawl_tree step rather than this\n             # first Exception which serves only to catch catastrophic errors.\n+            # If the line has a noqa directive, we should skip the error\n+            if any(directive.rules and \"TMP\" in directive.rules for directive in ignore_buff):\n+                return None, []\n             try:\n                 syntax_tree = env.parse(in_str)\n                 undefined_variables = meta.find_undeclared_variables(syntax_tree)\n@@ -392,6 +395,9 @@\n             # Get rid of any that *are* actually defined.\n             for val in live_context:\n                 if val in undefined_variables:\n+                    # If the line has a noqa directive, we should skip the error\n+                    if any(directive.rules and \"TMP\" in directive.rules for directive in ignore_buff):\n+                        continue\n                     undefined_variables.remove(val)\n \n             if undefined_variables:\n``` \n\nThis patch modifies the `lint_rendered` method in the `linter.py` file to check for `noqa: TMP` directives. If such a directive is present, it will skip the error related to undefined Jinja variables, allowing the linter to ignore the error as expected.", "model_patch": "--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -382,6 +382,9 @@\n             # Attempt to identify any undeclared variables. The majority\n             # will be found during the _crawl_tree step rather than this\n             # first Exception which serves only to catch catastrophic errors.\n+            # If the line has a noqa directive, we should skip the error\n+            if any(directive.rules and \"TMP\" in directive.rules for directive in ignore_buff):\n+                return None, []\n             try:\n                 syntax_tree = env.parse(in_str)\n                 undefined_variables = meta.find_undeclared_variables(syntax_tree)\n@@ -392,6 +395,9 @@\n             # Get rid of any that *are* actually defined.\n             for val in live_context:\n                 if val in undefined_variables:\n+                    # If the line has a noqa directive, we should skip the error\n+                    if any(directive.rules and \"TMP\" in directive.rules for directive in ignore_buff):\n+                        continue\n                     undefined_variables.remove(val)\n \n             if undefined_variables:\n"}
{"instance_id": "sqlfluff__sqlfluff-4777", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n`fix` per file linted instead of at the end\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### Description\n\nI am just testing sqlfluff on a small example project.\r\nWe have configured it as part of `pre-commit`.\r\n\r\n```\r\n-   repo: https://github.com/sqlfluff/sqlfluff\r\n    rev: 1.0.0\r\n    hooks:\r\n    -   id: sqlfluff-fix\r\n        args: [--config, \".sqlfluff\", --disable_progress_bar, --processes, \"2\", --bench]\r\n        files: \\.(sql)$\r\n        exclude: sp_whoisactive.sql\r\n```\r\n\r\nProcessing our example already takes 30 minutes, I thus think formatting any real project would take 4+ hours.\r\n\r\nAt the moment the files are all formated first and _all together_ written at the very end. I see no benefit in writing at the very end, why are they not written sequentially?\n\n### Use case\n\nInstead of writing all formatted sql at the end, I would like to see files written sequentially.\n\n### Dialect\n\nmost likely all, i am working with t-sql.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![Coveralls](https://img.shields.io/coverallsCoverage/github/sqlfluff/sqlfluff?logo=coveralls&style=flat-square)](https://coveralls.io/github/sqlfluff/sqlfluff?branch=main)\n13 [![GitHub Workflow Status](https://img.shields.io/github/actions/workflow/status/sqlfluff/sqlfluff/.github/workflows/ci-tests.yml?logo=github&style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)\n14 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n15 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n16 [![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)\n17 \n18 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed\n19 with ELT applications in mind, **SQLFluff** also works with Jinja templating\n20 and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus\n21 your time on what matters.\n22 \n23 ## Dialects Supported\n24 \n25 Although SQL is reasonably consistent in its implementations, there are several\n26 different dialects available with variations of syntax and grammar. **SQLFluff**\n27 currently supports the following SQL dialects (though perhaps not in full):\n28 \n29 - ANSI SQL - this is the base version and on occasion may not strictly follow\n30   the ANSI/ISO SQL definition\n31 - [Athena](https://aws.amazon.com/athena/)\n32 - [BigQuery](https://cloud.google.com/bigquery/)\n33 - [ClickHouse](https://clickhouse.com/)\n34 - [Databricks](https://databricks.com/) (note: this extends the `sparksql` dialect with\n35   [Unity Catalog](https://docs.databricks.com/data-governance/unity-catalog/index.html) syntax).\n36 - [Db2](https://www.ibm.com/analytics/db2)\n37 - [DuckDB](https://duckdb.org/)\n38 - [Exasol](https://www.exasol.com/)\n39 - [Greenplum](https://greenplum.org/)\n40 - [Hive](https://hive.apache.org/)\n41 - [Materialize](https://materialize.com/)\n42 - [MySQL](https://www.mysql.com/)\n43 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n44 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n45 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n46 - [Snowflake](https://www.snowflake.com/)\n47 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n48 - [SparkSQL](https://spark.apache.org/docs/latest/)\n49 - [SQLite](https://www.sqlite.org/)\n50 - [Teradata](https://www.teradata.com/)\n51 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n52 \n53 We aim to make it easy to expand on the support of these dialects and also\n54 add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues)\n55 (or upvote any existing issues) to let us know of demand for missing support.\n56 \n57 Pull requests from those that know the missing syntax or dialects are especially\n58 welcomed and are the question way for you to get support added. We are happy\n59 to work with any potential contributors on this to help them add this support.\n60 Please raise an issue first for any large feature change to ensure it is a good\n61 fit for this project before spending time on this work.\n62 \n63 ## Templates Supported\n64 \n65 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity),\n66 so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor)\n67 as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n68 \n69 **SQLFluff** supports the following templates:\n70 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n71 - [dbt](https://www.getdbt.com/)\n72 \n73 Again, please raise issues if you wish to support more templating languages/syntaxes.\n74 \n75 ## VS Code Extension\n76 \n77 We also have a VS Code extension:\n78 \n79 - [Github Repository](https://github.com/sqlfluff/vscode-sqlfluff)\n80 - [Extension in VS Code marketplace](https://marketplace.visualstudio.com/items?itemName=dorzey.vscode-sqlfluff)\n81 \n82 # Getting Started\n83 \n84 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n85 \n86 ```shell\n87 $ pip install sqlfluff\n88 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n89 $ sqlfluff lint test.sql --dialect ansi\n90 == [test.sql] FAIL\n91 L:   1 | P:   1 | LT01 | Expected only single space before 'SELECT' keyword.\n92                        | Found '  '. [layout.spacing]\n93 L:   1 | P:   1 | LT02 | First line should not be indented.\n94                        | [layout.indent]\n95 L:   1 | P:   1 | LT13 | Files must not begin with newlines or whitespace.\n96                        | [layout.start_of_file]\n97 L:   1 | P:  11 | LT01 | Expected only single space before binary operator '+'.\n98                        | Found '  '. [layout.spacing]\n99 L:   1 | P:  14 | LT01 | Expected only single space before naked identifier.\n100                        | Found '  '. [layout.spacing]\n101 L:   1 | P:  27 | LT01 | Unnecessary trailing whitespace at end of file.\n102                        | [layout.spacing]\n103 L:   1 | P:  27 | LT12 | Files must end with a single trailing newline.\n104                        | [layout.end_of_file]\n105 All Finished \ud83d\udcdc \ud83c\udf89!\n106 ```\n107 \n108 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff)\n109 or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n110 \n111 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and\n112 [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see\n113 [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n114 \n115 # Documentation\n116 \n117 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/).\n118 This documentation is generated from this repository so please raise\n119 [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests\n120 for any additions, corrections, or clarifications.\n121 \n122 # Releases\n123 \n124 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html),\n125 so breaking changes should be restricted to major versions releases. Some\n126 elements (such as the python API) are in a less stable state and may see more\n127 significant changes more often. For details on breaking changes and how\n128 to migrate between versions, see our\n129 [release notes](https://docs.sqlfluff.com/en/latest/releasenotes.html). See the\n130 [changelog](CHANGELOG.md) for more details. If you would like to join in, please\n131 consider [contributing](CONTRIBUTING.md).\n132 \n133 New releases are made monthly. For more information, visit\n134 [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n135 \n136 # SQLFluff on Slack\n137 \n138 We have a fast-growing community\n139 [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g),\n140 come and join us!\n141 \n142 # SQLFluff on Twitter\n143 \n144 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements\n145 and other related posts.\n146 \n147 # Contributing\n148 \n149 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors).\n150 There is a lot to do in this project, and we are just getting started.\n151 \n152 If you want to understand more about the architecture of **SQLFluff**, you can\n153 find [more here](https://docs.sqlfluff.com/en/latest/internals.html#architecture).\n154 \n155 If you would like to contribute, check out the\n156 [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see\n157 the guide to [contributing](CONTRIBUTING.md).\n158 \n159 # Sponsors\n160 \n161 <img src=\"images/datacoves.png\" alt=\"Datacoves\" width=\"150\"/><br>\n162 The turnkey analytics stack, find out more at [Datacoves.com](https://datacoves.com/).\n163 \n[end of README.md]\n[start of src/sqlfluff/cli/commands.py]\n1 \"\"\"Contains the CLI.\"\"\"\n2 \n3 from itertools import chain\n4 import os\n5 import sys\n6 import json\n7 import logging\n8 import time\n9 from logging import LogRecord\n10 from typing import Callable, Tuple, Optional, cast\n11 \n12 import yaml\n13 \n14 import click\n15 \n16 # For the profiler\n17 import pstats\n18 from io import StringIO\n19 \n20 # To enable colour cross platform\n21 import colorama\n22 from tqdm import tqdm\n23 from sqlfluff.cli.autocomplete import shell_completion_enabled, dialect_shell_complete\n24 \n25 from sqlfluff.cli import EXIT_SUCCESS, EXIT_ERROR, EXIT_FAIL\n26 from sqlfluff.cli.click_deprecated_option import (\n27     DeprecatedOption,\n28     DeprecatedOptionsCommand,\n29 )\n30 from sqlfluff.cli.formatters import (\n31     format_linting_result_header,\n32     OutputStreamFormatter,\n33 )\n34 from sqlfluff.cli.helpers import get_package_version\n35 from sqlfluff.cli.outputstream import make_output_stream, OutputStream\n36 \n37 # Import from sqlfluff core.\n38 from sqlfluff.core import (\n39     Linter,\n40     FluffConfig,\n41     SQLLintError,\n42     SQLTemplaterError,\n43     SQLFluffUserError,\n44     dialect_selector,\n45     dialect_readout,\n46 )\n47 from sqlfluff.core.linter import LintingResult\n48 from sqlfluff.core.config import progress_bar_configuration\n49 \n50 from sqlfluff.core.enums import FormatType, Color\n51 from sqlfluff.core.plugin.host import get_plugin_manager\n52 \n53 \n54 class StreamHandlerTqdm(logging.StreamHandler):\n55     \"\"\"Modified StreamHandler which takes care of writing within `tqdm` context.\n56 \n57     It uses `tqdm` write which takes care of conflicting prints with progressbar.\n58     Without it, there were left artifacts in DEBUG mode (not sure about another ones,\n59     but probably would happen somewhere).\n60     \"\"\"\n61 \n62     def emit(self, record: LogRecord) -> None:\n63         \"\"\"Behaves like original one except uses `tqdm` to write.\"\"\"\n64         try:\n65             msg = self.format(record)\n66             tqdm.write(msg, file=self.stream)\n67             self.flush()\n68         except Exception:  # pragma: no cover\n69             self.handleError(record)\n70 \n71 \n72 def set_logging_level(\n73     verbosity: int,\n74     formatter: OutputStreamFormatter,\n75     logger: Optional[logging.Logger] = None,\n76     stderr_output: bool = False,\n77 ) -> None:\n78     \"\"\"Set up logging for the CLI.\n79 \n80     We either set up global logging based on the verbosity\n81     or, if `logger` is specified, we only limit to a single\n82     sqlfluff logger. Verbosity is applied in the same way.\n83 \n84     Implementation: If `logger` is not specified, the handler\n85     is attached to the `sqlfluff` logger. If it is specified\n86     then it attaches the the logger in question. In addition\n87     if `logger` is specified, then that logger will also\n88     not propagate.\n89     \"\"\"\n90     fluff_logger = logging.getLogger(\"sqlfluff\")\n91     # Don't propagate logging\n92     fluff_logger.propagate = False\n93 \n94     # Enable colorama\n95     colorama.init()\n96 \n97     # Set up the log handler which is able to print messages without overlapping\n98     # with progressbars.\n99     handler = StreamHandlerTqdm(stream=sys.stderr if stderr_output else sys.stdout)\n100     # NB: the unicode character at the beginning is to squash any badly\n101     # tamed ANSI colour statements, and return us to normality.\n102     handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n103 \n104     # Set up a handler to colour warnings red.\n105     # See: https://docs.python.org/3/library/logging.html#filter-objects\n106     def red_log_filter(record: logging.LogRecord) -> bool:\n107         if record.levelno >= logging.WARNING:\n108             record.msg = f\"{formatter.colorize(record.msg, Color.red)} \"\n109         return True\n110 \n111     handler.addFilter(red_log_filter)\n112 \n113     if logger:\n114         focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n115         focus_logger.addHandler(handler)\n116     else:\n117         fluff_logger.addHandler(handler)\n118 \n119     # NB: We treat the parser logger slightly differently because it's noisier.\n120     # It's important that we set levels for all each time so\n121     # that we don't break tests by changing the granularity\n122     # between tests.\n123     parser_logger = logging.getLogger(\"sqlfluff.parser\")\n124     if verbosity < 3:\n125         fluff_logger.setLevel(logging.WARNING)\n126         parser_logger.setLevel(logging.NOTSET)\n127     elif verbosity == 3:\n128         fluff_logger.setLevel(logging.INFO)\n129         parser_logger.setLevel(logging.WARNING)\n130     elif verbosity == 4:\n131         fluff_logger.setLevel(logging.DEBUG)\n132         parser_logger.setLevel(logging.INFO)\n133     elif verbosity > 4:\n134         fluff_logger.setLevel(logging.DEBUG)\n135         parser_logger.setLevel(logging.DEBUG)\n136 \n137 \n138 class PathAndUserErrorHandler:\n139     \"\"\"Make an API call but with error handling for the CLI.\"\"\"\n140 \n141     def __init__(self, formatter):\n142         self.formatter = formatter\n143 \n144     def __enter__(self):\n145         return self\n146 \n147     def __exit__(self, exc_type, exc_val, exc_tb):\n148         if exc_type is SQLFluffUserError:\n149             click.echo(\n150                 \"\\nUser Error: \"\n151                 + self.formatter.colorize(\n152                     str(exc_val),\n153                     Color.red,\n154                 )\n155             )\n156             sys.exit(EXIT_ERROR)\n157 \n158 \n159 def common_options(f: Callable) -> Callable:\n160     \"\"\"Add common options to commands via a decorator.\n161 \n162     These are applied to all of the cli commands.\n163     \"\"\"\n164     f = click.version_option()(f)\n165     f = click.option(\n166         \"-v\",\n167         \"--verbose\",\n168         count=True,\n169         default=None,\n170         help=(\n171             \"Verbosity, how detailed should the output be. This is *stackable*, so \"\n172             \"`-vv` is more verbose than `-v`. For the most verbose option try `-vvvv` \"\n173             \"or `-vvvvv`.\"\n174         ),\n175     )(f)\n176     f = click.option(\n177         \"-n\",\n178         \"--nocolor\",\n179         is_flag=True,\n180         default=None,\n181         help=\"No color - output will be without ANSI color codes.\",\n182     )(f)\n183 \n184     return f\n185 \n186 \n187 def core_options(f: Callable) -> Callable:\n188     \"\"\"Add core operation options to commands via a decorator.\n189 \n190     These are applied to the main (but not all) cli commands like\n191     `parse`, `lint` and `fix`.\n192     \"\"\"\n193     # Only enable dialect completion if on version of click\n194     # that supports it\n195     if shell_completion_enabled:\n196         f = click.option(\n197             \"-d\",\n198             \"--dialect\",\n199             default=None,\n200             help=\"The dialect of SQL to lint\",\n201             shell_complete=dialect_shell_complete,\n202         )(f)\n203     else:  # pragma: no cover\n204         f = click.option(\n205             \"-d\",\n206             \"--dialect\",\n207             default=None,\n208             help=\"The dialect of SQL to lint\",\n209         )(f)\n210     f = click.option(\n211         \"-t\",\n212         \"--templater\",\n213         default=None,\n214         help=\"The templater to use (default=jinja)\",\n215         type=click.Choice(\n216             [\n217                 templater.name\n218                 for templater in chain.from_iterable(\n219                     get_plugin_manager().hook.get_templaters()\n220                 )\n221             ]\n222         ),\n223     )(f)\n224     f = click.option(\n225         \"-r\",\n226         \"--rules\",\n227         default=None,\n228         help=(\n229             \"Narrow the search to only specific rules. For example \"\n230             \"specifying `--rules LT01` will only search for rule `LT01` (Unnecessary \"\n231             \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n232             \"`--rules LT01,LT02` will specify only looking for violations of rule \"\n233             \"`LT01` and rule `LT02`.\"\n234         ),\n235     )(f)\n236     f = click.option(\n237         \"-e\",\n238         \"--exclude-rules\",\n239         default=None,\n240         help=(\n241             \"Exclude specific rules. For example \"\n242             \"specifying `--exclude-rules LT01` will remove rule `LT01` (Unnecessary \"\n243             \"trailing whitespace) from the set of considered rules. This could either \"\n244             \"be the allowlist, or the general set if there is no specific allowlist. \"\n245             \"Multiple rules can be specified with commas e.g. \"\n246             \"`--exclude-rules LT01,LT02` will exclude violations of rule \"\n247             \"`LT01` and rule `LT02`.\"\n248         ),\n249     )(f)\n250     f = click.option(\n251         \"--config\",\n252         \"extra_config_path\",\n253         default=None,\n254         help=(\n255             \"Include additional config file. By default the config is generated \"\n256             \"from the standard configuration files described in the documentation. \"\n257             \"This argument allows you to specify an additional configuration file that \"\n258             \"overrides the standard configuration files. N.B. cfg format is required.\"\n259         ),\n260         type=click.Path(),\n261     )(f)\n262     f = click.option(\n263         \"--ignore-local-config\",\n264         is_flag=True,\n265         help=(\n266             \"Ignore config files in default search path locations. \"\n267             \"This option allows the user to lint with the default config \"\n268             \"or can be used in conjunction with --config to only \"\n269             \"reference the custom config file.\"\n270         ),\n271     )(f)\n272     f = click.option(\n273         \"--encoding\",\n274         default=None,\n275         help=(\n276             \"Specify encoding to use when reading and writing files. Defaults to \"\n277             \"autodetect.\"\n278         ),\n279     )(f)\n280     f = click.option(\n281         \"-i\",\n282         \"--ignore\",\n283         default=None,\n284         help=(\n285             \"Ignore particular families of errors so that they don't cause a failed \"\n286             \"run. For example `--ignore parsing` would mean that any parsing errors \"\n287             \"are ignored and don't influence the success or fail of a run. \"\n288             \"`--ignore` behaves somewhat like `noqa` comments, except it \"\n289             \"applies globally. Multiple options are possible if comma separated: \"\n290             \"e.g. `--ignore parsing,templating`.\"\n291         ),\n292     )(f)\n293     f = click.option(\n294         \"--bench\",\n295         is_flag=True,\n296         help=\"Set this flag to engage the benchmarking tool output.\",\n297     )(f)\n298     f = click.option(\n299         \"--logger\",\n300         type=click.Choice(\n301             [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\", \"config\"],\n302             case_sensitive=False,\n303         ),\n304         help=\"Choose to limit the logging to one of the loggers.\",\n305     )(f)\n306     f = click.option(\n307         \"--disable-noqa\",\n308         is_flag=True,\n309         default=None,\n310         help=\"Set this flag to ignore inline noqa comments.\",\n311     )(f)\n312     return f\n313 \n314 \n315 def lint_options(f: Callable) -> Callable:\n316     \"\"\"Add lint operation options to commands via a decorator.\n317 \n318     These are cli commands that do linting, i.e. `lint` and `fix`.\n319     \"\"\"\n320     f = click.option(\n321         \"-p\",\n322         \"--processes\",\n323         type=int,\n324         default=None,\n325         help=(\n326             \"The number of parallel processes to run. Positive numbers work as \"\n327             \"expected. Zero and negative numbers will work as number_of_cpus - \"\n328             \"number. e.g  -1 means all cpus except one. 0 means all cpus.\"\n329         ),\n330     )(f)\n331     f = click.option(\n332         \"--disable_progress_bar\",\n333         \"--disable-progress-bar\",\n334         is_flag=True,\n335         help=\"Disables progress bars.\",\n336         cls=DeprecatedOption,\n337         deprecated=[\"--disable_progress_bar\"],\n338     )(f)\n339     f = click.option(\n340         \"--persist-timing\",\n341         default=None,\n342         help=(\n343             \"A filename to persist the timing information for a linting run to \"\n344             \"in csv format for external analysis. NOTE: This feature should be \"\n345             \"treated as beta, and the format of the csv file may change in \"\n346             \"future releases without warning.\"\n347         ),\n348     )(f)\n349     return f\n350 \n351 \n352 def get_config(\n353     extra_config_path: Optional[str] = None,\n354     ignore_local_config: bool = False,\n355     **kwargs,\n356 ) -> FluffConfig:\n357     \"\"\"Get a config object from kwargs.\"\"\"\n358     plain_output = OutputStreamFormatter.should_produce_plain_output(kwargs[\"nocolor\"])\n359     if kwargs.get(\"dialect\"):\n360         try:\n361             # We're just making sure it exists at this stage.\n362             # It will be fetched properly in the linter.\n363             dialect_selector(kwargs[\"dialect\"])\n364         except SQLFluffUserError as err:\n365             click.echo(\n366                 OutputStreamFormatter.colorize_helper(\n367                     plain_output,\n368                     f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n369                     color=Color.red,\n370                 )\n371             )\n372             sys.exit(EXIT_ERROR)\n373         except KeyError:\n374             click.echo(\n375                 OutputStreamFormatter.colorize_helper(\n376                     plain_output,\n377                     f\"Error: Unknown dialect '{kwargs['dialect']}'\",\n378                     color=Color.red,\n379                 )\n380             )\n381             sys.exit(EXIT_ERROR)\n382     from_root_kwargs = {}\n383     if \"require_dialect\" in kwargs:\n384         from_root_kwargs[\"require_dialect\"] = kwargs.pop(\"require_dialect\")\n385     # Instantiate a config object (filtering out the nulls)\n386     overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n387     try:\n388         return FluffConfig.from_root(\n389             extra_config_path=extra_config_path,\n390             ignore_local_config=ignore_local_config,\n391             overrides=overrides,\n392             **from_root_kwargs,\n393         )\n394     except SQLFluffUserError as err:  # pragma: no cover\n395         click.echo(\n396             OutputStreamFormatter.colorize_helper(\n397                 plain_output,\n398                 f\"Error loading config: {str(err)}\",\n399                 color=Color.red,\n400             )\n401         )\n402         sys.exit(EXIT_ERROR)\n403 \n404 \n405 def get_linter_and_formatter(\n406     cfg: FluffConfig, output_stream: Optional[OutputStream] = None\n407 ) -> Tuple[Linter, OutputStreamFormatter]:\n408     \"\"\"Get a linter object given a config.\"\"\"\n409     try:\n410         # We're just making sure it exists at this stage.\n411         # It will be fetched properly in the linter.\n412         dialect = cfg.get(\"dialect\")\n413         if dialect:\n414             dialect_selector(dialect)\n415     except KeyError:  # pragma: no cover\n416         click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n417         sys.exit(EXIT_ERROR)\n418     formatter = OutputStreamFormatter(\n419         output_stream=output_stream or make_output_stream(cfg),\n420         nocolor=cfg.get(\"nocolor\"),\n421         verbosity=cfg.get(\"verbose\"),\n422         output_line_length=cfg.get(\"output_line_length\"),\n423     )\n424     return Linter(config=cfg, formatter=formatter), formatter\n425 \n426 \n427 @click.group(\n428     context_settings={\"help_option_names\": [\"-h\", \"--help\"]},\n429     epilog=\"\"\"\\b\\bExamples:\\n\n430   sqlfluff lint --dialect postgres .\\n\n431   sqlfluff lint --dialect postgres --rules ST05 .\\n\n432   sqlfluff fix --dialect sqlite --rules LT10,ST05 src/queries\\n\n433   sqlfluff parse --dialect sqlite --templater jinja src/queries/common.sql\n434 \"\"\",\n435 )\n436 @click.version_option()\n437 def cli():\n438     \"\"\"SQLFluff is a modular SQL linter for humans.\"\"\"  # noqa D403\n439 \n440 \n441 @cli.command()\n442 @common_options\n443 def version(**kwargs) -> None:\n444     \"\"\"Show the version of sqlfluff.\"\"\"\n445     c = get_config(**kwargs, require_dialect=False)\n446     if c.get(\"verbose\") > 0:\n447         # Instantiate the linter\n448         lnt, formatter = get_linter_and_formatter(c)\n449         # Dispatch the detailed config from the linter.\n450         formatter.dispatch_config(lnt)\n451     else:\n452         # Otherwise just output the package version.\n453         click.echo(get_package_version(), color=c.get(\"color\"))\n454 \n455 \n456 @cli.command()\n457 @common_options\n458 def rules(**kwargs) -> None:\n459     \"\"\"Show the current rules in use.\"\"\"\n460     c = get_config(**kwargs, dialect=\"ansi\")\n461     lnt, formatter = get_linter_and_formatter(c)\n462     try:\n463         click.echo(formatter.format_rules(lnt), color=c.get(\"color\"))\n464     # No cover for clause covering poorly formatted rules.\n465     # Without creating a poorly formed plugin, these are hard to\n466     # test.\n467     except (SQLFluffUserError, AssertionError) as err:  # pragma: no cover\n468         click.echo(\n469             OutputStreamFormatter.colorize_helper(\n470                 c.get(\"color\"),\n471                 f\"Error loading rules: {str(err)}\",\n472                 color=Color.red,\n473             )\n474         )\n475         sys.exit(EXIT_ERROR)\n476 \n477 \n478 @cli.command()\n479 @common_options\n480 def dialects(**kwargs) -> None:\n481     \"\"\"Show the current dialects available.\"\"\"\n482     c = get_config(**kwargs, require_dialect=False)\n483     _, formatter = get_linter_and_formatter(c)\n484     click.echo(formatter.format_dialects(dialect_readout), color=c.get(\"color\"))\n485 \n486 \n487 def dump_file_payload(filename: Optional[str], payload: str):\n488     \"\"\"Write the output file content to stdout or file.\"\"\"\n489     # If there's a file specified to write to, write to it.\n490     if filename:\n491         with open(filename, \"w\") as out_file:\n492             out_file.write(payload)\n493     # Otherwise write to stdout\n494     else:\n495         click.echo(payload)\n496 \n497 \n498 @cli.command(cls=DeprecatedOptionsCommand)\n499 @common_options\n500 @core_options\n501 @lint_options\n502 @click.option(\n503     \"-f\",\n504     \"--format\",\n505     \"format\",\n506     default=\"human\",\n507     type=click.Choice([ft.value for ft in FormatType], case_sensitive=False),\n508     help=\"What format to return the lint result in (default=human).\",\n509 )\n510 @click.option(\n511     \"--write-output\",\n512     help=(\n513         \"Optionally provide a filename to write the results to, mostly used in \"\n514         \"tandem with --format. NB: Setting an output file re-enables normal \"\n515         \"stdout logging.\"\n516     ),\n517 )\n518 @click.option(\n519     \"--annotation-level\",\n520     default=\"notice\",\n521     type=click.Choice([\"notice\", \"warning\", \"failure\", \"error\"], case_sensitive=False),\n522     help=(\n523         \"When format is set to github-annotation or github-annotation-native, \"\n524         \"default annotation level (default=notice). failure and error are equivalent.\"\n525     ),\n526 )\n527 @click.option(\n528     \"--nofail\",\n529     is_flag=True,\n530     help=(\n531         \"If set, the exit code will always be zero, regardless of violations \"\n532         \"found. This is potentially useful during rollout.\"\n533     ),\n534 )\n535 @click.option(\n536     \"--disregard-sqlfluffignores\",\n537     is_flag=True,\n538     help=\"Perform the operation regardless of .sqlfluffignore configurations\",\n539 )\n540 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n541 def lint(\n542     paths: Tuple[str],\n543     format: str,\n544     write_output: Optional[str],\n545     annotation_level: str,\n546     nofail: bool,\n547     disregard_sqlfluffignores: bool,\n548     logger: Optional[logging.Logger] = None,\n549     bench: bool = False,\n550     processes: Optional[int] = None,\n551     disable_progress_bar: Optional[bool] = False,\n552     persist_timing: Optional[str] = None,\n553     extra_config_path: Optional[str] = None,\n554     ignore_local_config: bool = False,\n555     **kwargs,\n556 ) -> None:\n557     \"\"\"Lint SQL files via passing a list of files or using stdin.\n558 \n559     PATH is the path to a sql file or directory to lint. This can be either a\n560     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n561     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n562     be interpreted like passing the current working directory as a path argument.\n563 \n564     Linting SQL files:\n565 \n566         sqlfluff lint path/to/file.sql\n567         sqlfluff lint directory/of/sql/files\n568 \n569     Linting a file via stdin (note the lone '-' character):\n570 \n571         cat path/to/file.sql | sqlfluff lint -\n572         echo 'select col from tbl' | sqlfluff lint -\n573 \n574     \"\"\"\n575     config = get_config(\n576         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n577     )\n578     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n579     file_output = None\n580     output_stream = make_output_stream(config, format, write_output)\n581     lnt, formatter = get_linter_and_formatter(config, output_stream)\n582 \n583     verbose = config.get(\"verbose\")\n584     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n585 \n586     formatter.dispatch_config(lnt)\n587 \n588     # Set up logging.\n589     set_logging_level(\n590         verbosity=verbose,\n591         formatter=formatter,\n592         logger=logger,\n593         stderr_output=non_human_output,\n594     )\n595 \n596     # Output the results as we go\n597     if verbose >= 1 and not non_human_output:\n598         click.echo(format_linting_result_header())\n599 \n600     with PathAndUserErrorHandler(formatter):\n601         # add stdin if specified via lone '-'\n602         if (\"-\",) == paths:\n603             result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n604         else:\n605             result = lnt.lint_paths(\n606                 paths,\n607                 ignore_non_existent_files=False,\n608                 ignore_files=not disregard_sqlfluffignores,\n609                 processes=processes,\n610             )\n611 \n612     # Output the final stats\n613     if verbose >= 1 and not non_human_output:\n614         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n615 \n616     if format == FormatType.json.value:\n617         file_output = json.dumps(result.as_records())\n618     elif format == FormatType.yaml.value:\n619         file_output = yaml.dump(result.as_records(), sort_keys=False)\n620     elif format == FormatType.none.value:\n621         file_output = \"\"\n622     elif format == FormatType.github_annotation.value:\n623         if annotation_level == \"error\":\n624             annotation_level = \"failure\"\n625 \n626         github_result = []\n627         for record in result.as_records():\n628             filepath = record[\"filepath\"]\n629             for violation in record[\"violations\"]:\n630                 # NOTE: The output format is designed for this GitHub action:\n631                 # https://github.com/yuzutech/annotations-action\n632                 # It is similar, but not identical, to the native GitHub format:\n633                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n634                 github_result.append(\n635                     {\n636                         \"file\": filepath,\n637                         \"line\": violation[\"line_no\"],\n638                         \"start_column\": violation[\"line_pos\"],\n639                         \"end_column\": violation[\"line_pos\"],\n640                         \"title\": \"SQLFluff\",\n641                         \"message\": f\"{violation['code']}: {violation['description']}\",\n642                         \"annotation_level\": annotation_level,\n643                     }\n644                 )\n645         file_output = json.dumps(github_result)\n646     elif format == FormatType.github_annotation_native.value:\n647         if annotation_level == \"failure\":\n648             annotation_level = \"error\"\n649 \n650         github_result_native = []\n651         for record in result.as_records():\n652             filepath = record[\"filepath\"]\n653             for violation in record[\"violations\"]:\n654                 # NOTE: The output format is designed for GitHub action:\n655                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n656                 line = f\"::{annotation_level} \"\n657                 line += \"title=SQLFluff,\"\n658                 line += f\"file={filepath},\"\n659                 line += f\"line={violation['line_no']},\"\n660                 line += f\"col={violation['line_pos']}\"\n661                 line += \"::\"\n662                 line += f\"{violation['code']}: {violation['description']}\"\n663                 if violation[\"name\"]:\n664                     line += f\" [{violation['name']}]\"\n665 \n666                 github_result_native.append(line)\n667 \n668         file_output = \"\\n\".join(github_result_native)\n669 \n670     if file_output:\n671         dump_file_payload(write_output, cast(str, file_output))\n672 \n673     if persist_timing:\n674         result.persist_timing_records(persist_timing)\n675 \n676     output_stream.close()\n677     if bench:\n678         click.echo(\"==== overall timings ====\")\n679         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n680         timing_summary = result.timing_summary()\n681         for step in timing_summary:\n682             click.echo(f\"=== {step} ===\")\n683             click.echo(\n684                 formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n685             )\n686 \n687     if not nofail:\n688         if not non_human_output:\n689             formatter.completion_message()\n690         sys.exit(result.stats()[\"exit code\"])\n691     else:\n692         sys.exit(EXIT_SUCCESS)\n693 \n694 \n695 def do_fixes(\n696     result: LintingResult, formatter: Optional[OutputStreamFormatter] = None, **kwargs\n697 ):\n698     \"\"\"Actually do the fixes.\"\"\"\n699     if formatter and formatter.verbosity >= 0:\n700         click.echo(\"Persisting Changes...\")\n701     res = result.persist_changes(formatter=formatter, **kwargs)\n702     if all(res.values()):\n703         if formatter and formatter.verbosity >= 0:\n704             click.echo(\"Done. Please check your files to confirm.\")\n705         return True\n706     # If some failed then return false\n707     click.echo(\n708         \"Done. Some operations failed. Please check your files to confirm.\"\n709     )  # pragma: no cover\n710     click.echo(\n711         \"Some errors cannot be fixed or there is another error blocking it.\"\n712     )  # pragma: no cover\n713     return False  # pragma: no cover\n714 \n715 \n716 def _stdin_fix(linter: Linter, formatter, fix_even_unparsable):\n717     \"\"\"Handle fixing from stdin.\"\"\"\n718     exit_code = EXIT_SUCCESS\n719     stdin = sys.stdin.read()\n720 \n721     result = linter.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n722     templater_error = result.num_violations(types=SQLTemplaterError) > 0\n723     unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n724     if not fix_even_unparsable:\n725         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n726 \n727     if result.num_violations(types=SQLLintError, fixable=True) > 0:\n728         stdout = result.paths[0].files[0].fix_string()[0]\n729     else:\n730         stdout = stdin\n731 \n732     if templater_error:\n733         click.echo(\n734             formatter.colorize(\n735                 \"Fix aborted due to unparsable template variables.\",\n736                 Color.red,\n737             ),\n738             err=True,\n739         )\n740         click.echo(\n741             formatter.colorize(\n742                 \"Use --FIX-EVEN-UNPARSABLE' to attempt to fix the SQL anyway.\",\n743                 Color.red,\n744             ),\n745             err=True,\n746         )\n747 \n748     if unfixable_error:\n749         click.echo(\n750             formatter.colorize(\"Unfixable violations detected.\", Color.red),\n751             err=True,\n752         )\n753 \n754     click.echo(stdout, nl=False)\n755     sys.exit(EXIT_FAIL if templater_error or unfixable_error else exit_code)\n756 \n757 \n758 def _paths_fix(\n759     linter: Linter,\n760     formatter,\n761     paths,\n762     processes,\n763     fix_even_unparsable,\n764     force,\n765     fixed_suffix,\n766     bench,\n767     show_lint_violations,\n768     warn_force: bool = True,\n769     persist_timing: Optional[str] = None,\n770 ):\n771     \"\"\"Handle fixing from paths.\"\"\"\n772     # Lint the paths (not with the fix argument at this stage), outputting as we go.\n773     if formatter.verbosity >= 0:\n774         click.echo(\"==== finding fixable violations ====\")\n775     exit_code = EXIT_SUCCESS\n776 \n777     with PathAndUserErrorHandler(formatter):\n778         result: LintingResult = linter.lint_paths(\n779             paths,\n780             fix=True,\n781             ignore_non_existent_files=False,\n782             processes=processes,\n783         )\n784 \n785     if not fix_even_unparsable:\n786         exit_code = formatter.handle_files_with_tmp_or_prs_errors(result)\n787 \n788     # NB: We filter to linting violations here, because they're\n789     # the only ones which can be potentially fixed.\n790     num_fixable = result.num_violations(types=SQLLintError, fixable=True)\n791     if num_fixable > 0:\n792         if formatter.verbosity >= 0:\n793             click.echo(\"==== fixing violations ====\")\n794         click.echo(f\"{num_fixable} \" \"fixable linting violations found\")\n795         if force:\n796             if warn_force and formatter.verbosity >= 0:\n797                 click.echo(\n798                     f\"{formatter.colorize('FORCE MODE', Color.red)}: \"\n799                     \"Attempting fixes...\"\n800                 )\n801             success = do_fixes(\n802                 result,\n803                 formatter,\n804                 types=SQLLintError,\n805                 fixed_file_suffix=fixed_suffix,\n806             )\n807             if not success:\n808                 sys.exit(EXIT_FAIL)  # pragma: no cover\n809         else:\n810             click.echo(\n811                 \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n812             )\n813             c = click.getchar().lower()\n814             click.echo(\"...\")\n815             if c in (\"y\", \"\\r\", \"\\n\"):\n816                 if formatter.verbosity >= 0:\n817                     click.echo(\"Attempting fixes...\")\n818                 success = do_fixes(\n819                     result,\n820                     formatter,\n821                     types=SQLLintError,\n822                     fixed_file_suffix=fixed_suffix,\n823                 )\n824                 if not success:\n825                     sys.exit(EXIT_FAIL)  # pragma: no cover\n826                 else:\n827                     formatter.completion_message()\n828             elif c == \"n\":\n829                 click.echo(\"Aborting...\")\n830                 exit_code = EXIT_FAIL\n831             else:  # pragma: no cover\n832                 click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n833                 click.echo(\"Aborting...\")\n834                 exit_code = EXIT_FAIL\n835     else:\n836         if formatter.verbosity >= 0:\n837             click.echo(\"==== no fixable linting violations found ====\")\n838             formatter.completion_message()\n839 \n840     error_types = [\n841         (\n842             dict(types=SQLLintError, fixable=False),\n843             \"  [{} unfixable linting violations found]\",\n844             EXIT_FAIL,\n845         ),\n846     ]\n847     for num_violations_kwargs, message_format, error_level in error_types:\n848         num_violations = result.num_violations(**num_violations_kwargs)\n849         if num_violations > 0 and formatter.verbosity >= 0:\n850             click.echo(message_format.format(num_violations))\n851             exit_code = max(exit_code, error_level)\n852 \n853     if bench:\n854         click.echo(\"==== overall timings ====\")\n855         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n856         timing_summary = result.timing_summary()\n857         for step in timing_summary:\n858             click.echo(f\"=== {step} ===\")\n859             click.echo(\n860                 formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n861             )\n862 \n863     if show_lint_violations:\n864         click.echo(\"==== lint for unfixable violations ====\")\n865         all_results = result.violation_dict(**num_violations_kwargs)\n866         sorted_files = sorted(all_results.keys())\n867         for file in sorted_files:\n868             violations = all_results.get(file, [])\n869             click.echo(formatter.format_filename(file, success=(not violations)))\n870             for violation in violations:\n871                 click.echo(formatter.format_violation(violation))\n872 \n873     if persist_timing:\n874         result.persist_timing_records(persist_timing)\n875 \n876     sys.exit(exit_code)\n877 \n878 \n879 @cli.command(cls=DeprecatedOptionsCommand)\n880 @common_options\n881 @core_options\n882 @lint_options\n883 @click.option(\n884     \"-f\",\n885     \"--force\",\n886     is_flag=True,\n887     help=(\n888         \"Skip the confirmation prompt and go straight to applying \"\n889         \"fixes. **Use this with caution.**\"\n890     ),\n891 )\n892 @click.option(\n893     \"-q\",\n894     \"--quiet\",\n895     is_flag=True,\n896     help=(\n897         \"Reduces the amount of output to stdout to a minimal level. \"\n898         \"This is effectively the opposite of -v. NOTE: It will only \"\n899         \"take effect if -f/--force is also set.\"\n900     ),\n901 )\n902 @click.option(\n903     \"-x\",\n904     \"--fixed-suffix\",\n905     default=None,\n906     help=\"An optional suffix to add to fixed files.\",\n907 )\n908 @click.option(\n909     \"--FIX-EVEN-UNPARSABLE\",\n910     is_flag=True,\n911     default=None,\n912     help=(\n913         \"Enables fixing of files that have templating or parse errors. \"\n914         \"Note that the similar-sounding '--ignore' or 'noqa' features merely \"\n915         \"prevent errors from being *displayed*. For safety reasons, the 'fix'\"\n916         \"command will not make any fixes in files that have templating or parse \"\n917         \"errors unless '--FIX-EVEN-UNPARSABLE' is enabled on the command line\"\n918         \"or in the .sqlfluff config file.\"\n919     ),\n920 )\n921 @click.option(\n922     \"--show-lint-violations\",\n923     is_flag=True,\n924     help=\"Show lint violations\",\n925 )\n926 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n927 def fix(\n928     force: bool,\n929     paths: Tuple[str],\n930     bench: bool = False,\n931     quiet: bool = False,\n932     fixed_suffix: str = \"\",\n933     logger: Optional[logging.Logger] = None,\n934     processes: Optional[int] = None,\n935     disable_progress_bar: Optional[bool] = False,\n936     persist_timing: Optional[str] = None,\n937     extra_config_path: Optional[str] = None,\n938     ignore_local_config: bool = False,\n939     show_lint_violations: bool = False,\n940     **kwargs,\n941 ) -> None:\n942     \"\"\"Fix SQL files.\n943 \n944     PATH is the path to a sql file or directory to lint. This can be either a\n945     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n946     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n947     be interpreted like passing the current working directory as a path argument.\n948     \"\"\"\n949     # some quick checks\n950     fixing_stdin = (\"-\",) == paths\n951     if quiet:\n952         if kwargs[\"verbose\"]:\n953             click.echo(\n954                 \"ERROR: The --quiet flag can only be used if --verbose is not set.\",\n955             )\n956             sys.exit(EXIT_ERROR)\n957         kwargs[\"verbose\"] = -1\n958 \n959     config = get_config(\n960         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n961     )\n962     fix_even_unparsable = config.get(\"fix_even_unparsable\")\n963     output_stream = make_output_stream(\n964         config, None, os.devnull if fixing_stdin else None\n965     )\n966     lnt, formatter = get_linter_and_formatter(config, output_stream)\n967 \n968     verbose = config.get(\"verbose\")\n969     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n970 \n971     formatter.dispatch_config(lnt)\n972 \n973     # Set up logging.\n974     set_logging_level(\n975         verbosity=verbose,\n976         formatter=formatter,\n977         logger=logger,\n978         stderr_output=fixing_stdin,\n979     )\n980 \n981     # handle stdin case. should output formatted sql to stdout and nothing else.\n982     if fixing_stdin:\n983         _stdin_fix(lnt, formatter, fix_even_unparsable)\n984     else:\n985         _paths_fix(\n986             lnt,\n987             formatter,\n988             paths,\n989             processes,\n990             fix_even_unparsable,\n991             force,\n992             fixed_suffix,\n993             bench,\n994             show_lint_violations,\n995             persist_timing=persist_timing,\n996         )\n997 \n998 \n999 @cli.command(name=\"format\", cls=DeprecatedOptionsCommand)\n1000 @common_options\n1001 @core_options\n1002 @lint_options\n1003 @click.option(\n1004     \"-x\",\n1005     \"--fixed-suffix\",\n1006     default=None,\n1007     help=\"An optional suffix to add to fixed files.\",\n1008 )\n1009 @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n1010 def cli_format(\n1011     paths: Tuple[str],\n1012     bench: bool = False,\n1013     fixed_suffix: str = \"\",\n1014     logger: Optional[logging.Logger] = None,\n1015     processes: Optional[int] = None,\n1016     disable_progress_bar: Optional[bool] = False,\n1017     persist_timing: Optional[str] = None,\n1018     extra_config_path: Optional[str] = None,\n1019     ignore_local_config: bool = False,\n1020     **kwargs,\n1021 ) -> None:\n1022     \"\"\"Autoformat SQL files.\n1023 \n1024     This effectively force applies `sqlfluff fix` with a known subset of fairly\n1025     stable rules. Enabled rules are ignored, but rule exclusions (via CLI) or\n1026     config are still respected.\n1027 \n1028     PATH is the path to a sql file or directory to lint. This can be either a\n1029     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n1030     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n1031     be interpreted like passing the current working directory as a path argument.\n1032     \"\"\"\n1033     # some quick checks\n1034     fixing_stdin = (\"-\",) == paths\n1035 \n1036     if kwargs.get(\"rules\"):\n1037         click.echo(\n1038             \"Specifying rules is not supported for sqlfluff format.\",\n1039         )\n1040         sys.exit(EXIT_ERROR)\n1041 \n1042     # Override rules for sqlfluff format\n1043     kwargs[\"rules\"] = (\n1044         # All of the capitalisation rules\n1045         \"capitalisation,\"\n1046         # All of the layout rules\n1047         \"layout,\"\n1048         # Safe rules from other groups\n1049         \"ambiguous.union,\"\n1050         \"convention.not_equal,\"\n1051         \"convention.coalesce,\"\n1052         \"convention.select_trailing_comma,\"\n1053         \"convention.is_null,\"\n1054         \"jinja.padding,\"\n1055         \"structure.distinct,\"\n1056     )\n1057 \n1058     config = get_config(\n1059         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1060     )\n1061     output_stream = make_output_stream(\n1062         config, None, os.devnull if fixing_stdin else None\n1063     )\n1064     lnt, formatter = get_linter_and_formatter(config, output_stream)\n1065 \n1066     verbose = config.get(\"verbose\")\n1067     progress_bar_configuration.disable_progress_bar = disable_progress_bar\n1068 \n1069     formatter.dispatch_config(lnt)\n1070 \n1071     # Set up logging.\n1072     set_logging_level(\n1073         verbosity=verbose,\n1074         formatter=formatter,\n1075         logger=logger,\n1076         stderr_output=fixing_stdin,\n1077     )\n1078 \n1079     # handle stdin case. should output formatted sql to stdout and nothing else.\n1080     if fixing_stdin:\n1081         _stdin_fix(lnt, formatter, fix_even_unparsable=False)\n1082     else:\n1083         _paths_fix(\n1084             lnt,\n1085             formatter,\n1086             paths,\n1087             processes,\n1088             fix_even_unparsable=False,\n1089             force=True,  # Always force in format mode.\n1090             fixed_suffix=fixed_suffix,\n1091             bench=bench,\n1092             show_lint_violations=False,\n1093             warn_force=False,  # don't warn about being in force mode.\n1094             persist_timing=persist_timing,\n1095         )\n1096 \n1097 \n1098 def quoted_presenter(dumper, data):\n1099     \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n1100     if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n1101         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n1102     else:\n1103         return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n1104 \n1105 \n1106 @cli.command()\n1107 @common_options\n1108 @core_options\n1109 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n1110 @click.option(\n1111     \"--recurse\", default=0, help=\"The depth to recursively parse to (0 for unlimited)\"\n1112 )\n1113 @click.option(\n1114     \"-c\",\n1115     \"--code-only\",\n1116     is_flag=True,\n1117     help=\"Output only the code elements of the parse tree.\",\n1118 )\n1119 @click.option(\n1120     \"-m\",\n1121     \"--include-meta\",\n1122     is_flag=True,\n1123     help=(\n1124         \"Include meta segments (indents, dedents and placeholders) in the output. \"\n1125         \"This only applies when outputting json or yaml.\"\n1126     ),\n1127 )\n1128 @click.option(\n1129     \"-f\",\n1130     \"--format\",\n1131     default=FormatType.human.value,\n1132     type=click.Choice(\n1133         [\n1134             FormatType.human.value,\n1135             FormatType.json.value,\n1136             FormatType.yaml.value,\n1137             FormatType.none.value,\n1138         ],\n1139         case_sensitive=False,\n1140     ),\n1141     help=\"What format to return the parse result in.\",\n1142 )\n1143 @click.option(\n1144     \"--write-output\",\n1145     help=(\n1146         \"Optionally provide a filename to write the results to, mostly used in \"\n1147         \"tandem with --format. NB: Setting an output file re-enables normal \"\n1148         \"stdout logging.\"\n1149     ),\n1150 )\n1151 @click.option(\n1152     \"--profiler\", is_flag=True, help=\"Set this flag to engage the python profiler.\"\n1153 )\n1154 @click.option(\n1155     \"--nofail\",\n1156     is_flag=True,\n1157     help=(\n1158         \"If set, the exit code will always be zero, regardless of violations \"\n1159         \"found. This is potentially useful during rollout.\"\n1160     ),\n1161 )\n1162 def parse(\n1163     path: str,\n1164     code_only: bool,\n1165     include_meta: bool,\n1166     format: str,\n1167     write_output: Optional[str],\n1168     profiler: bool,\n1169     bench: bool,\n1170     nofail: bool,\n1171     logger: Optional[logging.Logger] = None,\n1172     extra_config_path: Optional[str] = None,\n1173     ignore_local_config: bool = False,\n1174     **kwargs,\n1175 ) -> None:\n1176     \"\"\"Parse SQL files and just spit out the result.\n1177 \n1178     PATH is the path to a sql file or directory to lint. This can be either a\n1179     file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n1180     character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n1181     be interpreted like passing the current working directory as a path argument.\n1182     \"\"\"\n1183     c = get_config(\n1184         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1185     )\n1186     # We don't want anything else to be logged if we want json or yaml output\n1187     # unless we're writing to a file.\n1188     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n1189     output_stream = make_output_stream(c, format, write_output)\n1190     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1191     verbose = c.get(\"verbose\")\n1192     recurse = c.get(\"recurse\")\n1193 \n1194     progress_bar_configuration.disable_progress_bar = True\n1195 \n1196     formatter.dispatch_config(lnt)\n1197 \n1198     # Set up logging.\n1199     set_logging_level(\n1200         verbosity=verbose,\n1201         formatter=formatter,\n1202         logger=logger,\n1203         stderr_output=non_human_output,\n1204     )\n1205 \n1206     # TODO: do this better\n1207 \n1208     if profiler:\n1209         # Set up the profiler if required\n1210         try:\n1211             import cProfile\n1212         except ImportError:  # pragma: no cover\n1213             click.echo(\"The cProfiler is not available on your platform.\")\n1214             sys.exit(EXIT_ERROR)\n1215         pr = cProfile.Profile()\n1216         pr.enable()\n1217 \n1218     t0 = time.monotonic()\n1219 \n1220     # handle stdin if specified via lone '-'\n1221     with PathAndUserErrorHandler(formatter):\n1222         if \"-\" == path:\n1223             parsed_strings = [\n1224                 lnt.parse_string(\n1225                     sys.stdin.read(),\n1226                     \"stdin\",\n1227                     recurse=recurse,\n1228                     config=lnt.config,\n1229                 ),\n1230             ]\n1231         else:\n1232             # A single path must be specified for this command\n1233             parsed_strings = list(\n1234                 lnt.parse_path(\n1235                     path=path,\n1236                     recurse=recurse,\n1237                 )\n1238             )\n1239 \n1240     total_time = time.monotonic() - t0\n1241     violations_count = 0\n1242 \n1243     # iterative print for human readout\n1244     if format == FormatType.human.value:\n1245         violations_count = formatter.print_out_violations_and_timing(\n1246             output_stream, bench, code_only, total_time, verbose, parsed_strings\n1247         )\n1248     else:\n1249         parsed_strings_dict = [\n1250             dict(\n1251                 filepath=linted_result.fname,\n1252                 segments=linted_result.tree.as_record(\n1253                     code_only=code_only, show_raw=True, include_meta=include_meta\n1254                 )\n1255                 if linted_result.tree\n1256                 else None,\n1257             )\n1258             for linted_result in parsed_strings\n1259         ]\n1260 \n1261         if format == FormatType.yaml.value:\n1262             # For yaml dumping always dump double quoted strings if they contain\n1263             # tabs or newlines.\n1264             yaml.add_representer(str, quoted_presenter)\n1265             file_output = yaml.dump(parsed_strings_dict, sort_keys=False)\n1266         elif format == FormatType.json.value:\n1267             file_output = json.dumps(parsed_strings_dict)\n1268         elif format == FormatType.none.value:\n1269             file_output = \"\"\n1270 \n1271         # Dump the output to stdout or to file as appropriate.\n1272         dump_file_payload(write_output, file_output)\n1273     if profiler:\n1274         pr.disable()\n1275         profiler_buffer = StringIO()\n1276         ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n1277         ps.print_stats()\n1278         click.echo(\"==== profiler stats ====\")\n1279         # Only print the first 50 lines of it\n1280         click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n1281 \n1282     if violations_count > 0 and not nofail:\n1283         sys.exit(EXIT_FAIL)  # pragma: no cover\n1284     else:\n1285         sys.exit(EXIT_SUCCESS)\n1286 \n1287 \n1288 @cli.command()\n1289 @common_options\n1290 @core_options\n1291 @click.argument(\"path\", nargs=1, type=click.Path(allow_dash=True))\n1292 def render(\n1293     path: str,\n1294     bench: bool,\n1295     logger: Optional[logging.Logger] = None,\n1296     extra_config_path: Optional[str] = None,\n1297     ignore_local_config: bool = False,\n1298     **kwargs,\n1299 ) -> None:\n1300     \"\"\"Render SQL files and just spit out the result.\n1301 \n1302     PATH is the path to a sql file. This should be either a single file\n1303     file ('path/to/file.sql') or a single ('-') character to indicate reading\n1304     from *stdin*.\n1305     \"\"\"\n1306     c = get_config(\n1307         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n1308     )\n1309     # We don't want anything else to be logged if we want json or yaml output\n1310     # unless we're writing to a file.\n1311     output_stream = make_output_stream(c, None, None)\n1312     lnt, formatter = get_linter_and_formatter(c, output_stream)\n1313     verbose = c.get(\"verbose\")\n1314 \n1315     progress_bar_configuration.disable_progress_bar = True\n1316 \n1317     formatter.dispatch_config(lnt)\n1318 \n1319     # Set up logging.\n1320     set_logging_level(\n1321         verbosity=verbose,\n1322         formatter=formatter,\n1323         logger=logger,\n1324         stderr_output=False,\n1325     )\n1326 \n1327     # handle stdin if specified via lone '-'\n1328     with PathAndUserErrorHandler(formatter):\n1329         if \"-\" == path:\n1330             raw_sql = sys.stdin.read()\n1331             fname = \"stdin\"\n1332             file_config = lnt.config\n1333         else:\n1334             raw_sql, file_config, _ = lnt.load_raw_file_and_config(path, lnt.config)\n1335             fname = path\n1336 \n1337     # Get file specific config\n1338     file_config.process_raw_file_for_config(raw_sql)\n1339     rendered = lnt.render_string(raw_sql, fname, file_config, \"utf8\")\n1340 \n1341     if rendered.templater_violations:\n1342         for v in rendered.templater_violations:\n1343             click.echo(formatter.format_violation(v))\n1344         sys.exit(EXIT_FAIL)\n1345     else:\n1346         click.echo(rendered.templated_file.templated_str)\n1347         sys.exit(EXIT_SUCCESS)\n1348 \n1349 \n1350 # This \"__main__\" handler allows invoking SQLFluff using \"python -m\", which\n1351 # simplifies the use of cProfile, e.g.:\n1352 # python -m cProfile -s cumtime -m sqlfluff.cli.commands lint slow_file.sql\n1353 if __name__ == \"__main__\":\n1354     cli.main(sys.argv[1:])  # pragma: no cover\n1355 \n[end of src/sqlfluff/cli/commands.py]\n[start of src/sqlfluff/core/linter/linted_dir.py]\n1 \"\"\"Defines the LintedDir class.\n2 \n3 This stores the idea of a collection of linted files at a single start path\n4 \n5 \"\"\"\n6 \n7 from typing import (\n8     Any,\n9     Dict,\n10     List,\n11     Optional,\n12     Union,\n13     overload,\n14 )\n15 from typing_extensions import Literal\n16 \n17 \n18 from sqlfluff.core.errors import (\n19     CheckTuple,\n20 )\n21 from sqlfluff.core.parser.segments.base import BaseSegment\n22 \n23 from sqlfluff.core.linter.linted_file import LintedFile\n24 \n25 \n26 class LintedDir:\n27     \"\"\"A class to store the idea of a collection of linted files at a single start path.\n28 \n29     A LintedDir may contain files in subdirectories, but they all share\n30     a common root.\n31     \"\"\"\n32 \n33     def __init__(self, path: str) -> None:\n34         self.files: List[LintedFile] = []\n35         self.path: str = path\n36 \n37     def add(self, file: LintedFile) -> None:\n38         \"\"\"Add a file to this path.\"\"\"\n39         self.files.append(file)\n40 \n41     @overload\n42     def check_tuples(\n43         self, by_path: Literal[False]\n44     ) -> List[CheckTuple]:  # pragma: no cover\n45         \"\"\"Return a List of CheckTuples when by_path is False.\"\"\"\n46         ...\n47 \n48     @overload\n49     def check_tuples(\n50         self, by_path: Literal[True]\n51     ) -> Dict[str, List[CheckTuple]]:  # pragma: no cover\n52         \"\"\"Return a Dict of paths and CheckTuples when by_path is True.\"\"\"\n53         ...\n54 \n55     @overload\n56     def check_tuples(self, by_path: bool = False):  # pragma: no cover\n57         \"\"\"Default overload method.\"\"\"\n58         ...\n59 \n60     def check_tuples(\n61         self, by_path=False, raise_on_non_linting_violations=True\n62     ) -> Union[List[CheckTuple], Dict[str, List[CheckTuple]]]:\n63         \"\"\"Compress all the tuples into one list.\n64 \n65         NB: This is a little crude, as you can't tell which\n66         file the violations are from. Good for testing though.\n67         For more control set the `by_path` argument to true.\n68         \"\"\"\n69         if by_path:\n70             return {\n71                 file.path: file.check_tuples(\n72                     raise_on_non_linting_violations=raise_on_non_linting_violations\n73                 )\n74                 for file in self.files\n75             }\n76         else:\n77             tuple_buffer: List[CheckTuple] = []\n78             for file in self.files:\n79                 tuple_buffer += file.check_tuples(\n80                     raise_on_non_linting_violations=raise_on_non_linting_violations\n81                 )\n82             return tuple_buffer\n83 \n84     def num_violations(self, **kwargs) -> int:\n85         \"\"\"Count the number of violations in the path.\"\"\"\n86         return sum(file.num_violations(**kwargs) for file in self.files)\n87 \n88     def get_violations(self, **kwargs) -> list:\n89         \"\"\"Return a list of violations in the path.\"\"\"\n90         buff: list = []\n91         for file in self.files:\n92             buff += file.get_violations(**kwargs)\n93         return buff\n94 \n95     def violation_dict(self, **kwargs) -> Dict[str, list]:\n96         \"\"\"Return a dict of violations by file path.\"\"\"\n97         return {file.path: file.get_violations(**kwargs) for file in self.files}\n98 \n99     def stats(self) -> Dict[str, int]:\n100         \"\"\"Return a dict containing linting stats about this path.\"\"\"\n101         return dict(\n102             files=len(self.files),\n103             clean=sum(file.is_clean() for file in self.files),\n104             unclean=sum(not file.is_clean() for file in self.files),\n105             violations=sum(file.num_violations() for file in self.files),\n106         )\n107 \n108     def persist_changes(\n109         self, formatter: Any = None, fixed_file_suffix: str = \"\", **kwargs\n110     ) -> Dict[str, Union[bool, str]]:\n111         \"\"\"Persist changes to files in the given path.\n112 \n113         This also logs the output as we go using the formatter if present.\n114         \"\"\"\n115         # Run all the fixes for all the files and return a dict\n116         buffer: Dict[str, Union[bool, str]] = {}\n117         for file in self.files:\n118             if file.num_violations(fixable=True, **kwargs) > 0:\n119                 buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n120                 result: Union[bool, str]\n121                 if buffer[file.path] is True:\n122                     result = \"FIXED\"\n123                 else:  # pragma: no cover\n124                     result = buffer[file.path]\n125             else:  # pragma: no cover TODO?\n126                 buffer[file.path] = True\n127                 result = \"SKIP\"\n128 \n129             if formatter:\n130                 formatter.dispatch_persist_filename(filename=file.path, result=result)\n131         return buffer\n132 \n133     @property\n134     def tree(self) -> Optional[BaseSegment]:\n135         \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n136         if len(self.files) > 1:  # pragma: no cover\n137             raise ValueError(\n138                 \".tree() cannot be called when a LintedDir contains more than one file.\"\n139             )\n140         return self.files[0].tree\n141 \n[end of src/sqlfluff/core/linter/linted_dir.py]\n[start of src/sqlfluff/core/linter/linted_file.py]\n1 \"\"\"Defines the LintedFile class.\n2 \n3 This holds linting results for a single file, and also\n4 contains all of the routines to apply fixes to that file\n5 post linting.\n6 \"\"\"\n7 \n8 import os\n9 import logging\n10 import shutil\n11 import stat\n12 import tempfile\n13 from collections import defaultdict\n14 from dataclasses import dataclass\n15 from typing import (\n16     Any,\n17     Iterable,\n18     List,\n19     NamedTuple,\n20     Optional,\n21     Tuple,\n22     Union,\n23     cast,\n24     Type,\n25     Dict,\n26 )\n27 \n28 from sqlfluff.core.errors import (\n29     SQLBaseError,\n30     SQLLintError,\n31     CheckTuple,\n32 )\n33 from sqlfluff.core.templaters import TemplatedFile, RawFileSlice\n34 \n35 # Classes needed only for type checking\n36 from sqlfluff.core.parser.segments import BaseSegment, FixPatch\n37 \n38 from sqlfluff.core.linter.common import NoQaDirective\n39 \n40 # Instantiate the linter logger\n41 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n42 \n43 \n44 @dataclass\n45 class FileTimings:\n46     \"\"\"A dataclass for holding the timings information for a file.\"\"\"\n47 \n48     step_timings: Dict[str, float]\n49     # NOTE: Because rules may run more than once for any\n50     # given file we record each run and then we can post\n51     # process this as we wish later.\n52     rule_timings: List[Tuple[str, str, float]]\n53 \n54     def get_rule_timing_dict(self) -> Dict[str, float]:\n55         \"\"\"Generate a summary to total time in each rule.\n56 \n57         This is primarily for csv export.\n58         \"\"\"\n59         total_times: Dict[str, float] = defaultdict(float)\n60 \n61         for code, _, time in self.rule_timings:\n62             total_times[code] += time\n63 \n64         # Return as plain dict\n65         return dict(total_times.items())\n66 \n67 \n68 class LintedFile(NamedTuple):\n69     \"\"\"A class to store the idea of a linted file.\"\"\"\n70 \n71     path: str\n72     violations: List[SQLBaseError]\n73     timings: Optional[FileTimings]\n74     tree: Optional[BaseSegment]\n75     ignore_mask: List[NoQaDirective]\n76     templated_file: TemplatedFile\n77     encoding: str\n78 \n79     def check_tuples(self, raise_on_non_linting_violations=True) -> List[CheckTuple]:\n80         \"\"\"Make a list of check_tuples.\n81 \n82         This assumes that all the violations found are\n83         linting violations. If they don't then this function\n84         raises that error.\n85         \"\"\"\n86         vs: List[CheckTuple] = []\n87         v: SQLLintError\n88         for v in self.get_violations():\n89             if isinstance(v, SQLLintError):\n90                 vs.append(v.check_tuple())\n91             elif raise_on_non_linting_violations:\n92                 raise v\n93         return vs\n94 \n95     @staticmethod\n96     def deduplicate_in_source_space(\n97         violations: List[SQLBaseError],\n98     ) -> List[SQLBaseError]:\n99         \"\"\"Removes duplicates in the source space.\n100 \n101         This is useful for templated files with loops, where we'll\n102         get a violation for each pass around the loop, but the user\n103         only cares about it once and we're only going to fix it once.\n104 \n105         By filtering them early we get a more a more helpful CLI\n106         output *and* and more efficient fixing routine (by handling\n107         fewer fixes).\n108         \"\"\"\n109         new_violations = []\n110         dedupe_buffer = set()\n111         for v in violations:\n112             signature = v.source_signature()\n113             if signature not in dedupe_buffer:\n114                 new_violations.append(v)\n115                 dedupe_buffer.add(signature)\n116             else:\n117                 linter_logger.debug(\"Removing duplicate source violation: %r\", v)\n118         return new_violations\n119 \n120     def get_violations(\n121         self,\n122         rules: Optional[Union[str, Tuple[str, ...]]] = None,\n123         types: Optional[Union[Type[SQLBaseError], Iterable[Type[SQLBaseError]]]] = None,\n124         filter_ignore: bool = True,\n125         filter_warning: bool = True,\n126         fixable: Optional[bool] = None,\n127     ) -> list:\n128         \"\"\"Get a list of violations, respecting filters and ignore options.\n129 \n130         Optionally now with filters.\n131         \"\"\"\n132         violations = self.violations\n133         # Filter types\n134         if types:\n135             # If it's a singular type, make it a single item in a tuple\n136             # otherwise coerce to tuple normally so that we can use it with\n137             # isinstance.\n138             if isinstance(types, type) and issubclass(types, SQLBaseError):\n139                 types = (types,)\n140             else:\n141                 types = tuple(types)  # pragma: no cover TODO?\n142             violations = [v for v in violations if isinstance(v, types)]\n143         # Filter rules\n144         if rules:\n145             if isinstance(rules, str):\n146                 rules = (rules,)\n147             else:\n148                 rules = tuple(rules)\n149             violations = [v for v in violations if v.rule_code() in rules]\n150         # Filter fixable\n151         if fixable is not None:\n152             # Assume that fixable is true or false if not None.\n153             # Fatal errors should always come through, regardless.\n154             violations = [v for v in violations if v.fixable is fixable or v.fatal]\n155         # Filter ignorable violations\n156         if filter_ignore:\n157             violations = [v for v in violations if not v.ignore]\n158             # Ignore any rules in the ignore mask\n159             if self.ignore_mask:\n160                 violations = self.ignore_masked_violations(violations, self.ignore_mask)\n161         # Filter warning violations\n162         if filter_warning:\n163             violations = [v for v in violations if not v.warning]\n164         return violations\n165 \n166     @staticmethod\n167     def _ignore_masked_violations_single_line(\n168         violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n169     ):\n170         \"\"\"Returns whether to ignore error for line-specific directives.\n171 \n172         The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n173         action=None.\n174         \"\"\"\n175         for ignore in ignore_mask:\n176             violations = [\n177                 v\n178                 for v in violations\n179                 if not (\n180                     v.line_no == ignore.line_no\n181                     and (ignore.rules is None or v.rule_code() in ignore.rules)\n182                 )\n183             ]\n184         return violations\n185 \n186     @staticmethod\n187     def _should_ignore_violation_line_range(\n188         line_no: int, ignore_rule: List[NoQaDirective]\n189     ):\n190         \"\"\"Returns whether to ignore a violation at line_no.\"\"\"\n191         # Loop through the NoQaDirectives to find the state of things at\n192         # line_no. Assumptions about \"ignore_rule\":\n193         # - Contains directives for only ONE RULE, i.e. the rule that was\n194         #   violated at line_no\n195         # - Sorted in ascending order by line number\n196         disable = False\n197         for ignore in ignore_rule:\n198             if ignore.line_no > line_no:\n199                 break\n200             disable = ignore.action == \"disable\"\n201         return disable\n202 \n203     @classmethod\n204     def _ignore_masked_violations_line_range(\n205         cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n206     ):\n207         \"\"\"Returns whether to ignore error for line-range directives.\n208 \n209         The \"ignore\" list is assumed to ONLY contain NoQaDirectives where\n210         action is \"enable\" or \"disable\".\n211         \"\"\"\n212         result = []\n213         for v in violations:\n214             # Find the directives that affect the violated rule \"v\", either\n215             # because they specifically reference it or because they don't\n216             # specify a list of rules, thus affecting ALL rules.\n217             ignore_rule = sorted(\n218                 (\n219                     ignore\n220                     for ignore in ignore_mask\n221                     if not ignore.rules\n222                     or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))\n223                 ),\n224                 key=lambda ignore: ignore.line_no,\n225             )\n226             # Determine whether to ignore the violation, based on the relevant\n227             # enable/disable directives.\n228             if not cls._should_ignore_violation_line_range(v.line_no, ignore_rule):\n229                 result.append(v)\n230         return result\n231 \n232     @classmethod\n233     def ignore_masked_violations(\n234         cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n235     ) -> List[SQLBaseError]:\n236         \"\"\"Remove any violations specified by ignore_mask.\n237 \n238         This involves two steps:\n239         1. Filter out violations affected by single-line \"noqa\" directives.\n240         2. Filter out violations affected by disable/enable \"noqa\" directives.\n241         \"\"\"\n242         ignore_specific = [ignore for ignore in ignore_mask if not ignore.action]\n243         ignore_range = [ignore for ignore in ignore_mask if ignore.action]\n244         violations = cls._ignore_masked_violations_single_line(\n245             violations, ignore_specific\n246         )\n247         violations = cls._ignore_masked_violations_line_range(violations, ignore_range)\n248         return violations\n249 \n250     def num_violations(self, **kwargs) -> int:\n251         \"\"\"Count the number of violations.\n252 \n253         Optionally now with filters.\n254         \"\"\"\n255         violations = self.get_violations(**kwargs)\n256         return len(violations)\n257 \n258     def is_clean(self) -> bool:\n259         \"\"\"Return True if there are no ignorable violations.\"\"\"\n260         return not any(self.get_violations(filter_ignore=True))\n261 \n262     @staticmethod\n263     def _log_hints(patch: FixPatch, templated_file: TemplatedFile):\n264         \"\"\"Log hints for debugging during patch generation.\"\"\"\n265         # This next bit is ALL FOR LOGGING AND DEBUGGING\n266         max_log_length = 10\n267         if patch.templated_slice.start >= max_log_length:\n268             pre_hint = templated_file.templated_str[\n269                 patch.templated_slice.start\n270                 - max_log_length : patch.templated_slice.start\n271             ]\n272         else:\n273             pre_hint = templated_file.templated_str[: patch.templated_slice.start]\n274         if patch.templated_slice.stop + max_log_length < len(\n275             templated_file.templated_str\n276         ):\n277             post_hint = templated_file.templated_str[\n278                 patch.templated_slice.stop : patch.templated_slice.stop + max_log_length\n279             ]\n280         else:\n281             post_hint = templated_file.templated_str[patch.templated_slice.stop :]\n282         linter_logger.debug(\n283             \"        Templated Hint: ...%r <> %r...\", pre_hint, post_hint\n284         )\n285 \n286     def fix_string(self) -> Tuple[Any, bool]:\n287         \"\"\"Obtain the changes to a path as a string.\n288 \n289         We use the source mapping features of TemplatedFile\n290         to generate a list of \"patches\" which cover the non\n291         templated parts of the file and refer back to the locations\n292         in the original file.\n293 \n294         NB: This is MUCH FASTER than the original approach\n295         using difflib in pre 0.4.0.\n296 \n297         There is an important distinction here between Slices and\n298         Segments. A Slice is a portion of a file which is determined\n299         by the templater based on which portions of the source file\n300         are templated or not, and therefore before Lexing and so is\n301         completely dialect agnostic. A Segment is determined by the\n302         Lexer from portions of strings after templating.\n303         \"\"\"\n304         linter_logger.debug(\"Original Tree: %r\", self.templated_file.templated_str)\n305         assert self.tree\n306         linter_logger.debug(\"Fixed Tree: %r\", self.tree.raw)\n307 \n308         # The sliced file is contiguous in the TEMPLATED space.\n309         # NB: It has gaps and repeats in the source space.\n310         # It's also not the FIXED file either.\n311         linter_logger.debug(\"### Templated File.\")\n312         for idx, file_slice in enumerate(self.templated_file.sliced_file):\n313             t_str = self.templated_file.templated_str[file_slice.templated_slice]\n314             s_str = self.templated_file.source_str[file_slice.source_slice]\n315             if t_str == s_str:\n316                 linter_logger.debug(\n317                     \"    File slice: %s %r [invariant]\", idx, file_slice\n318                 )\n319             else:\n320                 linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n321                 linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n322 \n323         original_source = self.templated_file.source_str\n324 \n325         # Generate patches from the fixed tree. In the process we sort\n326         # and deduplicate them so that the resultant list is in the\n327         # the right order for the source file without any duplicates.\n328         filtered_source_patches = self._generate_source_patches(\n329             self.tree, self.templated_file\n330         )\n331         linter_logger.debug(\"Filtered source patches:\")\n332         for idx, patch in enumerate(filtered_source_patches):\n333             linter_logger.debug(\"    %s: %s\", idx, patch)\n334 \n335         # Any Template tags in the source file are off limits, unless\n336         # we're explicitly fixing the source file.\n337         source_only_slices = self.templated_file.source_only_slices()\n338         linter_logger.debug(\"Source-only slices: %s\", source_only_slices)\n339 \n340         # We now slice up the file using the patches and any source only slices.\n341         # This gives us regions to apply changes to.\n342         slice_buff = self._slice_source_file_using_patches(\n343             filtered_source_patches, source_only_slices, self.templated_file.source_str\n344         )\n345 \n346         linter_logger.debug(\"Final slice buffer: %s\", slice_buff)\n347 \n348         # Iterate through the patches, building up the new string.\n349         fixed_source_string = self._build_up_fixed_source_string(\n350             slice_buff, filtered_source_patches, self.templated_file.source_str\n351         )\n352 \n353         # The success metric here is whether anything ACTUALLY changed.\n354         return fixed_source_string, fixed_source_string != original_source\n355 \n356     @classmethod\n357     def _generate_source_patches(\n358         cls, tree: BaseSegment, templated_file: TemplatedFile\n359     ) -> List[FixPatch]:\n360         \"\"\"Use the fixed tree to generate source patches.\n361 \n362         Importantly here we deduplicate and sort the patches\n363         from their position in the templated file into their\n364         intended order in the source file.\n365         \"\"\"\n366         # Iterate patches, filtering and translating as we go:\n367         linter_logger.debug(\"### Beginning Patch Iteration.\")\n368         filtered_source_patches = []\n369         dedupe_buffer = []\n370         # We use enumerate so that we get an index for each patch. This is entirely\n371         # so when debugging logs we can find a given patch again!\n372         for idx, patch in enumerate(tree.iter_patches(templated_file=templated_file)):\n373             linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n374             cls._log_hints(patch, templated_file)\n375 \n376             # Check for duplicates\n377             if patch.dedupe_tuple() in dedupe_buffer:\n378                 linter_logger.info(\n379                     \"      - Skipping. Source space Duplicate: %s\",\n380                     patch.dedupe_tuple(),\n381                 )\n382                 continue\n383 \n384             # We now evaluate patches in the source-space for whether they overlap\n385             # or disrupt any templated sections.\n386             # The intent here is that unless explicitly stated, a fix should never\n387             # disrupt a templated section.\n388             # NOTE: We rely here on the patches being generated in order.\n389             # TODO: Implement a mechanism for doing templated section fixes. Given\n390             # these patches are currently generated from fixed segments, there will\n391             # likely need to be an entirely different mechanism\n392 \n393             # Get the affected raw slices.\n394             local_raw_slices = templated_file.raw_slices_spanning_source_slice(\n395                 patch.source_slice\n396             )\n397             local_type_list = [slc.slice_type for slc in local_raw_slices]\n398 \n399             # Deal with the easy cases of 1) New code at end 2) only literals\n400             if not local_type_list or set(local_type_list) == {\"literal\"}:\n401                 linter_logger.info(\n402                     \"      * Keeping patch on new or literal-only section.\",\n403                 )\n404                 filtered_source_patches.append(patch)\n405                 dedupe_buffer.append(patch.dedupe_tuple())\n406             # Handle the easy case of an explicit source fix\n407             elif patch.patch_category == \"source\":\n408                 linter_logger.info(\n409                     \"      * Keeping explicit source fix patch.\",\n410                 )\n411                 filtered_source_patches.append(patch)\n412                 dedupe_buffer.append(patch.dedupe_tuple())\n413             # Is it a zero length patch.\n414             elif (\n415                 patch.source_slice.start == patch.source_slice.stop\n416                 and patch.source_slice.start == local_raw_slices[0].source_idx\n417             ):\n418                 linter_logger.info(\n419                     \"      * Keeping insertion patch on slice boundary.\",\n420                 )\n421                 filtered_source_patches.append(patch)\n422                 dedupe_buffer.append(patch.dedupe_tuple())\n423             else:  # pragma: no cover\n424                 # We've got a situation where the ends of our patch need to be\n425                 # more carefully mapped. This used to happen with greedy template\n426                 # element matching, but should now never happen. In the event that\n427                 # it does, we'll warn but carry on.\n428                 linter_logger.warning(\n429                     \"Skipping edit patch on uncertain templated section [%s], \"\n430                     \"Please report this warning on GitHub along with the query \"\n431                     \"that produced it.\",\n432                     (patch.patch_category, patch.source_slice),\n433                 )\n434                 continue\n435 \n436         # Sort the patches before building up the file.\n437         return sorted(filtered_source_patches, key=lambda x: x.source_slice.start)\n438 \n439     @staticmethod\n440     def _slice_source_file_using_patches(\n441         source_patches: List[FixPatch],\n442         source_only_slices: List[RawFileSlice],\n443         raw_source_string: str,\n444     ) -> List[slice]:\n445         \"\"\"Use patches to safely slice up the file before fixing.\n446 \n447         This uses source only slices to avoid overwriting sections\n448         of templated code in the source file (when we don't want to).\n449 \n450         We assume that the source patches have already been\n451         sorted and deduplicated. Sorting is important. If the slices\n452         aren't sorted then this function will miss chunks.\n453         If there are overlaps or duplicates then this function\n454         may produce strange results.\n455         \"\"\"\n456         # We now slice up the file using the patches and any source only slices.\n457         # This gives us regions to apply changes to.\n458         slice_buff = []\n459         source_idx = 0\n460         for patch in source_patches:\n461             # Are there templated slices at or before the start of this patch?\n462             # TODO: We'll need to explicit handling for template fixes here, because\n463             # they ARE source only slices. If we can get handling to work properly\n464             # here then this is the last hurdle and it will flow through\n465             # smoothly from here.\n466             while (\n467                 source_only_slices\n468                 and source_only_slices[0].source_idx < patch.source_slice.start\n469             ):\n470                 next_so_slice = source_only_slices.pop(0).source_slice()\n471                 # Add a pre-slice before the next templated slices if needed.\n472                 if next_so_slice.start > source_idx:\n473                     slice_buff.append(slice(source_idx, next_so_slice.start))\n474                 # Add the templated slice.\n475                 slice_buff.append(next_so_slice)\n476                 source_idx = next_so_slice.stop\n477 \n478             # Does this patch cover the next source-only slice directly?\n479             if (\n480                 source_only_slices\n481                 and patch.source_slice == source_only_slices[0].source_slice()\n482             ):\n483                 linter_logger.info(\n484                     \"Removing next source only slice from the stack because it \"\n485                     \"covers the same area of source file as the current patch: %s %s\",\n486                     source_only_slices[0],\n487                     patch,\n488                 )\n489                 # If it does, remove it so that we don't duplicate it.\n490                 source_only_slices.pop(0)\n491 \n492             # Is there a gap between current position and this patch?\n493             if patch.source_slice.start > source_idx:\n494                 # Add a slice up to this patch.\n495                 slice_buff.append(slice(source_idx, patch.source_slice.start))\n496 \n497             # Is this patch covering an area we've already covered?\n498             if patch.source_slice.start < source_idx:  # pragma: no cover\n499                 # NOTE: This shouldn't happen. With more detailed templating\n500                 # this shouldn't happen - but in the off-chance that this does\n501                 # happen - then this code path remains.\n502                 linter_logger.info(\n503                     \"Skipping overlapping patch at Index %s, Patch: %s\",\n504                     source_idx,\n505                     patch,\n506                 )\n507                 # Ignore the patch for now...\n508                 continue\n509 \n510             # Add this patch.\n511             slice_buff.append(patch.source_slice)\n512             source_idx = patch.source_slice.stop\n513         # Add a tail slice.\n514         if source_idx < len(raw_source_string):\n515             slice_buff.append(slice(source_idx, len(raw_source_string)))\n516 \n517         return slice_buff\n518 \n519     @staticmethod\n520     def _build_up_fixed_source_string(\n521         source_file_slices: List[slice],\n522         source_patches: List[FixPatch],\n523         raw_source_string: str,\n524     ) -> str:\n525         \"\"\"Use patches and raw file to fix the source file.\n526 \n527         This assumes that patches and slices have already\n528         been coordinated. If they haven't then this will\n529         fail because we rely on patches having a corresponding\n530         slice of exactly the right file in the list of file\n531         slices.\n532         \"\"\"\n533         # Iterate through the patches, building up the new string.\n534         str_buff = \"\"\n535         for source_slice in source_file_slices:\n536             # Is it one in the patch buffer:\n537             for patch in source_patches:\n538                 if patch.source_slice == source_slice:\n539                     # Use the patched version\n540                     linter_logger.debug(\n541                         \"%-30s    %s    %r > %r\",\n542                         f\"Appending {patch.patch_category} Patch:\",\n543                         patch.source_slice,\n544                         patch.source_str,\n545                         patch.fixed_raw,\n546                     )\n547                     str_buff += patch.fixed_raw\n548                     break\n549             else:\n550                 # Use the raw string\n551                 linter_logger.debug(\n552                     \"Appending Raw:                    %s     %r\",\n553                     source_slice,\n554                     raw_source_string[source_slice],\n555                 )\n556                 str_buff += raw_source_string[source_slice]\n557         return str_buff\n558 \n559     def persist_tree(self, suffix: str = \"\") -> bool:\n560         \"\"\"Persist changes to the given path.\"\"\"\n561         write_buff, success = self.fix_string()\n562 \n563         if success:\n564             fname = self.path\n565             # If there is a suffix specified, then use it.s\n566             if suffix:\n567                 root, ext = os.path.splitext(fname)\n568                 fname = root + suffix + ext\n569             self._safe_create_replace_file(self.path, fname, write_buff, self.encoding)\n570         return success\n571 \n572     @staticmethod\n573     def _safe_create_replace_file(\n574         input_path: str, output_path: str, write_buff: str, encoding: str\n575     ):\n576         # Write to a temporary file first, so in case of encoding or other\n577         # issues, we don't delete or corrupt the user's existing file.\n578 \n579         # Get file mode (i.e. permissions) on existing file. We'll preserve the\n580         # same permissions on the output file.\n581         mode = None\n582         try:\n583             status = os.stat(input_path)\n584         except FileNotFoundError:\n585             pass\n586         else:\n587             if stat.S_ISREG(status.st_mode):\n588                 mode = stat.S_IMODE(status.st_mode)\n589         dirname, basename = os.path.split(output_path)\n590         with tempfile.NamedTemporaryFile(\n591             mode=\"w\",\n592             encoding=encoding,\n593             newline=\"\",  # NOTE: No newline conversion. Write as read.\n594             prefix=basename,\n595             dir=dirname,\n596             suffix=os.path.splitext(output_path)[1],\n597             delete=False,\n598         ) as tmp:\n599             tmp.file.write(write_buff)\n600             tmp.flush()\n601             os.fsync(tmp.fileno())\n602         # Once the temp file is safely written, replace the existing file.\n603         if mode is not None:\n604             os.chmod(tmp.name, mode)\n605         shutil.move(tmp.name, output_path)\n606 \n[end of src/sqlfluff/core/linter/linted_file.py]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import (\n8     Any,\n9     Iterable,\n10     Iterator,\n11     List,\n12     Optional,\n13     Sequence,\n14     Set,\n15     Tuple,\n16     Type,\n17     Dict,\n18     cast,\n19 )\n20 \n21 import pathspec\n22 import regex\n23 from tqdm import tqdm\n24 \n25 from sqlfluff.core.errors import (\n26     SQLBaseError,\n27     SQLLexError,\n28     SQLLintError,\n29     SQLParseError,\n30     SQLFluffSkipFile,\n31     SQLFluffUserError,\n32 )\n33 from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n34 from sqlfluff.core.file_helpers import get_encoding\n35 from sqlfluff.core.templaters import TemplatedFile\n36 from sqlfluff.core.rules import get_ruleset\n37 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n38 \n39 # Classes needed only for type checking\n40 from sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n41 from sqlfluff.core.parser.segments.meta import MetaSegment\n42 from sqlfluff.core.parser.segments.raw import RawSegment\n43 from sqlfluff.core.rules import BaseRule, RulePack\n44 \n45 from sqlfluff.core.linter.common import (\n46     RuleTuple,\n47     ParsedString,\n48     NoQaDirective,\n49     RenderedFile,\n50 )\n51 from sqlfluff.core.linter.linted_file import LintedFile, FileTimings\n52 from sqlfluff.core.linter.linted_dir import LintedDir\n53 from sqlfluff.core.linter.linting_result import LintingResult\n54 \n55 \n56 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n57 RuleTimingsType = List[Tuple[str, str, float]]\n58 \n59 # Instantiate the linter logger\n60 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n61 \n62 \n63 class Linter:\n64     \"\"\"The interface class to interact with the linter.\"\"\"\n65 \n66     # Default to allowing process parallelism\n67     allow_process_parallelism = True\n68 \n69     def __init__(\n70         self,\n71         config: Optional[FluffConfig] = None,\n72         formatter: Any = None,\n73         dialect: Optional[str] = None,\n74         rules: Optional[List[str]] = None,\n75         user_rules: Optional[List[Type[BaseRule]]] = None,\n76         exclude_rules: Optional[List[str]] = None,\n77     ) -> None:\n78         # Store the config object\n79         self.config = FluffConfig.from_kwargs(\n80             config=config,\n81             dialect=dialect,\n82             rules=rules,\n83             exclude_rules=exclude_rules,\n84             # Don't require a dialect to be provided yet. Defer this until we\n85             # are actually linting something, since the directory we are linting\n86             # from may provide additional configuration, including a dialect.\n87             require_dialect=False,\n88         )\n89         # Get the dialect and templater\n90         self.dialect = self.config.get(\"dialect_obj\")\n91         self.templater = self.config.get(\"templater_obj\")\n92         # Store the formatter for output\n93         self.formatter = formatter\n94         # Store references to user rule classes\n95         self.user_rules = user_rules or []\n96 \n97     def get_rulepack(self, config: Optional[FluffConfig] = None) -> RulePack:\n98         \"\"\"Get hold of a set of rules.\"\"\"\n99         rs = get_ruleset()\n100         # Register any user rules\n101         for rule in self.user_rules:\n102             rs.register(rule)\n103         cfg = config or self.config\n104         return rs.get_rulepack(config=cfg)\n105 \n106     def rule_tuples(self) -> List[RuleTuple]:\n107         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n108         rs = self.get_rulepack()\n109         return [\n110             RuleTuple(rule.code, rule.name, rule.description, rule.groups, rule.aliases)\n111             for rule in rs.rules\n112         ]\n113 \n114     # #### Static methods\n115     # These are the building blocks of the linting process.\n116 \n117     @staticmethod\n118     def load_raw_file_and_config(\n119         fname: str, root_config: FluffConfig\n120     ) -> Tuple[str, FluffConfig, str]:\n121         \"\"\"Load a raw file and the associated config.\"\"\"\n122         file_config = root_config.make_child_from_path(fname)\n123         encoding = get_encoding(fname=fname, config=file_config)\n124         # Check file size before loading.\n125         limit = file_config.get(\"large_file_skip_byte_limit\")\n126         if limit:\n127             # Get the file size\n128             file_size = os.path.getsize(fname)\n129             if file_size > limit:\n130                 raise SQLFluffSkipFile(\n131                     f\"Length of file {fname!r} is {file_size} bytes which is over \"\n132                     f\"the limit of {limit} bytes. Skipping to avoid parser lock. \"\n133                     \"Users can increase this limit in their config by setting the \"\n134                     \"'large_file_skip_byte_limit' value, or disable by setting it \"\n135                     \"to zero.\"\n136                 )\n137         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n138             raw_file = target_file.read()\n139         # Scan the raw file for config commands.\n140         file_config.process_raw_file_for_config(raw_file)\n141         # Return the raw file and config\n142         return raw_file, file_config, encoding\n143 \n144     @staticmethod\n145     def _normalise_newlines(string: str) -> str:\n146         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n147         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n148 \n149     @staticmethod\n150     def _lex_templated_file(\n151         templated_file: TemplatedFile, config: FluffConfig\n152     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n153         \"\"\"Lex a templated file.\n154 \n155         NOTE: This potentially mutates the config, so make sure to\n156         use the returned one.\n157         \"\"\"\n158         violations = []\n159         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n160         # Get the lexer\n161         lexer = Lexer(config=config)\n162         # Lex the file and log any problems\n163         try:\n164             tokens, lex_vs = lexer.lex(templated_file)\n165             # We might just get the violations as a list\n166             violations += lex_vs\n167             linter_logger.info(\n168                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n169             )\n170         except SQLLexError as err:  # pragma: no cover\n171             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n172             violations.append(err)\n173             return None, violations, config\n174 \n175         if not tokens:  # pragma: no cover TODO?\n176             return None, violations, config\n177 \n178         # Check that we've got sensible indentation from the lexer.\n179         # We might need to suppress if it's a complicated file.\n180         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n181         if isinstance(templating_blocks_indent, str):\n182             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n183         else:\n184             force_block_indent = False\n185         templating_blocks_indent = bool(templating_blocks_indent)\n186         # If we're forcing it through we don't check.\n187         if templating_blocks_indent and not force_block_indent:\n188             indent_balance = sum(\n189                 getattr(elem, \"indent_val\", 0)\n190                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n191             )\n192             if indent_balance != 0:  # pragma: no cover\n193                 linter_logger.debug(\n194                     \"Indent balance test failed for %r. Template indents will not be \"\n195                     \"linted for this file.\",\n196                     templated_file.fname,\n197                 )\n198                 # Don't enable the templating blocks.\n199                 templating_blocks_indent = False\n200 \n201         # The file will have been lexed without config, so check all indents\n202         # are enabled.\n203         new_tokens = []\n204         for token in cast(Tuple[BaseSegment, ...], tokens):\n205             if token.is_meta:\n206                 token = cast(MetaSegment, token)\n207                 if token.indent_val != 0:\n208                     # Don't allow it if we're not linting templating block indents.\n209                     if not templating_blocks_indent:\n210                         continue  # pragma: no cover\n211             new_tokens.append(token)\n212 \n213         # Return new buffer\n214         return new_tokens, violations, config\n215 \n216     @staticmethod\n217     def _parse_tokens(\n218         tokens: Sequence[BaseSegment],\n219         config: FluffConfig,\n220         recurse: bool = True,\n221         fname: Optional[str] = None,\n222     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n223         parser = Parser(config=config)\n224         violations = []\n225         # Parse the file and log any problems\n226         try:\n227             parsed: Optional[BaseSegment] = parser.parse(\n228                 tokens,\n229                 recurse=recurse,\n230                 fname=fname,\n231             )\n232         except SQLParseError as err:\n233             linter_logger.info(\"PARSING FAILED! : %s\", err)\n234             violations.append(err)\n235             return None, violations\n236 \n237         if parsed:\n238             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n239             linter_logger.info(\"\\n\" + parsed.stringify())\n240             # We may succeed parsing, but still have unparsable segments. Extract them\n241             # here.\n242             for unparsable in parsed.iter_unparsables():\n243                 # No exception has been raised explicitly, but we still create one here\n244                 # so that we can use the common interface\n245                 violations.append(\n246                     SQLParseError(\n247                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n248                         \"{1!r}\".format(\n249                             unparsable.pos_marker.working_loc,\n250                             unparsable.raw\n251                             if len(unparsable.raw) < 40\n252                             else unparsable.raw[:40] + \"...\",\n253                         ),\n254                         segment=unparsable,\n255                     )\n256                 )\n257                 linter_logger.info(\"Found unparsable segment...\")\n258                 linter_logger.info(unparsable.stringify())\n259         return parsed, violations\n260 \n261     @staticmethod\n262     def parse_noqa(\n263         comment: str,\n264         line_no: int,\n265         reference_map: Dict[str, Set[str]],\n266     ):\n267         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n268         # Also trim any whitespace afterward\n269 \n270         # Comment lines can also have noqa e.g.\n271         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: LT05\n272         # Therefore extract last possible inline ignore.\n273         comment = [c.strip() for c in comment.split(\"--\")][-1]\n274 \n275         if comment.startswith(\"noqa\"):\n276             # This is an ignore identifier\n277             comment_remainder = comment[4:]\n278             if comment_remainder:\n279                 if not comment_remainder.startswith(\":\"):\n280                     return SQLParseError(\n281                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n282                         line_no=line_no,\n283                     )\n284                 comment_remainder = comment_remainder[1:].strip()\n285                 if comment_remainder:\n286                     action: Optional[str]\n287                     if \"=\" in comment_remainder:\n288                         action, rule_part = comment_remainder.split(\"=\", 1)\n289                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n290                             return SQLParseError(\n291                                 \"Malformed 'noqa' section. \"\n292                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n293                                 \"or 'noqa: disable=<rule>[,...] | all\",\n294                                 line_no=line_no,\n295                             )\n296                     else:\n297                         action = None\n298                         rule_part = comment_remainder\n299                         if rule_part in {\"disable\", \"enable\"}:\n300                             return SQLParseError(\n301                                 \"Malformed 'noqa' section. \"\n302                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n303                                 \"or 'noqa: disable=<rule>[,...] | all\",\n304                                 line_no=line_no,\n305                             )\n306                     rules: Optional[Tuple[str, ...]]\n307                     if rule_part != \"all\":\n308                         # Rules can be globs therefore we compare to the rule_set to\n309                         # expand the globs.\n310                         unexpanded_rules = tuple(\n311                             r.strip() for r in rule_part.split(\",\")\n312                         )\n313                         # We use a set to do natural deduplication.\n314                         expanded_rules: Set[str] = set()\n315                         for r in unexpanded_rules:\n316                             matched = False\n317                             for expanded in (\n318                                 reference_map[x]\n319                                 for x in fnmatch.filter(reference_map.keys(), r)\n320                             ):\n321                                 expanded_rules |= expanded\n322                                 matched = True\n323 \n324                             if not matched:\n325                                 # We were unable to expand the glob.\n326                                 # Therefore assume the user is referencing\n327                                 # a special error type (e.g. PRS, LXR, or TMP)\n328                                 # and add this to the list of rules to ignore.\n329                                 expanded_rules.add(r)\n330                         # Sort for consistency\n331                         rules = tuple(sorted(expanded_rules))\n332                     else:\n333                         rules = None\n334                     return NoQaDirective(line_no, rules, action)\n335             return NoQaDirective(line_no, None, None)\n336         return None\n337 \n338     @staticmethod\n339     def remove_templated_errors(\n340         linting_errors: List[SQLBaseError],\n341     ) -> List[SQLBaseError]:\n342         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n343         # Filter out any linting errors in templated sections if relevant.\n344         result: List[SQLBaseError] = []\n345         for e in linting_errors:\n346             if isinstance(e, SQLLintError):\n347                 if (\n348                     # Is it in a literal section?\n349                     e.segment.pos_marker.is_literal()\n350                     # Is it a rule that is designed to work on templated sections?\n351                     or e.rule.targets_templated\n352                 ):\n353                     result.append(e)\n354             else:\n355                 # If it's another type, just keep it. (E.g. SQLParseError from\n356                 # malformed \"noqa\" comment).\n357                 result.append(e)\n358         return result\n359 \n360     @staticmethod\n361     def _report_conflicting_fixes_same_anchor(message: str):  # pragma: no cover\n362         # This function exists primarily in order to let us monkeypatch it at\n363         # runtime (replacing it with a function that raises an exception).\n364         linter_logger.critical(message)\n365 \n366     @staticmethod\n367     def _warn_unfixable(code: str):\n368         linter_logger.warning(\n369             f\"One fix for {code} not applied, it would re-cause the same error.\"\n370         )\n371 \n372     # ### Class Methods\n373     # These compose the base static methods into useful recipes.\n374 \n375     @classmethod\n376     def parse_rendered(\n377         cls,\n378         rendered: RenderedFile,\n379         recurse: bool = True,\n380     ) -> ParsedString:\n381         \"\"\"Parse a rendered file.\"\"\"\n382         t0 = time.monotonic()\n383         violations = cast(List[SQLBaseError], rendered.templater_violations)\n384         tokens: Optional[Sequence[BaseSegment]]\n385         if rendered.templated_file:\n386             tokens, lvs, config = cls._lex_templated_file(\n387                 rendered.templated_file, rendered.config\n388             )\n389             violations += lvs\n390         else:\n391             tokens = None\n392 \n393         t1 = time.monotonic()\n394         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n395 \n396         if tokens:\n397             parsed, pvs = cls._parse_tokens(\n398                 tokens,\n399                 rendered.config,\n400                 recurse=recurse,\n401                 fname=rendered.fname,\n402             )\n403             violations += pvs\n404         else:\n405             parsed = None\n406 \n407         time_dict = {\n408             **rendered.time_dict,\n409             \"lexing\": t1 - t0,\n410             \"parsing\": time.monotonic() - t1,\n411         }\n412         return ParsedString(\n413             parsed,\n414             violations,\n415             time_dict,\n416             rendered.templated_file,\n417             rendered.config,\n418             rendered.fname,\n419             rendered.source_str,\n420         )\n421 \n422     @classmethod\n423     def extract_ignore_from_comment(\n424         cls,\n425         comment: RawSegment,\n426         reference_map: Dict[str, Set[str]],\n427     ):\n428         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n429         # Also trim any whitespace afterward\n430         comment_content = comment.raw_trimmed().strip()\n431         comment_line, _ = comment.pos_marker.source_position()\n432         result = cls.parse_noqa(comment_content, comment_line, reference_map)\n433         if isinstance(result, SQLParseError):\n434             result.segment = comment\n435         return result\n436 \n437     @classmethod\n438     def extract_ignore_mask_tree(\n439         cls,\n440         tree: BaseSegment,\n441         reference_map: Dict[str, Set[str]],\n442     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n443         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n444         ignore_buff: List[NoQaDirective] = []\n445         violations: List[SQLBaseError] = []\n446         for comment in tree.recursive_crawl(\"comment\"):\n447             if comment.is_type(\"inline_comment\"):\n448                 ignore_entry = cls.extract_ignore_from_comment(comment, reference_map)\n449                 if isinstance(ignore_entry, SQLParseError):\n450                     violations.append(ignore_entry)\n451                 elif ignore_entry:\n452                     ignore_buff.append(ignore_entry)\n453         if ignore_buff:\n454             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n455         return ignore_buff, violations\n456 \n457     @classmethod\n458     def extract_ignore_mask_source(\n459         cls,\n460         source: str,\n461         inline_comment_regex: RegexLexer,\n462         reference_map: Dict[str, Set[str]],\n463     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n464         \"\"\"Look for inline ignore comments and return NoQaDirectives.\n465 \n466         Very similar to extract_ignore_mask_tree(), but can be run on raw source\n467         (i.e. does not require the code to have parsed successfully).\n468         \"\"\"\n469         ignore_buff: List[NoQaDirective] = []\n470         violations: List[SQLBaseError] = []\n471         for idx, line in enumerate(source.split(\"\\n\")):\n472             match = inline_comment_regex.search(line) if line else None\n473             if match:\n474                 ignore_entry = cls.parse_noqa(\n475                     line[match[0] : match[1]], idx + 1, reference_map\n476                 )\n477                 if isinstance(ignore_entry, SQLParseError):\n478                     violations.append(ignore_entry)  # pragma: no cover\n479                 elif ignore_entry:\n480                     ignore_buff.append(ignore_entry)\n481         if ignore_buff:\n482             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n483         return ignore_buff, violations\n484 \n485     @classmethod\n486     def lint_fix_parsed(\n487         cls,\n488         tree: BaseSegment,\n489         config: FluffConfig,\n490         rule_pack: RulePack,\n491         fix: bool = False,\n492         fname: Optional[str] = None,\n493         templated_file: Optional[TemplatedFile] = None,\n494         formatter: Any = None,\n495     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective], RuleTimingsType]:\n496         \"\"\"Lint and optionally fix a tree object.\"\"\"\n497         # Keep track of the linting errors on the very first linter pass. The\n498         # list of issues output by \"lint\" and \"fix\" only includes issues present\n499         # in the initial SQL code, EXCLUDING any issues that may be created by\n500         # the fixes themselves.\n501         initial_linting_errors = []\n502         # A placeholder for the fixes we had on the previous loop\n503         last_fixes = None\n504         # Keep a set of previous versions to catch infinite loops.\n505         previous_versions: Set[Tuple[str, Tuple[SourceFix, ...]]] = {(tree.raw, ())}\n506         # Keep a buffer for recording rule timings.\n507         rule_timings: RuleTimingsType = []\n508 \n509         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n510         # once for linting.\n511         loop_limit = config.get(\"runaway_limit\") if fix else 1\n512 \n513         # Dispatch the output for the lint header\n514         if formatter:\n515             formatter.dispatch_lint_header(fname, sorted(rule_pack.codes()))\n516 \n517         # Look for comment segments which might indicate lines to ignore.\n518         if not config.get(\"disable_noqa\"):\n519             ignore_buff, ivs = cls.extract_ignore_mask_tree(\n520                 tree, rule_pack.reference_map\n521             )\n522             initial_linting_errors += ivs\n523         else:\n524             ignore_buff = []\n525 \n526         save_tree = tree\n527         # There are two phases of rule running.\n528         # 1. The main loop is for most rules. These rules are assumed to\n529         # interact and cause a cascade of fixes requiring multiple passes.\n530         # These are run the `runaway_limit` number of times (default 10).\n531         # 2. The post loop is for post-processing rules, not expected to trigger\n532         # any downstream rules, e.g. capitalization fixes. They are run on the\n533         # first loop and then twice at the end (once to fix, and once again to\n534         # check result of fixes), but not in the intervening loops.\n535         phases = [\"main\"]\n536         if fix:\n537             phases.append(\"post\")\n538         for phase in phases:\n539             if len(phases) > 1:\n540                 rules_this_phase = [\n541                     rule for rule in rule_pack.rules if rule.lint_phase == phase\n542                 ]\n543             else:\n544                 rules_this_phase = rule_pack.rules\n545             for loop in range(loop_limit if phase == \"main\" else 2):\n546 \n547                 def is_first_linter_pass():\n548                     return phase == phases[0] and loop == 0\n549 \n550                 # Additional newlines are to assist in scanning linting loops\n551                 # during debugging.\n552                 linter_logger.info(\n553                     f\"\\n\\nEntering linter phase {phase}, loop {loop+1}/{loop_limit}\\n\"\n554                 )\n555                 changed = False\n556 \n557                 if is_first_linter_pass():\n558                     # In order to compute initial_linting_errors correctly, need\n559                     # to run all rules on the first loop of the main phase.\n560                     rules_this_phase = rule_pack.rules\n561                 progress_bar_crawler = tqdm(\n562                     rules_this_phase,\n563                     desc=\"lint by rules\",\n564                     leave=False,\n565                     disable=progress_bar_configuration.disable_progress_bar,\n566                 )\n567 \n568                 for crawler in progress_bar_crawler:\n569                     # Performance: After first loop pass, skip rules that don't\n570                     # do fixes. Any results returned won't be seen by the user\n571                     # anyway (linting errors ADDED by rules changing SQL, are\n572                     # not reported back to the user - only initial linting errors),\n573                     # so there's absolutely no reason to run them.\n574                     if (\n575                         fix\n576                         and not is_first_linter_pass()\n577                         and not crawler.is_fix_compatible\n578                     ):\n579                         continue\n580 \n581                     progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n582                     t0 = time.monotonic()\n583 \n584                     # fixes should be a dict {} with keys edit, delete, create\n585                     # delete is just a list of segments to delete\n586                     # edit and create are list of tuples. The first element is\n587                     # the \"anchor\", the segment to look for either to edit or to\n588                     # insert BEFORE. The second is the element to insert or create.\n589                     linting_errors, _, fixes, _ = crawler.crawl(\n590                         tree,\n591                         dialect=config.get(\"dialect_obj\"),\n592                         fix=fix,\n593                         templated_file=templated_file,\n594                         ignore_mask=ignore_buff,\n595                         fname=fname,\n596                         config=config,\n597                     )\n598                     if is_first_linter_pass():\n599                         initial_linting_errors += linting_errors\n600 \n601                     if fix and fixes:\n602                         linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n603                         # Do some sanity checks on the fixes before applying.\n604                         anchor_info = BaseSegment.compute_anchor_edit_info(fixes)\n605                         if any(\n606                             not info.is_valid for info in anchor_info.values()\n607                         ):  # pragma: no cover\n608                             message = (\n609                                 f\"Rule {crawler.code} returned conflicting \"\n610                                 \"fixes with the same anchor. This is only \"\n611                                 \"supported for create_before+create_after, so \"\n612                                 \"the fixes will not be applied. \"\n613                             )\n614                             for uuid, info in anchor_info.items():\n615                                 if not info.is_valid:\n616                                     message += f\"\\n{uuid}:\"\n617                                     for fix in info.fixes:\n618                                         message += f\"\\n    {fix}\"\n619                             cls._report_conflicting_fixes_same_anchor(message)\n620                             for lint_result in linting_errors:\n621                                 lint_result.fixes = []\n622                         elif fixes == last_fixes:  # pragma: no cover\n623                             # If we generate the same fixes two times in a row,\n624                             # that means we're in a loop, and we want to stop.\n625                             # (Fixes should address issues, hence different\n626                             # and/or fewer fixes next time.)\n627                             cls._warn_unfixable(crawler.code)\n628                         else:\n629                             # This is the happy path. We have fixes, now we want to\n630                             # apply them.\n631                             last_fixes = fixes\n632                             new_tree, _, _ = tree.apply_fixes(\n633                                 config.get(\"dialect_obj\"), crawler.code, anchor_info\n634                             )\n635                             # Check for infinite loops. We use a combination of the\n636                             # fixed templated file and the list of source fixes to\n637                             # apply.\n638                             loop_check_tuple = (\n639                                 new_tree.raw,\n640                                 tuple(new_tree.source_fixes),\n641                             )\n642                             if loop_check_tuple not in previous_versions:\n643                                 # We've not seen this version of the file so\n644                                 # far. Continue.\n645                                 tree = new_tree\n646                                 previous_versions.add(loop_check_tuple)\n647                                 changed = True\n648                                 continue\n649                             else:\n650                                 # Applying these fixes took us back to a state\n651                                 # which we've seen before. We're in a loop, so\n652                                 # we want to stop.\n653                                 cls._warn_unfixable(crawler.code)\n654 \n655                     # Record rule timing\n656                     rule_timings.append(\n657                         (crawler.code, crawler.name, time.monotonic() - t0)\n658                     )\n659 \n660                 if fix and not changed:\n661                     # We did not change the file. Either the file is clean (no\n662                     # fixes), or any fixes which are present will take us back\n663                     # to a previous state.\n664                     linter_logger.info(\n665                         f\"Fix loop complete for {phase} phase. Stability \"\n666                         f\"achieved after {loop}/{loop_limit} loops.\"\n667                     )\n668                     break\n669             else:\n670                 if fix:\n671                     # The linter loop hit the limit before reaching a stable point\n672                     # (i.e. free of lint errors). If this happens, it's usually\n673                     # because one or more rules produced fixes which did not address\n674                     # the original issue **or** created new issues.\n675                     linter_logger.warning(\n676                         f\"Loop limit on fixes reached [{loop_limit}].\"\n677                     )\n678 \n679                     # Discard any fixes for the linting errors, since they caused a\n680                     # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n681                     # these linting errors are \"unfixable\". This is important,\n682                     # because when \"sqlfluff fix\" encounters unfixable lint errors,\n683                     # it exits with a \"failure\" exit code, which is exactly what we\n684                     # want in this situation. (Reason: Although this is more of an\n685                     # internal SQLFluff issue, users deserve to know about it,\n686                     # because it means their file(s) weren't fixed.\n687                     for violation in initial_linting_errors:\n688                         if isinstance(violation, SQLLintError):\n689                             violation.fixes = []\n690 \n691                     # Return the original parse tree, before any fixes were applied.\n692                     # Reason: When the linter hits the loop limit, the file is often\n693                     # messy, e.g. some of the fixes were applied repeatedly, possibly\n694                     # other weird things. We don't want the user to see this junk!\n695                     return save_tree, initial_linting_errors, ignore_buff, rule_timings\n696 \n697         if config.get(\"ignore_templated_areas\", default=True):\n698             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n699 \n700         linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Fixed Tree:\"))\n701         linter_logger.info(\"\\n\" + tree.stringify())\n702 \n703         return tree, initial_linting_errors, ignore_buff, rule_timings\n704 \n705     @classmethod\n706     def lint_parsed(\n707         cls,\n708         parsed: ParsedString,\n709         rule_pack: RulePack,\n710         fix: bool = False,\n711         formatter: Any = None,\n712         encoding: str = \"utf8\",\n713     ):\n714         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n715         violations = parsed.violations\n716         time_dict = parsed.time_dict\n717         tree: Optional[BaseSegment]\n718         if parsed.tree:\n719             t0 = time.monotonic()\n720             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n721             (\n722                 tree,\n723                 initial_linting_errors,\n724                 ignore_buff,\n725                 rule_timings,\n726             ) = cls.lint_fix_parsed(\n727                 parsed.tree,\n728                 config=parsed.config,\n729                 rule_pack=rule_pack,\n730                 fix=fix,\n731                 fname=parsed.fname,\n732                 templated_file=parsed.templated_file,\n733                 formatter=formatter,\n734             )\n735             # Update the timing dict\n736             time_dict[\"linting\"] = time.monotonic() - t0\n737 \n738             # We're only going to return the *initial* errors, rather\n739             # than any generated during the fixing cycle.\n740             violations += initial_linting_errors\n741         else:\n742             # If no parsed tree, set to None\n743             tree = None\n744             ignore_buff = []\n745             rule_timings = []\n746             if not parsed.config.get(\"disable_noqa\"):\n747                 # Templating and/or parsing have failed. Look for \"noqa\"\n748                 # comments (the normal path for identifying these comments\n749                 # requires access to the parse tree, and because of the failure,\n750                 # we don't have a parse tree).\n751                 ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n752                     parsed.source_str,\n753                     [\n754                         lm\n755                         for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n756                         if lm.name == \"inline_comment\"\n757                     ][0],\n758                     rule_pack.reference_map,\n759                 )\n760                 violations += ignore_violations\n761 \n762         # We process the ignore config here if appropriate\n763         for violation in violations:\n764             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n765             violation.warning_if_in(parsed.config.get(\"warnings\"))\n766 \n767         linted_file = LintedFile(\n768             parsed.fname,\n769             # Deduplicate violations\n770             LintedFile.deduplicate_in_source_space(violations),\n771             FileTimings(time_dict, rule_timings),\n772             tree,\n773             ignore_mask=ignore_buff,\n774             templated_file=parsed.templated_file,\n775             encoding=encoding,\n776         )\n777 \n778         # This is the main command line output from linting.\n779         if formatter:\n780             formatter.dispatch_file_violations(\n781                 parsed.fname, linted_file, only_fixable=fix\n782             )\n783 \n784         # Safety flag for unset dialects\n785         if linted_file.get_violations(\n786             fixable=True if fix else None, types=SQLParseError\n787         ):\n788             if formatter:  # pragma: no cover TODO?\n789                 formatter.dispatch_dialect_warning(parsed.config.get(\"dialect\"))\n790 \n791         return linted_file\n792 \n793     @classmethod\n794     def lint_rendered(\n795         cls,\n796         rendered: RenderedFile,\n797         rule_pack: RulePack,\n798         fix: bool = False,\n799         formatter: Any = None,\n800     ) -> LintedFile:\n801         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n802         parsed = cls.parse_rendered(rendered)\n803         return cls.lint_parsed(\n804             parsed,\n805             rule_pack=rule_pack,\n806             fix=fix,\n807             formatter=formatter,\n808             encoding=rendered.encoding,\n809         )\n810 \n811     # ### Instance Methods\n812     # These are tied to a specific instance and so are not necessarily\n813     # safe to use in parallel operations.\n814 \n815     def render_string(\n816         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n817     ) -> RenderedFile:\n818         \"\"\"Template the file.\"\"\"\n819         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n820 \n821         # Start the templating timer\n822         t0 = time.monotonic()\n823 \n824         # Newlines are normalised to unix-style line endings (\\n).\n825         # The motivation is that Jinja normalises newlines during templating and\n826         # we want consistent mapping between the raw and templated slices.\n827         in_str = self._normalise_newlines(in_str)\n828 \n829         # Since Linter.__init__() does not require a dialect to be specified,\n830         # check for one now. (We're processing a string, not a file, so we're\n831         # not going to pick up a .sqlfluff or other config file to provide a\n832         # missing dialect at this point.)\n833         config.verify_dialect_specified()\n834         if not config.get(\"templater_obj\") == self.templater:\n835             linter_logger.warning(\n836                 (\n837                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n838                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n839                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n840                     \"working directory. It can be set in a .sqlfluff in the current \"\n841                     \"working directory. See Nesting section of the docs for more \"\n842                     \"details.\"\n843                 )\n844             )\n845         try:\n846             templated_file, templater_violations = self.templater.process(\n847                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n848             )\n849         except SQLFluffSkipFile as s:  # pragma: no cover\n850             linter_logger.warning(str(s))\n851             templated_file = None\n852             templater_violations = []\n853 \n854         if not templated_file:\n855             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n856 \n857         # Record time\n858         time_dict = {\"templating\": time.monotonic() - t0}\n859 \n860         return RenderedFile(\n861             templated_file,\n862             templater_violations,\n863             config,\n864             time_dict,\n865             fname,\n866             encoding,\n867             in_str,\n868         )\n869 \n870     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n871         \"\"\"Load and render a file with relevant config.\"\"\"\n872         # Load the raw file.\n873         raw_file, config, encoding = self.load_raw_file_and_config(fname, root_config)\n874         # Render the file\n875         return self.render_string(raw_file, fname, config, encoding)\n876 \n877     def parse_string(\n878         self,\n879         in_str: str,\n880         fname: str = \"<string>\",\n881         recurse: bool = True,\n882         config: Optional[FluffConfig] = None,\n883         encoding: str = \"utf-8\",\n884     ) -> ParsedString:\n885         \"\"\"Parse a string.\"\"\"\n886         violations: List[SQLBaseError] = []\n887 \n888         # Dispatch the output for the template header (including the config diff)\n889         if self.formatter:\n890             self.formatter.dispatch_template_header(fname, self.config, config)\n891 \n892         # Just use the local config from here:\n893         config = config or self.config\n894 \n895         # Scan the raw file for config commands.\n896         config.process_raw_file_for_config(in_str)\n897         rendered = self.render_string(in_str, fname, config, encoding)\n898         violations += rendered.templater_violations\n899 \n900         # Dispatch the output for the parse header\n901         if self.formatter:\n902             self.formatter.dispatch_parse_header(fname)\n903 \n904         return self.parse_rendered(rendered, recurse=recurse)\n905 \n906     def fix(\n907         self,\n908         tree: BaseSegment,\n909         config: Optional[FluffConfig] = None,\n910         fname: Optional[str] = None,\n911         templated_file: Optional[TemplatedFile] = None,\n912     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n913         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n914         config = config or self.config\n915         rule_pack = self.get_rulepack(config=config)\n916         fixed_tree, violations, _, _ = self.lint_fix_parsed(\n917             tree,\n918             config,\n919             rule_pack,\n920             fix=True,\n921             fname=fname,\n922             templated_file=templated_file,\n923             formatter=self.formatter,\n924         )\n925         return fixed_tree, violations\n926 \n927     def lint(\n928         self,\n929         tree: BaseSegment,\n930         config: Optional[FluffConfig] = None,\n931         fname: Optional[str] = None,\n932         templated_file: Optional[TemplatedFile] = None,\n933     ) -> List[SQLBaseError]:\n934         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n935         config = config or self.config\n936         rule_pack = self.get_rulepack(config=config)\n937         _, violations, _, _ = self.lint_fix_parsed(\n938             tree,\n939             config,\n940             rule_pack,\n941             fix=False,\n942             fname=fname,\n943             templated_file=templated_file,\n944             formatter=self.formatter,\n945         )\n946         return violations\n947 \n948     def lint_string(\n949         self,\n950         in_str: str = \"\",\n951         fname: str = \"<string input>\",\n952         fix: bool = False,\n953         config: Optional[FluffConfig] = None,\n954         encoding: str = \"utf8\",\n955     ) -> LintedFile:\n956         \"\"\"Lint a string.\n957 \n958         Returns:\n959             :obj:`LintedFile`: an object representing that linted file.\n960 \n961         \"\"\"\n962         # Sort out config, defaulting to the built in config if no override\n963         config = config or self.config\n964         # Parse the string.\n965         parsed = self.parse_string(\n966             in_str=in_str,\n967             fname=fname,\n968             config=config,\n969         )\n970         # Get rules as appropriate\n971         rule_pack = self.get_rulepack(config=config)\n972         # Lint the file and return the LintedFile\n973         return self.lint_parsed(\n974             parsed,\n975             rule_pack,\n976             fix=fix,\n977             formatter=self.formatter,\n978             encoding=encoding,\n979         )\n980 \n981     def paths_from_path(\n982         self,\n983         path: str,\n984         ignore_file_name: str = \".sqlfluffignore\",\n985         ignore_non_existent_files: bool = False,\n986         ignore_files: bool = True,\n987         working_path: str = os.getcwd(),\n988     ) -> List[str]:\n989         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n990 \n991         Here we also deal with the .sqlfluffignore file if present.\n992 \n993         When a path to a file to be linted is explicitly passed\n994         we look for ignore files in all directories that are parents of the file,\n995         up to the current directory.\n996 \n997         If the current directory is not a parent of the file we only\n998         look for an ignore file in the direct parent of the file.\n999 \n1000         \"\"\"\n1001         if not os.path.exists(path):\n1002             if ignore_non_existent_files:\n1003                 return []\n1004             else:\n1005                 raise SQLFluffUserError(\n1006                     f\"Specified path does not exist. Check it/they exist(s): {path}.\"\n1007                 )\n1008 \n1009         # Files referred to exactly are also ignored if\n1010         # matched, but we warn the users when that happens\n1011         is_exact_file = os.path.isfile(path)\n1012 \n1013         path_walk: WalkableType\n1014         if is_exact_file:\n1015             # When the exact file to lint is passed, we\n1016             # fill path_walk with an input that follows\n1017             # the structure of `os.walk`:\n1018             #   (root, directories, files)\n1019             dirpath = os.path.dirname(path)\n1020             files = [os.path.basename(path)]\n1021             path_walk = [(dirpath, None, files)]\n1022         else:\n1023             path_walk = list(os.walk(path))\n1024 \n1025         ignore_file_paths = ConfigLoader.find_ignore_config_files(\n1026             path=path, working_path=working_path, ignore_file_name=ignore_file_name\n1027         )\n1028         # Add paths that could contain \"ignore files\"\n1029         # to the path_walk list\n1030         path_walk_ignore_file = [\n1031             (\n1032                 os.path.dirname(ignore_file_path),\n1033                 None,\n1034                 # Only one possible file, since we only\n1035                 # have one \"ignore file name\"\n1036                 [os.path.basename(ignore_file_path)],\n1037             )\n1038             for ignore_file_path in ignore_file_paths\n1039         ]\n1040         path_walk += path_walk_ignore_file\n1041 \n1042         # If it's a directory then expand the path!\n1043         buffer = []\n1044         ignores = {}\n1045         for dirpath, _, filenames in path_walk:\n1046             for fname in filenames:\n1047                 fpath = os.path.join(dirpath, fname)\n1048                 # Handle potential .sqlfluffignore files\n1049                 if ignore_files and fname == ignore_file_name:\n1050                     with open(fpath) as fh:\n1051                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n1052                         ignores[dirpath] = spec\n1053                     # We don't need to process the ignore file any further\n1054                     continue\n1055 \n1056                 # We won't purge files *here* because there's an edge case\n1057                 # that the ignore file is processed after the sql file.\n1058 \n1059                 # Scan for remaining files\n1060                 for ext in (\n1061                     self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n1062                 ):\n1063                     # is it a sql file?\n1064                     if fname.lower().endswith(ext):\n1065                         buffer.append(fpath)\n1066 \n1067         if not ignore_files:\n1068             return sorted(buffer)\n1069 \n1070         # Check the buffer for ignore items and normalise the rest.\n1071         # It's a set, so we can do natural deduplication.\n1072         filtered_buffer = set()\n1073 \n1074         for fpath in buffer:\n1075             abs_fpath = os.path.abspath(fpath)\n1076             for ignore_base, ignore_spec in ignores.items():\n1077                 abs_ignore_base = os.path.abspath(ignore_base)\n1078                 if abs_fpath.startswith(\n1079                     abs_ignore_base\n1080                     + (\n1081                         \"\"\n1082                         if os.path.dirname(abs_ignore_base) == abs_ignore_base\n1083                         else os.sep\n1084                     )\n1085                 ) and ignore_spec.match_file(\n1086                     os.path.relpath(abs_fpath, abs_ignore_base)\n1087                 ):\n1088                     # This file is ignored, skip it.\n1089                     if is_exact_file:\n1090                         linter_logger.warning(\n1091                             \"Exact file path %s was given but \"\n1092                             \"it was ignored by a %s pattern in %s, \"\n1093                             \"re-run with `--disregard-sqlfluffignores` to \"\n1094                             \"skip %s\"\n1095                             % (\n1096                                 path,\n1097                                 ignore_file_name,\n1098                                 ignore_base,\n1099                                 ignore_file_name,\n1100                             )\n1101                         )\n1102                     break\n1103             else:\n1104                 npath = os.path.normpath(fpath)\n1105                 # For debugging, log if we already have the file.\n1106                 if npath in filtered_buffer:\n1107                     linter_logger.debug(  # pragma: no cover\n1108                         \"Developer Warning: Path crawler attempted to \"\n1109                         \"requeue the same file twice. %s is already in \"\n1110                         \"filtered buffer.\",\n1111                         npath,\n1112                     )\n1113                 filtered_buffer.add(npath)\n1114 \n1115         # Return a sorted list\n1116         return sorted(filtered_buffer)\n1117 \n1118     def lint_string_wrapped(\n1119         self,\n1120         string: str,\n1121         fname: str = \"<string input>\",\n1122         fix: bool = False,\n1123     ) -> LintingResult:\n1124         \"\"\"Lint strings directly.\"\"\"\n1125         result = LintingResult()\n1126         linted_path = LintedDir(fname)\n1127         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n1128         result.add(linted_path)\n1129         result.stop_timer()\n1130         return result\n1131 \n1132     def lint_path(\n1133         self,\n1134         path: str,\n1135         fix: bool = False,\n1136         ignore_non_existent_files: bool = False,\n1137         ignore_files: bool = True,\n1138         processes: Optional[int] = None,\n1139     ) -> LintedDir:\n1140         \"\"\"Lint a path.\"\"\"\n1141         return self.lint_paths(\n1142             (path,), fix, ignore_non_existent_files, ignore_files, processes\n1143         ).paths[0]\n1144 \n1145     def lint_paths(\n1146         self,\n1147         paths: Tuple[str, ...],\n1148         fix: bool = False,\n1149         ignore_non_existent_files: bool = False,\n1150         ignore_files: bool = True,\n1151         processes: Optional[int] = None,\n1152     ) -> LintingResult:\n1153         \"\"\"Lint an iterable of paths.\"\"\"\n1154         # If no paths specified - assume local\n1155         if not paths:  # pragma: no cover\n1156             paths = (os.getcwd(),)\n1157         # Set up the result to hold what we get back\n1158         result = LintingResult()\n1159 \n1160         expanded_paths: List[str] = []\n1161         expanded_path_to_linted_dir = {}\n1162         for path in paths:\n1163             linted_dir = LintedDir(path)\n1164             result.add(linted_dir)\n1165             for fname in self.paths_from_path(\n1166                 path,\n1167                 ignore_non_existent_files=ignore_non_existent_files,\n1168                 ignore_files=ignore_files,\n1169             ):\n1170                 expanded_paths.append(fname)\n1171                 expanded_path_to_linted_dir[fname] = linted_dir\n1172 \n1173         files_count = len(expanded_paths)\n1174         if processes is None:\n1175             processes = self.config.get(\"processes\", default=1)\n1176 \n1177         # to avoid circular import\n1178         from sqlfluff.core.linter.runner import get_runner\n1179 \n1180         runner, effective_processes = get_runner(\n1181             self,\n1182             self.config,\n1183             processes=processes,\n1184             allow_process_parallelism=self.allow_process_parallelism,\n1185         )\n1186 \n1187         if self.formatter and effective_processes != 1:\n1188             self.formatter.dispatch_processing_header(effective_processes)\n1189 \n1190         # Show files progress bar only when there is more than one.\n1191         first_path = expanded_paths[0] if expanded_paths else \"\"\n1192         progress_bar_files = tqdm(\n1193             total=files_count,\n1194             desc=f\"file {first_path}\",\n1195             leave=False,\n1196             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1197         )\n1198 \n1199         for i, linted_file in enumerate(runner.run(expanded_paths, fix), start=1):\n1200             linted_dir = expanded_path_to_linted_dir[linted_file.path]\n1201             linted_dir.add(linted_file)\n1202             # If any fatal errors, then stop iteration.\n1203             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n1204                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n1205                 break\n1206 \n1207             # Progress bar for files is rendered only when there is more than one file.\n1208             # Additionally, as it's updated after each loop, we need to get file name\n1209             # from the next loop. This is why `enumerate` starts with `1` and there\n1210             # is `i < len` to not exceed files list length.\n1211             progress_bar_files.update(n=1)\n1212             if i < len(expanded_paths):\n1213                 progress_bar_files.set_description(f\"file {expanded_paths[i]}\")\n1214 \n1215         result.stop_timer()\n1216         return result\n1217 \n1218     def parse_path(\n1219         self,\n1220         path: str,\n1221         recurse: bool = True,\n1222     ) -> Iterator[ParsedString]:\n1223         \"\"\"Parse a path of sql files.\n1224 \n1225         NB: This a generator which will yield the result of each file\n1226         within the path iteratively.\n1227         \"\"\"\n1228         for fname in self.paths_from_path(path):\n1229             if self.formatter:\n1230                 self.formatter.dispatch_path(path)\n1231             # Load the file with the config and yield the result.\n1232             try:\n1233                 raw_file, config, encoding = self.load_raw_file_and_config(\n1234                     fname, self.config\n1235                 )\n1236             except SQLFluffSkipFile as s:\n1237                 linter_logger.warning(str(s))\n1238                 continue\n1239             yield self.parse_string(\n1240                 raw_file,\n1241                 fname=fname,\n1242                 recurse=recurse,\n1243                 config=config,\n1244                 encoding=encoding,\n1245             )\n1246 \n[end of src/sqlfluff/core/linter/linter.py]\n[start of src/sqlfluff/core/linter/linting_result.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import csv\n4 import time\n5 from typing import (\n6     Any,\n7     Dict,\n8     List,\n9     Optional,\n10     overload,\n11     Tuple,\n12     Union,\n13     Set,\n14 )\n15 from typing_extensions import Literal\n16 \n17 from sqlfluff.cli import EXIT_FAIL, EXIT_SUCCESS\n18 \n19 from sqlfluff.core.errors import (\n20     CheckTuple,\n21     SQLLintError,\n22     SQLParseError,\n23     SQLTemplaterError,\n24 )\n25 \n26 from sqlfluff.core.timing import TimingSummary, RuleTimingSummary\n27 \n28 # Classes needed only for type checking\n29 from sqlfluff.core.parser.segments.base import BaseSegment\n30 from sqlfluff.core.linter.linted_dir import LintedDir\n31 \n32 \n33 class LintingResult:\n34     \"\"\"A class to represent the result of a linting operation.\n35 \n36     Notably this might be a collection of paths, all with multiple\n37     potential files within them.\n38     \"\"\"\n39 \n40     def __init__(self) -> None:\n41         self.paths: List[LintedDir] = []\n42         self._start_time: float = time.monotonic()\n43         self.total_time: float = 0.0\n44 \n45     @staticmethod\n46     def sum_dicts(d1: Dict[str, Any], d2: Dict[str, Any]) -> Dict[str, Any]:\n47         \"\"\"Take the keys of two dictionaries and add them.\"\"\"\n48         keys = set(d1.keys()) | set(d2.keys())\n49         return {key: d1.get(key, 0) + d2.get(key, 0) for key in keys}\n50 \n51     @staticmethod\n52     def combine_dicts(*d: dict) -> dict:\n53         \"\"\"Take any set of dictionaries and combine them.\"\"\"\n54         dict_buffer: dict = {}\n55         for dct in d:\n56             dict_buffer.update(dct)\n57         return dict_buffer\n58 \n59     def add(self, path: LintedDir) -> None:\n60         \"\"\"Add a new `LintedDir` to this result.\"\"\"\n61         self.paths.append(path)\n62 \n63     def stop_timer(self):\n64         \"\"\"Stop the linting timer.\"\"\"\n65         self.total_time = time.monotonic() - self._start_time\n66 \n67     @overload\n68     def check_tuples(\n69         self, by_path: Literal[False]\n70     ) -> List[CheckTuple]:  # pragma: no cover\n71         \"\"\"Return a List of CheckTuples when by_path is False.\"\"\"\n72         ...\n73 \n74     @overload\n75     def check_tuples(\n76         self, by_path: Literal[True]\n77     ) -> Dict[LintedDir, List[CheckTuple]]:  # pragma: no cover\n78         \"\"\"Return a Dict of LintedDir and CheckTuples when by_path is True.\"\"\"\n79         ...\n80 \n81     @overload\n82     def check_tuples(self, by_path: bool = False):  # pragma: no cover\n83         \"\"\"Default overload method.\"\"\"\n84         ...\n85 \n86     def check_tuples(\n87         self, by_path=False\n88     ) -> Union[List[CheckTuple], Dict[LintedDir, List[CheckTuple]]]:\n89         \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n90 \n91         Args:\n92             by_path (:obj:`bool`, optional): When False, all the check_tuples\n93                 are aggregated into one flat list. When True, we return a `dict`\n94                 of paths, each with its own list of check_tuples. Defaults to False.\n95 \n96         \"\"\"\n97         if by_path:\n98             buff: Dict[LintedDir, List[CheckTuple]] = {}\n99             for path in self.paths:\n100                 buff.update(path.check_tuples(by_path=by_path))\n101             return buff\n102         else:\n103             tuple_buffer: List[CheckTuple] = []\n104             for path in self.paths:\n105                 tuple_buffer += path.check_tuples()\n106             return tuple_buffer\n107 \n108     def num_violations(self, **kwargs) -> int:\n109         \"\"\"Count the number of violations in the result.\"\"\"\n110         return sum(path.num_violations(**kwargs) for path in self.paths)\n111 \n112     def get_violations(self, **kwargs):\n113         \"\"\"Return a list of violations in the result.\"\"\"\n114         buff = []\n115         for path in self.paths:\n116             buff += path.get_violations(**kwargs)\n117         return buff\n118 \n119     def violation_dict(self, **kwargs):\n120         \"\"\"Return a dict of paths and violations.\"\"\"\n121         return self.combine_dicts(\n122             *(path.violation_dict(**kwargs) for path in self.paths)\n123         )\n124 \n125     def stats(self) -> Dict[str, Any]:\n126         \"\"\"Return a stats dictionary of this result.\"\"\"\n127         all_stats: Dict[str, Any] = dict(files=0, clean=0, unclean=0, violations=0)\n128         for path in self.paths:\n129             all_stats = self.sum_dicts(path.stats(), all_stats)\n130         if all_stats[\"files\"] > 0:\n131             all_stats[\"avg per file\"] = (\n132                 all_stats[\"violations\"] * 1.0 / all_stats[\"files\"]\n133             )\n134             all_stats[\"unclean rate\"] = all_stats[\"unclean\"] * 1.0 / all_stats[\"files\"]\n135         else:\n136             all_stats[\"avg per file\"] = 0\n137             all_stats[\"unclean rate\"] = 0\n138         all_stats[\"clean files\"] = all_stats[\"clean\"]\n139         all_stats[\"unclean files\"] = all_stats[\"unclean\"]\n140         all_stats[\"exit code\"] = (\n141             EXIT_FAIL if all_stats[\"violations\"] > 0 else EXIT_SUCCESS\n142         )\n143         all_stats[\"status\"] = \"FAIL\" if all_stats[\"violations\"] > 0 else \"PASS\"\n144         return all_stats\n145 \n146     def timing_summary(self) -> Dict[str, Dict[str, Any]]:\n147         \"\"\"Return a timing summary.\"\"\"\n148         timing = TimingSummary()\n149         rules_timing = RuleTimingSummary()\n150         for dir in self.paths:\n151             for file in dir.files:\n152                 if file.timings:\n153                     timing.add(file.timings.step_timings)\n154                     rules_timing.add(file.timings.rule_timings)\n155         return {**timing.summary(), **rules_timing.summary()}\n156 \n157     def persist_timing_records(self, filename: str) -> None:\n158         \"\"\"Persist the timing records as a csv for external analysis.\"\"\"\n159         meta_fields = [\n160             \"path\",\n161             \"source_chars\",\n162             \"templated_chars\",\n163             \"segments\",\n164             \"raw_segments\",\n165         ]\n166         timing_fields = [\"templating\", \"lexing\", \"parsing\", \"linting\"]\n167 \n168         # Iterate through all the files to get rule timing information so\n169         # we know what headings we're going to need.\n170         rule_codes: Set[str] = set()\n171         file_timing_dicts: Dict[str, dict] = {}\n172         for dir in self.paths:\n173             for file in dir.files:\n174                 if not file.timings:  # pragma: no cover\n175                     continue\n176                 file_timing_dicts[file.path] = file.timings.get_rule_timing_dict()\n177                 rule_codes.update(file_timing_dicts[file.path].keys())\n178 \n179         with open(filename, \"w\", newline=\"\") as f:\n180             writer = csv.DictWriter(\n181                 f, fieldnames=meta_fields + timing_fields + sorted(rule_codes)\n182             )\n183 \n184             writer.writeheader()\n185 \n186             for dir in self.paths:\n187                 for file in dir.files:\n188                     if not file.timings:  # pragma: no cover\n189                         continue\n190                     writer.writerow(\n191                         {\n192                             \"path\": file.path,\n193                             \"source_chars\": (\n194                                 len(file.templated_file.source_str)\n195                                 if file.templated_file\n196                                 else \"\"\n197                             ),\n198                             \"templated_chars\": (\n199                                 len(file.templated_file.templated_str)\n200                                 if file.templated_file\n201                                 else \"\"\n202                             ),\n203                             \"segments\": (\n204                                 file.tree.count_segments(raw_only=False)\n205                                 if file.tree\n206                                 else \"\"\n207                             ),\n208                             \"raw_segments\": (\n209                                 file.tree.count_segments(raw_only=True)\n210                                 if file.tree\n211                                 else \"\"\n212                             ),\n213                             **file.timings.step_timings,\n214                             **file_timing_dicts[file.path],\n215                         }\n216                     )\n217 \n218     def as_records(self) -> List[dict]:\n219         \"\"\"Return the result as a list of dictionaries.\n220 \n221         Each record contains a key specifying the filepath, and a list of violations.\n222         This method is useful for serialization as all objects will be builtin python\n223         types (ints, strs).\n224         \"\"\"\n225         return [\n226             {\n227                 \"filepath\": path,\n228                 \"violations\": sorted(\n229                     # Sort violations by line and then position\n230                     (v.get_info_dict() for v in violations),\n231                     # The tuple allows sorting by line number, then position, then code\n232                     key=lambda v: (v[\"line_no\"], v[\"line_pos\"], v[\"code\"]),\n233                 ),\n234             }\n235             for LintedDir in self.paths\n236             for path, violations in LintedDir.violation_dict().items()\n237             if violations\n238         ]\n239 \n240     def persist_changes(self, formatter, **kwargs) -> dict:\n241         \"\"\"Run all the fixes for all the files and return a dict.\"\"\"\n242         return self.combine_dicts(\n243             *(\n244                 path.persist_changes(formatter=formatter, **kwargs)\n245                 for path in self.paths\n246             )\n247         )\n248 \n249     @property\n250     def tree(self) -> Optional[BaseSegment]:  # pragma: no cover\n251         \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n252         if len(self.paths) > 1:\n253             raise ValueError(\n254                 \".tree() cannot be called when a LintingResult contains more than one \"\n255                 \"path.\"\n256             )\n257         return self.paths[0].tree\n258 \n259     TMP_PRS_ERROR_TYPES = (SQLTemplaterError, SQLParseError)\n260 \n261     def count_tmp_prs_errors(self) -> Tuple[int, int]:\n262         \"\"\"Count templating or parse errors before and after filtering.\"\"\"\n263         total_errors = self.num_violations(\n264             types=self.TMP_PRS_ERROR_TYPES, filter_ignore=False, filter_warning=False\n265         )\n266         num_filtered_errors = 0\n267         for linted_dir in self.paths:\n268             for linted_file in linted_dir.files:\n269                 num_filtered_errors += linted_file.num_violations(\n270                     types=self.TMP_PRS_ERROR_TYPES\n271                 )\n272         return total_errors, num_filtered_errors\n273 \n274     def discard_fixes_for_lint_errors_in_files_with_tmp_or_prs_errors(self) -> None:\n275         \"\"\"Discard lint fixes for files with templating or parse errors.\"\"\"\n276         total_errors = self.num_violations(\n277             types=self.TMP_PRS_ERROR_TYPES, filter_ignore=False, filter_warning=False\n278         )\n279         if total_errors:\n280             for linted_dir in self.paths:\n281                 for linted_file in linted_dir.files:\n282                     num_errors = linted_file.num_violations(\n283                         types=self.TMP_PRS_ERROR_TYPES,\n284                         filter_ignore=False,\n285                         filter_warning=False,\n286                     )\n287                     if num_errors:\n288                         # File has errors. Discard all the SQLLintError fixes:\n289                         # they are potentially unsafe.\n290                         for violation in linted_file.violations:\n291                             if isinstance(violation, SQLLintError):\n292                                 violation.fixes = []\n293 \n[end of src/sqlfluff/core/linter/linting_result.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n```", "model_patch": "--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n@@ -1,5 +1,5 @@\n \n     def lint_paths(\n-        self,\n+        self, \n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n"}
{"instance_id": "sqlfluff__sqlfluff-3066", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nJinja: sqlfluff fails in the presence of assignments with multiple targets\n### Search before asking\r\n\r\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\r\n\r\nI did search, and I think this _may_ be related, but since no more information was provided I cannot confirm it: https://github.com/sqlfluff/sqlfluff/issues/2947. For this reason, I opened a new issue.\r\n\r\n### What Happened\r\n\r\nJinja templates support multiple targets in [assignments](https://jinja.palletsprojects.com/en/3.0.x/templates/#assignments). However, `sqlfluff` fails to lint a file in the presence of an assignment with multiple targets.\r\n\r\nI traced this back to the `update_inside_set_or_macro` function, specifically [this line](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L244=).\r\n\r\nThe way `sqlfluff` is determining whether we are inside a [block assignment](https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments) is by checking for the presence of an equals in the second index of the trimmed parts of the current raw slice:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\r\n```\r\n\r\nThis condition is false for single target assignments:\r\n\r\n```sql\r\n{% set a = 1 %}\r\n```\r\n\r\nWhich produce the expected trimmed parts (with spaces removed as in [line 243](https://github.com/sqlfluff/sqlfluff/blob/main/src/sqlfluff/core/templaters/slicers/tracer.py#L243=)):\r\n\r\n```python\r\n['set', 'a', '=', '1']\r\n#             2    \r\n```\r\n\r\nHowever, with multiple targets:\r\n\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n```\r\n\r\n```python\r\n['set', 'a', ',', 'b', '=', '1', '2']\r\n#                       4    \r\n```\r\n\r\nEquals is no longer in the index 2, but has been bumped to index 4, yet we are not in the expanded block form of set assignments. This causes the `inside_set_or_macro` flag to be incorrectly set to `True`, as if we were using a block assignment, which causes the entire template to be ignored (or something like that), and leads to the eventual `ValueError` raised.\r\n\r\nI played around a bit with potential solutions: first, I tried incrementing the index of the equals by the number of commas:\r\n\r\n```python\r\nequals_index = 2 + sum((c == ',' for c in  filtered_trimmed_parts))\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[equals_index] != \"=\":\r\n```\r\n\r\nHowever, this would bring issues if using the expanded form of set assignments with any commas in it, or in the presence of an uneven number of commas on both sides of the assignment.\r\n\r\nAnother simpler option would be to check for the presence of a single equals:\r\n\r\n```python\r\nif len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\r\n```\r\n\r\nThis one seems more promising, specially considering that multiple targets appear not to be supported with block assignments (at least, that's what I think, as the docs don't mention it, and trying it locally raises a too many values to unpack error). Thus, the first condition will always be true for block assignments (so, even the presence of an equals in the body of the assignment would not cause issues).\r\n\r\n### Expected Behaviour\r\n\r\nsqlfluff should lint files properly, even in the presence of assignments with multiple targets.\r\n\r\n### Observed Behaviour\r\n\r\nLinting fails when an exception is raised:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10dbt/bin/sqlfluff\", line 8, in <module>\r\n    sys.exit(cli())\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1128, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1053, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1659, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 1395, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/click/core.py\", line 754, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/cli/commands.py\", line 541, in lint\r\n    result = lnt.lint_paths(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1098, in lint_paths\r\n    self.lint_path(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 1050, in lint_path\r\n    for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 101, in run\r\n    for fname, partial in self.iter_partials(fnames, fix=fix):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 54, in iter_partials\r\n    for fname, rendered in self.iter_rendered(fnames):\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/runner.py\", line 43, in iter_rendered\r\n    yield fname, self.linter.render_file(fname, self.config)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 771, in render_file\r\n    return self.render_string(raw_file, fname, config, encoding)\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/linter/linter.py\", line 742, in render_string\r\n    templated_file, templater_violations = self.templater.process(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/jinja.py\", line 394, in process\r\n    TemplatedFile(\r\n  File \"/home/tomasfarias/.pyenv/versions/3.10.2/envs/3.10dbt/lib/python3.10/site-packages/sqlfluff/core/templaters/base.py\", line 94, in __init__\r\n    raise ValueError(\"Cannot instantiate a templated file unsliced!\")\r\nValueError: Cannot instantiate a templated file unsliced!\r\n```\r\n\r\n### How to reproduce\r\n\r\n1. Save the following template to `model.sql` in an empty directory:\r\n```sql\r\n{% set a, b = 1, 2 %}\r\n\r\nSELECT {{ a }}\r\n```\r\n2. Run `sqlfluff lint model.sql --dialect 'postgres'`\r\n\r\n\r\n### Dialect\r\n\r\nTried with postgres and redshift dialects, however I think others may be affected as long as they use jinja templates.\r\n\r\n### Version\r\n\r\nv0.12.0\r\n\r\n### Configuration\r\n\r\nNothing, ran from an empty directory.\r\n\r\n### Are you willing to work on and submit a PR to address the issue?\r\n\r\n- [X] Yes I am willing to submit a PR!\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![CI Tests](https://github.com/sqlfluff/sqlfluff/workflows/CI%20Tests/badge.svg)](https://github.com/sqlfluff/sqlfluff/actions?query=workflow%3A%22CI+Tests%22)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 \n16 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n17 \n18 ## Dialects Supported\n19 \n20 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n21 \n22 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n23 - [BigQuery](https://cloud.google.com/bigquery/)\n24 - [Exasol](https://www.exasol.com/)\n25 - [Hive](https://hive.apache.org/)\n26 - [MySQL](https://www.mysql.com/)\n27 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n28 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n29 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n30 - [Snowflake](https://www.snowflake.com/)\n31 - [SparkSQL](https://spark.apache.org/docs/latest/)\n32 - [SQLite](https://www.sqlite.org/)\n33 - [Teradata](https://www.teradata.com/)\n34 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n35 \n36 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n37 \n38 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n39 \n40 ## Templates Supported\n41 \n42 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n43 \n44 **SQLFluff** supports the following templates:\n45 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n46 - [dbt](https://www.getdbt.com/)\n47 \n48 Again, please raise issues if you wish to support more templating languages/syntaxes.\n49 \n50 # Getting Started\n51 \n52 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n53 \n54 ```shell\n55 $ pip install sqlfluff\n56 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n57 $ sqlfluff lint test.sql\n58 == [test.sql] FAIL\n59 L:   1 | P:   1 | L003 | Single indentation uses a number of spaces not a multiple of 4\n60 L:   1 | P:  14 | L006 | Operators should be surrounded by a single space unless at the start/end of a line\n61 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace\n62 ```\n63 \n64 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n65 \n66 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n67 \n68 # Documentation\n69 \n70 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n71 \n72 # Releases\n73 \n74 **SQLFluff** is in beta phase - expect the tool to change significantly with potentially non-backwards compatible API and configuration changes in future releases. If you would like to join in please consider [contributing](CONTRIBUTING.md).\n75 \n76 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n77 \n78 # SQLFluff on Slack\n79 \n80 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n81 \n82 # SQLFluff on Twitter\n83 \n84 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n85 \n86 # Contributing\n87 \n88 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n89 \n90 If you want to understand more about the architecture of **SQLFluff**, you can find [more here](https://docs.sqlfluff.com/en/latest/architecture.html).\n91 \n92 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n93 \n[end of README.md]\n[start of src/sqlfluff/core/templaters/slicers/tracer.py]\n1 \"\"\"'Trace' Jinja template execution to map output back to the raw template.\n2 \n3 This is a newer slicing algorithm that handles cases heuristic.py does not.\n4 \"\"\"\n5 \n6 from dataclasses import dataclass, field\n7 import logging\n8 import regex\n9 from typing import Callable, cast, Dict, List, NamedTuple, Optional\n10 \n11 from jinja2 import Environment\n12 from jinja2.environment import Template\n13 \n14 from sqlfluff.core.templaters.base import (\n15     RawFileSlice,\n16     TemplatedFileSlice,\n17 )\n18 \n19 \n20 # Instantiate the templater logger\n21 templater_logger = logging.getLogger(\"sqlfluff.templater\")\n22 \n23 \n24 class JinjaTrace(NamedTuple):\n25     \"\"\"Returned by JinjaTracer.trace().\"\"\"\n26 \n27     # Template output\n28     templated_str: str\n29     # Raw (i.e. before rendering) Jinja template sliced into tokens\n30     raw_sliced: List[RawFileSlice]\n31     # Rendered Jinja template (i.e. output) mapped back to rwa_str source\n32     sliced_file: List[TemplatedFileSlice]\n33 \n34 \n35 @dataclass\n36 class RawSliceInfo:\n37     \"\"\"JinjaTracer-specific info about each RawFileSlice.\"\"\"\n38 \n39     unique_alternate_id: Optional[str]\n40     alternate_code: Optional[str]\n41     next_slice_indices: List[int] = field(default_factory=list)\n42 \n43 \n44 class JinjaTracer:\n45     \"\"\"Records execution path of a Jinja template.\"\"\"\n46 \n47     def __init__(\n48         self,\n49         raw_str: str,\n50         raw_sliced: List[RawFileSlice],\n51         raw_slice_info: Dict[RawFileSlice, RawSliceInfo],\n52         sliced_file: List[TemplatedFileSlice],\n53         make_template: Callable[[str], Template],\n54     ):\n55         # Input\n56         self.raw_str = raw_str\n57         self.raw_sliced = raw_sliced\n58         self.raw_slice_info = raw_slice_info\n59         self.sliced_file = sliced_file\n60         self.make_template = make_template\n61 \n62         # Internal bookkeeping\n63         self.program_counter: int = 0\n64         self.source_idx: int = 0\n65 \n66     def trace(self, append_to_templated: str = \"\") -> JinjaTrace:\n67         \"\"\"Executes raw_str. Returns template output and trace.\"\"\"\n68         trace_template_str = \"\".join(\n69             cast(str, self.raw_slice_info[rs].alternate_code)\n70             if self.raw_slice_info[rs].alternate_code is not None\n71             else rs.raw\n72             for rs in self.raw_sliced\n73         )\n74         trace_template = self.make_template(trace_template_str)\n75         trace_template_output = trace_template.render()\n76         # Split output by section. Each section has two possible formats.\n77         trace_entries = list(regex.finditer(r\"\\0\", trace_template_output))\n78         for match_idx, match in enumerate(trace_entries):\n79             pos1 = match.span()[0]\n80             try:\n81                 pos2 = trace_entries[match_idx + 1].span()[0]\n82             except IndexError:\n83                 pos2 = len(trace_template_output)\n84             p = trace_template_output[pos1 + 1 : pos2]\n85             m_id = regex.match(r\"^([0-9a-f]+)(_(\\d+))?\", p)\n86             if not m_id:\n87                 raise ValueError(  # pragma: no cover\n88                     \"Internal error. Trace template output does not match expected \"\n89                     \"format.\"\n90                 )\n91             if m_id.group(3):\n92                 # E.g. \"00000000000000000000000000000001_83\". The number after\n93                 # \"_\" is the length (in characters) of a corresponding literal\n94                 # in raw_str.\n95                 value = [m_id.group(1), int(m_id.group(3)), True]\n96             else:\n97                 # E.g. \"00000000000000000000000000000002 a < 10\". The characters\n98                 # after the slice ID are executable code from raw_str.\n99                 value = [m_id.group(0), p[len(m_id.group(0)) + 1 :], False]\n100             alt_id, content_info, literal = value\n101             target_slice_idx = self.find_slice_index(alt_id)\n102             slice_length = content_info if literal else len(str(content_info))\n103             self.move_to_slice(target_slice_idx, slice_length)\n104 \n105         # TRICKY: The 'append_to_templated' parameter is only used by the dbt\n106         # templater, passing \"\\n\" for this parameter if we need to add one back.\n107         # (The Jinja templater does not pass this parameter, so\n108         # 'append_to_templated' gets the default value of \"\", empty string.)\n109         # we receive the default value of \"\".) The dbt templater will\n110         # For more detail, see the comments near the call to slice_file() in\n111         # plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py.\n112         templated_str = self.make_template(self.raw_str).render() + append_to_templated\n113         return JinjaTrace(templated_str, self.raw_sliced, self.sliced_file)\n114 \n115     def find_slice_index(self, slice_identifier) -> int:\n116         \"\"\"Given a slice identifier, return its index.\n117 \n118         A slice identifier is a string like 00000000000000000000000000000002.\n119         \"\"\"\n120         raw_slices_search_result = [\n121             idx\n122             for idx, rs in enumerate(self.raw_sliced)\n123             if self.raw_slice_info[rs].unique_alternate_id == slice_identifier\n124         ]\n125         if len(raw_slices_search_result) != 1:\n126             raise ValueError(  # pragma: no cover\n127                 f\"Internal error. Unable to locate slice for {slice_identifier}.\"\n128             )\n129         return raw_slices_search_result[0]\n130 \n131     def move_to_slice(self, target_slice_idx, target_slice_length):\n132         \"\"\"Given a template location, walk execution to that point.\"\"\"\n133         while self.program_counter < len(self.raw_sliced):\n134             self.record_trace(\n135                 target_slice_length if self.program_counter == target_slice_idx else 0\n136             )\n137             current_raw_slice = self.raw_sliced[self.program_counter]\n138             if self.program_counter == target_slice_idx:\n139                 # Reached the target slice. Go to next location and stop.\n140                 self.program_counter += 1\n141                 break\n142             else:\n143                 # Choose the next step.\n144 \n145                 # We could simply go to the next slice (sequential execution).\n146                 candidates = [self.program_counter + 1]\n147                 # If we have other options, consider those.\n148                 for next_slice_idx in self.raw_slice_info[\n149                     current_raw_slice\n150                 ].next_slice_indices:\n151                     # It's a valid possibility if it does not take us past the\n152                     # target.\n153                     if next_slice_idx <= target_slice_idx:\n154                         candidates.append(next_slice_idx)\n155                 # Choose the candidate that takes us closest to the target.\n156                 candidates.sort(key=lambda c: abs(target_slice_idx - c))\n157                 self.program_counter = candidates[0]\n158 \n159     def record_trace(self, target_slice_length, slice_idx=None, slice_type=None):\n160         \"\"\"Add the specified (default: current) location to the trace.\"\"\"\n161         if slice_idx is None:\n162             slice_idx = self.program_counter\n163         if slice_type is None:\n164             slice_type = self.raw_sliced[slice_idx].slice_type\n165         self.sliced_file.append(\n166             TemplatedFileSlice(\n167                 slice_type,\n168                 slice(\n169                     self.raw_sliced[slice_idx].source_idx,\n170                     self.raw_sliced[slice_idx + 1].source_idx\n171                     if slice_idx + 1 < len(self.raw_sliced)\n172                     else len(self.raw_str),\n173                 ),\n174                 slice(self.source_idx, self.source_idx + target_slice_length),\n175             )\n176         )\n177         if slice_type in (\"literal\", \"templated\"):\n178             self.source_idx += target_slice_length\n179 \n180 \n181 class JinjaAnalyzer:\n182     \"\"\"Analyzes a Jinja template to prepare for tracing.\"\"\"\n183 \n184     re_open_tag = regex.compile(r\"^\\s*({[{%])[\\+\\-]?\\s*\")\n185     re_close_tag = regex.compile(r\"\\s*[\\+\\-]?([}%]})\\s*$\")\n186 \n187     def __init__(self, raw_str: str, env: Environment):\n188         # Input\n189         self.raw_str: str = raw_str\n190         self.env = env\n191 \n192         # Output\n193         self.raw_sliced: List[RawFileSlice] = []\n194         self.raw_slice_info: Dict[RawFileSlice, RawSliceInfo] = {}\n195         self.sliced_file: List[TemplatedFileSlice] = []\n196 \n197         # Internal bookkeeping\n198         self.slice_id: int = 0\n199         self.inside_set_or_macro: bool = False\n200         self.stack: List[int] = []\n201         self.idx_raw: int = 0\n202 \n203     def next_slice_id(self) -> str:\n204         \"\"\"Returns a new, unique slice ID.\"\"\"\n205         result = \"{0:#0{1}x}\".format(self.slice_id, 34)[2:]\n206         self.slice_id += 1\n207         return result\n208 \n209     def slice_info_for_literal(self, length, prefix=\"\") -> RawSliceInfo:\n210         \"\"\"Returns a RawSliceInfo for a literal.\n211 \n212         In the alternate template, literals are replaced with a uniquely\n213         numbered, easily-to-parse literal. JinjaTracer uses this output as\n214         a \"breadcrumb trail\" to deduce the execution path through the template.\n215 \n216         This is important even if the original literal (i.e. in the raw SQL\n217         file) was empty, as is the case when Jinja whitespace control is used\n218         (e.g. \"{%- endif -%}\"), because fewer breadcrumbs means JinjaTracer has\n219         to *guess* the path, in which case it assumes simple, straight-line\n220         execution, which can easily be wrong with loops and conditionals.\n221         \"\"\"\n222         unique_alternate_id = self.next_slice_id()\n223         alternate_code = f\"\\0{prefix}{unique_alternate_id}_{length}\"\n224         return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n225 \n226     def update_inside_set_or_macro(\n227         self, block_type: str, trimmed_parts: List[str]\n228     ) -> None:\n229         \"\"\"Based on block tag, update whether we're in a set/macro section.\"\"\"\n230         if block_type == \"block_start\" and trimmed_parts[0] in (\n231             \"macro\",\n232             \"set\",\n233         ):\n234             # Jinja supports two forms of {% set %}:\n235             # - {% set variable = value %}\n236             # - {% set variable %}value{% endset %}\n237             # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n238             # When the second format is used, set the field\n239             # 'inside_set_or_macro' to True. This info is used elsewhere,\n240             # as other code inside these regions require special handling.\n241             # (Generally speaking, JinjaTracer ignores the contents of these\n242             # blocks, treating them like opaque templated regions.)\n243             filtered_trimmed_parts = [p for p in trimmed_parts if not p.isspace()]\n244             if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\n245                 # Entering a set/macro block.\n246                 self.inside_set_or_macro = True\n247         elif block_type == \"block_end\" and (trimmed_parts[0] in (\"endmacro\", \"endset\")):\n248             # Exiting a set/macro block.\n249             self.inside_set_or_macro = False\n250 \n251     def make_raw_slice_info(\n252         self, unique_alternate_id: Optional[str], alternate_code: Optional[str]\n253     ) -> RawSliceInfo:\n254         \"\"\"Create RawSliceInfo as given, or \"empty\" if in set/macro block.\"\"\"\n255         if not self.inside_set_or_macro:\n256             return RawSliceInfo(unique_alternate_id, alternate_code, [])\n257         else:\n258             return RawSliceInfo(None, None, [])\n259 \n260     # We decide the \"kind\" of element we're dealing with using its _closing_\n261     # tag rather than its opening tag. The types here map back to similar types\n262     # of sections in the python slicer.\n263     block_types = {\n264         \"variable_end\": \"templated\",\n265         \"block_end\": \"block\",\n266         \"comment_end\": \"comment\",\n267         # Raw tags should behave like blocks. Note that\n268         # raw_end and raw_begin are whole tags rather\n269         # than blocks and comments where we get partial\n270         # tags.\n271         \"raw_end\": \"block\",\n272         \"raw_begin\": \"block\",\n273     }\n274 \n275     def analyze(self, make_template: Callable[[str], Template]) -> JinjaTracer:\n276         \"\"\"Slice template in jinja.\"\"\"\n277         # str_buff and str_parts are two ways we keep track of tokens received\n278         # from Jinja. str_buff concatenates them together, while str_parts\n279         # accumulates the individual strings. We generally prefer using\n280         # str_parts. That's because Jinja doesn't just split on whitespace, so\n281         # by keeping tokens as Jinja returns them, the code is more robust.\n282         # Consider the following:\n283         #   {% set col= \"col1\" %}\n284         # Note there's no space after col. Jinja splits this up for us. If we\n285         # simply concatenated the parts together and later split on whitespace,\n286         # we'd need some ugly, fragile logic to handle various whitespace\n287         # possibilities:\n288         #   {% set col= \"col1\" %}\n289         #   {% set col = \"col1\" %}\n290         #   {% set col =\"col1\" %}\n291         # By using str_parts and letting Jinja handle this, it just works.\n292 \n293         str_buff = \"\"\n294         str_parts = []\n295 \n296         # https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment.lex\n297         for _, elem_type, raw in self.env.lex(self.raw_str):\n298             if elem_type == \"data\":\n299                 self.track_literal(raw)\n300                 continue\n301             str_buff += raw\n302             str_parts.append(raw)\n303 \n304             if elem_type.endswith(\"_begin\"):\n305                 self.handle_left_whitespace_stripping(raw)\n306 \n307             raw_slice_info: RawSliceInfo = self.make_raw_slice_info(None, None)\n308             tag_contents = []\n309             # raw_end and raw_begin behave a little differently in\n310             # that the whole tag shows up in one go rather than getting\n311             # parts of the tag at a time.\n312             if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n313                 block_type = self.block_types[elem_type]\n314                 block_subtype = None\n315                 # Handle starts and ends of blocks\n316                 if block_type in (\"block\", \"templated\"):\n317                     m_open = self.re_open_tag.search(str_parts[0])\n318                     m_close = self.re_close_tag.search(str_parts[-1])\n319                     if m_open and m_close:\n320                         tag_contents = self.extract_tag_contents(\n321                             str_parts, m_close, m_open, str_buff\n322                         )\n323 \n324                     if block_type == \"block\" and tag_contents:\n325                         block_type, block_subtype = self.extract_block_type(\n326                             tag_contents[0], block_subtype\n327                         )\n328                     if block_type == \"templated\" and tag_contents:\n329                         assert m_open and m_close\n330                         raw_slice_info = self.track_templated(\n331                             m_open, m_close, tag_contents\n332                         )\n333                 self.update_inside_set_or_macro(block_type, tag_contents)\n334                 m_strip_right = regex.search(\n335                     r\"\\s+$\", raw, regex.MULTILINE | regex.DOTALL\n336                 )\n337                 if elem_type.endswith(\"_end\") and raw.startswith(\"-\") and m_strip_right:\n338                     # Right whitespace was stripped after closing block. Split\n339                     # off the trailing whitespace into a separate slice. The\n340                     # desired behavior is to behave similarly as the left\n341                     # stripping case. Note that the stakes are a bit lower here,\n342                     # because lex() hasn't *omitted* any characters from the\n343                     # strings it returns, it has simply grouped them differently\n344                     # than we want.\n345                     trailing_chars = len(m_strip_right.group(0))\n346                     self.raw_sliced.append(\n347                         RawFileSlice(\n348                             str_buff[:-trailing_chars],\n349                             block_type,\n350                             self.idx_raw,\n351                             block_subtype,\n352                         )\n353                     )\n354                     self.raw_slice_info[self.raw_sliced[-1]] = raw_slice_info\n355                     block_idx = len(self.raw_sliced) - 1\n356                     self.idx_raw += len(str_buff) - trailing_chars\n357                     self.raw_sliced.append(\n358                         RawFileSlice(\n359                             str_buff[-trailing_chars:],\n360                             \"literal\",\n361                             self.idx_raw,\n362                         )\n363                     )\n364                     self.raw_slice_info[\n365                         self.raw_sliced[-1]\n366                     ] = self.slice_info_for_literal(0)\n367                     self.idx_raw += trailing_chars\n368                 else:\n369                     self.raw_sliced.append(\n370                         RawFileSlice(\n371                             str_buff,\n372                             block_type,\n373                             self.idx_raw,\n374                             block_subtype,\n375                         )\n376                     )\n377                     self.raw_slice_info[self.raw_sliced[-1]] = raw_slice_info\n378                     block_idx = len(self.raw_sliced) - 1\n379                     self.idx_raw += len(str_buff)\n380                 if block_type.startswith(\"block\"):\n381                     self.track_block_end(block_type, tag_contents[0])\n382                     self.update_next_slice_indices(\n383                         block_idx, block_type, tag_contents[0]\n384                     )\n385                 str_buff = \"\"\n386                 str_parts = []\n387         return JinjaTracer(\n388             self.raw_str,\n389             self.raw_sliced,\n390             self.raw_slice_info,\n391             self.sliced_file,\n392             make_template,\n393         )\n394 \n395     def track_templated(\n396         self, m_open: regex.Match, m_close: regex.Match, tag_contents: List[str]\n397     ) -> RawSliceInfo:\n398         \"\"\"Compute tracking info for Jinja templated region, e.g. {{ foo }}.\"\"\"\n399         unique_alternate_id = self.next_slice_id()\n400         open_ = m_open.group(1)\n401         close_ = m_close.group(1)\n402         # Here, we still need to evaluate the original tag contents, e.g. in\n403         # case it has intentional side effects, but also return a slice ID\n404         # for tracking.\n405         alternate_code = (\n406             f\"\\0{unique_alternate_id} {open_} \" f\"{''.join(tag_contents)} {close_}\"\n407         )\n408         return self.make_raw_slice_info(unique_alternate_id, alternate_code)\n409 \n410     def track_literal(self, raw: str) -> None:\n411         \"\"\"Set up tracking for a Jinja literal.\"\"\"\n412         self.raw_sliced.append(\n413             RawFileSlice(\n414                 raw,\n415                 \"literal\",\n416                 self.idx_raw,\n417             )\n418         )\n419         # Replace literal text with a unique ID.\n420         self.raw_slice_info[self.raw_sliced[-1]] = self.slice_info_for_literal(\n421             len(raw), \"\"\n422         )\n423         self.idx_raw += len(raw)\n424 \n425     @staticmethod\n426     def extract_block_type(tag_name, block_subtype):\n427         \"\"\"Determine block type.\"\"\"\n428         # :TRICKY: Syntactically, the Jinja {% include %} directive looks like\n429         # a block, but its behavior is basically syntactic sugar for\n430         # {{ open(\"somefile).read() }}. Thus, treat it as templated code.\n431         if tag_name == \"include\":\n432             block_type = \"templated\"\n433         elif tag_name.startswith(\"end\"):\n434             block_type = \"block_end\"\n435         elif tag_name.startswith(\"el\"):\n436             # else, elif\n437             block_type = \"block_mid\"\n438         else:\n439             block_type = \"block_start\"\n440             if tag_name == \"for\":\n441                 block_subtype = \"loop\"\n442         return block_type, block_subtype\n443 \n444     @staticmethod\n445     def extract_tag_contents(\n446         str_parts: List[str],\n447         m_close: regex.Match,\n448         m_open: regex.Match,\n449         str_buff: str,\n450     ) -> List[str]:\n451         \"\"\"Given Jinja tag info, return the stuff inside the braces.\n452 \n453         I.e. Trim off the brackets and the whitespace.\n454         \"\"\"\n455         if len(str_parts) >= 3:\n456             # Handle a tag received as individual parts.\n457             trimmed_parts = str_parts[1:-1]\n458             if trimmed_parts[0].isspace():\n459                 del trimmed_parts[0]\n460             if trimmed_parts[-1].isspace():\n461                 del trimmed_parts[-1]\n462         else:\n463             # Handle a tag received in one go.\n464             trimmed_content = str_buff[len(m_open.group(0)) : -len(m_close.group(0))]\n465             trimmed_parts = trimmed_content.split()\n466         return trimmed_parts\n467 \n468     def track_block_end(self, block_type: str, tag_name: str) -> None:\n469         \"\"\"On ending a 'for' or 'if' block, set up tracking.\"\"\"\n470         if block_type == \"block_end\" and tag_name in (\n471             \"endfor\",\n472             \"endif\",\n473         ):\n474             # Replace RawSliceInfo for this slice with one that has alternate ID\n475             # and code for tracking. This ensures, for instance, that if a file\n476             # ends with \"{% endif %} (with no newline following), that we still\n477             # generate a TemplateSliceInfo for it.\n478             unique_alternate_id = self.next_slice_id()\n479             alternate_code = f\"{self.raw_sliced[-1].raw}\\0{unique_alternate_id}_0\"\n480             self.raw_slice_info[self.raw_sliced[-1]] = self.make_raw_slice_info(\n481                 unique_alternate_id, alternate_code\n482             )\n483 \n484     def update_next_slice_indices(\n485         self, block_idx: int, block_type: str, tag_name: str\n486     ) -> None:\n487         \"\"\"Based on block, update conditional jump info.\"\"\"\n488         if block_type == \"block_start\" and tag_name in (\n489             \"for\",\n490             \"if\",\n491         ):\n492             self.stack.append(block_idx)\n493         elif block_type == \"block_mid\":\n494             # Record potential forward jump over this block.\n495             self.raw_slice_info[\n496                 self.raw_sliced[self.stack[-1]]\n497             ].next_slice_indices.append(block_idx)\n498             self.stack.pop()\n499             self.stack.append(block_idx)\n500         elif block_type == \"block_end\" and tag_name in (\n501             \"endfor\",\n502             \"endif\",\n503         ):\n504             if not self.inside_set_or_macro:\n505                 # Record potential forward jump over this block.\n506                 self.raw_slice_info[\n507                     self.raw_sliced[self.stack[-1]]\n508                 ].next_slice_indices.append(block_idx)\n509                 if self.raw_sliced[self.stack[-1]].slice_subtype == \"loop\":\n510                     # Record potential backward jump to the loop beginning.\n511                     self.raw_slice_info[\n512                         self.raw_sliced[block_idx]\n513                     ].next_slice_indices.append(self.stack[-1] + 1)\n514                 self.stack.pop()\n515 \n516     def handle_left_whitespace_stripping(self, token: str) -> None:\n517         \"\"\"If block open uses whitespace stripping, record it.\n518 \n519         When a \"begin\" tag (whether block, comment, or data) uses whitespace\n520         stripping\n521         (https://jinja.palletsprojects.com/en/3.0.x/templates/#whitespace-control)\n522         the Jinja lex() function handles this by discarding adjacent whitespace\n523         from 'raw_str'. For more insight, see the tokeniter() function in this file:\n524         https://github.com/pallets/jinja/blob/main/src/jinja2/lexer.py\n525 \n526         We want to detect and correct for this in order to:\n527         - Correctly update \"idx\" (if this is wrong, that's a potential\n528           DISASTER because lint fixes use this info to update the source file,\n529           and incorrect values often result in CORRUPTING the user's file so\n530           it's no longer valid SQL. :-O\n531         - Guarantee that the slices we return fully \"cover\" the contents of\n532           'in_str'.\n533 \n534         We detect skipped characters by looking ahead in in_str for the token\n535         just returned from lex(). The token text will either be at the current\n536         'idx_raw' position (if whitespace stripping did not occur) OR it'll be\n537         farther along in 'raw_str', but we're GUARANTEED that lex() only skips\n538         over WHITESPACE; nothing else.\n539         \"\"\"\n540         # Find the token returned. Did lex() skip over any characters?\n541         num_chars_skipped = self.raw_str.index(token, self.idx_raw) - self.idx_raw\n542         if not num_chars_skipped:\n543             return\n544 \n545         # Yes. It skipped over some characters. Compute a string\n546         # containing the skipped characters.\n547         skipped_str = self.raw_str[self.idx_raw : self.idx_raw + num_chars_skipped]\n548 \n549         # Sanity check: Verify that Jinja only skips over\n550         # WHITESPACE, never anything else.\n551         if not skipped_str.isspace():  # pragma: no cover\n552             templater_logger.warning(\n553                 \"Jinja lex() skipped non-whitespace: %s\", skipped_str\n554             )\n555         # Treat the skipped whitespace as a literal.\n556         self.raw_sliced.append(RawFileSlice(skipped_str, \"literal\", self.idx_raw))\n557         self.raw_slice_info[self.raw_sliced[-1]] = self.slice_info_for_literal(0)\n558         self.idx_raw += num_chars_skipped\n559 \n[end of src/sqlfluff/core/templaters/slicers/tracer.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/core/templaters/slicers/tracer.py\n+++ b/src/sqlfluff/core/templaters/slicers/tracer.py\n@@ -244,7 +244,7 @@\n             # Jinja supports two forms of {% set %}:\n             # - {% set variable = value %}\n             # - {% set variable %}value{% endset %}\n-            # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n+            # https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments\n             # When the second format is used, set the field\n             # 'inside_set_or_macro' to True. This info is used elsewhere,\n             # as other code inside these regions require special handling.\n@@ -244,7 +244,7 @@\n             filtered_trimmed_parts = [p for p in trimmed_parts if not p.isspace()]\n-            if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\n+            if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\n                 # Entering a set/macro block.\n                 self.inside_set_or_macro = True\n```", "model_patch": "--- a/src/sqlfluff/core/templaters/slicers/tracer.py\n+++ b/src/sqlfluff/core/templaters/slicers/tracer.py\n@@ -244,7 +244,7 @@\n             # Jinja supports two forms of {% set %}:\n             # - {% set variable = value %}\n             # - {% set variable %}value{% endset %}\n-            # https://jinja.palletsprojects.com/en/2.10.x/templates/#block-assignments\n+            # https://jinja.palletsprojects.com/en/3.0.x/templates/#block-assignments\n             # When the second format is used, set the field\n             # 'inside_set_or_macro' to True. This info is used elsewhere,\n             # as other code inside these regions require special handling.\n@@ -244,7 +244,7 @@\n             filtered_trimmed_parts = [p for p in trimmed_parts if not p.isspace()]\n-            if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts[2] != \"=\":\n+            if len(filtered_trimmed_parts) < 3 or filtered_trimmed_parts.count(\"=\") != 1:\n                 # Entering a set/macro block.\n                 self.inside_set_or_macro = True\n"}
{"instance_id": "sqlfluff__sqlfluff-4084", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nMultiple processes not used when list of explicit filenames is passed\n### Search before asking\n\n- [X] I searched the [issues](https://github.com/sqlfluff/sqlfluff/issues) and found no similar issues.\n\n\n### What Happened\n\nWhen providing a long list of file names to `sqlfluff lint -p -1`, only a single CPU is used. This seems to stem from the fact that https://github.com/sqlfluff/sqlfluff/blob/a006378af8b670f9235653694dbcddd4c62d1ab9/src/sqlfluff/core/linter/linter.py#L1190 is iterating over the list of files. For each listed path there, it would run the found files in parallel. As we are inputting whole filenames here, a path equals a single file and thus `sqlfluff` would only process one file at a time.\r\n\r\nThe context here is the execution of `sqlfluff lint` inside a `pre-commit` hook.\n\n### Expected Behaviour\n\nAll CPU cores are used as `-p -1` is passed on the commandline.\n\n### Observed Behaviour\n\nOnly a single CPU core is used.\n\n### How to reproduce\n\nRun `sqlfluff lint -p -1` with a long list of files.\n\n### Dialect\n\nAffects all. \n\n### Version\n\n1.4.2\n\n### Configuration\n\nNone.\n\n### Are you willing to work on and submit a PR to address the issue?\n\n- [ ] Yes I am willing to submit a PR!\n\n### Code of Conduct\n\n- [X] I agree to follow this project's [Code of Conduct](https://github.com/sqlfluff/sqlfluff/blob/main/CODE_OF_CONDUCT.md)\n\n\n</issue>\n<code>\n[start of README.md]\n1 ![SQLFluff](https://raw.githubusercontent.com/sqlfluff/sqlfluff/main/images/sqlfluff-wide.png)\n2 \n3 # The SQL Linter for Humans\n4 \n5 [![PyPi Version](https://img.shields.io/pypi/v/sqlfluff.svg?style=flat-square&logo=PyPi)](https://pypi.org/project/sqlfluff/)\n6 [![PyPi License](https://img.shields.io/pypi/l/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n7 [![PyPi Python Versions](https://img.shields.io/pypi/pyversions/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n8 [![PyPi Status](https://img.shields.io/pypi/status/sqlfluff.svg?style=flat-square)](https://pypi.org/project/sqlfluff/)\n9 [![PyPi Downloads](https://img.shields.io/pypi/dm/sqlfluff?style=flat-square)](https://pypi.org/project/sqlfluff/)\n10 \n11 [![codecov](https://img.shields.io/codecov/c/gh/sqlfluff/sqlfluff.svg?style=flat-square&logo=Codecov)](https://codecov.io/gh/sqlfluff/sqlfluff)\n12 [![GitHub Workflow Status](https://img.shields.io/github/workflow/status/sqlfluff/sqlfluff/CI%20Tests?logo=github&style=flat-square)](https://github.com/sqlfluff/sqlfluff/actions/workflows/ci-tests.yml?query=branch%3Amain)\n13 [![ReadTheDocs](https://img.shields.io/readthedocs/sqlfluff?style=flat-square&logo=Read%20the%20Docs)](https://sqlfluff.readthedocs.io)\n14 [![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg?style=flat-square)](https://github.com/psf/black)\n15 [![Docker Pulls](https://img.shields.io/docker/pulls/sqlfluff/sqlfluff?logo=docker&style=flat-square)](https://hub.docker.com/r/sqlfluff/sqlfluff)\n16 \n17 **SQLFluff** is a dialect-flexible and configurable SQL linter. Designed with ELT applications in mind, **SQLFluff** also works with Jinja templating and dbt. **SQLFluff** will auto-fix most linting errors, allowing you to focus your time on what matters.\n18 \n19 ## Dialects Supported\n20 \n21 Although SQL is reasonably consistent in its implementations, there are several different dialects available with variations of syntax and grammar. **SQLFluff** currently supports the following SQL dialects (though perhaps not in full):\n22 \n23 - ANSI SQL - this is the base version and on occasion may not strictly follow the ANSI/ISO SQL definition\n24 - [Athena](https://aws.amazon.com/athena/)\n25 - [BigQuery](https://cloud.google.com/bigquery/)\n26 - [ClickHouse](https://clickhouse.com/)\n27 - [Databricks](https://databricks.com/) (note: currently this is just an alias for the `sparksql` dialect).\n28 - [Db2](https://www.ibm.com/analytics/db2)\n29 - [Exasol](https://www.exasol.com/)\n30 - [Hive](https://hive.apache.org/)\n31 - [MySQL](https://www.mysql.com/)\n32 - [Oracle](https://docs.oracle.com/en/database/oracle/oracle-database/21/sqlrf/index.html)\n33 - [PostgreSQL](https://www.postgresql.org/) (aka Postgres)\n34 - [Redshift](https://docs.aws.amazon.com/redshift/index.html)\n35 - [Snowflake](https://www.snowflake.com/)\n36 - [SOQL](https://developer.salesforce.com/docs/atlas.en-us.soql_sosl.meta/soql_sosl/sforce_api_calls_soql.htm)\n37 - [SparkSQL](https://spark.apache.org/docs/latest/)\n38 - [SQLite](https://www.sqlite.org/)\n39 - [Teradata](https://www.teradata.com/)\n40 - [Transact-SQL](https://docs.microsoft.com/en-us/sql/t-sql/language-reference) (aka T-SQL)\n41 \n42 We aim to make it easy to expand on the support of these dialects and also add other, currently unsupported, dialects. Please [raise issues](https://github.com/sqlfluff/sqlfluff/issues) (or upvote any existing issues) to let us know of demand for missing support.\n43 \n44 Pull requests from those that know the missing syntax or dialects are especially welcomed and are the question way for you to get support added. We are happy to work with any potential contributors on this to help them add this support. Please raise an issue first for any large feature change to ensure it is a good fit for this project before spending time on this work.\n45 \n46 ## Templates Supported\n47 \n48 SQL itself does not lend itself well to [modularity](https://docs.getdbt.com/docs/viewpoint#section-modularity), so to introduce some flexibility and reusability it is often [templated](https://en.wikipedia.org/wiki/Template_processor) as discussed more in [our modularity documentation](https://docs.sqlfluff.com/en/stable/realworld.html#modularity).\n49 \n50 **SQLFluff** supports the following templates:\n51 - [Jinja](https://jinja.palletsprojects.com/) (aka Jinja2)\n52 - [dbt](https://www.getdbt.com/)\n53 \n54 Again, please raise issues if you wish to support more templating languages/syntaxes.\n55 \n56 # Getting Started\n57 \n58 To get started, install the package and run `sqlfluff lint` or `sqlfluff fix`.\n59 \n60 ```shell\n61 $ pip install sqlfluff\n62 $ echo \"  SELECT a  +  b FROM tbl;  \" > test.sql\n63 $ sqlfluff lint test.sql --dialect ansi\n64 == [test.sql] FAIL\n65 L:   1 | P:   1 | L050 | Files must not begin with newlines or whitespace.\n66 L:   1 | P:   3 | L003 | First line has unexpected indent\n67 L:   1 | P:  11 | L039 | Unnecessary whitespace found.\n68 L:   1 | P:  14 | L039 | Unnecessary whitespace found.\n69 L:   1 | P:  27 | L001 | Unnecessary trailing whitespace.\n70 ```\n71 \n72 Alternatively, you can use the [**Official SQLFluff Docker Image**](https://hub.docker.com/r/sqlfluff/sqlfluff) or have a play using [**SQLFluff online**](https://online.sqlfluff.com/).\n73 \n74 For full [CLI usage](https://docs.sqlfluff.com/en/stable/cli.html) and [rules reference](https://docs.sqlfluff.com/en/stable/rules.html), see [the SQLFluff docs](https://docs.sqlfluff.com/en/stable/).\n75 \n76 # Documentation\n77 \n78 For full documentation visit [docs.sqlfluff.com](https://docs.sqlfluff.com/en/stable/). This documentation is generated from this repository so please raise [issues](https://github.com/sqlfluff/sqlfluff/issues) or pull requests for any additions, corrections, or clarifications.\n79 \n80 # Releases\n81 \n82 **SQLFluff** adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html), so breaking changes\n83 should be restricted to major versions releases. Some elements (such as the python API) are in a less\n84 stable state and may see more significant changes more often. See the [changelog](CHANGELOG.md) for more details.\n85 If you would like to join in please consider [contributing](CONTRIBUTING.md).\n86 \n87 New releases are made monthly. For more information, visit [Releases](https://github.com/sqlfluff/sqlfluff/releases).\n88 \n89 # SQLFluff on Slack\n90 \n91 We have a fast-growing community [on Slack](https://join.slack.com/t/sqlfluff/shared_invite/zt-o1f4x0e8-pZzarAIlQmKj_6ZwD16w0g), come and join us!\n92 \n93 # SQLFluff on Twitter\n94 \n95 Follow us [on Twitter @SQLFluff](https://twitter.com/SQLFluff) for announcements and other related posts.\n96 \n97 # Contributing\n98 \n99 We are grateful to all our [contributors](https://github.com/sqlfluff/sqlfluff/graphs/contributors). There is a lot to do in this project, and we are just getting started.\n100 \n101 If you want to understand more about the architecture of **SQLFluff**, you can\n102 find [more here](https://docs.sqlfluff.com/en/latest/internals.html#architecture).\n103 \n104 If you would like to contribute, check out the [open issues on GitHub](https://github.com/sqlfluff/sqlfluff/issues). You can also see the guide to [contributing](CONTRIBUTING.md).\n105 \n[end of README.md]\n[start of src/sqlfluff/core/linter/linter.py]\n1 \"\"\"Defines the linter class.\"\"\"\n2 \n3 import fnmatch\n4 import os\n5 import time\n6 import logging\n7 from typing import (\n8     Any,\n9     Iterable,\n10     Iterator,\n11     List,\n12     Optional,\n13     Sequence,\n14     Set,\n15     Tuple,\n16     Type,\n17     cast,\n18 )\n19 \n20 import pathspec\n21 import regex\n22 from tqdm import tqdm\n23 \n24 from sqlfluff.core.errors import (\n25     SQLBaseError,\n26     SQLLexError,\n27     SQLLintError,\n28     SQLParseError,\n29     SQLFluffSkipFile,\n30     SQLFluffUserError,\n31 )\n32 from sqlfluff.core.parser import Lexer, Parser, RegexLexer\n33 from sqlfluff.core.file_helpers import get_encoding\n34 from sqlfluff.core.templaters import TemplatedFile\n35 from sqlfluff.core.rules import get_ruleset\n36 from sqlfluff.core.rules.doc_decorators import is_fix_compatible\n37 from sqlfluff.core.config import FluffConfig, ConfigLoader, progress_bar_configuration\n38 \n39 # Classes needed only for type checking\n40 from sqlfluff.core.parser.segments.base import BaseSegment, SourceFix\n41 from sqlfluff.core.parser.segments.meta import MetaSegment\n42 from sqlfluff.core.parser.segments.raw import RawSegment\n43 from sqlfluff.core.rules import BaseRule\n44 \n45 from sqlfluff.core.linter.common import (\n46     RuleTuple,\n47     ParsedString,\n48     NoQaDirective,\n49     RenderedFile,\n50 )\n51 from sqlfluff.core.linter.linted_file import LintedFile\n52 from sqlfluff.core.linter.linted_dir import LintedDir\n53 from sqlfluff.core.linter.linting_result import LintingResult\n54 \n55 \n56 WalkableType = Iterable[Tuple[str, Optional[List[str]], List[str]]]\n57 \n58 # Instantiate the linter logger\n59 linter_logger: logging.Logger = logging.getLogger(\"sqlfluff.linter\")\n60 \n61 \n62 class Linter:\n63     \"\"\"The interface class to interact with the linter.\"\"\"\n64 \n65     # Default to allowing process parallelism\n66     allow_process_parallelism = True\n67 \n68     def __init__(\n69         self,\n70         config: Optional[FluffConfig] = None,\n71         formatter: Any = None,\n72         dialect: Optional[str] = None,\n73         rules: Optional[List[str]] = None,\n74         user_rules: Optional[List[Type[BaseRule]]] = None,\n75         exclude_rules: Optional[List[str]] = None,\n76     ) -> None:\n77         # Store the config object\n78         self.config = FluffConfig.from_kwargs(\n79             config=config,\n80             dialect=dialect,\n81             rules=rules,\n82             exclude_rules=exclude_rules,\n83             # Don't require a dialect to be provided yet. Defer this until we\n84             # are actually linting something, since the directory we are linting\n85             # from may provide additional configuration, including a dialect.\n86             require_dialect=False,\n87         )\n88         # Get the dialect and templater\n89         self.dialect = self.config.get(\"dialect_obj\")\n90         self.templater = self.config.get(\"templater_obj\")\n91         # Store the formatter for output\n92         self.formatter = formatter\n93         # Store references to user rule classes\n94         self.user_rules = user_rules or []\n95 \n96     def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n97         \"\"\"Get hold of a set of rules.\"\"\"\n98         rs = get_ruleset()\n99         # Register any user rules\n100         for rule in self.user_rules:\n101             rs.register(rule)\n102         cfg = config or self.config\n103         return rs.get_rulelist(config=cfg)\n104 \n105     def rule_tuples(self) -> List[RuleTuple]:\n106         \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n107         rs = self.get_ruleset()\n108         return [RuleTuple(rule.code, rule.description) for rule in rs]\n109 \n110     # #### Static methods\n111     # These are the building blocks of the linting process.\n112 \n113     @staticmethod\n114     def load_raw_file_and_config(\n115         fname: str, root_config: FluffConfig\n116     ) -> Tuple[str, FluffConfig, str]:\n117         \"\"\"Load a raw file and the associated config.\"\"\"\n118         file_config = root_config.make_child_from_path(fname)\n119         encoding = get_encoding(fname=fname, config=file_config)\n120         # Check file size before loading.\n121         limit = file_config.get(\"large_file_skip_byte_limit\")\n122         if limit:\n123             # Get the file size\n124             file_size = os.path.getsize(fname)\n125             if file_size > limit:\n126                 raise SQLFluffSkipFile(\n127                     f\"Length of file {fname!r} is {file_size} bytes which is over \"\n128                     f\"the limit of {limit} bytes. Skipping to avoid parser lock. \"\n129                     \"Users can increase this limit in their config by setting the \"\n130                     \"'large_file_skip_byte_limit' value, or disable by setting it \"\n131                     \"to zero.\"\n132                 )\n133         with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n134             raw_file = target_file.read()\n135         # Scan the raw file for config commands.\n136         file_config.process_raw_file_for_config(raw_file)\n137         # Return the raw file and config\n138         return raw_file, file_config, encoding\n139 \n140     @staticmethod\n141     def _normalise_newlines(string: str) -> str:\n142         \"\"\"Normalise newlines to unix-style line endings.\"\"\"\n143         return regex.sub(r\"\\r\\n|\\r\", \"\\n\", string)\n144 \n145     @staticmethod\n146     def _lex_templated_file(\n147         templated_file: TemplatedFile, config: FluffConfig\n148     ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n149         \"\"\"Lex a templated file.\n150 \n151         NOTE: This potentially mutates the config, so make sure to\n152         use the returned one.\n153         \"\"\"\n154         violations = []\n155         linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n156         # Get the lexer\n157         lexer = Lexer(config=config)\n158         # Lex the file and log any problems\n159         try:\n160             tokens, lex_vs = lexer.lex(templated_file)\n161             # We might just get the violations as a list\n162             violations += lex_vs\n163             linter_logger.info(\n164                 \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n165             )\n166         except SQLLexError as err:\n167             linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n168             violations.append(err)\n169             return None, violations, config\n170 \n171         if not tokens:  # pragma: no cover TODO?\n172             return None, violations, config\n173 \n174         # Check that we've got sensible indentation from the lexer.\n175         # We might need to suppress if it's a complicated file.\n176         templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n177         if isinstance(templating_blocks_indent, str):\n178             force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n179         else:\n180             force_block_indent = False\n181         templating_blocks_indent = bool(templating_blocks_indent)\n182         # If we're forcing it through we don't check.\n183         if templating_blocks_indent and not force_block_indent:\n184             indent_balance = sum(\n185                 getattr(elem, \"indent_val\", 0)\n186                 for elem in cast(Tuple[BaseSegment, ...], tokens)\n187             )\n188             if indent_balance != 0:\n189                 linter_logger.debug(\n190                     \"Indent balance test failed for %r. Template indents will not be \"\n191                     \"linted for this file.\",\n192                     templated_file.fname,\n193                 )\n194                 # Don't enable the templating blocks.\n195                 templating_blocks_indent = False\n196 \n197         # The file will have been lexed without config, so check all indents\n198         # are enabled.\n199         new_tokens = []\n200         for token in cast(Tuple[BaseSegment, ...], tokens):\n201             if token.is_meta:\n202                 token = cast(MetaSegment, token)\n203                 if token.indent_val != 0:\n204                     # Don't allow it if we're not linting templating block indents.\n205                     if not templating_blocks_indent:\n206                         continue\n207             new_tokens.append(token)\n208 \n209         # Return new buffer\n210         return new_tokens, violations, config\n211 \n212     @staticmethod\n213     def _parse_tokens(\n214         tokens: Sequence[BaseSegment],\n215         config: FluffConfig,\n216         recurse: bool = True,\n217         fname: Optional[str] = None,\n218     ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n219         parser = Parser(config=config)\n220         violations = []\n221         # Parse the file and log any problems\n222         try:\n223             parsed: Optional[BaseSegment] = parser.parse(\n224                 tokens,\n225                 recurse=recurse,\n226                 fname=fname,\n227             )\n228         except SQLParseError as err:\n229             linter_logger.info(\"PARSING FAILED! : %s\", err)\n230             violations.append(err)\n231             return None, violations\n232 \n233         if parsed:\n234             linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n235             linter_logger.info(\"\\n\" + parsed.stringify())\n236             # We may succeed parsing, but still have unparsable segments. Extract them\n237             # here.\n238             for unparsable in parsed.iter_unparsables():\n239                 # No exception has been raised explicitly, but we still create one here\n240                 # so that we can use the common interface\n241                 violations.append(\n242                     SQLParseError(\n243                         \"Line {0[0]}, Position {0[1]}: Found unparsable section: \"\n244                         \"{1!r}\".format(\n245                             unparsable.pos_marker.working_loc,\n246                             unparsable.raw\n247                             if len(unparsable.raw) < 40\n248                             else unparsable.raw[:40] + \"...\",\n249                         ),\n250                         segment=unparsable,\n251                     )\n252                 )\n253                 linter_logger.info(\"Found unparsable segment...\")\n254                 linter_logger.info(unparsable.stringify())\n255         return parsed, violations\n256 \n257     @staticmethod\n258     def parse_noqa(\n259         comment: str,\n260         line_no: int,\n261         rule_codes: List[str],\n262     ):\n263         \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n264         # Also trim any whitespace afterward\n265 \n266         # Comment lines can also have noqa e.g.\n267         # --dafhsdkfwdiruweksdkjdaffldfsdlfjksd -- noqa: L016\n268         # Therefore extract last possible inline ignore.\n269         comment = [c.strip() for c in comment.split(\"--\")][-1]\n270 \n271         if comment.startswith(\"noqa\"):\n272             # This is an ignore identifier\n273             comment_remainder = comment[4:]\n274             if comment_remainder:\n275                 if not comment_remainder.startswith(\":\"):\n276                     return SQLParseError(\n277                         \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n278                         line_no=line_no,\n279                     )\n280                 comment_remainder = comment_remainder[1:].strip()\n281                 if comment_remainder:\n282                     action: Optional[str]\n283                     if \"=\" in comment_remainder:\n284                         action, rule_part = comment_remainder.split(\"=\", 1)\n285                         if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n286                             return SQLParseError(\n287                                 \"Malformed 'noqa' section. \"\n288                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n289                                 \"or 'noqa: disable=<rule>[,...] | all\",\n290                                 line_no=line_no,\n291                             )\n292                     else:\n293                         action = None\n294                         rule_part = comment_remainder\n295                         if rule_part in {\"disable\", \"enable\"}:\n296                             return SQLParseError(\n297                                 \"Malformed 'noqa' section. \"\n298                                 \"Expected 'noqa: enable=<rule>[,...] | all' \"\n299                                 \"or 'noqa: disable=<rule>[,...] | all\",\n300                                 line_no=line_no,\n301                             )\n302                     rules: Optional[Tuple[str, ...]]\n303                     if rule_part != \"all\":\n304                         # Rules can be globs therefore we compare to the rule_set to\n305                         # expand the globs.\n306                         unexpanded_rules = tuple(\n307                             r.strip() for r in rule_part.split(\",\")\n308                         )\n309                         expanded_rules = []\n310                         for r in unexpanded_rules:\n311                             expanded_rule = [\n312                                 x\n313                                 for x in fnmatch.filter(rule_codes, r)\n314                                 if x not in expanded_rules\n315                             ]\n316                             if expanded_rule:\n317                                 expanded_rules.extend(expanded_rule)\n318                             elif r not in expanded_rules:\n319                                 # We were unable to expand the glob.\n320                                 # Therefore assume the user is referencing\n321                                 # a special error type (e.g. PRS, LXR, or TMP)\n322                                 # and add this to the list of rules to ignore.\n323                                 expanded_rules.append(r)\n324                         rules = tuple(expanded_rules)\n325                     else:\n326                         rules = None\n327                     return NoQaDirective(line_no, rules, action)\n328             return NoQaDirective(line_no, None, None)\n329         return None\n330 \n331     @staticmethod\n332     def remove_templated_errors(\n333         linting_errors: List[SQLBaseError],\n334     ) -> List[SQLBaseError]:\n335         \"\"\"Filter a list of lint errors, removing those from the templated slices.\"\"\"\n336         # Filter out any linting errors in templated sections if relevant.\n337         result: List[SQLBaseError] = []\n338         for e in linting_errors:\n339             if isinstance(e, SQLLintError):\n340                 if (\n341                     # Is it in a literal section?\n342                     e.segment.pos_marker.is_literal()\n343                     # Is it a rule that is designed to work on templated sections?\n344                     or e.rule.targets_templated\n345                 ):\n346                     result.append(e)\n347             else:\n348                 # If it's another type, just keep it. (E.g. SQLParseError from\n349                 # malformed \"noqa\" comment).\n350                 result.append(e)\n351         return result\n352 \n353     @staticmethod\n354     def _report_conflicting_fixes_same_anchor(message: str):  # pragma: no cover\n355         # This function exists primarily in order to let us monkeypatch it at\n356         # runtime (replacing it with a function that raises an exception).\n357         linter_logger.critical(message)\n358 \n359     @staticmethod\n360     def _warn_unfixable(code: str):\n361         linter_logger.warning(\n362             f\"One fix for {code} not applied, it would re-cause the same error.\"\n363         )\n364 \n365     # ### Class Methods\n366     # These compose the base static methods into useful recipes.\n367 \n368     @classmethod\n369     def parse_rendered(\n370         cls,\n371         rendered: RenderedFile,\n372         recurse: bool = True,\n373     ) -> ParsedString:\n374         \"\"\"Parse a rendered file.\"\"\"\n375         t0 = time.monotonic()\n376         violations = cast(List[SQLBaseError], rendered.templater_violations)\n377         tokens: Optional[Sequence[BaseSegment]]\n378         if rendered.templated_file:\n379             tokens, lvs, config = cls._lex_templated_file(\n380                 rendered.templated_file, rendered.config\n381             )\n382             violations += lvs\n383         else:\n384             tokens = None\n385 \n386         t1 = time.monotonic()\n387         linter_logger.info(\"PARSING (%s)\", rendered.fname)\n388 \n389         if tokens:\n390             parsed, pvs = cls._parse_tokens(\n391                 tokens,\n392                 rendered.config,\n393                 recurse=recurse,\n394                 fname=rendered.fname,\n395             )\n396             violations += pvs\n397         else:\n398             parsed = None\n399 \n400         time_dict = {\n401             **rendered.time_dict,\n402             \"lexing\": t1 - t0,\n403             \"parsing\": time.monotonic() - t1,\n404         }\n405         return ParsedString(\n406             parsed,\n407             violations,\n408             time_dict,\n409             rendered.templated_file,\n410             rendered.config,\n411             rendered.fname,\n412             rendered.source_str,\n413         )\n414 \n415     @classmethod\n416     def extract_ignore_from_comment(\n417         cls,\n418         comment: RawSegment,\n419         rule_codes: List[str],\n420     ):\n421         \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n422         # Also trim any whitespace afterward\n423         comment_content = comment.raw_trimmed().strip()\n424         comment_line, _ = comment.pos_marker.source_position()\n425         result = cls.parse_noqa(comment_content, comment_line, rule_codes)\n426         if isinstance(result, SQLParseError):\n427             result.segment = comment\n428         return result\n429 \n430     @classmethod\n431     def extract_ignore_mask_tree(\n432         cls,\n433         tree: BaseSegment,\n434         rule_codes: List[str],\n435     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n436         \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n437         ignore_buff: List[NoQaDirective] = []\n438         violations: List[SQLBaseError] = []\n439         for comment in tree.recursive_crawl(\"comment\"):\n440             if comment.is_type(\"inline_comment\"):\n441                 ignore_entry = cls.extract_ignore_from_comment(comment, rule_codes)\n442                 if isinstance(ignore_entry, SQLParseError):\n443                     violations.append(ignore_entry)\n444                 elif ignore_entry:\n445                     ignore_buff.append(ignore_entry)\n446         if ignore_buff:\n447             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n448         return ignore_buff, violations\n449 \n450     @classmethod\n451     def extract_ignore_mask_source(\n452         cls,\n453         source: str,\n454         inline_comment_regex: RegexLexer,\n455         rule_codes: List[str],\n456     ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n457         \"\"\"Look for inline ignore comments and return NoQaDirectives.\n458 \n459         Very similar to extract_ignore_mask_tree(), but can be run on raw source\n460         (i.e. does not require the code to have parsed successfully).\n461         \"\"\"\n462         ignore_buff: List[NoQaDirective] = []\n463         violations: List[SQLBaseError] = []\n464         for idx, line in enumerate(source.split(\"\\n\")):\n465             match = inline_comment_regex.search(line) if line else None\n466             if match:\n467                 ignore_entry = cls.parse_noqa(\n468                     line[match[0] : match[1]], idx + 1, rule_codes\n469                 )\n470                 if isinstance(ignore_entry, SQLParseError):\n471                     violations.append(ignore_entry)  # pragma: no cover\n472                 elif ignore_entry:\n473                     ignore_buff.append(ignore_entry)\n474         if ignore_buff:\n475             linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n476         return ignore_buff, violations\n477 \n478     @classmethod\n479     def lint_fix_parsed(\n480         cls,\n481         tree: BaseSegment,\n482         config: FluffConfig,\n483         rule_set: List[BaseRule],\n484         fix: bool = False,\n485         fname: Optional[str] = None,\n486         templated_file: Optional[TemplatedFile] = None,\n487         formatter: Any = None,\n488     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n489         \"\"\"Lint and optionally fix a tree object.\"\"\"\n490         # Keep track of the linting errors on the very first linter pass. The\n491         # list of issues output by \"lint\" and \"fix\" only includes issues present\n492         # in the initial SQL code, EXCLUDING any issues that may be created by\n493         # the fixes themselves.\n494         initial_linting_errors = []\n495         # A placeholder for the fixes we had on the previous loop\n496         last_fixes = None\n497         # Keep a set of previous versions to catch infinite loops.\n498         previous_versions: Set[Tuple[str, Tuple[SourceFix, ...]]] = {(tree.raw, ())}\n499 \n500         # If we are fixing then we want to loop up to the runaway_limit, otherwise just\n501         # once for linting.\n502         loop_limit = config.get(\"runaway_limit\") if fix else 1\n503 \n504         # Dispatch the output for the lint header\n505         if formatter:\n506             formatter.dispatch_lint_header(fname)\n507 \n508         # Look for comment segments which might indicate lines to ignore.\n509         if not config.get(\"disable_noqa\"):\n510             rule_codes = [r.code for r in rule_set]\n511             ignore_buff, ivs = cls.extract_ignore_mask_tree(tree, rule_codes)\n512             initial_linting_errors += ivs\n513         else:\n514             ignore_buff = []\n515 \n516         save_tree = tree\n517         # There are two phases of rule running.\n518         # 1. The main loop is for most rules. These rules are assumed to\n519         # interact and cause a cascade of fixes requiring multiple passes.\n520         # These are run the `runaway_limit` number of times (default 10).\n521         # 2. The post loop is for post-processing rules, not expected to trigger\n522         # any downstream rules, e.g. capitalization fixes. They are run on the\n523         # first loop and then twice at the end (once to fix, and once again to\n524         # check result of fixes), but not in the intervening loops.\n525         phases = [\"main\"]\n526         if fix:\n527             phases.append(\"post\")\n528         for phase in phases:\n529             if len(phases) > 1:\n530                 rules_this_phase = [\n531                     rule for rule in rule_set if rule.lint_phase == phase\n532                 ]\n533             else:\n534                 rules_this_phase = rule_set\n535             for loop in range(loop_limit if phase == \"main\" else 2):\n536 \n537                 def is_first_linter_pass():\n538                     return phase == phases[0] and loop == 0\n539 \n540                 # Additional newlines are to assist in scanning linting loops\n541                 # during debugging.\n542                 linter_logger.info(\n543                     f\"\\n\\nEntering linter phase {phase}, loop {loop+1}/{loop_limit}\\n\"\n544                 )\n545                 changed = False\n546 \n547                 if is_first_linter_pass():\n548                     # In order to compute initial_linting_errors correctly, need\n549                     # to run all rules on the first loop of the main phase.\n550                     rules_this_phase = rule_set\n551                 progress_bar_crawler = tqdm(\n552                     rules_this_phase,\n553                     desc=\"lint by rules\",\n554                     leave=False,\n555                     disable=progress_bar_configuration.disable_progress_bar,\n556                 )\n557 \n558                 for crawler in progress_bar_crawler:\n559                     # Performance: After first loop pass, skip rules that don't\n560                     # do fixes. Any results returned won't be seen by the user\n561                     # anyway (linting errors ADDED by rules changing SQL, are\n562                     # not reported back to the user - only initial linting errors),\n563                     # so there's absolutely no reason to run them.\n564                     if (\n565                         fix\n566                         and not is_first_linter_pass()\n567                         and not is_fix_compatible(crawler)\n568                     ):\n569                         continue\n570 \n571                     progress_bar_crawler.set_description(f\"rule {crawler.code}\")\n572 \n573                     # fixes should be a dict {} with keys edit, delete, create\n574                     # delete is just a list of segments to delete\n575                     # edit and create are list of tuples. The first element is\n576                     # the \"anchor\", the segment to look for either to edit or to\n577                     # insert BEFORE. The second is the element to insert or create.\n578                     linting_errors, _, fixes, _ = crawler.crawl(\n579                         tree,\n580                         dialect=config.get(\"dialect_obj\"),\n581                         fix=fix,\n582                         templated_file=templated_file,\n583                         ignore_mask=ignore_buff,\n584                         fname=fname,\n585                         config=config,\n586                     )\n587                     if is_first_linter_pass():\n588                         initial_linting_errors += linting_errors\n589 \n590                     if fix and fixes:\n591                         linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n592                         # Do some sanity checks on the fixes before applying.\n593                         anchor_info = BaseSegment.compute_anchor_edit_info(fixes)\n594                         if any(\n595                             not info.is_valid for info in anchor_info.values()\n596                         ):  # pragma: no cover\n597                             message = (\n598                                 f\"Rule {crawler.code} returned conflicting \"\n599                                 \"fixes with the same anchor. This is only \"\n600                                 \"supported for create_before+create_after, so \"\n601                                 f\"the fixes will not be applied. {fixes!r}\"\n602                             )\n603                             cls._report_conflicting_fixes_same_anchor(message)\n604                             for lint_result in linting_errors:\n605                                 lint_result.fixes = []\n606                         elif fixes == last_fixes:  # pragma: no cover\n607                             # If we generate the same fixes two times in a row,\n608                             # that means we're in a loop, and we want to stop.\n609                             # (Fixes should address issues, hence different\n610                             # and/or fewer fixes next time.)\n611                             cls._warn_unfixable(crawler.code)\n612                         else:\n613                             # This is the happy path. We have fixes, now we want to\n614                             # apply them.\n615                             last_fixes = fixes\n616                             new_tree, _, _ = tree.apply_fixes(\n617                                 config.get(\"dialect_obj\"), crawler.code, anchor_info\n618                             )\n619                             # Check for infinite loops. We use a combination of the\n620                             # fixed templated file and the list of source fixes to\n621                             # apply.\n622                             loop_check_tuple = (\n623                                 new_tree.raw,\n624                                 tuple(new_tree.source_fixes),\n625                             )\n626                             if loop_check_tuple not in previous_versions:\n627                                 # We've not seen this version of the file so\n628                                 # far. Continue.\n629                                 tree = new_tree\n630                                 previous_versions.add(loop_check_tuple)\n631                                 changed = True\n632                                 continue\n633                             else:\n634                                 # Applying these fixes took us back to a state\n635                                 # which we've seen before. We're in a loop, so\n636                                 # we want to stop.\n637                                 cls._warn_unfixable(crawler.code)\n638 \n639                 if fix and not changed:\n640                     # We did not change the file. Either the file is clean (no\n641                     # fixes), or any fixes which are present will take us back\n642                     # to a previous state.\n643                     linter_logger.info(\n644                         f\"Fix loop complete for {phase} phase. Stability \"\n645                         f\"achieved after {loop}/{loop_limit} loops.\"\n646                     )\n647                     break\n648             else:\n649                 if fix:\n650                     # The linter loop hit the limit before reaching a stable point\n651                     # (i.e. free of lint errors). If this happens, it's usually\n652                     # because one or more rules produced fixes which did not address\n653                     # the original issue **or** created new issues.\n654                     linter_logger.warning(\n655                         f\"Loop limit on fixes reached [{loop_limit}].\"\n656                     )\n657 \n658                     # Discard any fixes for the linting errors, since they caused a\n659                     # loop. IMPORTANT: By doing this, we are telling SQLFluff that\n660                     # these linting errors are \"unfixable\". This is important,\n661                     # because when \"sqlfluff fix\" encounters unfixable lint errors,\n662                     # it exits with a \"failure\" exit code, which is exactly what we\n663                     # want in this situation. (Reason: Although this is more of an\n664                     # internal SQLFluff issue, users deserve to know about it,\n665                     # because it means their file(s) weren't fixed.\n666                     for violation in initial_linting_errors:\n667                         if isinstance(violation, SQLLintError):\n668                             violation.fixes = []\n669 \n670                     # Return the original parse tree, before any fixes were applied.\n671                     # Reason: When the linter hits the loop limit, the file is often\n672                     # messy, e.g. some of the fixes were applied repeatedly, possibly\n673                     # other weird things. We don't want the user to see this junk!\n674                     return save_tree, initial_linting_errors, ignore_buff\n675 \n676         if config.get(\"ignore_templated_areas\", default=True):\n677             initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n678 \n679         return tree, initial_linting_errors, ignore_buff\n680 \n681     @classmethod\n682     def lint_parsed(\n683         cls,\n684         parsed: ParsedString,\n685         rule_set: List[BaseRule],\n686         fix: bool = False,\n687         formatter: Any = None,\n688         encoding: str = \"utf8\",\n689     ):\n690         \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n691         violations = parsed.violations\n692         time_dict = parsed.time_dict\n693         tree: Optional[BaseSegment]\n694         if parsed.tree:\n695             t0 = time.monotonic()\n696             linter_logger.info(\"LINTING (%s)\", parsed.fname)\n697             tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n698                 parsed.tree,\n699                 config=parsed.config,\n700                 rule_set=rule_set,\n701                 fix=fix,\n702                 fname=parsed.fname,\n703                 templated_file=parsed.templated_file,\n704                 formatter=formatter,\n705             )\n706             # Update the timing dict\n707             time_dict[\"linting\"] = time.monotonic() - t0\n708 \n709             # We're only going to return the *initial* errors, rather\n710             # than any generated during the fixing cycle.\n711             violations += initial_linting_errors\n712         else:\n713             # If no parsed tree, set to None\n714             tree = None\n715             ignore_buff = []\n716             if not parsed.config.get(\"disable_noqa\"):\n717                 # Templating and/or parsing have failed. Look for \"noqa\"\n718                 # comments (the normal path for identifying these comments\n719                 # requires access to the parse tree, and because of the failure,\n720                 # we don't have a parse tree).\n721                 ignore_buff, ignore_violations = cls.extract_ignore_mask_source(\n722                     parsed.source_str,\n723                     [\n724                         lm\n725                         for lm in parsed.config.get(\"dialect_obj\").lexer_matchers\n726                         if lm.name == \"inline_comment\"\n727                     ][0],\n728                     [r.code for r in rule_set],\n729                 )\n730                 violations += ignore_violations\n731 \n732         # We process the ignore config here if appropriate\n733         for violation in violations:\n734             violation.ignore_if_in(parsed.config.get(\"ignore\"))\n735             violation.warning_if_in(parsed.config.get(\"warnings\"))\n736 \n737         linted_file = LintedFile(\n738             parsed.fname,\n739             # Deduplicate violations\n740             LintedFile.deduplicate_in_source_space(violations),\n741             time_dict,\n742             tree,\n743             ignore_mask=ignore_buff,\n744             templated_file=parsed.templated_file,\n745             encoding=encoding,\n746         )\n747 \n748         # This is the main command line output from linting.\n749         if formatter:\n750             formatter.dispatch_file_violations(\n751                 parsed.fname, linted_file, only_fixable=fix\n752             )\n753 \n754         # Safety flag for unset dialects\n755         if linted_file.get_violations(\n756             fixable=True if fix else None, types=SQLParseError\n757         ):\n758             if formatter:  # pragma: no cover TODO?\n759                 formatter.dispatch_dialect_warning(parsed.config.get(\"dialect\"))\n760 \n761         return linted_file\n762 \n763     @classmethod\n764     def lint_rendered(\n765         cls,\n766         rendered: RenderedFile,\n767         rule_set: List[BaseRule],\n768         fix: bool = False,\n769         formatter: Any = None,\n770     ) -> LintedFile:\n771         \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n772         parsed = cls.parse_rendered(rendered)\n773         return cls.lint_parsed(\n774             parsed,\n775             rule_set=rule_set,\n776             fix=fix,\n777             formatter=formatter,\n778             encoding=rendered.encoding,\n779         )\n780 \n781     # ### Instance Methods\n782     # These are tied to a specific instance and so are not necessarily\n783     # safe to use in parallel operations.\n784 \n785     def render_string(\n786         self, in_str: str, fname: str, config: FluffConfig, encoding: str\n787     ) -> RenderedFile:\n788         \"\"\"Template the file.\"\"\"\n789         linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n790 \n791         # Start the templating timer\n792         t0 = time.monotonic()\n793 \n794         # Newlines are normalised to unix-style line endings (\\n).\n795         # The motivation is that Jinja normalises newlines during templating and\n796         # we want consistent mapping between the raw and templated slices.\n797         in_str = self._normalise_newlines(in_str)\n798 \n799         # Since Linter.__init__() does not require a dialect to be specified,\n800         # check for one now. (We're processing a string, not a file, so we're\n801         # not going to pick up a .sqlfluff or other config file to provide a\n802         # missing dialect at this point.)\n803         config.verify_dialect_specified()\n804         if not config.get(\"templater_obj\") == self.templater:\n805             linter_logger.warning(\n806                 (\n807                     f\"Attempt to set templater to {config.get('templater_obj').name} \"\n808                     f\"failed. Using {self.templater.name} templater. Templater cannot \"\n809                     \"be set in a .sqlfluff file in a subdirectory of the current \"\n810                     \"working directory. It can be set in a .sqlfluff in the current \"\n811                     \"working directory. See Nesting section of the docs for more \"\n812                     \"details.\"\n813                 )\n814             )\n815         try:\n816             templated_file, templater_violations = self.templater.process(\n817                 in_str=in_str, fname=fname, config=config, formatter=self.formatter\n818             )\n819         except SQLFluffSkipFile as s:  # pragma: no cover\n820             linter_logger.warning(str(s))\n821             templated_file = None\n822             templater_violations = []\n823 \n824         if not templated_file:\n825             linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n826 \n827         # Record time\n828         time_dict = {\"templating\": time.monotonic() - t0}\n829 \n830         return RenderedFile(\n831             templated_file,\n832             templater_violations,\n833             config,\n834             time_dict,\n835             fname,\n836             encoding,\n837             in_str,\n838         )\n839 \n840     def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n841         \"\"\"Load and render a file with relevant config.\"\"\"\n842         # Load the raw file.\n843         raw_file, config, encoding = self.load_raw_file_and_config(fname, root_config)\n844         # Render the file\n845         return self.render_string(raw_file, fname, config, encoding)\n846 \n847     def parse_string(\n848         self,\n849         in_str: str,\n850         fname: str = \"<string>\",\n851         recurse: bool = True,\n852         config: Optional[FluffConfig] = None,\n853         encoding: str = \"utf-8\",\n854     ) -> ParsedString:\n855         \"\"\"Parse a string.\"\"\"\n856         violations: List[SQLBaseError] = []\n857 \n858         # Dispatch the output for the template header (including the config diff)\n859         if self.formatter:\n860             self.formatter.dispatch_template_header(fname, self.config, config)\n861 \n862         # Just use the local config from here:\n863         config = config or self.config\n864 \n865         # Scan the raw file for config commands.\n866         config.process_raw_file_for_config(in_str)\n867         rendered = self.render_string(in_str, fname, config, encoding)\n868         violations += rendered.templater_violations\n869 \n870         # Dispatch the output for the parse header\n871         if self.formatter:\n872             self.formatter.dispatch_parse_header(fname)\n873 \n874         return self.parse_rendered(rendered, recurse=recurse)\n875 \n876     def fix(\n877         self,\n878         tree: BaseSegment,\n879         config: Optional[FluffConfig] = None,\n880         fname: Optional[str] = None,\n881         templated_file: Optional[TemplatedFile] = None,\n882     ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n883         \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n884         config = config or self.config\n885         rule_set = self.get_ruleset(config=config)\n886         fixed_tree, violations, _ = self.lint_fix_parsed(\n887             tree,\n888             config,\n889             rule_set,\n890             fix=True,\n891             fname=fname,\n892             templated_file=templated_file,\n893             formatter=self.formatter,\n894         )\n895         return fixed_tree, violations\n896 \n897     def lint(\n898         self,\n899         tree: BaseSegment,\n900         config: Optional[FluffConfig] = None,\n901         fname: Optional[str] = None,\n902         templated_file: Optional[TemplatedFile] = None,\n903     ) -> List[SQLBaseError]:\n904         \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n905         config = config or self.config\n906         rule_set = self.get_ruleset(config=config)\n907         _, violations, _ = self.lint_fix_parsed(\n908             tree,\n909             config,\n910             rule_set,\n911             fix=False,\n912             fname=fname,\n913             templated_file=templated_file,\n914             formatter=self.formatter,\n915         )\n916         return violations\n917 \n918     def lint_string(\n919         self,\n920         in_str: str = \"\",\n921         fname: str = \"<string input>\",\n922         fix: bool = False,\n923         config: Optional[FluffConfig] = None,\n924         encoding: str = \"utf8\",\n925     ) -> LintedFile:\n926         \"\"\"Lint a string.\n927 \n928         Returns:\n929             :obj:`LintedFile`: an object representing that linted file.\n930 \n931         \"\"\"\n932         # Sort out config, defaulting to the built in config if no override\n933         config = config or self.config\n934         # Parse the string.\n935         parsed = self.parse_string(\n936             in_str=in_str,\n937             fname=fname,\n938             config=config,\n939         )\n940         # Get rules as appropriate\n941         rule_set = self.get_ruleset(config=config)\n942         # Lint the file and return the LintedFile\n943         return self.lint_parsed(\n944             parsed,\n945             rule_set,\n946             fix=fix,\n947             formatter=self.formatter,\n948             encoding=encoding,\n949         )\n950 \n951     def paths_from_path(\n952         self,\n953         path: str,\n954         ignore_file_name: str = \".sqlfluffignore\",\n955         ignore_non_existent_files: bool = False,\n956         ignore_files: bool = True,\n957         working_path: str = os.getcwd(),\n958     ) -> List[str]:\n959         \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n960 \n961         Here we also deal with the .sqlfluffignore file if present.\n962 \n963         When a path to a file to be linted is explicitly passed\n964         we look for ignore files in all directories that are parents of the file,\n965         up to the current directory.\n966 \n967         If the current directory is not a parent of the file we only\n968         look for an ignore file in the direct parent of the file.\n969 \n970         \"\"\"\n971         if not os.path.exists(path):\n972             if ignore_non_existent_files:\n973                 return []\n974             else:\n975                 raise SQLFluffUserError(\n976                     f\"Specified path does not exist. Check it/they exist(s): {path}.\"\n977                 )\n978 \n979         # Files referred to exactly are also ignored if\n980         # matched, but we warn the users when that happens\n981         is_exact_file = os.path.isfile(path)\n982 \n983         path_walk: WalkableType\n984         if is_exact_file:\n985             # When the exact file to lint is passed, we\n986             # fill path_walk with an input that follows\n987             # the structure of `os.walk`:\n988             #   (root, directories, files)\n989             dirpath = os.path.dirname(path)\n990             files = [os.path.basename(path)]\n991             path_walk = [(dirpath, None, files)]\n992         else:\n993             path_walk = list(os.walk(path))\n994 \n995         ignore_file_paths = ConfigLoader.find_ignore_config_files(\n996             path=path, working_path=working_path, ignore_file_name=ignore_file_name\n997         )\n998         # Add paths that could contain \"ignore files\"\n999         # to the path_walk list\n1000         path_walk_ignore_file = [\n1001             (\n1002                 os.path.dirname(ignore_file_path),\n1003                 None,\n1004                 # Only one possible file, since we only\n1005                 # have one \"ignore file name\"\n1006                 [os.path.basename(ignore_file_path)],\n1007             )\n1008             for ignore_file_path in ignore_file_paths\n1009         ]\n1010         path_walk += path_walk_ignore_file\n1011 \n1012         # If it's a directory then expand the path!\n1013         buffer = []\n1014         ignores = {}\n1015         for dirpath, _, filenames in path_walk:\n1016             for fname in filenames:\n1017                 fpath = os.path.join(dirpath, fname)\n1018                 # Handle potential .sqlfluffignore files\n1019                 if ignore_files and fname == ignore_file_name:\n1020                     with open(fpath) as fh:\n1021                         spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n1022                         ignores[dirpath] = spec\n1023                     # We don't need to process the ignore file any further\n1024                     continue\n1025 \n1026                 # We won't purge files *here* because there's an edge case\n1027                 # that the ignore file is processed after the sql file.\n1028 \n1029                 # Scan for remaining files\n1030                 for ext in (\n1031                     self.config.get(\"sql_file_exts\", default=\".sql\").lower().split(\",\")\n1032                 ):\n1033                     # is it a sql file?\n1034                     if fname.lower().endswith(ext):\n1035                         buffer.append(fpath)\n1036 \n1037         if not ignore_files:\n1038             return sorted(buffer)\n1039 \n1040         # Check the buffer for ignore items and normalise the rest.\n1041         # It's a set, so we can do natural deduplication.\n1042         filtered_buffer = set()\n1043 \n1044         for fpath in buffer:\n1045             abs_fpath = os.path.abspath(fpath)\n1046             for ignore_base, ignore_spec in ignores.items():\n1047                 abs_ignore_base = os.path.abspath(ignore_base)\n1048                 if abs_fpath.startswith(\n1049                     abs_ignore_base + os.sep\n1050                 ) and ignore_spec.match_file(\n1051                     os.path.relpath(abs_fpath, abs_ignore_base)\n1052                 ):\n1053                     # This file is ignored, skip it.\n1054                     if is_exact_file:\n1055                         linter_logger.warning(\n1056                             \"Exact file path %s was given but \"\n1057                             \"it was ignored by a %s pattern in %s, \"\n1058                             \"re-run with `--disregard-sqlfluffignores` to \"\n1059                             \"skip %s\"\n1060                             % (\n1061                                 path,\n1062                                 ignore_file_name,\n1063                                 ignore_base,\n1064                                 ignore_file_name,\n1065                             )\n1066                         )\n1067                     break\n1068             else:\n1069                 npath = os.path.normpath(fpath)\n1070                 # For debugging, log if we already have the file.\n1071                 if npath in filtered_buffer:\n1072                     linter_logger.debug(  # pragma: no cover\n1073                         \"Developer Warning: Path crawler attempted to \"\n1074                         \"requeue the same file twice. %s is already in \"\n1075                         \"filtered buffer.\",\n1076                         npath,\n1077                     )\n1078                 filtered_buffer.add(npath)\n1079 \n1080         # Return a sorted list\n1081         return sorted(filtered_buffer)\n1082 \n1083     def lint_string_wrapped(\n1084         self,\n1085         string: str,\n1086         fname: str = \"<string input>\",\n1087         fix: bool = False,\n1088     ) -> LintingResult:\n1089         \"\"\"Lint strings directly.\"\"\"\n1090         result = LintingResult()\n1091         linted_path = LintedDir(fname)\n1092         linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n1093         result.add(linted_path)\n1094         result.stop_timer()\n1095         return result\n1096 \n1097     def lint_path(\n1098         self,\n1099         path: str,\n1100         fix: bool = False,\n1101         ignore_non_existent_files: bool = False,\n1102         ignore_files: bool = True,\n1103         processes: Optional[int] = None,\n1104     ) -> LintedDir:\n1105         \"\"\"Lint a path.\"\"\"\n1106         linted_path = LintedDir(path)\n1107         if self.formatter:\n1108             self.formatter.dispatch_path(path)\n1109         fnames = list(\n1110             self.paths_from_path(\n1111                 path,\n1112                 ignore_non_existent_files=ignore_non_existent_files,\n1113                 ignore_files=ignore_files,\n1114             )\n1115         )\n1116 \n1117         if processes is None:\n1118             processes = self.config.get(\"processes\", default=1)\n1119 \n1120         # to avoid circular import\n1121         from sqlfluff.core.linter.runner import get_runner\n1122 \n1123         runner, effective_processes = get_runner(\n1124             self,\n1125             self.config,\n1126             processes=processes,\n1127             allow_process_parallelism=self.allow_process_parallelism,\n1128         )\n1129 \n1130         if self.formatter and effective_processes != 1:\n1131             self.formatter.dispatch_processing_header(effective_processes)\n1132 \n1133         # Show files progress bar only when there is more than one.\n1134         files_count = len(fnames)\n1135         progress_bar_files = tqdm(\n1136             total=files_count,\n1137             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n1138             leave=False,\n1139             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1140         )\n1141 \n1142         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n1143             linted_path.add(linted_file)\n1144             # If any fatal errors, then stop iteration.\n1145             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n1146                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n1147                 break\n1148 \n1149             # Progress bar for files is rendered only when there is more than one file.\n1150             # Additionally as it's updated after each loop, we need to get file name\n1151             # from the next loop. This is why `enumerate` starts with `1` and there\n1152             # is `i < len` to not exceed files list length.\n1153             progress_bar_files.update(n=1)\n1154             if i < len(fnames):\n1155                 progress_bar_files.set_description(\n1156                     f\"file {os.path.basename(fnames[i])}\"\n1157                 )\n1158 \n1159         return linted_path\n1160 \n1161     def lint_paths(\n1162         self,\n1163         paths: Tuple[str, ...],\n1164         fix: bool = False,\n1165         ignore_non_existent_files: bool = False,\n1166         ignore_files: bool = True,\n1167         processes: Optional[int] = None,\n1168     ) -> LintingResult:\n1169         \"\"\"Lint an iterable of paths.\"\"\"\n1170         paths_count = len(paths)\n1171 \n1172         # If no paths specified - assume local\n1173         if not paths_count:  # pragma: no cover\n1174             paths = (os.getcwd(),)\n1175         # Set up the result to hold what we get back\n1176         result = LintingResult()\n1177 \n1178         progress_bar_paths = tqdm(\n1179             total=paths_count,\n1180             desc=\"path\",\n1181             leave=False,\n1182             disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n1183         )\n1184         for path in paths:\n1185             progress_bar_paths.set_description(f\"path {path}\")\n1186 \n1187             # Iterate through files recursively in the specified directory (if it's a\n1188             # directory) or read the file directly if it's not\n1189             result.add(\n1190                 self.lint_path(\n1191                     path,\n1192                     fix=fix,\n1193                     ignore_non_existent_files=ignore_non_existent_files,\n1194                     ignore_files=ignore_files,\n1195                     processes=processes,\n1196                 )\n1197             )\n1198 \n1199             progress_bar_paths.update(1)\n1200 \n1201         result.stop_timer()\n1202         return result\n1203 \n1204     def parse_path(\n1205         self,\n1206         path: str,\n1207         recurse: bool = True,\n1208     ) -> Iterator[ParsedString]:\n1209         \"\"\"Parse a path of sql files.\n1210 \n1211         NB: This a generator which will yield the result of each file\n1212         within the path iteratively.\n1213         \"\"\"\n1214         for fname in self.paths_from_path(path):\n1215             if self.formatter:\n1216                 self.formatter.dispatch_path(path)\n1217             # Load the file with the config and yield the result.\n1218             try:\n1219                 raw_file, config, encoding = self.load_raw_file_and_config(\n1220                     fname, self.config\n1221                 )\n1222             except SQLFluffSkipFile as s:\n1223                 linter_logger.warning(str(s))\n1224                 continue\n1225             yield self.parse_string(\n1226                 raw_file,\n1227                 fname=fname,\n1228                 recurse=recurse,\n1229                 config=config,\n1230                 encoding=encoding,\n1231             )\n1232 \n[end of src/sqlfluff/core/linter/linter.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1116,7 +1116,7 @@\n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n-        runner, effective_processes = get_runner(\n+        runner, effective_processes = get_runner(self, self.config, processes=processes, allow_process_parallelism=self.allow_process_parallelism)\n \n         if self.formatter and effective_processes != 1:\n             self.formatter.dispatch_processing_header(effective_processes)\n@@ -1134,7 +1134,7 @@\n         # Show files progress bar only when there is more than one.\n         files_count = len(fnames)\n         progress_bar_files = tqdm(\n-            total=files_count,\n+            total=files_count if files_count > 1 else 1,\n             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n             leave=False,\n             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n@@ -1142,6 +1142,9 @@\n         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n             linted_path.add(linted_file)\n             # If any fatal errors, then stop iteration.\n+            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n+                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n+                break\n \n             # Progress bar for files is rendered only when there is more than one file.\n             # Additionally as it's updated after each loop, we need to get file name\n``` \n\nThis patch modifies the `lint_paths` method in the `linter.py` file to ensure that multiple processes are utilized when a list of explicit filenames is passed to the `sqlfluff lint` command. The changes ensure that the runner can handle multiple files concurrently, thus utilizing all available CPU cores as expected when `-p -1` is specified.", "model_patch": "--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1116,7 +1116,7 @@\n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n-        runner, effective_processes = get_runner(\n+        runner, effective_processes = get_runner(self, self.config, processes=processes, allow_process_parallelism=self.allow_process_parallelism)\n \n         if self.formatter and effective_processes != 1:\n             self.formatter.dispatch_processing_header(effective_processes)\n@@ -1134,7 +1134,7 @@\n         # Show files progress bar only when there is more than one.\n         files_count = len(fnames)\n         progress_bar_files = tqdm(\n-            total=files_count,\n+            total=files_count if files_count > 1 else 1,\n             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n             leave=False,\n             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n@@ -1142,6 +1142,9 @@\n         for i, linted_file in enumerate(runner.run(fnames, fix), start=1):\n             linted_path.add(linted_file)\n             # If any fatal errors, then stop iteration.\n+            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n+                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n+                break\n \n             # Progress bar for files is rendered only when there is more than one file.\n             # Additionally as it's updated after each loop, we need to get file name\n"}
{"instance_id": "marshmallow-code__marshmallow-2123", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nfields.URL should allow relative-only validation\nRelative URLs may be used to redirect the user within the site, such as to sign in, and allowing absolute URLs without extra validation opens up a possibility of nefarious redirects.\r\n\r\nCurrent `fields.URL(relative = True)` allows relative URLs _in addition_ to absolute URLs, so one must set up extra validation to catch either all absolute URLs or just those that don't have a valid domain names.\r\n\r\nIt would be helpful if there was a way to set up URL validation to allow only relative URLs. \r\n\r\n~One quick and dirty way to do this would be if there was a `validate.Not` operator, then at the expense of matching the value twice, it would be possible to use something like this:~\r\n\r\n~`fields.URL(relative = True, validate=validate.Not(validate.URL()))`~\r\n\r\nEDIT: Never mind the crossed out thought above - failed validations are handled only via exceptions and while failing the inner validator works in general, it requires suppressing exception handlers and is just not a good way to go about it. \n\n</issue>\n<code>\n[start of README.rst]\n1 ********************************************\n2 marshmallow: simplified object serialization\n3 ********************************************\n4 \n5 .. image:: https://badgen.net/pypi/v/marshmallow\n6     :target: https://pypi.org/project/marshmallow/\n7     :alt: Latest version\n8 \n9 .. image:: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml/badge.svg\n10     :target: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml\n11     :alt: Build status\n12 \n13 .. image:: https://results.pre-commit.ci/badge/github/marshmallow-code/marshmallow/dev.svg\n14    :target: https://results.pre-commit.ci/latest/github/marshmallow-code/marshmallow/dev\n15    :alt: pre-commit.ci status\n16 \n17 .. image:: https://readthedocs.org/projects/marshmallow/badge/\n18    :target: https://marshmallow.readthedocs.io/\n19    :alt: Documentation\n20    \n21 .. image:: https://badgen.net/badge/code%20style/black/000\n22     :target: https://github.com/ambv/black\n23     :alt: code style: black\n24 \n25 \n26 **marshmallow** is an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes.\n27 \n28 .. code-block:: python\n29 \n30     from datetime import date\n31     from pprint import pprint\n32 \n33     from marshmallow import Schema, fields\n34 \n35 \n36     class ArtistSchema(Schema):\n37         name = fields.Str()\n38 \n39 \n40     class AlbumSchema(Schema):\n41         title = fields.Str()\n42         release_date = fields.Date()\n43         artist = fields.Nested(ArtistSchema())\n44 \n45 \n46     bowie = dict(name=\"David Bowie\")\n47     album = dict(artist=bowie, title=\"Hunky Dory\", release_date=date(1971, 12, 17))\n48 \n49     schema = AlbumSchema()\n50     result = schema.dump(album)\n51     pprint(result, indent=2)\n52     # { 'artist': {'name': 'David Bowie'},\n53     #   'release_date': '1971-12-17',\n54     #   'title': 'Hunky Dory'}\n55 \n56 \n57 In short, marshmallow schemas can be used to:\n58 \n59 - **Validate** input data.\n60 - **Deserialize** input data to app-level objects.\n61 - **Serialize** app-level objects to primitive Python types. The serialized objects can then be rendered to standard formats such as JSON for use in an HTTP API.\n62 \n63 Get It Now\n64 ==========\n65 \n66 ::\n67 \n68     $ pip install -U marshmallow\n69 \n70 \n71 Documentation\n72 =============\n73 \n74 Full documentation is available at https://marshmallow.readthedocs.io/ .\n75 \n76 Requirements\n77 ============\n78 \n79 - Python >= 3.7\n80 \n81 Ecosystem\n82 =========\n83 \n84 A list of marshmallow-related libraries can be found at the GitHub wiki here:\n85 \n86 https://github.com/marshmallow-code/marshmallow/wiki/Ecosystem\n87 \n88 Credits\n89 =======\n90 \n91 Contributors\n92 ------------\n93 \n94 This project exists thanks to all the people who contribute.\n95 \n96 **You're highly encouraged to participate in marshmallow's development.**\n97 Check out the `Contributing Guidelines <https://marshmallow.readthedocs.io/en/latest/contributing.html>`_ to see how you can help.\n98 \n99 Thank you to all who have already contributed to marshmallow!\n100 \n101 .. image:: https://opencollective.com/marshmallow/contributors.svg?width=890&button=false\n102     :target: https://marshmallow.readthedocs.io/en/latest/authors.html\n103     :alt: Contributors\n104 \n105 Backers\n106 -------\n107 \n108 If you find marshmallow useful, please consider supporting the team with\n109 a donation. Your donation helps move marshmallow forward.\n110 \n111 Thank you to all our backers! [`Become a backer`_]\n112 \n113 .. _`Become a backer`: https://opencollective.com/marshmallow#backer\n114 \n115 .. image:: https://opencollective.com/marshmallow/backers.svg?width=890\n116     :target: https://opencollective.com/marshmallow#backers\n117     :alt: Backers\n118 \n119 Sponsors\n120 --------\n121 \n122 Support this project by becoming a sponsor (or ask your company to support this project by becoming a sponsor).\n123 Your logo will show up here with a link to your website. [`Become a sponsor`_]\n124 \n125 .. _`Become a sponsor`: https://opencollective.com/marshmallow#sponsor\n126 \n127 .. image:: https://opencollective.com/marshmallow/sponsor/0/avatar.svg\n128     :target: https://opencollective.com/marshmallow/sponsor/0/website\n129     :alt: Sponsors\n130 \n131 .. image:: https://opencollective.com/static/images/become_sponsor.svg\n132     :target: https://opencollective.com/marshmallow#sponsor\n133     :alt: Become a sponsor\n134 \n135 \n136 Professional Support\n137 ====================\n138 \n139 Professionally-supported marshmallow is now available through the\n140 `Tidelift Subscription <https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme>`_.\n141 \n142 Tidelift gives software development teams a single source for purchasing and maintaining their software,\n143 with professional-grade assurances from the experts who know it best,\n144 while seamlessly integrating with existing tools. [`Get professional support`_]\n145 \n146 .. _`Get professional support`: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=marshmallow&utm_medium=referral&utm_campaign=github\n147 \n148 .. image:: https://user-images.githubusercontent.com/2379650/45126032-50b69880-b13f-11e8-9c2c-abd16c433495.png\n149     :target: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme\n150     :alt: Get supported marshmallow with Tidelift\n151 \n152 \n153 Project Links\n154 =============\n155 \n156 - Docs: https://marshmallow.readthedocs.io/\n157 - Changelog: https://marshmallow.readthedocs.io/en/latest/changelog.html\n158 - Contributing Guidelines: https://marshmallow.readthedocs.io/en/latest/contributing.html\n159 - PyPI: https://pypi.python.org/pypi/marshmallow\n160 - Issues: https://github.com/marshmallow-code/marshmallow/issues\n161 - Donate: https://opencollective.com/marshmallow\n162 \n163 License\n164 =======\n165 \n166 MIT licensed. See the bundled `LICENSE <https://github.com/marshmallow-code/marshmallow/blob/dev/LICENSE>`_ file for more details.\n167 \n[end of README.rst]\n[start of src/marshmallow/fields.py]\n1 \"\"\"Field classes for various types of data.\"\"\"\n2 from __future__ import annotations\n3 \n4 import collections\n5 import copy\n6 import datetime as dt\n7 import numbers\n8 import uuid\n9 import ipaddress\n10 import decimal\n11 import math\n12 import typing\n13 import warnings\n14 from enum import Enum as EnumType\n15 from collections.abc import Mapping as _Mapping\n16 \n17 from marshmallow import validate, utils, class_registry, types\n18 from marshmallow.base import FieldABC, SchemaABC\n19 from marshmallow.utils import (\n20     is_collection,\n21     missing as missing_,\n22     resolve_field_instance,\n23     is_aware,\n24 )\n25 from marshmallow.exceptions import (\n26     ValidationError,\n27     StringNotCollectionError,\n28     FieldInstanceResolutionError,\n29 )\n30 from marshmallow.validate import And, Length\n31 from marshmallow.warnings import RemovedInMarshmallow4Warning\n32 \n33 __all__ = [\n34     \"Field\",\n35     \"Raw\",\n36     \"Nested\",\n37     \"Mapping\",\n38     \"Dict\",\n39     \"List\",\n40     \"Tuple\",\n41     \"String\",\n42     \"UUID\",\n43     \"Number\",\n44     \"Integer\",\n45     \"Decimal\",\n46     \"Boolean\",\n47     \"Float\",\n48     \"DateTime\",\n49     \"NaiveDateTime\",\n50     \"AwareDateTime\",\n51     \"Time\",\n52     \"Date\",\n53     \"TimeDelta\",\n54     \"Url\",\n55     \"URL\",\n56     \"Email\",\n57     \"IP\",\n58     \"IPv4\",\n59     \"IPv6\",\n60     \"IPInterface\",\n61     \"IPv4Interface\",\n62     \"IPv6Interface\",\n63     \"Enum\",\n64     \"Method\",\n65     \"Function\",\n66     \"Str\",\n67     \"Bool\",\n68     \"Int\",\n69     \"Constant\",\n70     \"Pluck\",\n71 ]\n72 \n73 _T = typing.TypeVar(\"_T\")\n74 \n75 \n76 class Field(FieldABC):\n77     \"\"\"Basic field from which other fields should extend. It applies no\n78     formatting by default, and should only be used in cases where\n79     data does not need to be formatted before being serialized or deserialized.\n80     On error, the name of the field will be returned.\n81 \n82     :param dump_default: If set, this value will be used during serialization if the\n83         input value is missing. If not set, the field will be excluded from the\n84         serialized output if the input value is missing. May be a value or a callable.\n85     :param load_default: Default deserialization value for the field if the field is not\n86         found in the input data. May be a value or a callable.\n87     :param data_key: The name of the dict key in the external representation, i.e.\n88         the input of `load` and the output of `dump`.\n89         If `None`, the key will match the name of the field.\n90     :param attribute: The name of the attribute to get the value from when serializing.\n91         If `None`, assumes the attribute has the same name as the field.\n92         Note: This should only be used for very specific use cases such as\n93         outputting multiple fields for a single attribute. In most cases,\n94         you should use ``data_key`` instead.\n95     :param validate: Validator or collection of validators that are called\n96         during deserialization. Validator takes a field's input value as\n97         its only parameter and returns a boolean.\n98         If it returns `False`, an :exc:`ValidationError` is raised.\n99     :param required: Raise a :exc:`ValidationError` if the field value\n100         is not supplied during deserialization.\n101     :param allow_none: Set this to `True` if `None` should be considered a valid value during\n102         validation/deserialization. If ``load_default=None`` and ``allow_none`` is unset,\n103         will default to ``True``. Otherwise, the default is ``False``.\n104     :param load_only: If `True` skip this field during serialization, otherwise\n105         its value will be present in the serialized data.\n106     :param dump_only: If `True` skip this field during deserialization, otherwise\n107         its value will be present in the deserialized object. In the context of an\n108         HTTP API, this effectively marks the field as \"read-only\".\n109     :param dict error_messages: Overrides for `Field.default_error_messages`.\n110     :param metadata: Extra information to be stored as field metadata.\n111 \n112     .. versionchanged:: 2.0.0\n113         Removed `error` parameter. Use ``error_messages`` instead.\n114 \n115     .. versionchanged:: 2.0.0\n116         Added `allow_none` parameter, which makes validation/deserialization of `None`\n117         consistent across fields.\n118 \n119     .. versionchanged:: 2.0.0\n120         Added `load_only` and `dump_only` parameters, which allow field skipping\n121         during the (de)serialization process.\n122 \n123     .. versionchanged:: 2.0.0\n124         Added `missing` parameter, which indicates the value for a field if the field\n125         is not found during deserialization.\n126 \n127     .. versionchanged:: 2.0.0\n128         ``default`` value is only used if explicitly set. Otherwise, missing values\n129         inputs are excluded from serialized output.\n130 \n131     .. versionchanged:: 3.0.0b8\n132         Add ``data_key`` parameter for the specifying the key in the input and\n133         output data. This parameter replaced both ``load_from`` and ``dump_to``.\n134     \"\"\"\n135 \n136     # Some fields, such as Method fields and Function fields, are not expected\n137     #  to exist as attributes on the objects to serialize. Set this to False\n138     #  for those fields\n139     _CHECK_ATTRIBUTE = True\n140     _creation_index = 0  # Used for sorting\n141 \n142     #: Default error messages for various kinds of errors. The keys in this dictionary\n143     #: are passed to `Field.make_error`. The values are error messages passed to\n144     #: :exc:`marshmallow.exceptions.ValidationError`.\n145     default_error_messages = {\n146         \"required\": \"Missing data for required field.\",\n147         \"null\": \"Field may not be null.\",\n148         \"validator_failed\": \"Invalid value.\",\n149     }\n150 \n151     def __init__(\n152         self,\n153         *,\n154         load_default: typing.Any = missing_,\n155         missing: typing.Any = missing_,\n156         dump_default: typing.Any = missing_,\n157         default: typing.Any = missing_,\n158         data_key: str | None = None,\n159         attribute: str | None = None,\n160         validate: (\n161             None\n162             | typing.Callable[[typing.Any], typing.Any]\n163             | typing.Iterable[typing.Callable[[typing.Any], typing.Any]]\n164         ) = None,\n165         required: bool = False,\n166         allow_none: bool | None = None,\n167         load_only: bool = False,\n168         dump_only: bool = False,\n169         error_messages: dict[str, str] | None = None,\n170         metadata: typing.Mapping[str, typing.Any] | None = None,\n171         **additional_metadata,\n172     ) -> None:\n173         # handle deprecated `default` and `missing` parameters\n174         if default is not missing_:\n175             warnings.warn(\n176                 \"The 'default' argument to fields is deprecated. \"\n177                 \"Use 'dump_default' instead.\",\n178                 RemovedInMarshmallow4Warning,\n179                 stacklevel=2,\n180             )\n181             if dump_default is missing_:\n182                 dump_default = default\n183         if missing is not missing_:\n184             warnings.warn(\n185                 \"The 'missing' argument to fields is deprecated. \"\n186                 \"Use 'load_default' instead.\",\n187                 RemovedInMarshmallow4Warning,\n188                 stacklevel=2,\n189             )\n190             if load_default is missing_:\n191                 load_default = missing\n192         self.dump_default = dump_default\n193         self.load_default = load_default\n194 \n195         self.attribute = attribute\n196         self.data_key = data_key\n197         self.validate = validate\n198         if validate is None:\n199             self.validators = []\n200         elif callable(validate):\n201             self.validators = [validate]\n202         elif utils.is_iterable_but_not_string(validate):\n203             self.validators = list(validate)\n204         else:\n205             raise ValueError(\n206                 \"The 'validate' parameter must be a callable \"\n207                 \"or a collection of callables.\"\n208             )\n209 \n210         # If allow_none is None and load_default is None\n211         # None should be considered valid by default\n212         self.allow_none = load_default is None if allow_none is None else allow_none\n213         self.load_only = load_only\n214         self.dump_only = dump_only\n215         if required is True and load_default is not missing_:\n216             raise ValueError(\"'load_default' must not be set for required fields.\")\n217         self.required = required\n218 \n219         metadata = metadata or {}\n220         self.metadata = {**metadata, **additional_metadata}\n221         if additional_metadata:\n222             warnings.warn(\n223                 \"Passing field metadata as keyword arguments is deprecated. Use the \"\n224                 \"explicit `metadata=...` argument instead. \"\n225                 f\"Additional metadata: {additional_metadata}\",\n226                 RemovedInMarshmallow4Warning,\n227                 stacklevel=2,\n228             )\n229 \n230         self._creation_index = Field._creation_index\n231         Field._creation_index += 1\n232 \n233         # Collect default error message from self and parent classes\n234         messages = {}  # type: dict[str, str]\n235         for cls in reversed(self.__class__.__mro__):\n236             messages.update(getattr(cls, \"default_error_messages\", {}))\n237         messages.update(error_messages or {})\n238         self.error_messages = messages\n239 \n240     def __repr__(self) -> str:\n241         return (\n242             \"<fields.{ClassName}(dump_default={self.dump_default!r}, \"\n243             \"attribute={self.attribute!r}, \"\n244             \"validate={self.validate}, required={self.required}, \"\n245             \"load_only={self.load_only}, dump_only={self.dump_only}, \"\n246             \"load_default={self.load_default}, allow_none={self.allow_none}, \"\n247             \"error_messages={self.error_messages})>\".format(\n248                 ClassName=self.__class__.__name__, self=self\n249             )\n250         )\n251 \n252     def __deepcopy__(self, memo):\n253         return copy.copy(self)\n254 \n255     def get_value(self, obj, attr, accessor=None, default=missing_):\n256         \"\"\"Return the value for a given key from an object.\n257 \n258         :param object obj: The object to get the value from.\n259         :param str attr: The attribute/key in `obj` to get the value from.\n260         :param callable accessor: A callable used to retrieve the value of `attr` from\n261             the object `obj`. Defaults to `marshmallow.utils.get_value`.\n262         \"\"\"\n263         accessor_func = accessor or utils.get_value\n264         check_key = attr if self.attribute is None else self.attribute\n265         return accessor_func(obj, check_key, default)\n266 \n267     def _validate(self, value):\n268         \"\"\"Perform validation on ``value``. Raise a :exc:`ValidationError` if validation\n269         does not succeed.\n270         \"\"\"\n271         self._validate_all(value)\n272 \n273     @property\n274     def _validate_all(self):\n275         return And(*self.validators, error=self.error_messages[\"validator_failed\"])\n276 \n277     def make_error(self, key: str, **kwargs) -> ValidationError:\n278         \"\"\"Helper method to make a `ValidationError` with an error message\n279         from ``self.error_messages``.\n280         \"\"\"\n281         try:\n282             msg = self.error_messages[key]\n283         except KeyError as error:\n284             class_name = self.__class__.__name__\n285             message = (\n286                 \"ValidationError raised by `{class_name}`, but error key `{key}` does \"\n287                 \"not exist in the `error_messages` dictionary.\"\n288             ).format(class_name=class_name, key=key)\n289             raise AssertionError(message) from error\n290         if isinstance(msg, (str, bytes)):\n291             msg = msg.format(**kwargs)\n292         return ValidationError(msg)\n293 \n294     def fail(self, key: str, **kwargs):\n295         \"\"\"Helper method that raises a `ValidationError` with an error message\n296         from ``self.error_messages``.\n297 \n298         .. deprecated:: 3.0.0\n299             Use `make_error <marshmallow.fields.Field.make_error>` instead.\n300         \"\"\"\n301         warnings.warn(\n302             '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.'.format(\n303                 key\n304             ),\n305             RemovedInMarshmallow4Warning,\n306             stacklevel=2,\n307         )\n308         raise self.make_error(key=key, **kwargs)\n309 \n310     def _validate_missing(self, value):\n311         \"\"\"Validate missing values. Raise a :exc:`ValidationError` if\n312         `value` should be considered missing.\n313         \"\"\"\n314         if value is missing_ and self.required:\n315             raise self.make_error(\"required\")\n316         if value is None and not self.allow_none:\n317             raise self.make_error(\"null\")\n318 \n319     def serialize(\n320         self,\n321         attr: str,\n322         obj: typing.Any,\n323         accessor: typing.Callable[[typing.Any, str, typing.Any], typing.Any]\n324         | None = None,\n325         **kwargs,\n326     ):\n327         \"\"\"Pulls the value for the given key from the object, applies the\n328         field's formatting and returns the result.\n329 \n330         :param attr: The attribute/key to get from the object.\n331         :param obj: The object to access the attribute/key from.\n332         :param accessor: Function used to access values from ``obj``.\n333         :param kwargs: Field-specific keyword arguments.\n334         \"\"\"\n335         if self._CHECK_ATTRIBUTE:\n336             value = self.get_value(obj, attr, accessor=accessor)\n337             if value is missing_:\n338                 default = self.dump_default\n339                 value = default() if callable(default) else default\n340             if value is missing_:\n341                 return value\n342         else:\n343             value = None\n344         return self._serialize(value, attr, obj, **kwargs)\n345 \n346     def deserialize(\n347         self,\n348         value: typing.Any,\n349         attr: str | None = None,\n350         data: typing.Mapping[str, typing.Any] | None = None,\n351         **kwargs,\n352     ):\n353         \"\"\"Deserialize ``value``.\n354 \n355         :param value: The value to deserialize.\n356         :param attr: The attribute/key in `data` to deserialize.\n357         :param data: The raw input data passed to `Schema.load`.\n358         :param kwargs: Field-specific keyword arguments.\n359         :raise ValidationError: If an invalid value is passed or if a required value\n360             is missing.\n361         \"\"\"\n362         # Validate required fields, deserialize, then validate\n363         # deserialized value\n364         self._validate_missing(value)\n365         if value is missing_:\n366             _miss = self.load_default\n367             return _miss() if callable(_miss) else _miss\n368         if self.allow_none and value is None:\n369             return None\n370         output = self._deserialize(value, attr, data, **kwargs)\n371         self._validate(output)\n372         return output\n373 \n374     # Methods for concrete classes to override.\n375 \n376     def _bind_to_schema(self, field_name, schema):\n377         \"\"\"Update field with values from its parent schema. Called by\n378         :meth:`Schema._bind_field <marshmallow.Schema._bind_field>`.\n379 \n380         :param str field_name: Field name set in schema.\n381         :param Schema|Field schema: Parent object.\n382         \"\"\"\n383         self.parent = self.parent or schema\n384         self.name = self.name or field_name\n385         self.root = self.root or (\n386             self.parent.root if isinstance(self.parent, FieldABC) else self.parent\n387         )\n388 \n389     def _serialize(\n390         self, value: typing.Any, attr: str | None, obj: typing.Any, **kwargs\n391     ):\n392         \"\"\"Serializes ``value`` to a basic Python datatype. Noop by default.\n393         Concrete :class:`Field` classes should implement this method.\n394 \n395         Example: ::\n396 \n397             class TitleCase(Field):\n398                 def _serialize(self, value, attr, obj, **kwargs):\n399                     if not value:\n400                         return ''\n401                     return str(value).title()\n402 \n403         :param value: The value to be serialized.\n404         :param str attr: The attribute or key on the object to be serialized.\n405         :param object obj: The object the value was pulled from.\n406         :param dict kwargs: Field-specific keyword arguments.\n407         :return: The serialized value\n408         \"\"\"\n409         return value\n410 \n411     def _deserialize(\n412         self,\n413         value: typing.Any,\n414         attr: str | None,\n415         data: typing.Mapping[str, typing.Any] | None,\n416         **kwargs,\n417     ):\n418         \"\"\"Deserialize value. Concrete :class:`Field` classes should implement this method.\n419 \n420         :param value: The value to be deserialized.\n421         :param attr: The attribute/key in `data` to be deserialized.\n422         :param data: The raw input data passed to the `Schema.load`.\n423         :param kwargs: Field-specific keyword arguments.\n424         :raise ValidationError: In case of formatting or validation failure.\n425         :return: The deserialized value.\n426 \n427         .. versionchanged:: 2.0.0\n428             Added ``attr`` and ``data`` parameters.\n429 \n430         .. versionchanged:: 3.0.0\n431             Added ``**kwargs`` to signature.\n432         \"\"\"\n433         return value\n434 \n435     # Properties\n436 \n437     @property\n438     def context(self):\n439         \"\"\"The context dictionary for the parent :class:`Schema`.\"\"\"\n440         return self.parent.context\n441 \n442     # the default and missing properties are provided for compatibility and\n443     # emit warnings when they are accessed and set\n444     @property\n445     def default(self):\n446         warnings.warn(\n447             \"The 'default' attribute of fields is deprecated. \"\n448             \"Use 'dump_default' instead.\",\n449             RemovedInMarshmallow4Warning,\n450             stacklevel=2,\n451         )\n452         return self.dump_default\n453 \n454     @default.setter\n455     def default(self, value):\n456         warnings.warn(\n457             \"The 'default' attribute of fields is deprecated. \"\n458             \"Use 'dump_default' instead.\",\n459             RemovedInMarshmallow4Warning,\n460             stacklevel=2,\n461         )\n462         self.dump_default = value\n463 \n464     @property\n465     def missing(self):\n466         warnings.warn(\n467             \"The 'missing' attribute of fields is deprecated. \"\n468             \"Use 'load_default' instead.\",\n469             RemovedInMarshmallow4Warning,\n470             stacklevel=2,\n471         )\n472         return self.load_default\n473 \n474     @missing.setter\n475     def missing(self, value):\n476         warnings.warn(\n477             \"The 'missing' attribute of fields is deprecated. \"\n478             \"Use 'load_default' instead.\",\n479             RemovedInMarshmallow4Warning,\n480             stacklevel=2,\n481         )\n482         self.load_default = value\n483 \n484 \n485 class Raw(Field):\n486     \"\"\"Field that applies no formatting.\"\"\"\n487 \n488 \n489 class Nested(Field):\n490     \"\"\"Allows you to nest a :class:`Schema <marshmallow.Schema>`\n491     inside a field.\n492 \n493     Examples: ::\n494 \n495         class ChildSchema(Schema):\n496             id = fields.Str()\n497             name = fields.Str()\n498             # Use lambda functions when you need two-way nesting or self-nesting\n499             parent = fields.Nested(lambda: ParentSchema(only=(\"id\",)), dump_only=True)\n500             siblings = fields.List(fields.Nested(lambda: ChildSchema(only=(\"id\", \"name\"))))\n501 \n502         class ParentSchema(Schema):\n503             id = fields.Str()\n504             children = fields.List(\n505                 fields.Nested(ChildSchema(only=(\"id\", \"parent\", \"siblings\")))\n506             )\n507             spouse = fields.Nested(lambda: ParentSchema(only=(\"id\",)))\n508 \n509     When passing a `Schema <marshmallow.Schema>` instance as the first argument,\n510     the instance's ``exclude``, ``only``, and ``many`` attributes will be respected.\n511 \n512     Therefore, when passing the ``exclude``, ``only``, or ``many`` arguments to `fields.Nested`,\n513     you should pass a `Schema <marshmallow.Schema>` class (not an instance) as the first argument.\n514 \n515     ::\n516 \n517         # Yes\n518         author = fields.Nested(UserSchema, only=('id', 'name'))\n519 \n520         # No\n521         author = fields.Nested(UserSchema(), only=('id', 'name'))\n522 \n523     :param nested: `Schema` instance, class, class name (string), dictionary, or callable that\n524         returns a `Schema` or dictionary. Dictionaries are converted with `Schema.from_dict`.\n525     :param exclude: A list or tuple of fields to exclude.\n526     :param only: A list or tuple of fields to marshal. If `None`, all fields are marshalled.\n527         This parameter takes precedence over ``exclude``.\n528     :param many: Whether the field is a collection of objects.\n529     :param unknown: Whether to exclude, include, or raise an error for unknown\n530         fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`.\n531     :param kwargs: The same keyword arguments that :class:`Field` receives.\n532     \"\"\"\n533 \n534     #: Default error messages.\n535     default_error_messages = {\"type\": \"Invalid type.\"}\n536 \n537     def __init__(\n538         self,\n539         nested: SchemaABC\n540         | type\n541         | str\n542         | dict[str, Field | type]\n543         | typing.Callable[[], SchemaABC | dict[str, Field | type]],\n544         *,\n545         dump_default: typing.Any = missing_,\n546         default: typing.Any = missing_,\n547         only: types.StrSequenceOrSet | None = None,\n548         exclude: types.StrSequenceOrSet = (),\n549         many: bool = False,\n550         unknown: str | None = None,\n551         **kwargs,\n552     ):\n553         # Raise error if only or exclude is passed as string, not list of strings\n554         if only is not None and not is_collection(only):\n555             raise StringNotCollectionError('\"only\" should be a collection of strings.')\n556         if not is_collection(exclude):\n557             raise StringNotCollectionError(\n558                 '\"exclude\" should be a collection of strings.'\n559             )\n560         if nested == \"self\":\n561             warnings.warn(\n562                 \"Passing 'self' to `Nested` is deprecated. \"\n563                 \"Use `Nested(lambda: MySchema(...))` instead.\",\n564                 RemovedInMarshmallow4Warning,\n565                 stacklevel=2,\n566             )\n567         self.nested = nested\n568         self.only = only\n569         self.exclude = exclude\n570         self.many = many\n571         self.unknown = unknown\n572         self._schema = None  # Cached Schema instance\n573         super().__init__(default=default, dump_default=dump_default, **kwargs)\n574 \n575     @property\n576     def schema(self):\n577         \"\"\"The nested Schema object.\n578 \n579         .. versionchanged:: 1.0.0\n580             Renamed from `serializer` to `schema`.\n581         \"\"\"\n582         if not self._schema:\n583             # Inherit context from parent.\n584             context = getattr(self.parent, \"context\", {})\n585             if callable(self.nested) and not isinstance(self.nested, type):\n586                 nested = self.nested()\n587             else:\n588                 nested = self.nested\n589             if isinstance(nested, dict):\n590                 # defer the import of `marshmallow.schema` to avoid circular imports\n591                 from marshmallow.schema import Schema\n592 \n593                 nested = Schema.from_dict(nested)\n594 \n595             if isinstance(nested, SchemaABC):\n596                 self._schema = copy.copy(nested)\n597                 self._schema.context.update(context)\n598                 # Respect only and exclude passed from parent and re-initialize fields\n599                 set_class = self._schema.set_class\n600                 if self.only is not None:\n601                     if self._schema.only is not None:\n602                         original = self._schema.only\n603                     else:  # only=None -> all fields\n604                         original = self._schema.fields.keys()\n605                     self._schema.only = set_class(self.only) & set_class(original)\n606                 if self.exclude:\n607                     original = self._schema.exclude\n608                     self._schema.exclude = set_class(self.exclude) | set_class(original)\n609                 self._schema._init_fields()\n610             else:\n611                 if isinstance(nested, type) and issubclass(nested, SchemaABC):\n612                     schema_class = nested\n613                 elif not isinstance(nested, (str, bytes)):\n614                     raise ValueError(\n615                         \"`Nested` fields must be passed a \"\n616                         \"`Schema`, not {}.\".format(nested.__class__)\n617                     )\n618                 elif nested == \"self\":\n619                     schema_class = self.root.__class__\n620                 else:\n621                     schema_class = class_registry.get_class(nested)\n622                 self._schema = schema_class(\n623                     many=self.many,\n624                     only=self.only,\n625                     exclude=self.exclude,\n626                     context=context,\n627                     load_only=self._nested_normalized_option(\"load_only\"),\n628                     dump_only=self._nested_normalized_option(\"dump_only\"),\n629                 )\n630         return self._schema\n631 \n632     def _nested_normalized_option(self, option_name: str) -> list[str]:\n633         nested_field = \"%s.\" % self.name\n634         return [\n635             field.split(nested_field, 1)[1]\n636             for field in getattr(self.root, option_name, set())\n637             if field.startswith(nested_field)\n638         ]\n639 \n640     def _serialize(self, nested_obj, attr, obj, **kwargs):\n641         # Load up the schema first. This allows a RegistryError to be raised\n642         # if an invalid schema name was passed\n643         schema = self.schema\n644         if nested_obj is None:\n645             return None\n646         many = schema.many or self.many\n647         return schema.dump(nested_obj, many=many)\n648 \n649     def _test_collection(self, value):\n650         many = self.schema.many or self.many\n651         if many and not utils.is_collection(value):\n652             raise self.make_error(\"type\", input=value, type=value.__class__.__name__)\n653 \n654     def _load(self, value, data, partial=None):\n655         try:\n656             valid_data = self.schema.load(value, unknown=self.unknown, partial=partial)\n657         except ValidationError as error:\n658             raise ValidationError(\n659                 error.messages, valid_data=error.valid_data\n660             ) from error\n661         return valid_data\n662 \n663     def _deserialize(self, value, attr, data, partial=None, **kwargs):\n664         \"\"\"Same as :meth:`Field._deserialize` with additional ``partial`` argument.\n665 \n666         :param bool|tuple partial: For nested schemas, the ``partial``\n667             parameter passed to `Schema.load`.\n668 \n669         .. versionchanged:: 3.0.0\n670             Add ``partial`` parameter.\n671         \"\"\"\n672         self._test_collection(value)\n673         return self._load(value, data, partial=partial)\n674 \n675 \n676 class Pluck(Nested):\n677     \"\"\"Allows you to replace nested data with one of the data's fields.\n678 \n679     Example: ::\n680 \n681         from marshmallow import Schema, fields\n682 \n683         class ArtistSchema(Schema):\n684             id = fields.Int()\n685             name = fields.Str()\n686 \n687         class AlbumSchema(Schema):\n688             artist = fields.Pluck(ArtistSchema, 'id')\n689 \n690 \n691         in_data = {'artist': 42}\n692         loaded = AlbumSchema().load(in_data) # => {'artist': {'id': 42}}\n693         dumped = AlbumSchema().dump(loaded)  # => {'artist': 42}\n694 \n695     :param Schema nested: The Schema class or class name (string)\n696         to nest, or ``\"self\"`` to nest the :class:`Schema` within itself.\n697     :param str field_name: The key to pluck a value from.\n698     :param kwargs: The same keyword arguments that :class:`Nested` receives.\n699     \"\"\"\n700 \n701     def __init__(\n702         self,\n703         nested: SchemaABC | type | str | typing.Callable[[], SchemaABC],\n704         field_name: str,\n705         **kwargs,\n706     ):\n707         super().__init__(nested, only=(field_name,), **kwargs)\n708         self.field_name = field_name\n709 \n710     @property\n711     def _field_data_key(self):\n712         only_field = self.schema.fields[self.field_name]\n713         return only_field.data_key or self.field_name\n714 \n715     def _serialize(self, nested_obj, attr, obj, **kwargs):\n716         ret = super()._serialize(nested_obj, attr, obj, **kwargs)\n717         if ret is None:\n718             return None\n719         if self.many:\n720             return utils.pluck(ret, key=self._field_data_key)\n721         return ret[self._field_data_key]\n722 \n723     def _deserialize(self, value, attr, data, partial=None, **kwargs):\n724         self._test_collection(value)\n725         if self.many:\n726             value = [{self._field_data_key: v} for v in value]\n727         else:\n728             value = {self._field_data_key: value}\n729         return self._load(value, data, partial=partial)\n730 \n731 \n732 class List(Field):\n733     \"\"\"A list field, composed with another `Field` class or\n734     instance.\n735 \n736     Example: ::\n737 \n738         numbers = fields.List(fields.Float())\n739 \n740     :param cls_or_instance: A field class or instance.\n741     :param kwargs: The same keyword arguments that :class:`Field` receives.\n742 \n743     .. versionchanged:: 2.0.0\n744         The ``allow_none`` parameter now applies to deserialization and\n745         has the same semantics as the other fields.\n746 \n747     .. versionchanged:: 3.0.0rc9\n748         Does not serialize scalar values to single-item lists.\n749     \"\"\"\n750 \n751     #: Default error messages.\n752     default_error_messages = {\"invalid\": \"Not a valid list.\"}\n753 \n754     def __init__(self, cls_or_instance: Field | type, **kwargs):\n755         super().__init__(**kwargs)\n756         try:\n757             self.inner = resolve_field_instance(cls_or_instance)\n758         except FieldInstanceResolutionError as error:\n759             raise ValueError(\n760                 \"The list elements must be a subclass or instance of \"\n761                 \"marshmallow.base.FieldABC.\"\n762             ) from error\n763         if isinstance(self.inner, Nested):\n764             self.only = self.inner.only\n765             self.exclude = self.inner.exclude\n766 \n767     def _bind_to_schema(self, field_name, schema):\n768         super()._bind_to_schema(field_name, schema)\n769         self.inner = copy.deepcopy(self.inner)\n770         self.inner._bind_to_schema(field_name, self)\n771         if isinstance(self.inner, Nested):\n772             self.inner.only = self.only\n773             self.inner.exclude = self.exclude\n774 \n775     def _serialize(self, value, attr, obj, **kwargs) -> list[typing.Any] | None:\n776         if value is None:\n777             return None\n778         return [self.inner._serialize(each, attr, obj, **kwargs) for each in value]\n779 \n780     def _deserialize(self, value, attr, data, **kwargs) -> list[typing.Any]:\n781         if not utils.is_collection(value):\n782             raise self.make_error(\"invalid\")\n783 \n784         result = []\n785         errors = {}\n786         for idx, each in enumerate(value):\n787             try:\n788                 result.append(self.inner.deserialize(each, **kwargs))\n789             except ValidationError as error:\n790                 if error.valid_data is not None:\n791                     result.append(error.valid_data)\n792                 errors.update({idx: error.messages})\n793         if errors:\n794             raise ValidationError(errors, valid_data=result)\n795         return result\n796 \n797 \n798 class Tuple(Field):\n799     \"\"\"A tuple field, composed of a fixed number of other `Field` classes or\n800     instances\n801 \n802     Example: ::\n803 \n804         row = Tuple((fields.String(), fields.Integer(), fields.Float()))\n805 \n806     .. note::\n807         Because of the structured nature of `collections.namedtuple` and\n808         `typing.NamedTuple`, using a Schema within a Nested field for them is\n809         more appropriate than using a `Tuple` field.\n810 \n811     :param Iterable[Field] tuple_fields: An iterable of field classes or\n812         instances.\n813     :param kwargs: The same keyword arguments that :class:`Field` receives.\n814 \n815     .. versionadded:: 3.0.0rc4\n816     \"\"\"\n817 \n818     #: Default error messages.\n819     default_error_messages = {\"invalid\": \"Not a valid tuple.\"}\n820 \n821     def __init__(self, tuple_fields, *args, **kwargs):\n822         super().__init__(*args, **kwargs)\n823         if not utils.is_collection(tuple_fields):\n824             raise ValueError(\n825                 \"tuple_fields must be an iterable of Field classes or \" \"instances.\"\n826             )\n827 \n828         try:\n829             self.tuple_fields = [\n830                 resolve_field_instance(cls_or_instance)\n831                 for cls_or_instance in tuple_fields\n832             ]\n833         except FieldInstanceResolutionError as error:\n834             raise ValueError(\n835                 'Elements of \"tuple_fields\" must be subclasses or '\n836                 \"instances of marshmallow.base.FieldABC.\"\n837             ) from error\n838 \n839         self.validate_length = Length(equal=len(self.tuple_fields))\n840 \n841     def _bind_to_schema(self, field_name, schema):\n842         super()._bind_to_schema(field_name, schema)\n843         new_tuple_fields = []\n844         for field in self.tuple_fields:\n845             field = copy.deepcopy(field)\n846             field._bind_to_schema(field_name, self)\n847             new_tuple_fields.append(field)\n848 \n849         self.tuple_fields = new_tuple_fields\n850 \n851     def _serialize(self, value, attr, obj, **kwargs) -> tuple | None:\n852         if value is None:\n853             return None\n854 \n855         return tuple(\n856             field._serialize(each, attr, obj, **kwargs)\n857             for field, each in zip(self.tuple_fields, value)\n858         )\n859 \n860     def _deserialize(self, value, attr, data, **kwargs) -> tuple:\n861         if not utils.is_collection(value):\n862             raise self.make_error(\"invalid\")\n863 \n864         self.validate_length(value)\n865 \n866         result = []\n867         errors = {}\n868 \n869         for idx, (field, each) in enumerate(zip(self.tuple_fields, value)):\n870             try:\n871                 result.append(field.deserialize(each, **kwargs))\n872             except ValidationError as error:\n873                 if error.valid_data is not None:\n874                     result.append(error.valid_data)\n875                 errors.update({idx: error.messages})\n876         if errors:\n877             raise ValidationError(errors, valid_data=result)\n878 \n879         return tuple(result)\n880 \n881 \n882 class String(Field):\n883     \"\"\"A string field.\n884 \n885     :param kwargs: The same keyword arguments that :class:`Field` receives.\n886     \"\"\"\n887 \n888     #: Default error messages.\n889     default_error_messages = {\n890         \"invalid\": \"Not a valid string.\",\n891         \"invalid_utf8\": \"Not a valid utf-8 string.\",\n892     }\n893 \n894     def _serialize(self, value, attr, obj, **kwargs) -> str | None:\n895         if value is None:\n896             return None\n897         return utils.ensure_text_type(value)\n898 \n899     def _deserialize(self, value, attr, data, **kwargs) -> typing.Any:\n900         if not isinstance(value, (str, bytes)):\n901             raise self.make_error(\"invalid\")\n902         try:\n903             return utils.ensure_text_type(value)\n904         except UnicodeDecodeError as error:\n905             raise self.make_error(\"invalid_utf8\") from error\n906 \n907 \n908 class UUID(String):\n909     \"\"\"A UUID field.\"\"\"\n910 \n911     #: Default error messages.\n912     default_error_messages = {\"invalid_uuid\": \"Not a valid UUID.\"}\n913 \n914     def _validated(self, value) -> uuid.UUID | None:\n915         \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n916         if value is None:\n917             return None\n918         if isinstance(value, uuid.UUID):\n919             return value\n920         try:\n921             if isinstance(value, bytes) and len(value) == 16:\n922                 return uuid.UUID(bytes=value)\n923             else:\n924                 return uuid.UUID(value)\n925         except (ValueError, AttributeError, TypeError) as error:\n926             raise self.make_error(\"invalid_uuid\") from error\n927 \n928     def _deserialize(self, value, attr, data, **kwargs) -> uuid.UUID | None:\n929         return self._validated(value)\n930 \n931 \n932 class Number(Field):\n933     \"\"\"Base class for number fields.\n934 \n935     :param bool as_string: If `True`, format the serialized value as a string.\n936     :param kwargs: The same keyword arguments that :class:`Field` receives.\n937     \"\"\"\n938 \n939     num_type = float  # type: typing.Type\n940 \n941     #: Default error messages.\n942     default_error_messages = {\n943         \"invalid\": \"Not a valid number.\",\n944         \"too_large\": \"Number too large.\",\n945     }\n946 \n947     def __init__(self, *, as_string: bool = False, **kwargs):\n948         self.as_string = as_string\n949         super().__init__(**kwargs)\n950 \n951     def _format_num(self, value) -> typing.Any:\n952         \"\"\"Return the number value for value, given this field's `num_type`.\"\"\"\n953         return self.num_type(value)\n954 \n955     def _validated(self, value) -> _T | None:\n956         \"\"\"Format the value or raise a :exc:`ValidationError` if an error occurs.\"\"\"\n957         if value is None:\n958             return None\n959         # (value is True or value is False) is ~5x faster than isinstance(value, bool)\n960         if value is True or value is False:\n961             raise self.make_error(\"invalid\", input=value)\n962         try:\n963             return self._format_num(value)\n964         except (TypeError, ValueError) as error:\n965             raise self.make_error(\"invalid\", input=value) from error\n966         except OverflowError as error:\n967             raise self.make_error(\"too_large\", input=value) from error\n968 \n969     def _to_string(self, value) -> str:\n970         return str(value)\n971 \n972     def _serialize(self, value, attr, obj, **kwargs) -> str | _T | None:\n973         \"\"\"Return a string if `self.as_string=True`, otherwise return this field's `num_type`.\"\"\"\n974         if value is None:\n975             return None\n976         ret = self._format_num(value)  # type: _T\n977         return self._to_string(ret) if self.as_string else ret\n978 \n979     def _deserialize(self, value, attr, data, **kwargs) -> _T | None:\n980         return self._validated(value)\n981 \n982 \n983 class Integer(Number):\n984     \"\"\"An integer field.\n985 \n986     :param strict: If `True`, only integer types are valid.\n987         Otherwise, any value castable to `int` is valid.\n988     :param kwargs: The same keyword arguments that :class:`Number` receives.\n989     \"\"\"\n990 \n991     num_type = int\n992 \n993     #: Default error messages.\n994     default_error_messages = {\"invalid\": \"Not a valid integer.\"}\n995 \n996     def __init__(self, *, strict: bool = False, **kwargs):\n997         self.strict = strict\n998         super().__init__(**kwargs)\n999 \n1000     # override Number\n1001     def _validated(self, value):\n1002         if self.strict and not isinstance(value, numbers.Integral):\n1003             raise self.make_error(\"invalid\", input=value)\n1004         return super()._validated(value)\n1005 \n1006 \n1007 class Float(Number):\n1008     \"\"\"A double as an IEEE-754 double precision string.\n1009 \n1010     :param bool allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,\n1011         even though they are illegal according to the JSON specification.\n1012     :param bool as_string: If `True`, format the value as a string.\n1013     :param kwargs: The same keyword arguments that :class:`Number` receives.\n1014     \"\"\"\n1015 \n1016     num_type = float\n1017 \n1018     #: Default error messages.\n1019     default_error_messages = {\n1020         \"special\": \"Special numeric values (nan or infinity) are not permitted.\"\n1021     }\n1022 \n1023     def __init__(self, *, allow_nan: bool = False, as_string: bool = False, **kwargs):\n1024         self.allow_nan = allow_nan\n1025         super().__init__(as_string=as_string, **kwargs)\n1026 \n1027     def _validated(self, value):\n1028         num = super()._validated(value)\n1029         if self.allow_nan is False:\n1030             if math.isnan(num) or num == float(\"inf\") or num == float(\"-inf\"):\n1031                 raise self.make_error(\"special\")\n1032         return num\n1033 \n1034 \n1035 class Decimal(Number):\n1036     \"\"\"A field that (de)serializes to the Python ``decimal.Decimal`` type.\n1037     It's safe to use when dealing with money values, percentages, ratios\n1038     or other numbers where precision is critical.\n1039 \n1040     .. warning::\n1041 \n1042         This field serializes to a `decimal.Decimal` object by default. If you need\n1043         to render your data as JSON, keep in mind that the `json` module from the\n1044         standard library does not encode `decimal.Decimal`. Therefore, you must use\n1045         a JSON library that can handle decimals, such as `simplejson`, or serialize\n1046         to a string by passing ``as_string=True``.\n1047 \n1048     .. warning::\n1049 \n1050         If a JSON `float` value is passed to this field for deserialization it will\n1051         first be cast to its corresponding `string` value before being deserialized\n1052         to a `decimal.Decimal` object. The default `__str__` implementation of the\n1053         built-in Python `float` type may apply a destructive transformation upon\n1054         its input data and therefore cannot be relied upon to preserve precision.\n1055         To avoid this, you can instead pass a JSON `string` to be deserialized\n1056         directly.\n1057 \n1058     :param places: How many decimal places to quantize the value. If `None`, does\n1059         not quantize the value.\n1060     :param rounding: How to round the value during quantize, for example\n1061         `decimal.ROUND_UP`. If `None`, uses the rounding value from\n1062         the current thread's context.\n1063     :param allow_nan: If `True`, `NaN`, `Infinity` and `-Infinity` are allowed,\n1064         even though they are illegal according to the JSON specification.\n1065     :param as_string: If `True`, serialize to a string instead of a Python\n1066         `decimal.Decimal` type.\n1067     :param kwargs: The same keyword arguments that :class:`Number` receives.\n1068 \n1069     .. versionadded:: 1.2.0\n1070     \"\"\"\n1071 \n1072     num_type = decimal.Decimal\n1073 \n1074     #: Default error messages.\n1075     default_error_messages = {\n1076         \"special\": \"Special numeric values (nan or infinity) are not permitted.\"\n1077     }\n1078 \n1079     def __init__(\n1080         self,\n1081         places: int | None = None,\n1082         rounding: str | None = None,\n1083         *,\n1084         allow_nan: bool = False,\n1085         as_string: bool = False,\n1086         **kwargs,\n1087     ):\n1088         self.places = (\n1089             decimal.Decimal((0, (1,), -places)) if places is not None else None\n1090         )\n1091         self.rounding = rounding\n1092         self.allow_nan = allow_nan\n1093         super().__init__(as_string=as_string, **kwargs)\n1094 \n1095     # override Number\n1096     def _format_num(self, value):\n1097         num = decimal.Decimal(str(value))\n1098         if self.allow_nan:\n1099             if num.is_nan():\n1100                 return decimal.Decimal(\"NaN\")  # avoid sNaN, -sNaN and -NaN\n1101         if self.places is not None and num.is_finite():\n1102             num = num.quantize(self.places, rounding=self.rounding)\n1103         return num\n1104 \n1105     # override Number\n1106     def _validated(self, value):\n1107         try:\n1108             num = super()._validated(value)\n1109         except decimal.InvalidOperation as error:\n1110             raise self.make_error(\"invalid\") from error\n1111         if not self.allow_nan and (num.is_nan() or num.is_infinite()):\n1112             raise self.make_error(\"special\")\n1113         return num\n1114 \n1115     # override Number\n1116     def _to_string(self, value):\n1117         return format(value, \"f\")\n1118 \n1119 \n1120 class Boolean(Field):\n1121     \"\"\"A boolean field.\n1122 \n1123     :param truthy: Values that will (de)serialize to `True`. If an empty\n1124         set, any non-falsy value will deserialize to `True`. If `None`,\n1125         `marshmallow.fields.Boolean.truthy` will be used.\n1126     :param falsy: Values that will (de)serialize to `False`. If `None`,\n1127         `marshmallow.fields.Boolean.falsy` will be used.\n1128     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1129     \"\"\"\n1130 \n1131     #: Default truthy values.\n1132     truthy = {\n1133         \"t\",\n1134         \"T\",\n1135         \"true\",\n1136         \"True\",\n1137         \"TRUE\",\n1138         \"on\",\n1139         \"On\",\n1140         \"ON\",\n1141         \"y\",\n1142         \"Y\",\n1143         \"yes\",\n1144         \"Yes\",\n1145         \"YES\",\n1146         \"1\",\n1147         1,\n1148         True,\n1149     }\n1150     #: Default falsy values.\n1151     falsy = {\n1152         \"f\",\n1153         \"F\",\n1154         \"false\",\n1155         \"False\",\n1156         \"FALSE\",\n1157         \"off\",\n1158         \"Off\",\n1159         \"OFF\",\n1160         \"n\",\n1161         \"N\",\n1162         \"no\",\n1163         \"No\",\n1164         \"NO\",\n1165         \"0\",\n1166         0,\n1167         0.0,\n1168         False,\n1169     }\n1170 \n1171     #: Default error messages.\n1172     default_error_messages = {\"invalid\": \"Not a valid boolean.\"}\n1173 \n1174     def __init__(\n1175         self,\n1176         *,\n1177         truthy: set | None = None,\n1178         falsy: set | None = None,\n1179         **kwargs,\n1180     ):\n1181         super().__init__(**kwargs)\n1182 \n1183         if truthy is not None:\n1184             self.truthy = set(truthy)\n1185         if falsy is not None:\n1186             self.falsy = set(falsy)\n1187 \n1188     def _serialize(self, value, attr, obj, **kwargs):\n1189         if value is None:\n1190             return None\n1191 \n1192         try:\n1193             if value in self.truthy:\n1194                 return True\n1195             if value in self.falsy:\n1196                 return False\n1197         except TypeError:\n1198             pass\n1199 \n1200         return bool(value)\n1201 \n1202     def _deserialize(self, value, attr, data, **kwargs):\n1203         if not self.truthy:\n1204             return bool(value)\n1205         try:\n1206             if value in self.truthy:\n1207                 return True\n1208             if value in self.falsy:\n1209                 return False\n1210         except TypeError as error:\n1211             raise self.make_error(\"invalid\", input=value) from error\n1212         raise self.make_error(\"invalid\", input=value)\n1213 \n1214 \n1215 class DateTime(Field):\n1216     \"\"\"A formatted datetime string.\n1217 \n1218     Example: ``'2014-12-22T03:12:58.019077+00:00'``\n1219 \n1220     :param format: Either ``\"rfc\"`` (for RFC822), ``\"iso\"`` (for ISO8601),\n1221         ``\"timestamp\"``, ``\"timestamp_ms\"`` (for a POSIX timestamp) or a date format string.\n1222         If `None`, defaults to \"iso\".\n1223     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1224 \n1225     .. versionchanged:: 3.0.0rc9\n1226         Does not modify timezone information on (de)serialization.\n1227     .. versionchanged:: 3.19\n1228         Add timestamp as a format.\n1229     \"\"\"\n1230 \n1231     SERIALIZATION_FUNCS = {\n1232         \"iso\": utils.isoformat,\n1233         \"iso8601\": utils.isoformat,\n1234         \"rfc\": utils.rfcformat,\n1235         \"rfc822\": utils.rfcformat,\n1236         \"timestamp\": utils.timestamp,\n1237         \"timestamp_ms\": utils.timestamp_ms,\n1238     }  # type: typing.Dict[str, typing.Callable[[typing.Any], str | float]]\n1239 \n1240     DESERIALIZATION_FUNCS = {\n1241         \"iso\": utils.from_iso_datetime,\n1242         \"iso8601\": utils.from_iso_datetime,\n1243         \"rfc\": utils.from_rfc,\n1244         \"rfc822\": utils.from_rfc,\n1245         \"timestamp\": utils.from_timestamp,\n1246         \"timestamp_ms\": utils.from_timestamp_ms,\n1247     }  # type: typing.Dict[str, typing.Callable[[str], typing.Any]]\n1248 \n1249     DEFAULT_FORMAT = \"iso\"\n1250 \n1251     OBJ_TYPE = \"datetime\"\n1252 \n1253     SCHEMA_OPTS_VAR_NAME = \"datetimeformat\"\n1254 \n1255     #: Default error messages.\n1256     default_error_messages = {\n1257         \"invalid\": \"Not a valid {obj_type}.\",\n1258         \"invalid_awareness\": \"Not a valid {awareness} {obj_type}.\",\n1259         \"format\": '\"{input}\" cannot be formatted as a {obj_type}.',\n1260     }\n1261 \n1262     def __init__(self, format: str | None = None, **kwargs) -> None:\n1263         super().__init__(**kwargs)\n1264         # Allow this to be None. It may be set later in the ``_serialize``\n1265         # or ``_deserialize`` methods. This allows a Schema to dynamically set the\n1266         # format, e.g. from a Meta option\n1267         self.format = format\n1268 \n1269     def _bind_to_schema(self, field_name, schema):\n1270         super()._bind_to_schema(field_name, schema)\n1271         self.format = (\n1272             self.format\n1273             or getattr(self.root.opts, self.SCHEMA_OPTS_VAR_NAME)\n1274             or self.DEFAULT_FORMAT\n1275         )\n1276 \n1277     def _serialize(self, value, attr, obj, **kwargs) -> str | float | None:\n1278         if value is None:\n1279             return None\n1280         data_format = self.format or self.DEFAULT_FORMAT\n1281         format_func = self.SERIALIZATION_FUNCS.get(data_format)\n1282         if format_func:\n1283             return format_func(value)\n1284         else:\n1285             return value.strftime(data_format)\n1286 \n1287     def _deserialize(self, value, attr, data, **kwargs) -> dt.datetime:\n1288         if not value:  # Falsy values, e.g. '', None, [] are not valid\n1289             raise self.make_error(\"invalid\", input=value, obj_type=self.OBJ_TYPE)\n1290         data_format = self.format or self.DEFAULT_FORMAT\n1291         func = self.DESERIALIZATION_FUNCS.get(data_format)\n1292         if func:\n1293             try:\n1294                 return func(value)\n1295             except (TypeError, AttributeError, ValueError) as error:\n1296                 raise self.make_error(\n1297                     \"invalid\", input=value, obj_type=self.OBJ_TYPE\n1298                 ) from error\n1299         else:\n1300             try:\n1301                 return self._make_object_from_format(value, data_format)\n1302             except (TypeError, AttributeError, ValueError) as error:\n1303                 raise self.make_error(\n1304                     \"invalid\", input=value, obj_type=self.OBJ_TYPE\n1305                 ) from error\n1306 \n1307     @staticmethod\n1308     def _make_object_from_format(value, data_format) -> dt.datetime:\n1309         return dt.datetime.strptime(value, data_format)\n1310 \n1311 \n1312 class NaiveDateTime(DateTime):\n1313     \"\"\"A formatted naive datetime string.\n1314 \n1315     :param format: See :class:`DateTime`.\n1316     :param timezone: Used on deserialization. If `None`,\n1317         aware datetimes are rejected. If not `None`, aware datetimes are\n1318         converted to this timezone before their timezone information is\n1319         removed.\n1320     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1321 \n1322     .. versionadded:: 3.0.0rc9\n1323     \"\"\"\n1324 \n1325     AWARENESS = \"naive\"\n1326 \n1327     def __init__(\n1328         self,\n1329         format: str | None = None,\n1330         *,\n1331         timezone: dt.timezone | None = None,\n1332         **kwargs,\n1333     ) -> None:\n1334         super().__init__(format=format, **kwargs)\n1335         self.timezone = timezone\n1336 \n1337     def _deserialize(self, value, attr, data, **kwargs) -> dt.datetime:\n1338         ret = super()._deserialize(value, attr, data, **kwargs)\n1339         if is_aware(ret):\n1340             if self.timezone is None:\n1341                 raise self.make_error(\n1342                     \"invalid_awareness\",\n1343                     awareness=self.AWARENESS,\n1344                     obj_type=self.OBJ_TYPE,\n1345                 )\n1346             ret = ret.astimezone(self.timezone).replace(tzinfo=None)\n1347         return ret\n1348 \n1349 \n1350 class AwareDateTime(DateTime):\n1351     \"\"\"A formatted aware datetime string.\n1352 \n1353     :param format: See :class:`DateTime`.\n1354     :param default_timezone: Used on deserialization. If `None`, naive\n1355         datetimes are rejected. If not `None`, naive datetimes are set this\n1356         timezone.\n1357     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1358 \n1359     .. versionadded:: 3.0.0rc9\n1360     \"\"\"\n1361 \n1362     AWARENESS = \"aware\"\n1363 \n1364     def __init__(\n1365         self,\n1366         format: str | None = None,\n1367         *,\n1368         default_timezone: dt.tzinfo | None = None,\n1369         **kwargs,\n1370     ) -> None:\n1371         super().__init__(format=format, **kwargs)\n1372         self.default_timezone = default_timezone\n1373 \n1374     def _deserialize(self, value, attr, data, **kwargs) -> dt.datetime:\n1375         ret = super()._deserialize(value, attr, data, **kwargs)\n1376         if not is_aware(ret):\n1377             if self.default_timezone is None:\n1378                 raise self.make_error(\n1379                     \"invalid_awareness\",\n1380                     awareness=self.AWARENESS,\n1381                     obj_type=self.OBJ_TYPE,\n1382                 )\n1383             ret = ret.replace(tzinfo=self.default_timezone)\n1384         return ret\n1385 \n1386 \n1387 class Time(DateTime):\n1388     \"\"\"A formatted time string.\n1389 \n1390     Example: ``'03:12:58.019077'``\n1391 \n1392     :param format: Either ``\"iso\"`` (for ISO8601) or a date format string.\n1393         If `None`, defaults to \"iso\".\n1394     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1395     \"\"\"\n1396 \n1397     SERIALIZATION_FUNCS = {\"iso\": utils.to_iso_time, \"iso8601\": utils.to_iso_time}\n1398 \n1399     DESERIALIZATION_FUNCS = {\"iso\": utils.from_iso_time, \"iso8601\": utils.from_iso_time}\n1400 \n1401     DEFAULT_FORMAT = \"iso\"\n1402 \n1403     OBJ_TYPE = \"time\"\n1404 \n1405     SCHEMA_OPTS_VAR_NAME = \"timeformat\"\n1406 \n1407     @staticmethod\n1408     def _make_object_from_format(value, data_format):\n1409         return dt.datetime.strptime(value, data_format).time()\n1410 \n1411 \n1412 class Date(DateTime):\n1413     \"\"\"ISO8601-formatted date string.\n1414 \n1415     :param format: Either ``\"iso\"`` (for ISO8601) or a date format string.\n1416         If `None`, defaults to \"iso\".\n1417     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1418     \"\"\"\n1419 \n1420     #: Default error messages.\n1421     default_error_messages = {\n1422         \"invalid\": \"Not a valid date.\",\n1423         \"format\": '\"{input}\" cannot be formatted as a date.',\n1424     }\n1425 \n1426     SERIALIZATION_FUNCS = {\"iso\": utils.to_iso_date, \"iso8601\": utils.to_iso_date}\n1427 \n1428     DESERIALIZATION_FUNCS = {\"iso\": utils.from_iso_date, \"iso8601\": utils.from_iso_date}\n1429 \n1430     DEFAULT_FORMAT = \"iso\"\n1431 \n1432     OBJ_TYPE = \"date\"\n1433 \n1434     SCHEMA_OPTS_VAR_NAME = \"dateformat\"\n1435 \n1436     @staticmethod\n1437     def _make_object_from_format(value, data_format):\n1438         return dt.datetime.strptime(value, data_format).date()\n1439 \n1440 \n1441 class TimeDelta(Field):\n1442     \"\"\"A field that (de)serializes a :class:`datetime.timedelta` object to an\n1443     integer or float and vice versa. The integer or float can represent the\n1444     number of days, seconds or microseconds.\n1445 \n1446     :param precision: Influences how the integer or float is interpreted during\n1447         (de)serialization. Must be 'days', 'seconds', 'microseconds',\n1448         'milliseconds', 'minutes', 'hours' or 'weeks'.\n1449     :param serialization_type: Whether to (de)serialize to a `int` or `float`.\n1450     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1451 \n1452     Integer Caveats\n1453     ---------------\n1454     Any fractional parts (which depends on the precision used) will be truncated\n1455     when serializing using `int`.\n1456 \n1457     Float Caveats\n1458     -------------\n1459     Use of `float` when (de)serializing may result in data precision loss due\n1460     to the way machines handle floating point values.\n1461 \n1462     Regardless of the precision chosen, the fractional part when using `float`\n1463     will always be truncated to microseconds.\n1464     For example, `1.12345` interpreted as microseconds will result in `timedelta(microseconds=1)`.\n1465 \n1466     .. versionchanged:: 2.0.0\n1467         Always serializes to an integer value to avoid rounding errors.\n1468         Add `precision` parameter.\n1469     .. versionchanged:: 3.17.0\n1470         Allow (de)serialization to `float` through use of a new `serialization_type` parameter.\n1471         `int` is the default to retain previous behaviour.\n1472     \"\"\"\n1473 \n1474     DAYS = \"days\"\n1475     SECONDS = \"seconds\"\n1476     MICROSECONDS = \"microseconds\"\n1477     MILLISECONDS = \"milliseconds\"\n1478     MINUTES = \"minutes\"\n1479     HOURS = \"hours\"\n1480     WEEKS = \"weeks\"\n1481 \n1482     #: Default error messages.\n1483     default_error_messages = {\n1484         \"invalid\": \"Not a valid period of time.\",\n1485         \"format\": \"{input!r} cannot be formatted as a timedelta.\",\n1486     }\n1487 \n1488     def __init__(\n1489         self,\n1490         precision: str = SECONDS,\n1491         serialization_type: type[int | float] = int,\n1492         **kwargs,\n1493     ):\n1494         precision = precision.lower()\n1495         units = (\n1496             self.DAYS,\n1497             self.SECONDS,\n1498             self.MICROSECONDS,\n1499             self.MILLISECONDS,\n1500             self.MINUTES,\n1501             self.HOURS,\n1502             self.WEEKS,\n1503         )\n1504 \n1505         if precision not in units:\n1506             msg = 'The precision must be {} or \"{}\".'.format(\n1507                 \", \".join([f'\"{each}\"' for each in units[:-1]]), units[-1]\n1508             )\n1509             raise ValueError(msg)\n1510 \n1511         if serialization_type not in (int, float):\n1512             raise ValueError(\"The serialization type must be one of int or float\")\n1513 \n1514         self.precision = precision\n1515         self.serialization_type = serialization_type\n1516         super().__init__(**kwargs)\n1517 \n1518     def _serialize(self, value, attr, obj, **kwargs):\n1519         if value is None:\n1520             return None\n1521 \n1522         base_unit = dt.timedelta(**{self.precision: 1})\n1523 \n1524         if self.serialization_type is int:\n1525             delta = utils.timedelta_to_microseconds(value)\n1526             unit = utils.timedelta_to_microseconds(base_unit)\n1527             return delta // unit\n1528         else:\n1529             assert self.serialization_type is float\n1530             return value.total_seconds() / base_unit.total_seconds()\n1531 \n1532     def _deserialize(self, value, attr, data, **kwargs):\n1533         try:\n1534             value = self.serialization_type(value)\n1535         except (TypeError, ValueError) as error:\n1536             raise self.make_error(\"invalid\") from error\n1537 \n1538         kwargs = {self.precision: value}\n1539 \n1540         try:\n1541             return dt.timedelta(**kwargs)\n1542         except OverflowError as error:\n1543             raise self.make_error(\"invalid\") from error\n1544 \n1545 \n1546 class Mapping(Field):\n1547     \"\"\"An abstract class for objects with key-value pairs.\n1548 \n1549     :param keys: A field class or instance for dict keys.\n1550     :param values: A field class or instance for dict values.\n1551     :param kwargs: The same keyword arguments that :class:`Field` receives.\n1552 \n1553     .. note::\n1554         When the structure of nested data is not known, you may omit the\n1555         `keys` and `values` arguments to prevent content validation.\n1556 \n1557     .. versionadded:: 3.0.0rc4\n1558     \"\"\"\n1559 \n1560     mapping_type = dict\n1561 \n1562     #: Default error messages.\n1563     default_error_messages = {\"invalid\": \"Not a valid mapping type.\"}\n1564 \n1565     def __init__(\n1566         self,\n1567         keys: Field | type | None = None,\n1568         values: Field | type | None = None,\n1569         **kwargs,\n1570     ):\n1571         super().__init__(**kwargs)\n1572         if keys is None:\n1573             self.key_field = None\n1574         else:\n1575             try:\n1576                 self.key_field = resolve_field_instance(keys)\n1577             except FieldInstanceResolutionError as error:\n1578                 raise ValueError(\n1579                     '\"keys\" must be a subclass or instance of '\n1580                     \"marshmallow.base.FieldABC.\"\n1581                 ) from error\n1582 \n1583         if values is None:\n1584             self.value_field = None\n1585         else:\n1586             try:\n1587                 self.value_field = resolve_field_instance(values)\n1588             except FieldInstanceResolutionError as error:\n1589                 raise ValueError(\n1590                     '\"values\" must be a subclass or instance of '\n1591                     \"marshmallow.base.FieldABC.\"\n1592                 ) from error\n1593             if isinstance(self.value_field, Nested):\n1594                 self.only = self.value_field.only\n1595                 self.exclude = self.value_field.exclude\n1596 \n1597     def _bind_to_schema(self, field_name, schema):\n1598         super()._bind_to_schema(field_name, schema)\n1599         if self.value_field:\n1600             self.value_field = copy.deepcopy(self.value_field)\n1601             self.value_field._bind_to_schema(field_name, self)\n1602         if isinstance(self.value_field, Nested):\n1603             self.value_field.only = self.only\n1604             self.value_field.exclude = self.exclude\n1605         if self.key_field:\n1606             self.key_field = copy.deepcopy(self.key_field)\n1607             self.key_field._bind_to_schema(field_name, self)\n1608 \n1609     def _serialize(self, value, attr, obj, **kwargs):\n1610         if value is None:\n1611             return None\n1612         if not self.value_field and not self.key_field:\n1613             return self.mapping_type(value)\n1614 \n1615         # \u00a0Serialize keys\n1616         if self.key_field is None:\n1617             keys = {k: k for k in value.keys()}\n1618         else:\n1619             keys = {\n1620                 k: self.key_field._serialize(k, None, None, **kwargs)\n1621                 for k in value.keys()\n1622             }\n1623 \n1624         # \u00a0Serialize values\n1625         result = self.mapping_type()\n1626         if self.value_field is None:\n1627             for k, v in value.items():\n1628                 if k in keys:\n1629                     result[keys[k]] = v\n1630         else:\n1631             for k, v in value.items():\n1632                 result[keys[k]] = self.value_field._serialize(v, None, None, **kwargs)\n1633 \n1634         return result\n1635 \n1636     def _deserialize(self, value, attr, data, **kwargs):\n1637         if not isinstance(value, _Mapping):\n1638             raise self.make_error(\"invalid\")\n1639         if not self.value_field and not self.key_field:\n1640             return self.mapping_type(value)\n1641 \n1642         errors = collections.defaultdict(dict)\n1643 \n1644         # \u00a0Deserialize keys\n1645         if self.key_field is None:\n1646             keys = {k: k for k in value.keys()}\n1647         else:\n1648             keys = {}\n1649             for key in value.keys():\n1650                 try:\n1651                     keys[key] = self.key_field.deserialize(key, **kwargs)\n1652                 except ValidationError as error:\n1653                     errors[key][\"key\"] = error.messages\n1654 \n1655         # \u00a0Deserialize values\n1656         result = self.mapping_type()\n1657         if self.value_field is None:\n1658             for k, v in value.items():\n1659                 if k in keys:\n1660                     result[keys[k]] = v\n1661         else:\n1662             for key, val in value.items():\n1663                 try:\n1664                     deser_val = self.value_field.deserialize(val, **kwargs)\n1665                 except ValidationError as error:\n1666                     errors[key][\"value\"] = error.messages\n1667                     if error.valid_data is not None and key in keys:\n1668                         result[keys[key]] = error.valid_data\n1669                 else:\n1670                     if key in keys:\n1671                         result[keys[key]] = deser_val\n1672 \n1673         if errors:\n1674             raise ValidationError(errors, valid_data=result)\n1675 \n1676         return result\n1677 \n1678 \n1679 class Dict(Mapping):\n1680     \"\"\"A dict field. Supports dicts and dict-like objects. Extends\n1681     Mapping with dict as the mapping_type.\n1682 \n1683     Example: ::\n1684 \n1685         numbers = fields.Dict(keys=fields.Str(), values=fields.Float())\n1686 \n1687     :param kwargs: The same keyword arguments that :class:`Mapping` receives.\n1688 \n1689     .. versionadded:: 2.1.0\n1690     \"\"\"\n1691 \n1692     mapping_type = dict\n1693 \n1694 \n1695 class Url(String):\n1696     \"\"\"An URL field.\n1697 \n1698     :param default: Default value for the field if the attribute is not set.\n1699     :param relative: Whether to allow relative URLs.\n1700     :param require_tld: Whether to reject non-FQDN hostnames.\n1701     :param schemes: Valid schemes. By default, ``http``, ``https``,\n1702         ``ftp``, and ``ftps`` are allowed.\n1703     :param kwargs: The same keyword arguments that :class:`String` receives.\n1704     \"\"\"\n1705 \n1706     #: Default error messages.\n1707     default_error_messages = {\"invalid\": \"Not a valid URL.\"}\n1708 \n1709     def __init__(\n1710         self,\n1711         *,\n1712         relative: bool = False,\n1713         schemes: types.StrSequenceOrSet | None = None,\n1714         require_tld: bool = True,\n1715         **kwargs,\n1716     ):\n1717         super().__init__(**kwargs)\n1718 \n1719         self.relative = relative\n1720         self.require_tld = require_tld\n1721         # Insert validation into self.validators so that multiple errors can be stored.\n1722         validator = validate.URL(\n1723             relative=self.relative,\n1724             schemes=schemes,\n1725             require_tld=self.require_tld,\n1726             error=self.error_messages[\"invalid\"],\n1727         )\n1728         self.validators.insert(0, validator)\n1729 \n1730 \n1731 class Email(String):\n1732     \"\"\"An email field.\n1733 \n1734     :param args: The same positional arguments that :class:`String` receives.\n1735     :param kwargs: The same keyword arguments that :class:`String` receives.\n1736     \"\"\"\n1737 \n1738     #: Default error messages.\n1739     default_error_messages = {\"invalid\": \"Not a valid email address.\"}\n1740 \n1741     def __init__(self, *args, **kwargs) -> None:\n1742         super().__init__(*args, **kwargs)\n1743         # Insert validation into self.validators so that multiple errors can be stored.\n1744         validator = validate.Email(error=self.error_messages[\"invalid\"])\n1745         self.validators.insert(0, validator)\n1746 \n1747 \n1748 class IP(Field):\n1749     \"\"\"A IP address field.\n1750 \n1751     :param bool exploded: If `True`, serialize ipv6 address in long form, ie. with groups\n1752         consisting entirely of zeros included.\n1753 \n1754     .. versionadded:: 3.8.0\n1755     \"\"\"\n1756 \n1757     default_error_messages = {\"invalid_ip\": \"Not a valid IP address.\"}\n1758 \n1759     DESERIALIZATION_CLASS = None  # type: typing.Optional[typing.Type]\n1760 \n1761     def __init__(self, *args, exploded=False, **kwargs):\n1762         super().__init__(*args, **kwargs)\n1763         self.exploded = exploded\n1764 \n1765     def _serialize(self, value, attr, obj, **kwargs) -> str | None:\n1766         if value is None:\n1767             return None\n1768         if self.exploded:\n1769             return value.exploded\n1770         return value.compressed\n1771 \n1772     def _deserialize(\n1773         self, value, attr, data, **kwargs\n1774     ) -> ipaddress.IPv4Address | ipaddress.IPv6Address | None:\n1775         if value is None:\n1776             return None\n1777         try:\n1778             return (self.DESERIALIZATION_CLASS or ipaddress.ip_address)(\n1779                 utils.ensure_text_type(value)\n1780             )\n1781         except (ValueError, TypeError) as error:\n1782             raise self.make_error(\"invalid_ip\") from error\n1783 \n1784 \n1785 class IPv4(IP):\n1786     \"\"\"A IPv4 address field.\n1787 \n1788     .. versionadded:: 3.8.0\n1789     \"\"\"\n1790 \n1791     default_error_messages = {\"invalid_ip\": \"Not a valid IPv4 address.\"}\n1792 \n1793     DESERIALIZATION_CLASS = ipaddress.IPv4Address\n1794 \n1795 \n1796 class IPv6(IP):\n1797     \"\"\"A IPv6 address field.\n1798 \n1799     .. versionadded:: 3.8.0\n1800     \"\"\"\n1801 \n1802     default_error_messages = {\"invalid_ip\": \"Not a valid IPv6 address.\"}\n1803 \n1804     DESERIALIZATION_CLASS = ipaddress.IPv6Address\n1805 \n1806 \n1807 class IPInterface(Field):\n1808     \"\"\"A IPInterface field.\n1809 \n1810     IP interface is the non-strict form of the IPNetwork type where arbitrary host\n1811     addresses are always accepted.\n1812 \n1813     IPAddress and mask e.g. '192.168.0.2/24' or '192.168.0.2/255.255.255.0'\n1814 \n1815     see https://python.readthedocs.io/en/latest/library/ipaddress.html#interface-objects\n1816 \n1817     :param bool exploded: If `True`, serialize ipv6 interface in long form, ie. with groups\n1818         consisting entirely of zeros included.\n1819     \"\"\"\n1820 \n1821     default_error_messages = {\"invalid_ip_interface\": \"Not a valid IP interface.\"}\n1822 \n1823     DESERIALIZATION_CLASS = None  # type: typing.Optional[typing.Type]\n1824 \n1825     def __init__(self, *args, exploded: bool = False, **kwargs):\n1826         super().__init__(*args, **kwargs)\n1827         self.exploded = exploded\n1828 \n1829     def _serialize(self, value, attr, obj, **kwargs) -> str | None:\n1830         if value is None:\n1831             return None\n1832         if self.exploded:\n1833             return value.exploded\n1834         return value.compressed\n1835 \n1836     def _deserialize(\n1837         self, value, attr, data, **kwargs\n1838     ) -> None | (ipaddress.IPv4Interface | ipaddress.IPv6Interface):\n1839         if value is None:\n1840             return None\n1841         try:\n1842             return (self.DESERIALIZATION_CLASS or ipaddress.ip_interface)(\n1843                 utils.ensure_text_type(value)\n1844             )\n1845         except (ValueError, TypeError) as error:\n1846             raise self.make_error(\"invalid_ip_interface\") from error\n1847 \n1848 \n1849 class IPv4Interface(IPInterface):\n1850     \"\"\"A IPv4 Network Interface field.\"\"\"\n1851 \n1852     default_error_messages = {\"invalid_ip_interface\": \"Not a valid IPv4 interface.\"}\n1853 \n1854     DESERIALIZATION_CLASS = ipaddress.IPv4Interface\n1855 \n1856 \n1857 class IPv6Interface(IPInterface):\n1858     \"\"\"A IPv6 Network Interface field.\"\"\"\n1859 \n1860     default_error_messages = {\"invalid_ip_interface\": \"Not a valid IPv6 interface.\"}\n1861 \n1862     DESERIALIZATION_CLASS = ipaddress.IPv6Interface\n1863 \n1864 \n1865 class Enum(Field):\n1866     \"\"\"An Enum field (de)serializing enum members by symbol (name) or by value.\n1867 \n1868     :param enum Enum: Enum class\n1869     :param boolean|Schema|Field by_value: Whether to (de)serialize by value or by name,\n1870         or Field class or instance to use to (de)serialize by value. Defaults to False.\n1871 \n1872     If `by_value` is `False` (default), enum members are (de)serialized by symbol (name).\n1873     If it is `True`, they are (de)serialized by value using :class:`Field`.\n1874     If it is a field instance or class, they are (de)serialized by value using this field.\n1875 \n1876     .. versionadded:: 3.18.0\n1877     \"\"\"\n1878 \n1879     default_error_messages = {\n1880         \"unknown\": \"Must be one of: {choices}.\",\n1881     }\n1882 \n1883     def __init__(\n1884         self,\n1885         enum: type[EnumType],\n1886         *,\n1887         by_value: bool | Field | type = False,\n1888         **kwargs,\n1889     ):\n1890         super().__init__(**kwargs)\n1891         self.enum = enum\n1892         self.by_value = by_value\n1893 \n1894         # Serialization by name\n1895         if by_value is False:\n1896             self.field: Field = String()\n1897             self.choices_text = \", \".join(\n1898                 str(self.field._serialize(m, None, None)) for m in enum.__members__\n1899             )\n1900         # Serialization by value\n1901         else:\n1902             if by_value is True:\n1903                 self.field = Field()\n1904             else:\n1905                 try:\n1906                     self.field = resolve_field_instance(by_value)\n1907                 except FieldInstanceResolutionError as error:\n1908                     raise ValueError(\n1909                         '\"by_value\" must be either a bool or a subclass or instance of '\n1910                         \"marshmallow.base.FieldABC.\"\n1911                     ) from error\n1912             self.choices_text = \", \".join(\n1913                 str(self.field._serialize(m.value, None, None)) for m in enum\n1914             )\n1915 \n1916     def _serialize(self, value, attr, obj, **kwargs):\n1917         if value is None:\n1918             return None\n1919         if self.by_value:\n1920             val = value.value\n1921         else:\n1922             val = value.name\n1923         return self.field._serialize(val, attr, obj, **kwargs)\n1924 \n1925     def _deserialize(self, value, attr, data, **kwargs):\n1926         val = self.field._deserialize(value, attr, data, **kwargs)\n1927         if self.by_value:\n1928             try:\n1929                 return self.enum(val)\n1930             except ValueError as error:\n1931                 raise self.make_error(\"unknown\", choices=self.choices_text) from error\n1932         try:\n1933             return getattr(self.enum, val)\n1934         except AttributeError as error:\n1935             raise self.make_error(\"unknown\", choices=self.choices_text) from error\n1936 \n1937 \n1938 class Method(Field):\n1939     \"\"\"A field that takes the value returned by a `Schema` method.\n1940 \n1941     :param str serialize: The name of the Schema method from which\n1942         to retrieve the value. The method must take an argument ``obj``\n1943         (in addition to self) that is the object to be serialized.\n1944     :param str deserialize: Optional name of the Schema method for deserializing\n1945         a value The method must take a single argument ``value``, which is the\n1946         value to deserialize.\n1947 \n1948     .. versionchanged:: 2.0.0\n1949         Removed optional ``context`` parameter on methods. Use ``self.context`` instead.\n1950 \n1951     .. versionchanged:: 2.3.0\n1952         Deprecated ``method_name`` parameter in favor of ``serialize`` and allow\n1953         ``serialize`` to not be passed at all.\n1954 \n1955     .. versionchanged:: 3.0.0\n1956         Removed ``method_name`` parameter.\n1957     \"\"\"\n1958 \n1959     _CHECK_ATTRIBUTE = False\n1960 \n1961     def __init__(\n1962         self,\n1963         serialize: str | None = None,\n1964         deserialize: str | None = None,\n1965         **kwargs,\n1966     ):\n1967         # Set dump_only and load_only based on arguments\n1968         kwargs[\"dump_only\"] = bool(serialize) and not bool(deserialize)\n1969         kwargs[\"load_only\"] = bool(deserialize) and not bool(serialize)\n1970         super().__init__(**kwargs)\n1971         self.serialize_method_name = serialize\n1972         self.deserialize_method_name = deserialize\n1973         self._serialize_method = None\n1974         self._deserialize_method = None\n1975 \n1976     def _bind_to_schema(self, field_name, schema):\n1977         if self.serialize_method_name:\n1978             self._serialize_method = utils.callable_or_raise(\n1979                 getattr(schema, self.serialize_method_name)\n1980             )\n1981 \n1982         if self.deserialize_method_name:\n1983             self._deserialize_method = utils.callable_or_raise(\n1984                 getattr(schema, self.deserialize_method_name)\n1985             )\n1986 \n1987         super()._bind_to_schema(field_name, schema)\n1988 \n1989     def _serialize(self, value, attr, obj, **kwargs):\n1990         if self._serialize_method is not None:\n1991             return self._serialize_method(obj)\n1992         return missing_\n1993 \n1994     def _deserialize(self, value, attr, data, **kwargs):\n1995         if self._deserialize_method is not None:\n1996             return self._deserialize_method(value)\n1997         return value\n1998 \n1999 \n2000 class Function(Field):\n2001     \"\"\"A field that takes the value returned by a function.\n2002 \n2003     :param serialize: A callable from which to retrieve the value.\n2004         The function must take a single argument ``obj`` which is the object\n2005         to be serialized. It can also optionally take a ``context`` argument,\n2006         which is a dictionary of context variables passed to the serializer.\n2007         If no callable is provided then the ```load_only``` flag will be set\n2008         to True.\n2009     :param deserialize: A callable from which to retrieve the value.\n2010         The function must take a single argument ``value`` which is the value\n2011         to be deserialized. It can also optionally take a ``context`` argument,\n2012         which is a dictionary of context variables passed to the deserializer.\n2013         If no callable is provided then ```value``` will be passed through\n2014         unchanged.\n2015 \n2016     .. versionchanged:: 2.3.0\n2017         Deprecated ``func`` parameter in favor of ``serialize``.\n2018 \n2019     .. versionchanged:: 3.0.0a1\n2020         Removed ``func`` parameter.\n2021     \"\"\"\n2022 \n2023     _CHECK_ATTRIBUTE = False\n2024 \n2025     def __init__(\n2026         self,\n2027         serialize: (\n2028             None\n2029             | typing.Callable[[typing.Any], typing.Any]\n2030             | typing.Callable[[typing.Any, dict], typing.Any]\n2031         ) = None,\n2032         deserialize: (\n2033             None\n2034             | typing.Callable[[typing.Any], typing.Any]\n2035             | typing.Callable[[typing.Any, dict], typing.Any]\n2036         ) = None,\n2037         **kwargs,\n2038     ):\n2039         # Set dump_only and load_only based on arguments\n2040         kwargs[\"dump_only\"] = bool(serialize) and not bool(deserialize)\n2041         kwargs[\"load_only\"] = bool(deserialize) and not bool(serialize)\n2042         super().__init__(**kwargs)\n2043         self.serialize_func = serialize and utils.callable_or_raise(serialize)\n2044         self.deserialize_func = deserialize and utils.callable_or_raise(deserialize)\n2045 \n2046     def _serialize(self, value, attr, obj, **kwargs):\n2047         return self._call_or_raise(self.serialize_func, obj, attr)\n2048 \n2049     def _deserialize(self, value, attr, data, **kwargs):\n2050         if self.deserialize_func:\n2051             return self._call_or_raise(self.deserialize_func, value, attr)\n2052         return value\n2053 \n2054     def _call_or_raise(self, func, value, attr):\n2055         if len(utils.get_func_args(func)) > 1:\n2056             if self.parent.context is None:\n2057                 msg = f\"No context available for Function field {attr!r}\"\n2058                 raise ValidationError(msg)\n2059             return func(value, self.parent.context)\n2060         else:\n2061             return func(value)\n2062 \n2063 \n2064 class Constant(Field):\n2065     \"\"\"A field that (de)serializes to a preset constant.  If you only want the\n2066     constant added for serialization or deserialization, you should use\n2067     ``dump_only=True`` or ``load_only=True`` respectively.\n2068 \n2069     :param constant: The constant to return for the field attribute.\n2070 \n2071     .. versionadded:: 2.0.0\n2072     \"\"\"\n2073 \n2074     _CHECK_ATTRIBUTE = False\n2075 \n2076     def __init__(self, constant: typing.Any, **kwargs):\n2077         super().__init__(**kwargs)\n2078         self.constant = constant\n2079         self.load_default = constant\n2080         self.dump_default = constant\n2081 \n2082     def _serialize(self, value, *args, **kwargs):\n2083         return self.constant\n2084 \n2085     def _deserialize(self, value, *args, **kwargs):\n2086         return self.constant\n2087 \n2088 \n2089 class Inferred(Field):\n2090     \"\"\"A field that infers how to serialize, based on the value type.\n2091 \n2092     .. warning::\n2093 \n2094         This class is treated as private API.\n2095         Users should not need to use this class directly.\n2096     \"\"\"\n2097 \n2098     def __init__(self):\n2099         super().__init__()\n2100         # We memoize the fields to avoid creating and binding new fields\n2101         # every time on serialization.\n2102         self._field_cache = {}\n2103 \n2104     def _serialize(self, value, attr, obj, **kwargs):\n2105         field_cls = self.root.TYPE_MAPPING.get(type(value))\n2106         if field_cls is None:\n2107             field = super()\n2108         else:\n2109             field = self._field_cache.get(field_cls)\n2110             if field is None:\n2111                 field = field_cls()\n2112                 field._bind_to_schema(self.name, self.parent)\n2113                 self._field_cache[field_cls] = field\n2114         return field._serialize(value, attr, obj, **kwargs)\n2115 \n2116 \n2117 # Aliases\n2118 URL = Url\n2119 Str = String\n2120 Bool = Boolean\n2121 Int = Integer\n2122 \n[end of src/marshmallow/fields.py]\n[start of src/marshmallow/validate.py]\n1 \"\"\"Validation classes for various types of data.\"\"\"\n2 from __future__ import annotations\n3 \n4 import re\n5 import typing\n6 from abc import ABC, abstractmethod\n7 from itertools import zip_longest\n8 from operator import attrgetter\n9 \n10 from marshmallow import types\n11 from marshmallow.exceptions import ValidationError\n12 \n13 _T = typing.TypeVar(\"_T\")\n14 \n15 \n16 class Validator(ABC):\n17     \"\"\"Abstract base class for validators.\n18 \n19     .. note::\n20         This class does not provide any validation behavior. It is only used to\n21         add a useful `__repr__` implementation for validators.\n22     \"\"\"\n23 \n24     error = None  # type: str | None\n25 \n26     def __repr__(self) -> str:\n27         args = self._repr_args()\n28         args = f\"{args}, \" if args else \"\"\n29 \n30         return \"<{self.__class__.__name__}({args}error={self.error!r})>\".format(\n31             self=self, args=args\n32         )\n33 \n34     def _repr_args(self) -> str:\n35         \"\"\"A string representation of the args passed to this validator. Used by\n36         `__repr__`.\n37         \"\"\"\n38         return \"\"\n39 \n40     @abstractmethod\n41     def __call__(self, value: typing.Any) -> typing.Any:\n42         ...\n43 \n44 \n45 class And(Validator):\n46     \"\"\"Compose multiple validators and combine their error messages.\n47 \n48     Example: ::\n49 \n50         from marshmallow import validate, ValidationError\n51 \n52         def is_even(value):\n53             if value % 2 != 0:\n54                 raise ValidationError(\"Not an even value.\")\n55 \n56         validator = validate.And(validate.Range(min=0), is_even)\n57         validator(-1)\n58         # ValidationError: ['Must be greater than or equal to 0.', 'Not an even value.']\n59 \n60     :param validators: Validators to combine.\n61     :param error: Error message to use when a validator returns ``False``.\n62     \"\"\"\n63 \n64     default_error_message = \"Invalid value.\"\n65 \n66     def __init__(self, *validators: types.Validator, error: str | None = None):\n67         self.validators = tuple(validators)\n68         self.error = error or self.default_error_message  # type: str\n69 \n70     def _repr_args(self) -> str:\n71         return f\"validators={self.validators!r}\"\n72 \n73     def __call__(self, value: typing.Any) -> typing.Any:\n74         errors = []\n75         kwargs = {}\n76         for validator in self.validators:\n77             try:\n78                 r = validator(value)\n79                 if not isinstance(validator, Validator) and r is False:\n80                     raise ValidationError(self.error)\n81             except ValidationError as err:\n82                 kwargs.update(err.kwargs)\n83                 if isinstance(err.messages, dict):\n84                     errors.append(err.messages)\n85                 else:\n86                     # FIXME : Get rid of cast\n87                     errors.extend(typing.cast(list, err.messages))\n88         if errors:\n89             raise ValidationError(errors, **kwargs)\n90         return value\n91 \n92 \n93 class URL(Validator):\n94     \"\"\"Validate a URL.\n95 \n96     :param relative: Whether to allow relative URLs.\n97     :param error: Error message to raise in case of a validation error.\n98         Can be interpolated with `{input}`.\n99     :param schemes: Valid schemes. By default, ``http``, ``https``,\n100         ``ftp``, and ``ftps`` are allowed.\n101     :param require_tld: Whether to reject non-FQDN hostnames.\n102     \"\"\"\n103 \n104     class RegexMemoizer:\n105         def __init__(self):\n106             self._memoized = {}\n107 \n108         def _regex_generator(self, relative: bool, require_tld: bool) -> typing.Pattern:\n109             return re.compile(\n110                 r\"\".join(\n111                     (\n112                         r\"^\",\n113                         r\"(\" if relative else r\"\",\n114                         r\"(?:[a-z0-9\\.\\-\\+]*)://\",  # scheme is validated separately\n115                         r\"(?:[^:@]+?(:[^:@]*?)?@|)\",  # basic auth\n116                         r\"(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\",\n117                         r\"(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|\",  # domain...\n118                         r\"localhost|\",  # localhost...\n119                         (\n120                             r\"(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.?)|\"\n121                             if not require_tld\n122                             else r\"\"\n123                         ),  # allow dotless hostnames\n124                         r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}|\",  # ...or ipv4\n125                         r\"\\[[A-F0-9]*:[A-F0-9:]+\\])\",  # ...or ipv6\n126                         r\"(?::\\d+)?\",  # optional port\n127                         r\")?\"\n128                         if relative\n129                         else r\"\",  # host is optional, allow for relative URLs\n130                         r\"(?:/?|[/?]\\S+)\\Z\",\n131                     )\n132                 ),\n133                 re.IGNORECASE,\n134             )\n135 \n136         def __call__(self, relative: bool, require_tld: bool) -> typing.Pattern:\n137             key = (relative, require_tld)\n138             if key not in self._memoized:\n139                 self._memoized[key] = self._regex_generator(relative, require_tld)\n140 \n141             return self._memoized[key]\n142 \n143     _regex = RegexMemoizer()\n144 \n145     default_message = \"Not a valid URL.\"\n146     default_schemes = {\"http\", \"https\", \"ftp\", \"ftps\"}\n147 \n148     def __init__(\n149         self,\n150         *,\n151         relative: bool = False,\n152         schemes: types.StrSequenceOrSet | None = None,\n153         require_tld: bool = True,\n154         error: str | None = None,\n155     ):\n156         self.relative = relative\n157         self.error = error or self.default_message  # type: str\n158         self.schemes = schemes or self.default_schemes\n159         self.require_tld = require_tld\n160 \n161     def _repr_args(self) -> str:\n162         return f\"relative={self.relative!r}\"\n163 \n164     def _format_error(self, value) -> str:\n165         return self.error.format(input=value)\n166 \n167     def __call__(self, value: str) -> str:\n168         message = self._format_error(value)\n169         if not value:\n170             raise ValidationError(message)\n171 \n172         # Check first if the scheme is valid\n173         if \"://\" in value:\n174             scheme = value.split(\"://\")[0].lower()\n175             if scheme not in self.schemes:\n176                 raise ValidationError(message)\n177 \n178         regex = self._regex(self.relative, self.require_tld)\n179 \n180         if not regex.search(value):\n181             raise ValidationError(message)\n182 \n183         return value\n184 \n185 \n186 class Email(Validator):\n187     \"\"\"Validate an email address.\n188 \n189     :param error: Error message to raise in case of a validation error. Can be\n190         interpolated with `{input}`.\n191     \"\"\"\n192 \n193     USER_REGEX = re.compile(\n194         r\"(^[-!#$%&'*+/=?^`{}|~\\w]+(\\.[-!#$%&'*+/=?^`{}|~\\w]+)*\\Z\"  # dot-atom\n195         # quoted-string\n196         r'|^\"([\\001-\\010\\013\\014\\016-\\037!#-\\[\\]-\\177]'\n197         r'|\\\\[\\001-\\011\\013\\014\\016-\\177])*\"\\Z)',\n198         re.IGNORECASE | re.UNICODE,\n199     )\n200 \n201     DOMAIN_REGEX = re.compile(\n202         # domain\n203         r\"(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+\" r\"(?:[A-Z]{2,6}|[A-Z0-9-]{2,})\\Z\"\n204         # literal form, ipv4 address (SMTP 4.1.3)\n205         r\"|^\\[(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)\"\n206         r\"(\\.(25[0-5]|2[0-4]\\d|[0-1]?\\d?\\d)){3}\\]\\Z\",\n207         re.IGNORECASE | re.UNICODE,\n208     )\n209 \n210     DOMAIN_WHITELIST = (\"localhost\",)\n211 \n212     default_message = \"Not a valid email address.\"\n213 \n214     def __init__(self, *, error: str | None = None):\n215         self.error = error or self.default_message  # type: str\n216 \n217     def _format_error(self, value: str) -> str:\n218         return self.error.format(input=value)\n219 \n220     def __call__(self, value: str) -> str:\n221         message = self._format_error(value)\n222 \n223         if not value or \"@\" not in value:\n224             raise ValidationError(message)\n225 \n226         user_part, domain_part = value.rsplit(\"@\", 1)\n227 \n228         if not self.USER_REGEX.match(user_part):\n229             raise ValidationError(message)\n230 \n231         if domain_part not in self.DOMAIN_WHITELIST:\n232             if not self.DOMAIN_REGEX.match(domain_part):\n233                 try:\n234                     domain_part = domain_part.encode(\"idna\").decode(\"ascii\")\n235                 except UnicodeError:\n236                     pass\n237                 else:\n238                     if self.DOMAIN_REGEX.match(domain_part):\n239                         return value\n240                 raise ValidationError(message)\n241 \n242         return value\n243 \n244 \n245 class Range(Validator):\n246     \"\"\"Validator which succeeds if the value passed to it is within the specified\n247     range. If ``min`` is not specified, or is specified as `None`,\n248     no lower bound exists. If ``max`` is not specified, or is specified as `None`,\n249     no upper bound exists. The inclusivity of the bounds (if they exist) is configurable.\n250     If ``min_inclusive`` is not specified, or is specified as `True`, then\n251     the ``min`` bound is included in the range. If ``max_inclusive`` is not specified,\n252     or is specified as `True`, then the ``max`` bound is included in the range.\n253 \n254     :param min: The minimum value (lower bound). If not provided, minimum\n255         value will not be checked.\n256     :param max: The maximum value (upper bound). If not provided, maximum\n257         value will not be checked.\n258     :param min_inclusive: Whether the `min` bound is included in the range.\n259     :param max_inclusive: Whether the `max` bound is included in the range.\n260     :param error: Error message to raise in case of a validation error.\n261         Can be interpolated with `{input}`, `{min}` and `{max}`.\n262     \"\"\"\n263 \n264     message_min = \"Must be {min_op} {{min}}.\"\n265     message_max = \"Must be {max_op} {{max}}.\"\n266     message_all = \"Must be {min_op} {{min}} and {max_op} {{max}}.\"\n267 \n268     message_gte = \"greater than or equal to\"\n269     message_gt = \"greater than\"\n270     message_lte = \"less than or equal to\"\n271     message_lt = \"less than\"\n272 \n273     def __init__(\n274         self,\n275         min=None,\n276         max=None,\n277         *,\n278         min_inclusive: bool = True,\n279         max_inclusive: bool = True,\n280         error: str | None = None,\n281     ):\n282         self.min = min\n283         self.max = max\n284         self.error = error\n285         self.min_inclusive = min_inclusive\n286         self.max_inclusive = max_inclusive\n287 \n288         # interpolate messages based on bound inclusivity\n289         self.message_min = self.message_min.format(\n290             min_op=self.message_gte if self.min_inclusive else self.message_gt\n291         )\n292         self.message_max = self.message_max.format(\n293             max_op=self.message_lte if self.max_inclusive else self.message_lt\n294         )\n295         self.message_all = self.message_all.format(\n296             min_op=self.message_gte if self.min_inclusive else self.message_gt,\n297             max_op=self.message_lte if self.max_inclusive else self.message_lt,\n298         )\n299 \n300     def _repr_args(self) -> str:\n301         return \"min={!r}, max={!r}, min_inclusive={!r}, max_inclusive={!r}\".format(\n302             self.min, self.max, self.min_inclusive, self.max_inclusive\n303         )\n304 \n305     def _format_error(self, value: _T, message: str) -> str:\n306         return (self.error or message).format(input=value, min=self.min, max=self.max)\n307 \n308     def __call__(self, value: _T) -> _T:\n309         if self.min is not None and (\n310             value < self.min if self.min_inclusive else value <= self.min\n311         ):\n312             message = self.message_min if self.max is None else self.message_all\n313             raise ValidationError(self._format_error(value, message))\n314 \n315         if self.max is not None and (\n316             value > self.max if self.max_inclusive else value >= self.max\n317         ):\n318             message = self.message_max if self.min is None else self.message_all\n319             raise ValidationError(self._format_error(value, message))\n320 \n321         return value\n322 \n323 \n324 class Length(Validator):\n325     \"\"\"Validator which succeeds if the value passed to it has a\n326     length between a minimum and maximum. Uses len(), so it\n327     can work for strings, lists, or anything with length.\n328 \n329     :param min: The minimum length. If not provided, minimum length\n330         will not be checked.\n331     :param max: The maximum length. If not provided, maximum length\n332         will not be checked.\n333     :param equal: The exact length. If provided, maximum and minimum\n334         length will not be checked.\n335     :param error: Error message to raise in case of a validation error.\n336         Can be interpolated with `{input}`, `{min}` and `{max}`.\n337     \"\"\"\n338 \n339     message_min = \"Shorter than minimum length {min}.\"\n340     message_max = \"Longer than maximum length {max}.\"\n341     message_all = \"Length must be between {min} and {max}.\"\n342     message_equal = \"Length must be {equal}.\"\n343 \n344     def __init__(\n345         self,\n346         min: int | None = None,\n347         max: int | None = None,\n348         *,\n349         equal: int | None = None,\n350         error: str | None = None,\n351     ):\n352         if equal is not None and any([min, max]):\n353             raise ValueError(\n354                 \"The `equal` parameter was provided, maximum or \"\n355                 \"minimum parameter must not be provided.\"\n356             )\n357 \n358         self.min = min\n359         self.max = max\n360         self.error = error\n361         self.equal = equal\n362 \n363     def _repr_args(self) -> str:\n364         return f\"min={self.min!r}, max={self.max!r}, equal={self.equal!r}\"\n365 \n366     def _format_error(self, value: typing.Sized, message: str) -> str:\n367         return (self.error or message).format(\n368             input=value, min=self.min, max=self.max, equal=self.equal\n369         )\n370 \n371     def __call__(self, value: typing.Sized) -> typing.Sized:\n372         length = len(value)\n373 \n374         if self.equal is not None:\n375             if length != self.equal:\n376                 raise ValidationError(self._format_error(value, self.message_equal))\n377             return value\n378 \n379         if self.min is not None and length < self.min:\n380             message = self.message_min if self.max is None else self.message_all\n381             raise ValidationError(self._format_error(value, message))\n382 \n383         if self.max is not None and length > self.max:\n384             message = self.message_max if self.min is None else self.message_all\n385             raise ValidationError(self._format_error(value, message))\n386 \n387         return value\n388 \n389 \n390 class Equal(Validator):\n391     \"\"\"Validator which succeeds if the ``value`` passed to it is\n392     equal to ``comparable``.\n393 \n394     :param comparable: The object to compare to.\n395     :param error: Error message to raise in case of a validation error.\n396         Can be interpolated with `{input}` and `{other}`.\n397     \"\"\"\n398 \n399     default_message = \"Must be equal to {other}.\"\n400 \n401     def __init__(self, comparable, *, error: str | None = None):\n402         self.comparable = comparable\n403         self.error = error or self.default_message  # type: str\n404 \n405     def _repr_args(self) -> str:\n406         return f\"comparable={self.comparable!r}\"\n407 \n408     def _format_error(self, value: _T) -> str:\n409         return self.error.format(input=value, other=self.comparable)\n410 \n411     def __call__(self, value: _T) -> _T:\n412         if value != self.comparable:\n413             raise ValidationError(self._format_error(value))\n414         return value\n415 \n416 \n417 class Regexp(Validator):\n418     \"\"\"Validator which succeeds if the ``value`` matches ``regex``.\n419 \n420     .. note::\n421 \n422         Uses `re.match`, which searches for a match at the beginning of a string.\n423 \n424     :param regex: The regular expression string to use. Can also be a compiled\n425         regular expression pattern.\n426     :param flags: The regexp flags to use, for example re.IGNORECASE. Ignored\n427         if ``regex`` is not a string.\n428     :param error: Error message to raise in case of a validation error.\n429         Can be interpolated with `{input}` and `{regex}`.\n430     \"\"\"\n431 \n432     default_message = \"String does not match expected pattern.\"\n433 \n434     def __init__(\n435         self,\n436         regex: str | bytes | typing.Pattern,\n437         flags: int = 0,\n438         *,\n439         error: str | None = None,\n440     ):\n441         self.regex = (\n442             re.compile(regex, flags) if isinstance(regex, (str, bytes)) else regex\n443         )\n444         self.error = error or self.default_message  # type: str\n445 \n446     def _repr_args(self) -> str:\n447         return f\"regex={self.regex!r}\"\n448 \n449     def _format_error(self, value: str | bytes) -> str:\n450         return self.error.format(input=value, regex=self.regex.pattern)\n451 \n452     @typing.overload\n453     def __call__(self, value: str) -> str:\n454         ...\n455 \n456     @typing.overload\n457     def __call__(self, value: bytes) -> bytes:\n458         ...\n459 \n460     def __call__(self, value):\n461         if self.regex.match(value) is None:\n462             raise ValidationError(self._format_error(value))\n463 \n464         return value\n465 \n466 \n467 class Predicate(Validator):\n468     \"\"\"Call the specified ``method`` of the ``value`` object. The\n469     validator succeeds if the invoked method returns an object that\n470     evaluates to True in a Boolean context. Any additional keyword\n471     argument will be passed to the method.\n472 \n473     :param method: The name of the method to invoke.\n474     :param error: Error message to raise in case of a validation error.\n475         Can be interpolated with `{input}` and `{method}`.\n476     :param kwargs: Additional keyword arguments to pass to the method.\n477     \"\"\"\n478 \n479     default_message = \"Invalid input.\"\n480 \n481     def __init__(self, method: str, *, error: str | None = None, **kwargs):\n482         self.method = method\n483         self.error = error or self.default_message  # type: str\n484         self.kwargs = kwargs\n485 \n486     def _repr_args(self) -> str:\n487         return f\"method={self.method!r}, kwargs={self.kwargs!r}\"\n488 \n489     def _format_error(self, value: typing.Any) -> str:\n490         return self.error.format(input=value, method=self.method)\n491 \n492     def __call__(self, value: typing.Any) -> typing.Any:\n493         method = getattr(value, self.method)\n494 \n495         if not method(**self.kwargs):\n496             raise ValidationError(self._format_error(value))\n497 \n498         return value\n499 \n500 \n501 class NoneOf(Validator):\n502     \"\"\"Validator which fails if ``value`` is a member of ``iterable``.\n503 \n504     :param iterable: A sequence of invalid values.\n505     :param error: Error message to raise in case of a validation error. Can be\n506         interpolated using `{input}` and `{values}`.\n507     \"\"\"\n508 \n509     default_message = \"Invalid input.\"\n510 \n511     def __init__(self, iterable: typing.Iterable, *, error: str | None = None):\n512         self.iterable = iterable\n513         self.values_text = \", \".join(str(each) for each in self.iterable)\n514         self.error = error or self.default_message  # type: str\n515 \n516     def _repr_args(self) -> str:\n517         return f\"iterable={self.iterable!r}\"\n518 \n519     def _format_error(self, value) -> str:\n520         return self.error.format(input=value, values=self.values_text)\n521 \n522     def __call__(self, value: typing.Any) -> typing.Any:\n523         try:\n524             if value in self.iterable:\n525                 raise ValidationError(self._format_error(value))\n526         except TypeError:\n527             pass\n528 \n529         return value\n530 \n531 \n532 class OneOf(Validator):\n533     \"\"\"Validator which succeeds if ``value`` is a member of ``choices``.\n534 \n535     :param choices: A sequence of valid values.\n536     :param labels: Optional sequence of labels to pair with the choices.\n537     :param error: Error message to raise in case of a validation error. Can be\n538         interpolated with `{input}`, `{choices}` and `{labels}`.\n539     \"\"\"\n540 \n541     default_message = \"Must be one of: {choices}.\"\n542 \n543     def __init__(\n544         self,\n545         choices: typing.Iterable,\n546         labels: typing.Iterable[str] | None = None,\n547         *,\n548         error: str | None = None,\n549     ):\n550         self.choices = choices\n551         self.choices_text = \", \".join(str(choice) for choice in self.choices)\n552         self.labels = labels if labels is not None else []\n553         self.labels_text = \", \".join(str(label) for label in self.labels)\n554         self.error = error or self.default_message  # type: str\n555 \n556     def _repr_args(self) -> str:\n557         return f\"choices={self.choices!r}, labels={self.labels!r}\"\n558 \n559     def _format_error(self, value) -> str:\n560         return self.error.format(\n561             input=value, choices=self.choices_text, labels=self.labels_text\n562         )\n563 \n564     def __call__(self, value: typing.Any) -> typing.Any:\n565         try:\n566             if value not in self.choices:\n567                 raise ValidationError(self._format_error(value))\n568         except TypeError as error:\n569             raise ValidationError(self._format_error(value)) from error\n570 \n571         return value\n572 \n573     def options(\n574         self,\n575         valuegetter: str | typing.Callable[[typing.Any], typing.Any] = str,\n576     ) -> typing.Iterable[tuple[typing.Any, str]]:\n577         \"\"\"Return a generator over the (value, label) pairs, where value\n578         is a string associated with each choice. This convenience method\n579         is useful to populate, for instance, a form select field.\n580 \n581         :param valuegetter: Can be a callable or a string. In the former case, it must\n582             be a one-argument callable which returns the value of a\n583             choice. In the latter case, the string specifies the name\n584             of an attribute of the choice objects. Defaults to `str()`\n585             or `str()`.\n586         \"\"\"\n587         valuegetter = valuegetter if callable(valuegetter) else attrgetter(valuegetter)\n588         pairs = zip_longest(self.choices, self.labels, fillvalue=\"\")\n589 \n590         return ((valuegetter(choice), label) for choice, label in pairs)\n591 \n592 \n593 class ContainsOnly(OneOf):\n594     \"\"\"Validator which succeeds if ``value`` is a sequence and each element\n595     in the sequence is also in the sequence passed as ``choices``. Empty input\n596     is considered valid.\n597 \n598     :param iterable choices: Same as :class:`OneOf`.\n599     :param iterable labels: Same as :class:`OneOf`.\n600     :param str error: Same as :class:`OneOf`.\n601 \n602     .. versionchanged:: 3.0.0b2\n603         Duplicate values are considered valid.\n604     .. versionchanged:: 3.0.0b2\n605         Empty input is considered valid. Use `validate.Length(min=1) <marshmallow.validate.Length>`\n606         to validate against empty inputs.\n607     \"\"\"\n608 \n609     default_message = \"One or more of the choices you made was not in: {choices}.\"\n610 \n611     def _format_error(self, value) -> str:\n612         value_text = \", \".join(str(val) for val in value)\n613         return super()._format_error(value_text)\n614 \n615     def __call__(self, value: typing.Sequence[_T]) -> typing.Sequence[_T]:\n616         # We can't use set.issubset because does not handle unhashable types\n617         for val in value:\n618             if val not in self.choices:\n619                 raise ValidationError(self._format_error(value))\n620         return value\n621 \n622 \n623 class ContainsNoneOf(NoneOf):\n624     \"\"\"Validator which fails if ``value`` is a sequence and any element\n625     in the sequence is a member of the sequence passed as ``iterable``. Empty input\n626     is considered valid.\n627 \n628     :param iterable iterable: Same as :class:`NoneOf`.\n629     :param str error: Same as :class:`NoneOf`.\n630 \n631     .. versionadded:: 3.6.0\n632     \"\"\"\n633 \n634     default_message = \"One or more of the choices you made was in: {values}.\"\n635 \n636     def _format_error(self, value) -> str:\n637         value_text = \", \".join(str(val) for val in value)\n638         return super()._format_error(value_text)\n639 \n640     def __call__(self, value: typing.Sequence[_T]) -> typing.Sequence[_T]:\n641         for val in value:\n642             if val in self.iterable:\n643                 raise ValidationError(self._format_error(value))\n644         return value\n645 \n[end of src/marshmallow/validate.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/src/marshmallow/fields.py\n+++ b/src/marshmallow/fields.py\n@@ -1697,6 +1697,10 @@\n     \"\"\"\n \n     #: Default error messages.\n+    default_error_messages = {\n+        \"invalid\": \"Not a valid URL.\",\n+        \"relative_only\": \"Only relative URLs are allowed.\"\n+    }\n     def __init__(\n         self,\n         *,\n@@ -1720,6 +1724\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n \n+        if relative and require_tld:\n+            raise ValueError(\"Cannot require TLD when allowing relative URLs.\")\n         self.relative = relative\n         self.require_tld = require_tld\n         # Insert validation into self.validators so that multiple errors can be stored.\n@@ -1725,7 +1729\n         validator = validate.URL(\n             relative=self.relative,\n             schemes=schemes,\n             require_tld=self.require_tld,\n-            error=self.error_messages[\"invalid\"],\n+            error=self.error_messages[\"relative_only\"] if self.relative else self.error_messages[\"invalid\"],\n         )\n         self.validators.insert(0, validator)\n--- a/src/marshmallow/validate.py\n+++ b/src/marshmallow/validate.py\n@@ -181\n     \"\"\"\n \n     class RegexMemoizer:\n@@ -182\n     _regex = RegexMemoizer()\n \n     default_message = \"Not a valid URL.\"\n+    relative_only_message = \"Only relative URLs are allowed.\"\n \n     def __init__(\n         self,\n@@ -161\n         error: str | None = None,\n+        relative_only: bool = False,\n     ):\n         self.relative = relative\n         self.error = error or self.default_message  # type: str\n         self.schemes = schemes or self.default_schemes\n         self.require_tld = require_tld\n+        self.relative_only = relative_only\n \n     def _repr_args(self) -> str:\n         return f\"relative={self.relative!r}\"\n@@ -181\n         if \"://\" in value:\n             scheme = value.split(\"://\")[0].lower()\n             if scheme not in self.schemes:\n                 raise ValidationError(message)\n \n+        if self.relative_only and not self.relative:\n+            raise ValidationError(self.relative_only_message)\n \n         regex = self._regex(self.relative, self.require_tld)\n \n         if not regex.search(value):\n             raise ValidationError(message)\n \n         return value\n```", "model_patch": "--- a/src/marshmallow/fields.py\n+++ b/src/marshmallow/fields.py\n@@ -1697,6 +1697,10 @@\n     \"\"\"\n \n     #: Default error messages.\n+    default_error_messages = {\n+        \"invalid\": \"Not a valid URL.\",\n+        \"relative_only\": \"Only relative URLs are allowed.\"\n+    }\n     def __init__(\n         self,\n         *,\n@@ -1720,6 +1724\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n \n+        if relative and require_tld:\n+            raise ValueError(\"Cannot require TLD when allowing relative URLs.\")\n         self.relative = relative\n         self.require_tld = require_tld\n         # Insert validation into self.validators so that multiple errors can be stored.\n@@ -1725,7 +1729\n         validator = validate.URL(\n             relative=self.relative,\n             schemes=schemes,\n             require_tld=self.require_tld,\n-            error=self.error_messages[\"invalid\"],\n+            error=self.error_messages[\"relative_only\"] if self.relative else self.error_messages[\"invalid\"],\n         )\n         self.validators.insert(0, validator)\n--- a/src/marshmallow/validate.py\n+++ b/src/marshmallow/validate.py\n@@ -181\n     \"\"\"\n \n     class RegexMemoizer:\n@@ -182\n     _regex = RegexMemoizer()\n \n     default_message = \"Not a valid URL.\"\n+    relative_only_message = \"Only relative URLs are allowed.\"\n \n     def __init__(\n         self,\n@@ -161\n         error: str | None = None,\n+        relative_only: bool = False,\n     ):\n         self.relative = relative\n         self.error = error or self.default_message  # type: str\n         self.schemes = schemes or self.default_schemes\n         self.require_tld = require_tld\n+        self.relative_only = relative_only\n \n     def _repr_args(self) -> str:\n         return f\"relative={self.relative!r}\"\n@@ -181\n         if \"://\" in value:\n             scheme = value.split(\"://\")[0].lower()\n             if scheme not in self.schemes:\n                 raise ValidationError(message)\n \n+        if self.relative_only and not self.relative:\n+            raise ValidationError(self.relative_only_message)\n \n         regex = self._regex(self.relative, self.require_tld)\n \n         if not regex.search(value):\n             raise ValidationError(message)\n \n         return value\n"}
{"instance_id": "pvlib__pvlib-python-1738", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n`pvlib.soiling.hsu` takes `tilt` instead of `surface_tilt`\n`pvlib.soiling.hsu` takes a `tilt` parameter representing the same thing we normally call `surface_tilt`:\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L13-L14\r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/7a2ec9b4765124463bf0ddd0a49dcfedc4cbcad7/pvlib/soiling.py#L33-L34\r\n\r\nI don't see any good reason for this naming inconsistency (I suspect `tilt` just got copied from the matlab implementation) and suggest we rename the parameter to `surface_tilt` with a deprecation.\r\n\r\nAlso, the docstring parameter type description says it must be `float`, but the model's reference explicitly says time series tilt is allowed: \r\n\r\n> The angle is variable for tracking systems and is taken as the average angle over the time step.\r\n\r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/main/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml?query=branch%3Amain\">\n32       <img src=\"https://github.com/pvlib/pvlib-python/actions/workflows/pytest.yml/badge.svg?branch=main\" alt=\"GitHub Actions Testing Status\" />\n33     </a>\n34     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n35     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/main/graph/badge.svg\" alt=\"codecov coverage\" />\n36     </a>\n37   </td>\n38 </tr>\n39 <tr>\n40   <td>Benchmarks</td>\n41   <td>\n42     <a href=\"https://pvlib.github.io/pvlib-benchmarks/\">\n43     <img src=\"https://img.shields.io/badge/benchmarks-asv-lightgrey\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Publications</td>\n49   <td>\n50     <a href=\"https://doi.org/10.5281/zenodo.593284\">\n51     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.593284.svg\" alt=\"zenodo reference\">\n52     </a>\n53     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n54     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Downloads</td>\n60   <td>\n61     <a href=\"https://pypi.org/project/pvlib/\">\n62     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n63     </a>\n64     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n65     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n66     </a>\n67   </td>\n68 </tr>\n69 </table>\n70 \n71 \n72 pvlib python is a community supported tool that provides a set of\n73 functions and classes for simulating the performance of photovoltaic\n74 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n75 toolbox developed at Sandia National Laboratories and it implements many\n76 of the models and methods developed at the Labs. More information on\n77 Sandia Labs PV performance modeling programs can be found at\n78 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n79 but operate independently of it.\n80 \n81 \n82 Documentation\n83 =============\n84 \n85 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/),\n86 including an [FAQ](http://pvlib-python.readthedocs.io/en/stable/user_guide/faq.html) page.\n87 \n88 Installation\n89 ============\n90 \n91 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n92 Please see the [Installation page](https://pvlib-python.readthedocs.io/en/stable/user_guide/installation.html) of the documentation for complete instructions.\n93 \n94 \n95 Contributing\n96 ============\n97 \n98 We need your help to make pvlib-python a great tool!\n99 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n100 The long-term success of pvlib-python requires substantial community support.\n101 \n102 \n103 Citing\n104 ======\n105 \n106 If you use pvlib-python in a published work, please cite:\n107 \n108   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n109   \"pvlib python: a python package for modeling solar energy systems.\"\n110   Journal of Open Source Software, 3(29), 884, (2018).\n111   https://doi.org/10.21105/joss.00884\n112 \n113 Please also cite the DOI corresponding to the specific version of\n114 pvlib-python that you used. pvlib-python DOIs are listed at\n115 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n116 \n117 If you use pvlib-python in a commercial or publicly-available application, please\n118 consider displaying one of the \"powered by pvlib\" logos:\n119 \n120 <img src=\"docs/sphinx/source/_images/pvlib_powered_logo_vert.png\" width=\"300\"><img src=\"docs/sphinx/source/_images/pvlib_powered_logo_horiz.png\" width=\"300\">\n121 \n122 Getting support\n123 ===============\n124 \n125 pvlib usage questions can be asked on\n126 [Stack Overflow](http://stackoverflow.com) and tagged with\n127 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n128 \n129 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n130 is used for discussing various topics of interest to the pvlib-python\n131 community. We also make new version announcements on the google group.\n132 \n133 If you suspect that you may have discovered a bug or if you'd like to\n134 change something about pvlib, then please make an issue on our\n135 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n136 \n137 \n138 \n139 License\n140 =======\n141 \n142 BSD 3-clause.\n143 \n144 \n145 NumFOCUS\n146 ========\n147 \n148 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n149 \n150 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n151 \n[end of README.md]\n[start of pvlib/soiling.py]\n1 \"\"\"\n2 This module contains functions for soiling models\n3 \"\"\"\n4 \n5 import datetime\n6 import numpy as np\n7 import pandas as pd\n8 from scipy.special import erf\n9 \n10 from pvlib.tools import cosd\n11 \n12 \n13 def hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n14         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n15     \"\"\"\n16     Calculates soiling ratio given particulate and rain data using the\n17     Fixed Velocity model from Humboldt State University (HSU).\n18 \n19     The HSU soiling model [1]_ returns the soiling ratio, a value between zero\n20     and one which is equivalent to (1 - transmission loss). Therefore a soiling\n21     ratio of 1.0 is equivalent to zero transmission loss.\n22 \n23     Parameters\n24     ----------\n25 \n26     rainfall : Series\n27         Rain accumulated in each time period. [mm]\n28 \n29     cleaning_threshold : float\n30         Amount of rain in an accumulation period needed to clean the PV\n31         modules. [mm]\n32 \n33     tilt : float\n34         Tilt of the PV panels from horizontal. [degree]\n35 \n36     pm2_5 : numeric\n37         Concentration of airborne particulate matter (PM) with\n38         aerodynamic diameter less than 2.5 microns. [g/m^3]\n39 \n40     pm10 : numeric\n41         Concentration of airborne particulate matter (PM) with\n42         aerodynamicdiameter less than 10 microns. [g/m^3]\n43 \n44     depo_veloc : dict, default {'2_5': 0.0009, '10': 0.004}\n45         Deposition or settling velocity of particulates. [m/s]\n46 \n47     rain_accum_period : Timedelta, default 1 hour\n48         Period for accumulating rainfall to check against `cleaning_threshold`\n49         It is recommended that `rain_accum_period` be between 1 hour and\n50         24 hours.\n51 \n52     Returns\n53     -------\n54     soiling_ratio : Series\n55         Values between 0 and 1. Equal to 1 - transmission loss.\n56 \n57     References\n58     -----------\n59     .. [1] M. Coello and L. Boyle, \"Simple Model For Predicting Time Series\n60        Soiling of Photovoltaic Panels,\" in IEEE Journal of Photovoltaics.\n61        doi: 10.1109/JPHOTOV.2019.2919628\n62     .. [2] Atmospheric Chemistry and Physics: From Air Pollution to Climate\n63        Change. J. Seinfeld and S. Pandis. Wiley and Sons 2001.\n64 \n65     \"\"\"\n66     # never use mutable input arguments\n67     if depo_veloc is None:\n68         depo_veloc = {'2_5': 0.0009, '10': 0.004}\n69 \n70     # accumulate rainfall into periods for comparison with threshold\n71     accum_rain = rainfall.rolling(rain_accum_period, closed='right').sum()\n72     # cleaning is True for intervals with rainfall greater than threshold\n73     cleaning_times = accum_rain.index[accum_rain >= cleaning_threshold]\n74 \n75     # determine the time intervals in seconds (dt_sec)\n76     dt = rainfall.index\n77     # subtract shifted values from original and convert to seconds\n78     dt_diff = (dt[1:] - dt[:-1]).total_seconds()\n79     # ensure same number of elements in the array, assuming that the interval\n80     # prior to the first value is equal in length to the first interval\n81     dt_sec = np.append(dt_diff[0], dt_diff).astype('float64')\n82 \n83     horiz_mass_rate = (\n84         pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n85         * depo_veloc['10']) * dt_sec\n86     tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n87 \n88     # tms -> tilt_mass_rate\n89     tms_cumsum = np.cumsum(tilted_mass_rate * np.ones(rainfall.shape))\n90 \n91     mass_no_cleaning = pd.Series(index=rainfall.index, data=tms_cumsum)\n92     # specify dtype so pandas doesn't assume object\n93     mass_removed = pd.Series(index=rainfall.index, dtype='float64')\n94     mass_removed[0] = 0.\n95     mass_removed[cleaning_times] = mass_no_cleaning[cleaning_times]\n96     accum_mass = mass_no_cleaning - mass_removed.ffill()\n97 \n98     soiling_ratio = 1 - 0.3437 * erf(0.17 * accum_mass**0.8473)\n99 \n100     return soiling_ratio\n101 \n102 \n103 def kimber(rainfall, cleaning_threshold=6, soiling_loss_rate=0.0015,\n104            grace_period=14, max_soiling=0.3, manual_wash_dates=None,\n105            initial_soiling=0, rain_accum_period=24):\n106     \"\"\"\n107     Calculates fraction of energy lost due to soiling given rainfall data and\n108     daily loss rate using the Kimber model.\n109 \n110     Kimber soiling model [1]_ assumes soiling builds up at a daily rate unless\n111     the daily rainfall is greater than a threshold. The model also assumes that\n112     if daily rainfall has exceeded the threshold within a grace period, then\n113     the ground is too damp to cause soiling build-up. The model also assumes\n114     there is a maximum soiling build-up. Scheduled manual washes and rain\n115     events are assumed to reset soiling to zero.\n116 \n117     Parameters\n118     ----------\n119     rainfall: pandas.Series\n120         Accumulated rainfall at the end of each time period. [mm]\n121     cleaning_threshold: float, default 6\n122         Amount of daily rainfall required to clean the panels. [mm]\n123     soiling_loss_rate: float, default 0.0015\n124         Fraction of energy lost due to one day of soiling. [unitless]\n125     grace_period : int, default 14\n126         Number of days after a rainfall event when it's assumed the ground is\n127         damp, and so it's assumed there is no soiling. [days]\n128     max_soiling : float, default 0.3\n129         Maximum fraction of energy lost due to soiling. Soiling will build up\n130         until this value. [unitless]\n131     manual_wash_dates : sequence or None, default None\n132         List or tuple of dates as Python ``datetime.date`` when the panels were\n133         washed manually. Note there is no grace period after a manual wash, so\n134         soiling begins to build up immediately.\n135     initial_soiling : float, default 0\n136         Initial fraction of energy lost due to soiling at time zero in the\n137         `rainfall` series input. [unitless]\n138     rain_accum_period : int, default 24\n139         Period for accumulating rainfall to check against `cleaning_threshold`.\n140         The Kimber model defines this period as one day. [hours]\n141 \n142     Returns\n143     -------\n144     pandas.Series\n145         fraction of energy lost due to soiling, has same intervals as input\n146 \n147     Notes\n148     -----\n149     The soiling loss rate depends on both the geographical region and the\n150     soiling environment type. Rates measured by Kimber [1]_ are summarized in\n151     the following table:\n152 \n153     ===================  =======  =========  ======================\n154     Region/Environment   Rural    Suburban   Urban/Highway/Airport\n155     ===================  =======  =========  ======================\n156     Central Valley       0.0011   0.0019     0.0020\n157     Northern CA          0.0011   0.0010     0.0016\n158     Southern CA          0        0.0016     0.0019\n159     Desert               0.0030   0.0030     0.0030\n160     ===================  =======  =========  ======================\n161 \n162     Rainfall thresholds and grace periods may also vary by region. Please\n163     consult [1]_ for more information.\n164 \n165     References\n166     ----------\n167     .. [1] \"The Effect of Soiling on Large Grid-Connected Photovoltaic Systems\n168        in California and the Southwest Region of the United States,\" Adrianne\n169        Kimber, et al., IEEE 4th World Conference on Photovoltaic Energy\n170        Conference, 2006, :doi:`10.1109/WCPEC.2006.279690`\n171     \"\"\"\n172     # convert rain_accum_period to timedelta\n173     rain_accum_period = datetime.timedelta(hours=rain_accum_period)\n174 \n175     # convert grace_period to timedelta\n176     grace_period = datetime.timedelta(days=grace_period)\n177 \n178     # get indices as numpy datetime64, calculate timestep as numpy timedelta64,\n179     # and convert timestep to fraction of days\n180     rain_index_vals = rainfall.index.values\n181     timestep_interval = (rain_index_vals[1] - rain_index_vals[0])\n182     day_fraction = timestep_interval / np.timedelta64(24, 'h')\n183 \n184     # accumulate rainfall\n185     accumulated_rainfall = rainfall.rolling(\n186         rain_accum_period, closed='right').sum()\n187 \n188     # soiling rate\n189     soiling = np.ones_like(rainfall.values) * soiling_loss_rate * day_fraction\n190     soiling[0] = initial_soiling\n191     soiling = np.cumsum(soiling)\n192     soiling = pd.Series(soiling, index=rainfall.index, name='soiling')\n193 \n194     # rainfall events that clean the panels\n195     rain_events = accumulated_rainfall > cleaning_threshold\n196 \n197     # grace periods windows during which ground is assumed damp, so no soiling\n198     grace_windows = rain_events.rolling(grace_period, closed='right').sum() > 0\n199 \n200     # clean panels by subtracting soiling for indices in grace period windows\n201     cleaning = pd.Series(float('NaN'), index=rainfall.index)\n202     cleaning.iloc[0] = 0.0\n203     cleaning[grace_windows] = soiling[grace_windows]\n204 \n205     # manual wash dates\n206     if manual_wash_dates is not None:\n207         rain_tz = rainfall.index.tz\n208         # convert manual wash dates to datetime index in the timezone of rain\n209         manual_wash_dates = pd.DatetimeIndex(manual_wash_dates, tz=rain_tz)\n210         cleaning[manual_wash_dates] = soiling[manual_wash_dates]\n211 \n212     # remove soiling by foward filling cleaning where NaN\n213     soiling -= cleaning.ffill()\n214 \n215     # check if soiling has reached the maximum\n216     return soiling.where(soiling < max_soiling, max_soiling)\n217 \n[end of pvlib/soiling.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/soiling.py\n+++ b/pvlib/soiling.py\n@@ -13,14 +13,14 @@\n def hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n-         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n+         depo_veloc=None, rain_accum_period=pd.Timedelta('1h'), surface_tilt=None):\n     \"\"\"\n      Calculates soiling ratio given particulate and rain data using the\n      Fixed Velocity model from Humboldt State University (HSU).\n \n      The HSU soiling model [1]_ returns the soiling ratio, a value between zero\n      and one which is equivalent to (1 - transmission loss). Therefore a soiling\n      ratio of 1.0 is equivalent to zero transmission loss.\n \n      Parameters\n      ----------\n \n      rainfall : Series\n          Rain accumulated in each time period. [mm]\n \n      cleaning_threshold : float\n          Amount of rain in an accumulation period needed to clean the PV\n          modules. [mm]\n \n-     tilt : float\n+     tilt : float, optional\n+     surface_tilt : float, optional\n          Tilt of the PV panels from horizontal. [degree]\n \n      pm2_5 : numeric\n          Concentration of airborne particulate matter (PM) with\n          aerodynamic diameter less than 2.5 microns. [g/m^3]\n \n      pm10 : numeric\n          Concentration of airborne particulate matter (PM) with\n          aerodynamicdiameter less than 10 microns. [g/m^3]\n \n      depo_veloc : dict, default {'2_5': 0.0009, '10': 0.004}\n          Deposition or settling velocity of particulates. [m/s]\n \n      rain_accum_period : Timedelta, default 1 hour\n          Period for accumulating rainfall to check against `cleaning_threshold`\n          It is recommended that `rain_accum_period` be between 1 hour and\n          24 hours.\n \n      Returns\n      -------\n      soiling_ratio : Series\n          Values between 0 and 1. Equal to 1 - transmission loss.\n \n      References\n      -----------\n      .. [1] M. Coello and L. Boyle, \"Simple Model For Predicting Time Series\n         Soiling of Photovoltaic Panels,\" in IEEE Journal of Photovoltaics.\n         doi: 10.1109/JPHOTOV.2019.2919628\n      .. [2] Atmospheric Chemistry and Physics: From Air Pollution to Climate\n         Change. J. Seinfeld and S. Pandis. Wiley and Sons 2001.\n \n     \"\"\"\n \n+    if surface_tilt is not None:\n+        import warnings\n+        warnings.warn(\"The 'surface_tilt' parameter is deprecated, use 'tilt' instead.\", DeprecationWarning)\n \n     # never use mutable input arguments\n     if depo_veloc is None:\n         depo_veloc = {'2_5': 0.0009, '10': 0.004}\n \n     # accumulate rainfall into periods for comparison with threshold\n     accum_rain = rainfall.rolling(rain_accum_period, closed='right').sum()\n     # cleaning is True for intervals with rainfall greater than threshold\n     cleaning_times = accum_rain.index[accum_rain >= cleaning_threshold]\n \n     # determine the time intervals in seconds (dt_sec)\n     dt = rainfall.index\n     # subtract shifted values from original and convert to seconds\n     dt_diff = (dt[1:] - dt[:-1]).total_seconds()\n     # ensure same number of elements in the array, assuming that the interval\n     # prior to the first value is equal in length to the first interval\n     dt_sec = np.append(dt_diff[0], dt_diff).astype('float64')\n \n     horiz_mass_rate = (\n         pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n         * depo_veloc['10']) * dt_sec\n-    tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n+    tilted_mass_rate = horiz_mass_rate * cosd(tilt if surface_tilt is None else surface_tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate\n     tms_cumsum = np.cumsum(tilted_mass_rate * np.ones(rainfall.shape))\n \n     mass_no_cleaning = pd.Series(index=rainfall.index, data=tms_cumsum)\n     # specify dtype so pandas doesn't assume object\n     mass_removed = pd.Series(index=rainfall.index, dtype='float64')\n     mass_removed[0] = 0.\n     mass_removed[cleaning_times] = mass_no_cleaning[cleaning_times]\n     accum_mass = mass_no_cleaning - mass_removed.ffill()\n \n     soiling_ratio = 1 - 0.3437 * erf(0.17 * accum_mass**0.8473)\n \n     return soiling_ratio\n```", "model_patch": "--- a/pvlib/soiling.py\n+++ b/pvlib/soiling.py\n@@ -13,14 +13,14 @@\n def hsu(rainfall, cleaning_threshold, tilt, pm2_5, pm10,\n-         depo_veloc=None, rain_accum_period=pd.Timedelta('1h')):\n+         depo_veloc=None, rain_accum_period=pd.Timedelta('1h'), surface_tilt=None):\n     \"\"\"\n      Calculates soiling ratio given particulate and rain data using the\n      Fixed Velocity model from Humboldt State University (HSU).\n \n      The HSU soiling model [1]_ returns the soiling ratio, a value between zero\n      and one which is equivalent to (1 - transmission loss). Therefore a soiling\n      ratio of 1.0 is equivalent to zero transmission loss.\n \n      Parameters\n      ----------\n \n      rainfall : Series\n          Rain accumulated in each time period. [mm]\n \n      cleaning_threshold : float\n          Amount of rain in an accumulation period needed to clean the PV\n          modules. [mm]\n \n-     tilt : float\n+     tilt : float, optional\n+     surface_tilt : float, optional\n          Tilt of the PV panels from horizontal. [degree]\n \n      pm2_5 : numeric\n          Concentration of airborne particulate matter (PM) with\n          aerodynamic diameter less than 2.5 microns. [g/m^3]\n \n      pm10 : numeric\n          Concentration of airborne particulate matter (PM) with\n          aerodynamicdiameter less than 10 microns. [g/m^3]\n \n      depo_veloc : dict, default {'2_5': 0.0009, '10': 0.004}\n          Deposition or settling velocity of particulates. [m/s]\n \n      rain_accum_period : Timedelta, default 1 hour\n          Period for accumulating rainfall to check against `cleaning_threshold`\n          It is recommended that `rain_accum_period` be between 1 hour and\n          24 hours.\n \n      Returns\n      -------\n      soiling_ratio : Series\n          Values between 0 and 1. Equal to 1 - transmission loss.\n \n      References\n      -----------\n      .. [1] M. Coello and L. Boyle, \"Simple Model For Predicting Time Series\n         Soiling of Photovoltaic Panels,\" in IEEE Journal of Photovoltaics.\n         doi: 10.1109/JPHOTOV.2019.2919628\n      .. [2] Atmospheric Chemistry and Physics: From Air Pollution to Climate\n         Change. J. Seinfeld and S. Pandis. Wiley and Sons 2001.\n \n     \"\"\"\n \n+    if surface_tilt is not None:\n+        import warnings\n+        warnings.warn(\"The 'surface_tilt' parameter is deprecated, use 'tilt' instead.\", DeprecationWarning)\n \n     # never use mutable input arguments\n     if depo_veloc is None:\n         depo_veloc = {'2_5': 0.0009, '10': 0.004}\n \n     # accumulate rainfall into periods for comparison with threshold\n     accum_rain = rainfall.rolling(rain_accum_period, closed='right').sum()\n     # cleaning is True for intervals with rainfall greater than threshold\n     cleaning_times = accum_rain.index[accum_rain >= cleaning_threshold]\n \n     # determine the time intervals in seconds (dt_sec)\n     dt = rainfall.index\n     # subtract shifted values from original and convert to seconds\n     dt_diff = (dt[1:] - dt[:-1]).total_seconds()\n     # ensure same number of elements in the array, assuming that the interval\n     # prior to the first value is equal in length to the first interval\n     dt_sec = np.append(dt_diff[0], dt_diff).astype('float64')\n \n     horiz_mass_rate = (\n         pm2_5 * depo_veloc['2_5'] + np.maximum(pm10 - pm2_5, 0.)\n         * depo_veloc['10']) * dt_sec\n-    tilted_mass_rate = horiz_mass_rate * cosd(tilt)  # assuming no rain\n+    tilted_mass_rate = horiz_mass_rate * cosd(tilt if surface_tilt is None else surface_tilt)  # assuming no rain\n \n     # tms -> tilt_mass_rate\n     tms_cumsum = np.cumsum(tilted_mass_rate * np.ones(rainfall.shape))\n \n     mass_no_cleaning = pd.Series(index=rainfall.index, data=tms_cumsum)\n     # specify dtype so pandas doesn't assume object\n     mass_removed = pd.Series(index=rainfall.index, dtype='float64')\n     mass_removed[0] = 0.\n     mass_removed[cleaning_times] = mass_no_cleaning[cleaning_times]\n     accum_mass = mass_no_cleaning - mass_removed.ffill()\n \n     soiling_ratio = 1 - 0.3437 * erf(0.17 * accum_mass**0.8473)\n \n     return soiling_ratio\n"}
{"instance_id": "pvlib__pvlib-python-1213", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nBug within scaling.py wavelet calculation methodology\n**Describe the bug**\r\nMathematical error within the wavelet computation for the scaling.py WVM implementation. Error arises from the methodology, as opposed to just a software bug. \r\n\r\n**To Reproduce**\r\nSteps to reproduce the behavior:\r\n```\r\nimport numpy as np\r\nfrom pvlib import scaling\r\ncs = np.random.rand(2**14)\r\nw, ts = scaling._compute_wavelet(cs,1)\r\nprint(np.all( (sum(w)-cs) < 1e-8 ))  # Returns False, expect True\r\n```\r\n\r\n**Expected behavior**\r\nFor a discrete wavelet transform (DWT) the sum of all wavelet modes should equate to the original data. \r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.7.2\r\n - ``pandas.__version__``: 1.2.3\r\n - python: 3.8.8\r\n\r\n**Additional context**\r\nThis bug is also present in the [PV_LIB](https://pvpmc.sandia.gov/applications/wavelet-variability-model/) Matlab version that was used as the basis for this code (I did reach out to them using the PVLIB MATLAB email form, but don't know who actually wrote that code). Essentially, the existing code throws away the highest level of Detail Coefficient in the transform and keeps an extra level of Approximation coefficient. The impact on the calculation is small, but leads to an incorrect DWT and reconstruction. I have a fix that makes the code pass the theoretical test about the DWT proposed under 'To Reproduce' but there may be some question as to whether this should be corrected or left alone to match the MATLAB code it was based on. \r\n\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n32       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n33     </a>\n34   </td>\n35 </tr>\n36 <tr>\n37   <td>Code Quality</td>\n38  \u00a0<td>\n39     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n40     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n41     </a>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n43     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Coverage</td>\n49  \u00a0<td>\n50     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n51     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n52     </a>\n53     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n54     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Publications</td>\n60   <td>\n61     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n62     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n63     </a>\n64     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n65     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n66     </a>\n67   </td>\n68 </tr>\n69 <tr>\n70   <td>Downloads</td>\n71   <td>\n72     <a href=\"https://pypi.org/project/pvlib/\">\n73     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n74     </a>\n75     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n76     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n77     </a>\n78   </td>\n79 </tr>\n80 </table>\n81 \n82 \n83 pvlib python is a community supported tool that provides a set of\n84 functions and classes for simulating the performance of photovoltaic\n85 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n86 toolbox developed at Sandia National Laboratories and it implements many\n87 of the models and methods developed at the Labs. More information on\n88 Sandia Labs PV performance modeling programs can be found at\n89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n90 but operate independently of it.\n91 \n92 \n93 Documentation\n94 =============\n95 \n96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n97 \n98 \n99 Installation\n100 ============\n101 \n102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n104 \n105 \n106 Contributing\n107 ============\n108 \n109 We need your help to make pvlib-python a great tool!\n110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n111 The long-term success of pvlib-python requires substantial community support.\n112 \n113 \n114 License\n115 =======\n116 \n117 BSD 3-clause\n118 \n119 \n120 Getting support\n121 ===============\n122 \n123 pvlib usage questions can be asked on\n124 [Stack Overflow](http://stackoverflow.com) and tagged with\n125 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n126 \n127 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n128 is used for discussing various topics of interest to the pvlib-python\n129 community. We also make new version announcements on the google group.\n130 \n131 If you suspect that you may have discovered a bug or if you'd like to\n132 change something about pvlib, then please make an issue on our\n133 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n134 \n135 \n136 Citing\n137 ======\n138 \n139 If you use pvlib-python in a published work, please cite:\n140 \n141   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n142   \"pvlib python: a python package for modeling solar energy systems.\"\n143   Journal of Open Source Software, 3(29), 884, (2018).\n144   https://doi.org/10.21105/joss.00884\n145 \n146 Please also cite the DOI corresponding to the specific version of\n147 pvlib-python that you used. pvlib-python DOIs are listed at\n148 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n149 \n150 NumFOCUS\n151 ========\n152 \n153 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n154 \n155 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n156 \n[end of README.md]\n[start of pvlib/scaling.py]\n1 \"\"\"\n2 The ``scaling`` module contains functions for manipulating irradiance\n3 or other variables to account for temporal or spatial characteristics.\n4 \"\"\"\n5 \n6 import numpy as np\n7 import pandas as pd\n8 \n9 import scipy.optimize\n10 from scipy.spatial.distance import pdist\n11 \n12 \n13 def wvm(clearsky_index, positions, cloud_speed, dt=None):\n14     \"\"\"\n15     Compute spatial aggregation time series smoothing on clear sky index based\n16     on the Wavelet Variability model of Lave et al [1-2]. Implementation is\n17     basically a port of the Matlab version of the code [3].\n18 \n19     Parameters\n20     ----------\n21     clearsky_index : numeric or pandas.Series\n22         Clear Sky Index time series that will be smoothed.\n23 \n24     positions : numeric\n25         Array of coordinate distances as (x,y) pairs representing the\n26         easting, northing of the site positions in meters [m]. Distributed\n27         plants could be simulated by gridded points throughout the plant\n28         footprint.\n29 \n30     cloud_speed : numeric\n31         Speed of cloud movement in meters per second [m/s].\n32 \n33     dt : float, default None\n34         The time series time delta. By default, is inferred from the\n35         clearsky_index. Must be specified for a time series that doesn't\n36         include an index. Units of seconds [s].\n37 \n38     Returns\n39     -------\n40     smoothed : numeric or pandas.Series\n41         The Clear Sky Index time series smoothed for the described plant.\n42 \n43     wavelet: numeric\n44         The individual wavelets for the time series before smoothing.\n45 \n46     tmscales: numeric\n47         The timescales associated with the wavelets in seconds [s].\n48 \n49     References\n50     ----------\n51     [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n52     Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n53     Energy, vol. 4, no. 2, pp. 501-509, 2013.\n54 \n55     [2] M. Lave and J. Kleissl. Cloud speed impact on solar variability\n56     scaling - Application to the wavelet variability model. Solar Energy,\n57     vol. 91, pp. 11-21, 2013.\n58 \n59     [3] Wavelet Variability Model - Matlab Code:\n60     https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n61     \"\"\"\n62 \n63     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n64 \n65     pos = np.array(positions)\n66     dist = pdist(pos, 'euclidean')\n67     wavelet, tmscales = _compute_wavelet(clearsky_index, dt)\n68 \n69     # Find effective length of position vector, 'dist' is full pairwise\n70     n_pairs = len(dist)\n71 \n72     def fn(x):\n73         return np.abs((x ** 2 - x) / 2 - n_pairs)\n74     n_dist = np.round(scipy.optimize.fmin(fn, np.sqrt(n_pairs), disp=False))\n75 \n76     # Compute VR\n77     A = cloud_speed / 2  # Resultant fit for A from [2]\n78     vr = np.zeros(tmscales.shape)\n79     for i, tmscale in enumerate(tmscales):\n80         rho = np.exp(-1 / A * dist / tmscale)  # Eq 5 from [1]\n81 \n82         # 2*rho is because rho_ij = rho_ji. +n_dist accounts for sum(rho_ii=1)\n83         denominator = 2 * np.sum(rho) + n_dist\n84         vr[i] = n_dist ** 2 / denominator  # Eq 6 of [1]\n85 \n86     # Scale each wavelet by VR (Eq 7 in [1])\n87     wavelet_smooth = np.zeros_like(wavelet)\n88     for i in np.arange(len(tmscales)):\n89         if i < len(tmscales) - 1:  # Treat the lowest freq differently\n90             wavelet_smooth[i, :] = wavelet[i, :] / np.sqrt(vr[i])\n91         else:\n92             wavelet_smooth[i, :] = wavelet[i, :]\n93 \n94     outsignal = np.sum(wavelet_smooth, 0)\n95 \n96     try:  # See if there's an index already, if so, return as a pandas Series\n97         smoothed = pd.Series(outsignal, index=clearsky_index.index)\n98     except AttributeError:\n99         smoothed = outsignal  # just output the numpy signal\n100 \n101     return smoothed, wavelet, tmscales\n102 \n103 \n104 def latlon_to_xy(coordinates):\n105     \"\"\"\n106     Convert latitude and longitude in degrees to a coordinate system measured\n107     in meters from zero deg latitude, zero deg longitude.\n108 \n109     This is a convenience method to support inputs to wvm. Note that the\n110     methodology used is only suitable for short distances. For conversions of\n111     longer distances, users should consider use of Universal Transverse\n112     Mercator (UTM) or other suitable cartographic projection. Consider\n113     packages built for cartographic projection such as pyproj (e.g.\n114     pyproj.transform()) [2].\n115 \n116     Parameters\n117     ----------\n118 \n119     coordinates : numeric\n120         Array or list of (latitude, longitude) coordinate pairs. Use decimal\n121         degrees notation.\n122 \n123     Returns\n124     -------\n125     xypos : numeric\n126         Array of coordinate distances as (x,y) pairs representing the\n127         easting, northing of the position in meters [m].\n128 \n129     References\n130     ----------\n131     [1] H. Moritz. Geodetic Reference System 1980, Journal of Geodesy, vol. 74,\n132     no. 1, pp 128\u2013133, 2000.\n133 \n134     [2] https://pypi.org/project/pyproj/\n135 \n136     [3] Wavelet Variability Model - Matlab Code:\n137     https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n138     \"\"\"\n139 \n140     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n141 \n142     r_earth = 6371008.7714  # mean radius of Earth, in meters\n143     m_per_deg_lat = r_earth * np.pi / 180\n144     try:\n145         meanlat = np.mean([lat for (lat, lon) in coordinates])  # Mean latitude\n146     except TypeError:  # Assume it's a single value?\n147         meanlat = coordinates[0]\n148     m_per_deg_lon = r_earth * np.cos(np.pi/180 * meanlat) * np.pi/180\n149 \n150     # Conversion\n151     pos = coordinates * np.array(m_per_deg_lat, m_per_deg_lon)\n152 \n153     # reshape as (x,y) pairs to return\n154     try:\n155         return np.column_stack([pos[:, 1], pos[:, 0]])\n156     except IndexError:  # Assume it's a single value, which has a 1D shape\n157         return np.array((pos[1], pos[0]))\n158 \n159 \n160 def _compute_wavelet(clearsky_index, dt=None):\n161     \"\"\"\n162     Compute the wavelet transform on the input clear_sky time series.\n163 \n164     Parameters\n165     ----------\n166     clearsky_index : numeric or pandas.Series\n167         Clear Sky Index time series that will be smoothed.\n168 \n169     dt : float, default None\n170         The time series time delta. By default, is inferred from the\n171         clearsky_index. Must be specified for a time series that doesn't\n172         include an index. Units of seconds [s].\n173 \n174     Returns\n175     -------\n176     wavelet: numeric\n177         The individual wavelets for the time series\n178 \n179     tmscales: numeric\n180         The timescales associated with the wavelets in seconds [s]\n181 \n182     References\n183     ----------\n184     [1] M. Lave, J. Kleissl and J.S. Stein. A Wavelet-Based Variability\n185     Model (WVM) for Solar PV Power Plants. IEEE Transactions on Sustainable\n186     Energy, vol. 4, no. 2, pp. 501-509, 2013.\n187 \n188     [3] Wavelet Variability Model - Matlab Code:\n189     https://pvpmc.sandia.gov/applications/wavelet-variability-model/\n190     \"\"\"\n191 \n192     # Added by Joe Ranalli (@jranalli), Penn State Hazleton, 2019\n193 \n194     try:  # Assume it's a pandas type\n195         vals = clearsky_index.values.flatten()\n196     except AttributeError:  # Assume it's a numpy type\n197         vals = clearsky_index.flatten()\n198         if dt is None:\n199             raise ValueError(\"dt must be specified for numpy type inputs.\")\n200     else:  # flatten() succeeded, thus it's a pandas type, so get its dt\n201         try:  # Assume it's a time series type index\n202             dt = (clearsky_index.index[1] - clearsky_index.index[0]).seconds\n203         except AttributeError:  # It must just be a numeric index\n204             dt = (clearsky_index.index[1] - clearsky_index.index[0])\n205 \n206     # Pad the series on both ends in time and place in a dataframe\n207     cs_long = np.pad(vals, (len(vals), len(vals)), 'symmetric')\n208     cs_long = pd.DataFrame(cs_long)\n209 \n210     # Compute wavelet time scales\n211     min_tmscale = np.ceil(np.log(dt)/np.log(2))  # Minimum wavelet timescale\n212     max_tmscale = int(12 - min_tmscale)  # maximum wavelet timescale\n213 \n214     tmscales = np.zeros(max_tmscale)\n215     csi_mean = np.zeros([max_tmscale, len(cs_long)])\n216     # Loop for all time scales we will consider\n217     for i in np.arange(0, max_tmscale):\n218         j = i+1\n219         tmscales[i] = 2**j * dt  # Wavelet integration time scale\n220         intvlen = 2**j  # Wavelet integration time series interval\n221         # Rolling average, retains only lower frequencies than interval\n222         df = cs_long.rolling(window=intvlen, center=True, min_periods=1).mean()\n223         # Fill nan's in both directions\n224         df = df.fillna(method='bfill').fillna(method='ffill')\n225         # Pop values back out of the dataframe and store\n226         csi_mean[i, :] = df.values.flatten()\n227 \n228     # Calculate the wavelets by isolating the rolling mean frequency ranges\n229     wavelet_long = np.zeros(csi_mean.shape)\n230     for i in np.arange(0, max_tmscale-1):\n231         wavelet_long[i, :] = csi_mean[i, :] - csi_mean[i+1, :]\n232     wavelet_long[max_tmscale-1, :] = csi_mean[max_tmscale-1, :]  # Lowest freq\n233 \n234     # Clip off the padding and just return the original time window\n235     wavelet = np.zeros([max_tmscale, len(vals)])\n236     for i in np.arange(0, max_tmscale):\n237         wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n238 \n239     return wavelet, tmscales\n240 \n[end of pvlib/scaling.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/scaling.py\n+++ b/pvlib/scaling.py\n@@ -239,7 +239,7 @@\n     return wavelet, tmscales\n     \n-def _compute_wavelet(clearsky_index, dt=None):\n+def _compute_wavelet(clearsky_index, dt=None, discard_highest_detail=True):\n     \"\"\"\n     Compute the wavelet transform on the input clear_sky time series.\n     ...\n     \"\"\"\n@@ -232,7 +232,7 @@\n     wavelet_long[max_tmscale-1, :] = csi_mean[max_tmscale-1, :]  # Lowest freq\n     \n-    # Clip off the padding and just return the original time window\n+    # Clip off the padding and just return the original time window, adjusting for detail coefficients\n     wavelet = np.zeros([max_tmscale, len(vals)])\n     for i in np.arange(0, max_tmscale):\n         wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n@@ -239,7 +239,7 @@\n     return wavelet, tmscales\n     \n-    # Adjust for the highest detail coefficient if needed\n+    if discard_highest_detail:\n         wavelet = wavelet[:-1, :]  # Remove the highest detail coefficient\n     \n     return wavelet, tmscales\n```", "model_patch": "--- a/pvlib/scaling.py\n+++ b/pvlib/scaling.py\n@@ -239,7 +239,7 @@\n     return wavelet, tmscales\n     \n-def _compute_wavelet(clearsky_index, dt=None):\n+def _compute_wavelet(clearsky_index, dt=None, discard_highest_detail=True):\n     \"\"\"\n     Compute the wavelet transform on the input clear_sky time series.\n     ...\n     \"\"\"\n@@ -232,7 +232,7 @@\n     wavelet_long[max_tmscale-1, :] = csi_mean[max_tmscale-1, :]  # Lowest freq\n     \n-    # Clip off the padding and just return the original time window\n+    # Clip off the padding and just return the original time window, adjusting for detail coefficients\n     wavelet = np.zeros([max_tmscale, len(vals)])\n     for i in np.arange(0, max_tmscale):\n         wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n@@ -239,7 +239,7 @@\n     return wavelet, tmscales\n     \n-    # Adjust for the highest detail coefficient if needed\n+    if discard_highest_detail:\n         wavelet = wavelet[:-1, :]  # Remove the highest detail coefficient\n     \n     return wavelet, tmscales\n"}
{"instance_id": "pvlib__pvlib-python-1368", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\n`read_crn` returns -99999 instead of `NaN`\n**Describe the bug**\r\n`read_crn` fails to map -99999 to `NaN`\r\n\r\n**To Reproduce**\r\n```python\r\nfrom pvlib.iotools import read_crn\r\ncrn = read_crn('https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/2021/CRNS0101-05-2021-NY_Millbrook_3_W.txt')\r\ncrn.loc['2021-12-14 0930':'2021-12-14 1130', 'ghi']\r\n2021-12-14 09:30:00+00:00        0.0\r\n2021-12-14 09:35:00+00:00        0.0\r\n2021-12-14 09:40:00+00:00        0.0\r\n2021-12-14 09:45:00+00:00        0.0\r\n2021-12-14 09:50:00+00:00        0.0\r\n2021-12-14 09:55:00+00:00        0.0\r\n2021-12-14 10:00:00+00:00        0.0\r\n2021-12-14 10:05:00+00:00   -99999.0\r\n2021-12-14 10:10:00+00:00   -99999.0\r\n2021-12-14 10:15:00+00:00   -99999.0\r\n2021-12-14 10:20:00+00:00   -99999.0\r\n2021-12-14 10:25:00+00:00   -99999.0\r\n2021-12-14 10:30:00+00:00   -99999.0\r\n2021-12-14 10:35:00+00:00   -99999.0\r\n2021-12-14 10:40:00+00:00   -99999.0\r\n2021-12-14 10:45:00+00:00   -99999.0\r\n2021-12-14 10:50:00+00:00   -99999.0\r\n2021-12-14 10:55:00+00:00   -99999.0\r\n2021-12-14 11:00:00+00:00   -99999.0\r\n2021-12-14 11:05:00+00:00        0.0\r\n2021-12-14 11:10:00+00:00        0.0\r\n2021-12-14 11:15:00+00:00        0.0\r\n2021-12-14 11:20:00+00:00        0.0\r\n2021-12-14 11:25:00+00:00        0.0\r\n2021-12-14 11:30:00+00:00        0.0\r\nName: ghi, dtype: float64\r\n```\r\n\r\n**Expected behavior**\r\nShould return `NaN` instead of -99999\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.9.0\r\n - ``pandas.__version__``: 1.0.3 (doesn't matter)\r\n - python: 3.7\r\n\r\n**Additional context**\r\n\r\nDocumentation [here](https://www.ncei.noaa.gov/pub/data/uscrn/products/subhourly01/) says\r\n\r\n>          C.  Missing data are indicated by the lowest possible integer for a \r\n>             given column format, such as -9999.0 for 7-character fields with \r\n>             one decimal place or -99.000 for 7-character fields with three\r\n>             decimal places.\r\n\r\nSo we should change \r\n\r\nhttps://github.com/pvlib/pvlib-python/blob/1ab0eb20f9cd9fb9f7a0ddf35f81283f2648e34a/pvlib/iotools/crn.py#L112-L117\r\n\r\nto include -99999 and perhaps -999999. Or do the smarter thing as discussed in the comment.\r\n\r\nalso https://github.com/SolarArbiter/solarforecastarbiter-core/issues/773\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n32       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n33     </a>\n34   </td>\n35 </tr>\n36 <tr>\n37   <td>Code Quality</td>\n38  \u00a0<td>\n39     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n40     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n41     </a>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n43     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Coverage</td>\n49  \u00a0<td>\n50     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n51     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n52     </a>\n53     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n54     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Publications</td>\n60   <td>\n61     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n62     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n63     </a>\n64     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n65     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n66     </a>\n67   </td>\n68 </tr>\n69 <tr>\n70   <td>Downloads</td>\n71   <td>\n72     <a href=\"https://pypi.org/project/pvlib/\">\n73     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n74     </a>\n75     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n76     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n77     </a>\n78   </td>\n79 </tr>\n80 </table>\n81 \n82 \n83 pvlib python is a community supported tool that provides a set of\n84 functions and classes for simulating the performance of photovoltaic\n85 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n86 toolbox developed at Sandia National Laboratories and it implements many\n87 of the models and methods developed at the Labs. More information on\n88 Sandia Labs PV performance modeling programs can be found at\n89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n90 but operate independently of it.\n91 \n92 \n93 Documentation\n94 =============\n95 \n96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n97 \n98 \n99 Installation\n100 ============\n101 \n102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n104 \n105 \n106 Contributing\n107 ============\n108 \n109 We need your help to make pvlib-python a great tool!\n110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n111 The long-term success of pvlib-python requires substantial community support.\n112 \n113 \n114 Citing\n115 ======\n116 \n117 If you use pvlib-python in a published work, please cite:\n118 \n119   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n120   \"pvlib python: a python package for modeling solar energy systems.\"\n121   Journal of Open Source Software, 3(29), 884, (2018).\n122   https://doi.org/10.21105/joss.00884\n123 \n124 Please also cite the DOI corresponding to the specific version of\n125 pvlib-python that you used. pvlib-python DOIs are listed at\n126 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n127 \n128 If you use pvlib-python in a commercial or publicly-available application, please\n129 consider displaying one of the \"powered by pvlib\" logos:\n130 \n131 <img src=\"docs/sphinx/source/_images/pvlib_powered_logo_vert.png\" width=\"300\"><img src=\"docs/sphinx/source/_images/pvlib_powered_logo_horiz.png\" width=\"300\">\n132 \n133 Getting support\n134 ===============\n135 \n136 pvlib usage questions can be asked on\n137 [Stack Overflow](http://stackoverflow.com) and tagged with\n138 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n139 \n140 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n141 is used for discussing various topics of interest to the pvlib-python\n142 community. We also make new version announcements on the google group.\n143 \n144 If you suspect that you may have discovered a bug or if you'd like to\n145 change something about pvlib, then please make an issue on our\n146 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n147 \n148 \n149 \n150 License\n151 =======\n152 \n153 BSD 3-clause.\n154 \n155 \n156 NumFOCUS\n157 ========\n158 \n159 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n160 \n161 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n162 \n[end of README.md]\n[start of pvlib/iotools/crn.py]\n1 \"\"\"Functions to read data from the US Climate Reference Network (CRN).\n2 \"\"\"\n3 \n4 import pandas as pd\n5 import numpy as np\n6 \n7 \n8 HEADERS = (\n9     'WBANNO UTC_DATE UTC_TIME LST_DATE LST_TIME CRX_VN LONGITUDE LATITUDE '\n10     'AIR_TEMPERATURE PRECIPITATION SOLAR_RADIATION SR_FLAG '\n11     'SURFACE_TEMPERATURE ST_TYPE ST_FLAG RELATIVE_HUMIDITY RH_FLAG '\n12     'SOIL_MOISTURE_5 SOIL_TEMPERATURE_5 WETNESS WET_FLAG WIND_1_5 WIND_FLAG'\n13 )\n14 \n15 VARIABLE_MAP = {\n16     'LONGITUDE': 'longitude',\n17     'LATITUDE': 'latitude',\n18     'AIR_TEMPERATURE': 'temp_air',\n19     'SOLAR_RADIATION': 'ghi',\n20     'SR_FLAG': 'ghi_flag',\n21     'RELATIVE_HUMIDITY': 'relative_humidity',\n22     'RH_FLAG': 'relative_humidity_flag',\n23     'WIND_1_5': 'wind_speed',\n24     'WIND_FLAG': 'wind_speed_flag'\n25 }\n26 \n27 # as specified in CRN README.txt file. excludes 1 space between columns\n28 WIDTHS = [5, 8, 4, 8, 4, 6, 7, 7, 7, 7, 6, 1, 7, 1, 1, 5, 1, 7, 7, 5, 1, 6, 1]\n29 # add 1 to make fields contiguous (required by pandas.read_fwf)\n30 WIDTHS = [w + 1 for w in WIDTHS]\n31 # no space after last column\n32 WIDTHS[-1] -= 1\n33 \n34 # specify dtypes for potentially problematic values\n35 DTYPES = [\n36     'int64', 'int64', 'int64', 'int64', 'int64', 'str', 'float64', 'float64',\n37     'float64', 'float64', 'float64', 'int64', 'float64', 'O', 'int64',\n38     'float64', 'int64', 'float64', 'float64', 'int64', 'int64', 'float64',\n39     'int64'\n40 ]\n41 \n42 \n43 def read_crn(filename):\n44     \"\"\"\n45     Read a NOAA USCRN fixed-width file into pandas dataframe.  The CRN is\n46     described in [1]_ and [2]_.\n47 \n48     Parameters\n49     ----------\n50     filename: str, path object, or file-like\n51         filepath or url to read for the fixed-width file.\n52 \n53     Returns\n54     -------\n55     data: Dataframe\n56         A dataframe with DatetimeIndex and all of the variables in the\n57         file.\n58 \n59     Notes\n60     -----\n61     CRN files contain 5 minute averages labeled by the interval ending\n62     time. Here, missing data is flagged as NaN, rather than the lowest\n63     possible integer for a field (e.g. -999 or -99). Air temperature in\n64     deg C. Wind speed in m/s at a height of 1.5 m above ground level.\n65 \n66     Variables corresponding to standard pvlib variables are renamed,\n67     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n68     `pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n69 \n70     CRN files occasionally have a set of null characters on a line\n71     instead of valid data. This function drops those lines. Sometimes\n72     these null characters appear on a line of their own and sometimes\n73     they occur on the same line as valid data. In the latter case, the\n74     valid data will not be returned. Users may manually remove the null\n75     characters and reparse the file if they need that line.\n76 \n77     References\n78     ----------\n79     .. [1] U.S. Climate Reference Network\n80        `https://www.ncdc.noaa.gov/crn/qcdatasets.html\n81        <https://www.ncdc.noaa.gov/crn/qcdatasets.html>`_\n82 \n83     .. [2] Diamond, H. J. et. al., 2013: U.S. Climate Reference Network\n84        after one decade of operations: status and assessment. Bull.\n85        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n86     \"\"\"\n87 \n88     # read in data. set fields with NUL characters to NaN\n89     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n90                        widths=WIDTHS, na_values=['\\x00\\x00\\x00\\x00\\x00\\x00'])\n91     # at this point we only have NaNs from NUL characters, not -999 etc.\n92     # these bad rows need to be removed so that dtypes can be set.\n93     # NaNs require float dtype so we run into errors if we don't do this.\n94     data = data.dropna(axis=0)\n95     # loop here because dtype kwarg not supported in read_fwf until 0.20\n96     for (col, _dtype) in zip(data.columns, DTYPES):\n97         data[col] = data[col].astype(_dtype)\n98 \n99     # set index\n100     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n101     # with %H%M format\n102     dts = data[['UTC_DATE', 'UTC_TIME']].astype(str)\n103     dtindex = pd.to_datetime(dts['UTC_DATE'] + dts['UTC_TIME'].str.zfill(4),\n104                              format='%Y%m%d%H%M', utc=True)\n105     data = data.set_index(dtindex)\n106     try:\n107         # to_datetime(utc=True) does not work in older versions of pandas\n108         data = data.tz_localize('UTC')\n109     except TypeError:\n110         pass\n111 \n112     # Now we can set nans. This could be done a per column basis to be\n113     # safer, since in principle a real -99 value could occur in a -9999\n114     # column. Very unlikely to see that in the real world.\n115     for val in [-99, -999, -9999]:\n116         # consider replacing with .replace([-99, -999, -9999])\n117         data = data.where(data != val, np.nan)\n118 \n119     data = data.rename(columns=VARIABLE_MAP)\n120 \n121     return data\n122 \n[end of pvlib/iotools/crn.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/iotools/crn.py\n+++ b/pvlib/iotools/crn.py\n@@ -115,18 +115,20 @@\n     for val in [-99, -999, -9999]:\n         # consider replacing with .replace([-99, -999, -9999])\n         data = data.where(data != val, np.nan)\n+    \n+    # Add additional values to be replaced with NaN\n+    for val in [-99999, -999999]:\n+        data = data.where(data != val, np.nan)\n\n     data = data.rename(columns=VARIABLE_MAP)\n \n     return data\n```", "model_patch": "--- a/pvlib/iotools/crn.py\n+++ b/pvlib/iotools/crn.py\n@@ -115,18 +115,20 @@\n     for val in [-99, -999, -9999]:\n         # consider replacing with .replace([-99, -999, -9999])\n         data = data.where(data != val, np.nan)\n+    \n+    # Add additional values to be replaced with NaN\n+    for val in [-99999, -999999]:\n+        data = data.where(data != val, np.nan)\n\n     data = data.rename(columns=VARIABLE_MAP)\n \n     return data\n"}
{"instance_id": "pvlib__pvlib-python-1048", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nwarnings in test_sdm\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=209\r\n\r\nthrough\r\n\r\nhttps://dev.azure.com/solararbiter/pvlib%20python/_build/results?buildId=4054&view=logs&j=fc432b8b-e2e3-594e-d8b1-15597b6c1d62&t=309866e1-2cf4-5f00-3d0a-999fc3a0f279&l=295\r\n\r\nSo almost 100 lines of warnings.\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"https://travis-ci.org/pvlib/pvlib-python\">\n29     <img src=\"https://travis-ci.org/pvlib/pvlib-python.svg?branch=master\" alt=\"travis build status\" />\n30     </a>\n31     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n32     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n33     </a>\n34     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n35       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n36     </a>\n37   </td>\n38 </tr>\n39 <tr>\n40   <td>Code Quality</td>\n41  \u00a0<td>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n43     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n44     </a>\n45     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n46     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n47     </a>\n48   </td>\n49 </tr>\n50 <tr>\n51   <td>Coverage</td>\n52  \u00a0<td>\n53     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n54     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n55     </a>\n56     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n57     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n58     </a>\n59   </td>\n60 </tr>\n61 <tr>\n62   <td>Publications</td>\n63   <td>\n64     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n65     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n66     </a>\n67     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n68     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n69     </a>\n70   </td>\n71 </tr>\n72 <tr>\n73   <td>Downloads</td>\n74   <td>\n75     <a href=\"https://pypi.org/project/pvlib/\">\n76     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n77     </a>\n78     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n79     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n80     </a>\n81   </td>\n82 </tr>\n83 </table>\n84 \n85 \n86 pvlib python is a community supported tool that provides a set of\n87 functions and classes for simulating the performance of photovoltaic\n88 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n89 toolbox developed at Sandia National Laboratories and it implements many\n90 of the models and methods developed at the Labs. More information on\n91 Sandia Labs PV performance modeling programs can be found at\n92 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n93 but operate independently of it.\n94 \n95 \n96 Documentation\n97 =============\n98 \n99 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n100 \n101 \n102 Installation\n103 ============\n104 \n105 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n106 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n107 \n108 pvlib-python is compatible with Python 3.5 and above.\n109 \n110 **Python 2.7 support ended on June 1, 2019, with pvlib-python 0.6.3.**\n111 \n112 \n113 Contributing\n114 ============\n115 \n116 We need your help to make pvlib-python a great tool!\n117 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n118 The long-term success of pvlib-python requires substantial community support.\n119 \n120 \n121 License\n122 =======\n123 \n124 BSD 3-clause\n125 \n126 \n127 Getting support\n128 ===============\n129 \n130 pvlib usage questions can be asked on\n131 [Stack Overflow](http://stackoverflow.com) and tagged with\n132 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n133 \n134 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n135 is used for discussing various topics of interest to the pvlib-python\n136 community. We also make new version announcements on the google group.\n137 \n138 If you suspect that you may have discovered a bug or if you'd like to\n139 change something about pvlib, then please make an issue on our\n140 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n141 \n142 \n143 Citing\n144 ======\n145 \n146 If you use pvlib-python in a published work, please cite:\n147 \n148   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n149   \"pvlib python: a python package for modeling solar energy systems.\"\n150   Journal of Open Source Software, 3(29), 884, (2018).\n151   https://doi.org/10.21105/joss.00884\n152 \n153 Please also cite the DOI corresponding to the specific version of\n154 pvlib-python that you used. pvlib-python DOIs are listed at\n155 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n156 \n157 NumFOCUS\n158 ========\n159 \n160 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n161 \n162 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n163 \n[end of README.md]\n[start of pvlib/ivtools/__init__.py]\n1 \"\"\"\n2 The ``ivtools`` module contains functions to fit the single diode equation\n3 and diode models.\n4 \n5 \"\"\"\n6 \n7 from pvlib.ivtools import sde, sdm, utility  # noqa: F401\n8 \n[end of pvlib/ivtools/__init__.py]\n[start of pvlib/ivtools/sde.py]\n1 \"\"\"\n2 The ``sde`` module contains functions to fit the single diode equation.\n3 \n4 Function names should follow the pattern \"fit_\" + fitting method.\n5 \n6 \"\"\"\n7 \n8 import numpy as np\n9 \n10 from pvlib.ivtools.utility import _schumaker_qspline\n11 \n12 \n13 # set constant for numpy.linalg.lstsq parameter rcond\n14 # rcond=-1 for numpy<1.14, rcond=None for numpy>=1.14\n15 # TODO remove after minimum numpy version >= 1.14\n16 minor = int(np.__version__.split('.')[1])\n17 if minor < 14:\n18     RCOND = -1\n19 else:\n20     RCOND = None\n21 \n22 \n23 def fit_sandia_simple(voltage, current, v_oc=None, i_sc=None, v_mp_i_mp=None,\n24                       vlim=0.2, ilim=0.1):\n25     r\"\"\"\n26     Fits the single diode equation (SDE) to an IV curve.\n27 \n28     Parameters\n29     ----------\n30     voltage : ndarray\n31         1D array of `float` type containing voltage at each point on the IV\n32         curve, increasing from 0 to ``v_oc`` inclusive. [V]\n33 \n34     current : ndarray\n35         1D array of `float` type containing current at each point on the IV\n36         curve, from ``i_sc`` to 0 inclusive. [A]\n37 \n38     v_oc : float, default None\n39         Open circuit voltage. If not provided, ``v_oc`` is taken as the\n40         last point in the ``voltage`` array. [V]\n41 \n42     i_sc : float, default None\n43         Short circuit current. If not provided, ``i_sc`` is taken as the\n44         first point in the ``current`` array. [A]\n45 \n46     v_mp_i_mp : tuple of float, default None\n47         Voltage, current at maximum power point. If not provided, the maximum\n48         power point is found at the maximum of ``voltage`` \\times ``current``.\n49         [V], [A]\n50 \n51     vlim : float, default 0.2\n52         Defines portion of IV curve where the exponential term in the single\n53         diode equation can be neglected, i.e.\n54         ``voltage`` <= ``vlim`` x ``v_oc``. [V]\n55 \n56     ilim : float, default 0.1\n57         Defines portion of the IV curve where the exponential term in the\n58         single diode equation is significant, approximately defined by\n59         ``current`` < (1 - ``ilim``) x ``i_sc``. [A]\n60 \n61     Returns\n62     -------\n63     photocurrent : float\n64         photocurrent [A]\n65     saturation_current : float\n66         dark (saturation) current [A]\n67     resistance_series : float\n68         series resistance [ohm]\n69     resistance_shunt : float\n70         shunt (parallel) resistance [ohm]\n71     nNsVth : float\n72         product of thermal voltage ``Vth`` [V], diode ideality factor\n73         ``n``, and number of series cells ``Ns``. [V]\n74 \n75     Raises\n76     ------\n77     RuntimeError if parameter extraction is not successful.\n78 \n79     Notes\n80     -----\n81     Inputs ``voltage``, ``current``, ``v_oc``, ``i_sc`` and ``v_mp_i_mp`` are\n82     assumed to be from a single IV curve at constant irradiance and cell\n83     temperature.\n84 \n85     :py:func:`fit_sandia_simple` obtains values for the five parameters for\n86     the single diode equation [1]_:\n87 \n88     .. math::\n89 \n90         I = I_{L} - I_{0} (\\exp \\frac{V + I R_{s}}{nNsVth} - 1)\n91         - \\frac{V + I R_{s}}{R_{sh}}\n92 \n93     See :py:func:`pvsystem.singlediode` for definition of the parameters.\n94 \n95     The extraction method [2]_ proceeds in six steps.\n96 \n97     1. In the single diode equation, replace :math:`R_{sh} = 1/G_{p}` and\n98        re-arrange\n99 \n100     .. math::\n101 \n102         I = \\frac{I_{L}}{1 + G_{p} R_{s}} - \\frac{G_{p} V}{1 + G_{p} R_{s}}\n103         - \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nN_sV_{th}})\n104         - 1)\n105 \n106     2. The linear portion of the IV curve is defined as\n107        :math:`V \\le vlim \\times v_{oc}`. Over this portion of the IV curve,\n108 \n109     .. math::\n110 \n111         \\frac{I_{0}}{1 + G_{p} R_{s}} (\\exp(\\frac{V + I R_{s}}{nN_sV_{th}})\n112         - 1) \\approx 0\n113 \n114     3. Fit the linear portion of the IV curve with a line.\n115 \n116     .. math::\n117 \n118         I &\\approx \\frac{I_{L}}{1 + G_{p} R_{s}}\n119         - \\frac{G_{p}}{1 + G_{p}R_{s}} V\n120         &= \\beta_{0} + \\beta_{1} V\n121 \n122     4. The exponential portion of the IV curve is defined by\n123        :math:`\\beta_{0} + \\beta_{1} \\times V - I > ilim \\times i_{sc}`.\n124        Over this portion of the curve,\n125        :math:`\\exp((V + IR_s)/{nN_sV_{th}}) \\gg 1` so that\n126 \n127     .. math::\n128 \n129         \\exp(\\frac{V + I R_{s}}{nN_sV_{th}}) - 1 \\approx\n130         \\exp(\\frac{V + I R_{s}}{nN_sV_{th}})\n131 \n132     5. Fit the exponential portion of the IV curve.\n133 \n134     .. math::\n135 \n136         \\log(\\beta_{0} - \\beta_{1} V - I)\n137         &\\approx \\log(\\frac{I_{0}}{1 + G_{p} R_{s}} + \\frac{V}{nN_sV_{th}}\n138         + \\frac{I R_{s}}{nN_sV_{th}}) \\\\\n139         &= \\beta_{2} + \\beta_{3} V + \\beta_{4} I\n140 \n141     6. Calculate values for ``IL, I0, Rs, Rsh,`` and ``nNsVth`` from the\n142        regression coefficents :math:`\\beta_{0}, \\beta_{1}, \\beta_{3}` and\n143        :math:`\\beta_{4}`.\n144 \n145 \n146     References\n147     ----------\n148     .. [1] S.R. Wenham, M.A. Green, M.E. Watt, \"Applied Photovoltaics\" ISBN\n149        0 86758 909 4\n150     .. [2] C. B. Jones, C. W. Hansen, \"Single Diode Parameter Extraction from\n151        In-Field Photovoltaic I-V Curves on a Single Board Computer\", 46th IEEE\n152        Photovoltaic Specialist Conference, Chicago, IL, 2019\n153     \"\"\"\n154 \n155     # If not provided, extract v_oc, i_sc, v_mp and i_mp from the IV curve data\n156     if v_oc is None:\n157         v_oc = voltage[-1]\n158     if i_sc is None:\n159         i_sc = current[0]\n160     if v_mp_i_mp is not None:\n161         v_mp, i_mp = v_mp_i_mp\n162     else:\n163         v_mp, i_mp = _find_mp(voltage, current)\n164 \n165     # Find beta0 and beta1 from linear portion of the IV curve\n166     beta0, beta1 = _sandia_beta0_beta1(voltage, current, vlim, v_oc)\n167 \n168     # Find beta3 and beta4 from the exponential portion of the IV curve\n169     beta3, beta4 = _sandia_beta3_beta4(voltage, current, beta0, beta1, ilim,\n170                                        i_sc)\n171 \n172     # calculate single diode parameters from regression coefficients\n173     return _sandia_simple_params(beta0, beta1, beta3, beta4, v_mp, i_mp, v_oc)\n174 \n175 \n176 def _find_mp(voltage, current):\n177     \"\"\"\n178     Finds voltage and current at maximum power point.\n179 \n180     Parameters\n181     ----------\n182     voltage : ndarray\n183         1D array containing voltage at each point on the IV curve, increasing\n184         from 0 to v_oc inclusive, of `float` type. [V]\n185 \n186     current : ndarray\n187         1D array containing current at each point on the IV curve, decreasing\n188         from i_sc to 0 inclusive, of `float` type. [A]\n189 \n190     Returns\n191     -------\n192     v_mp, i_mp : tuple\n193         voltage ``v_mp`` and current ``i_mp`` at the maximum power point. [V],\n194         [A]\n195     \"\"\"\n196     p = voltage * current\n197     idx = np.argmax(p)\n198     return voltage[idx], current[idx]\n199 \n200 \n201 def _sandia_beta0_beta1(v, i, vlim, v_oc):\n202     # Used by fit_sandia_simple.\n203     # Get intercept and slope of linear portion of IV curve.\n204     # Start with V =< vlim * v_oc, extend by adding points until slope is\n205     # negative (downward).\n206     beta0 = np.nan\n207     beta1 = np.nan\n208     first_idx = np.searchsorted(v, vlim * v_oc)\n209     for idx in range(first_idx, len(v)):\n210         coef = np.polyfit(v[:idx], i[:idx], deg=1)\n211         if coef[0] < 0:\n212             # intercept term\n213             beta0 = coef[1].item()\n214             # sign change of slope to get positive parameter value\n215             beta1 = -coef[0].item()\n216             break\n217     if any(np.isnan([beta0, beta1])):\n218         raise RuntimeError(\"Parameter extraction failed: beta0={}, beta1={}\"\n219                            .format(beta0, beta1))\n220     else:\n221         return beta0, beta1\n222 \n223 \n224 def _sandia_beta3_beta4(voltage, current, beta0, beta1, ilim, i_sc):\n225     # Used by fit_sde_sandia.\n226     # Subtract the IV curve from the linear fit.\n227     y = beta0 - beta1 * voltage - current\n228     x = np.array([np.ones_like(voltage), voltage, current]).T\n229     # Select points where y > ilim * i_sc to regress log(y) onto x\n230     idx = (y > ilim * i_sc)\n231     result = np.linalg.lstsq(x[idx], np.log(y[idx]), rcond=RCOND)\n232     coef = result[0]\n233     beta3 = coef[1].item()\n234     beta4 = coef[2].item()\n235     if any(np.isnan([beta3, beta4])):\n236         raise RuntimeError(\"Parameter extraction failed: beta3={}, beta4={}\"\n237                            .format(beta3, beta4))\n238     else:\n239         return beta3, beta4\n240 \n241 \n242 def _sandia_simple_params(beta0, beta1, beta3, beta4, v_mp, i_mp, v_oc):\n243     # Used by fit_sandia_simple.\n244     nNsVth = 1.0 / beta3\n245     rs = beta4 / beta3\n246     gsh = beta1 / (1.0 - rs * beta1)\n247     rsh = 1.0 / gsh\n248     iph = (1 + gsh * rs) * beta0\n249     # calculate I0\n250     io_vmp = _calc_I0(v_mp, i_mp, iph, gsh, rs, nNsVth)\n251     io_voc = _calc_I0(v_oc, 0, iph, gsh, rs, nNsVth)\n252     if any(np.isnan([io_vmp, io_voc])) or ((io_vmp <= 0) and (io_voc <= 0)):\n253         raise RuntimeError(\"Parameter extraction failed: I0 is undetermined.\")\n254     elif (io_vmp > 0) and (io_voc > 0):\n255         io = 0.5 * (io_vmp + io_voc)\n256     elif (io_vmp > 0):\n257         io = io_vmp\n258     else:  # io_voc > 0\n259         io = io_voc\n260     return iph, io, rs, rsh, nNsVth\n261 \n262 \n263 def _calc_I0(voltage, current, iph, gsh, rs, nNsVth):\n264     return (iph - current - gsh * (voltage + rs * current)) / \\\n265         np.expm1((voltage + rs * current) / nNsVth)\n266 \n267 \n268 def _fit_sandia_cocontent(voltage, current, nsvth):\n269     \"\"\"\n270     Regression technique to fit the single diode equation to data for a single\n271     IV curve.\n272 \n273     In general, not reliable for estimating parameters other than Rsh.\n274 \n275     Parameters\n276     ----------\n277     voltage : numeric\n278         voltage for the IV curve in increasing order, the first value must be\n279         0, the last value is taken as ``Voc``. [V]\n280     current : numeric\n281         current for the IV curve corresponding to ``voltage``, the first value\n282         is taken as ``Isc``, the last value must be 0. [A]\n283     nsvth : numeric\n284         the thermal voltage for the module, equal to ``Ns`` (number of cells in\n285         series) times ``Vth`` (thermal voltage per cell). [V]\n286 \n287     Returns\n288     -------\n289     iph : numeric\n290         photocurrent [A]\n291     io : numeric\n292         dark current [A]\n293     rs : numeric\n294         shunt resistance [ohm]\n295     rsh : numeric\n296         series resistance [ohm]\n297     n : numeric\n298         diode (ideality) factor [unitless]\n299 \n300     Raises\n301     ------\n302     ValueError if ``voltage`` and ``current`` are different lengths.\n303     ValueError if ``len(voltage)`` < 6\n304 \n305     Notes\n306     -----\n307     Ported from PVLib Matlab [1]_. This function uses a regression technique\n308     described in [2]_ to fit the single diode equation to data for a single IV\n309     curve. The method extends ideas in [3]_ where the co-content is regressed\n310     onto predictors involving voltage and current. Here, a principal component\n311     transformation of ``(voltage, current)`` prior to regression to reduce the\n312     effects of colinearity between voltage and current.\n313     Although values for each of the five parameters are returned,\n314     testing has shown that values for ``Rs``, ``Io`` and ``n`` may be negative\n315     or imaginary even for IV curve data without obvious flaws.\n316 \n317     References\n318     ----------\n319     .. [1] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n320     .. [2] C. Hansen, \"Parameter Estimation for Single Diode Models of\n321        Photovoltaic Modules\", Sandia National Laboratories Report SAND2015-2065\n322     .. [3] A. Ortiz-Conde, F. Garci'a Sa'nchez, J. Murci, \"New method to\n323        extract the model parameters of solar cells from the explicit analytic\n324        solutions of their illuminated I-V characteristics\", Solar Energy\n325        Materials and Solar Cells 90, pp 352 - 361, 2006.\n326     \"\"\"\n327 \n328     if len(current) != len(voltage):\n329         raise ValueError(\"voltage and current should have the same \"\n330                          \"length\")\n331     if len(voltage) < 6:\n332         raise ValueError(\"at least 6 voltage points are required; ~50 are \"\n333                          \"recommended\")\n334     isc = current[0]  # short circuit current\n335     voc = voltage[-1]  # open circuit voltage\n336 \n337     # Fit quadratic spline to IV curve in order to compute the co-content\n338     # (i.e., integral of Isc - I over V) more accurately\n339 \n340     [t, c, yhat, kflag] = _schumaker_qspline(voltage, current)\n341 \n342     # Calculate co-content integral\n343     cci = _cocontent(t, c, isc, kflag)\n344 \n345     # Regress co-content onto voltage and current predictors\n346     beta = _cocontent_regress(voltage, current, voc, isc, cci)\n347 \n348     # Extract five parameter values from regression coefficients.\n349     # Equation 11, [3]\n350     betagp = beta[3] * 2.\n351 \n352     # Equation 12, [3]\n353     betars = (np.sqrt(1. + 16. * beta[3] * beta[4]) - 1.) / (4. * beta[3])\n354 \n355     # Equation 13, [3]\n356     betan = (beta[0] * (np.sqrt(1. + 16. * beta[3] * beta[4]) - 1.) + 4. *\n357              beta[1] * beta[3]) / (4. * beta[3] * nsvth)\n358 \n359     # Single diode equation at Voc, approximating Iph + Io by Isc\n360     betaio = (isc - voc * betagp) / (np.exp(voc / (betan * nsvth)))\n361 \n362     # Single diode equation at Isc, using Rsh, Rs, n and Io that were\n363     # determined above\n364     betaiph = isc - betaio + betaio * np.exp(isc / (betan * nsvth)) + \\\n365         isc * betars * betagp\n366 \n367     iph = betaiph\n368     io = betaio\n369     rsh = 1 / betagp\n370     rs = betars\n371     n = betan\n372 \n373     return iph, io, rs, rsh, n\n374 \n375 \n376 def _cocontent(v, c, isc, kflag):\n377     # Used by fit_sandia_cocontent\n378     # calculate co-content integral by numerical integration of\n379     # i = (Isc - I) over v\n380     # Here, i = Isc - I is assumed to be represented by the quadratic spline\n381     # with coefficients in input c, at the discrete sequence of knots in v\n382     xn = len(v)\n383     delx = v[1:] - v[:-1]\n384     tmp = np.array([1. / 3., .5, 1.])\n385     ss = np.tile(tmp, [xn - 1, 1])\n386     cc = c * ss  # cast coefficients to a convenient shape\n387     # compute integral on each interval\n388     tmpint = np.sum(cc * np.array([delx ** 3, delx ** 2, delx]).T, 1)\n389     tmpint = np.append(0., tmpint)\n390 \n391     # compute co-content = Int_0^V (Isc - I) dV\n392     scc = np.zeros(xn)\n393     # Use trapezoid rule for the first 5 intervals due to spline being\n394     # unreliable near the left endpoint\n395     scc[0:5] = isc * v[0:5] - np.cumsum(tmpint[0:5])  # by spline\n396     scc[5:(xn - 5)] = isc * (v[5:(xn - 5)] - v[4]) - \\\n397         np.cumsum(tmpint[5:(xn - 5)]) + scc[4]\n398 \n399     # Use trapezoid rule for the last 5 intervals due to spline being\n400     # unreliable near the right endpoint\n401     scc[(xn - 5):xn] = isc * (v[(xn - 5):xn] - v[xn - 6]) - \\\n402         np.cumsum(tmpint[(xn - 5):xn]) + scc[xn - 6]\n403 \n404     # For estimating diode equation parameters only use original data points,\n405     # not at any knots added by the quadratic spline fit\n406     cci = scc[~kflag.astype(bool)]\n407     return cci\n408 \n409 \n410 def _cocontent_regress(v, i, voc, isc, cci):\n411     # Used by fit_sandia_content\n412     # For the method coded here see Appendix C of [2] SAND2015-2065\n413     # predictor variables for regression of CC\n414     x = np.vstack((v, isc - i, v * (isc - i), v * v, (i - isc) ** 2)).T\n415 \n416     # define principal components transformation to shift, scale and rotate\n417     # V and I before the regression.\n418     tmpx = x[:, 0:2]\n419     tmpx_length = tmpx.shape[0]\n420 \n421     tmpx_mean = np.mean(tmpx, axis=0)\n422     tmpx_std = np.std(tmpx, axis=0, ddof=1)\n423     tmpx_zscore = (tmpx - np.tile(tmpx_mean, [tmpx_length, 1])) / \\\n424         np.tile(tmpx_std, [tmpx_length, 1])\n425 \n426     tmpx_d, tmpx_v = np.linalg.eig(np.cov(tmpx_zscore.T))\n427 \n428     idx = np.argsort(tmpx_d)[::-1]\n429 \n430     ev1 = tmpx_v[:, idx[0]]\n431 \n432     # Second component set to be orthogonal and rotated counterclockwise by 90.\n433     ev2 = np.dot(np.array([[0., -1.], [1., 0.]]), ev1)\n434     r = np.array([ev1, ev2])  # principal components transformation\n435 \n436     s = np.dot(tmpx_zscore, r)\n437     # [V, I] shift and scaled by zscore, rotated by r\n438 \n439     scc = cci - np.mean(cci, axis=0)  # center co-content values\n440     col1 = np.ones(len(scc))\n441 \n442     # predictors. Shifting makes a constant term necessary in the regression\n443     # model\n444     sx = np.vstack((s[:, 0], s[:, 1], s[:, 0] * s[:, 1], s[:, 0] * s[:, 0],\n445                     s[:, 1] * s[:, 1], col1)).T\n446 \n447     gamma = np.linalg.lstsq(sx, scc, rcond=RCOND)[0]\n448     # coefficients from regression in rotated coordinates\n449 \n450     # Principle components transformation steps\n451     # Matrix which relates principal components transformation R to the mapping\n452     # between [V' I' V'I' V'^2 I'^2] and sx, where prime ' indicates shifted\n453     # and scaled data. Used to translate from regression coefficients in\n454     # rotated coordinates to coefficients in initial V, I coordinates.\n455     mb = np.array([[r[0, 0], r[1, 0], 0., 0., 0.], [r[0, 1], r[1, 1], 0., 0.,\n456                                                     0.],\n457                    [0., 0., r[0, 0] * r[1, 1] + r[0, 1] * r[1, 0], 2. *\n458                     r[0, 0] * r[0, 1], 2. * r[1, 0] * r[1, 1]],\n459                    [0., 0., r[0, 0] * r[1, 0], r[0, 0] ** 2., r[1, 0] ** 2.],\n460                    [0., 0., r[0, 1] * r[1, 1], r[0, 1] ** 2., r[1, 1] ** 2.]])\n461 \n462     # matrix which is used to undo effect of shifting and scaling on regression\n463     # coefficients.\n464     ma = np.array([[np.std(v, ddof=1), 0., np.std(v, ddof=1) *\n465                     np.mean(isc - i), 2. * np.std(v, ddof=1) * np.mean(v),\n466                    0.], [0., np.std(isc - i, ddof=1), np.std(isc - i, ddof=1)\n467                          * np.mean(v), 0.,\n468                    2. * np.std(isc - i, ddof=1) * np.mean(isc - i)],\n469                    [0., 0., np.std(v, ddof=1) * np.std(isc - i, ddof=1), 0.,\n470                     0.],\n471                    [0., 0., 0., np.std(v, ddof=1) ** 2., 0.],\n472                    [0., 0., 0., 0., np.std(isc - i, ddof=1) ** 2.]])\n473 \n474     # translate from coefficients in rotated space (gamma) to coefficients in\n475     # original coordinates (beta)\n476     beta = np.linalg.lstsq(np.dot(mb, ma), gamma[0:5], rcond=RCOND)[0]\n477     return beta\n478 \n[end of pvlib/ivtools/sde.py]\n[start of pvlib/ivtools/sdm.py]\n1 \"\"\"\n2 The ``sdm`` module contains functions to fit single diode models.\n3 \n4 Function names should follow the pattern \"fit_\" + name of model + \"_\" +\n5  fitting method.\n6 \n7 \"\"\"\n8 \n9 import numpy as np\n10 \n11 from pvlib.pvsystem import singlediode, v_from_i\n12 \n13 from pvlib.ivtools.utility import constants, rectify_iv_curve, _numdiff\n14 from pvlib.ivtools.sde import _fit_sandia_cocontent\n15 \n16 \n17 def fit_cec_sam(celltype, v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc,\n18                 gamma_pmp, cells_in_series, temp_ref=25):\n19     \"\"\"\n20     Estimates parameters for the CEC single diode model (SDM) using the SAM\n21     SDK.\n22 \n23     Parameters\n24     ----------\n25     celltype : str\n26         Value is one of 'monoSi', 'multiSi', 'polySi', 'cis', 'cigs', 'cdte',\n27         'amorphous'\n28     v_mp : float\n29         Voltage at maximum power point [V]\n30     i_mp : float\n31         Current at maximum power point [A]\n32     v_oc : float\n33         Open circuit voltage [V]\n34     i_sc : float\n35         Short circuit current [A]\n36     alpha_sc : float\n37         Temperature coefficient of short circuit current [A/C]\n38     beta_voc : float\n39         Temperature coefficient of open circuit voltage [V/C]\n40     gamma_pmp : float\n41         Temperature coefficient of power at maximum point point [%/C]\n42     cells_in_series : int\n43         Number of cells in series\n44     temp_ref : float, default 25\n45         Reference temperature condition [C]\n46 \n47     Returns\n48     -------\n49     I_L_ref : float\n50         The light-generated current (or photocurrent) at reference\n51         conditions [A]\n52     I_o_ref : float\n53         The dark or diode reverse saturation current at reference\n54         conditions [A]\n55     R_s : float\n56         The series resistance at reference conditions, in ohms.\n57     R_sh_ref : float\n58         The shunt resistance at reference conditions, in ohms.\n59     a_ref : float\n60         The product of the usual diode ideality factor ``n`` (unitless),\n61         number of cells in series ``Ns``, and cell thermal voltage at\n62         reference conditions [V]\n63     Adjust : float\n64         The adjustment to the temperature coefficient for short circuit\n65         current, in percent.\n66 \n67     Raises\n68     ------\n69         ImportError if NREL-PySAM is not installed.\n70 \n71         RuntimeError if parameter extraction is not successful.\n72 \n73     Notes\n74     -----\n75     The CEC model and estimation method  are described in [1]_.\n76     Inputs ``v_mp``, ``i_mp``, ``v_oc`` and ``i_sc`` are assumed to be from a\n77     single IV curve at constant irradiance and cell temperature. Irradiance is\n78     not explicitly used by the fitting procedure. The irradiance level at which\n79     the input IV curve is determined and the specified cell temperature\n80     ``temp_ref`` are the reference conditions for the output parameters\n81     ``I_L_ref``, ``I_o_ref``, ``R_s``, ``R_sh_ref``, ``a_ref`` and ``Adjust``.\n82 \n83     References\n84     ----------\n85     .. [1] A. Dobos, \"An Improved Coefficient Calculator for the California\n86        Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n87        Solar Energy Engineering, vol 134, 2012.\n88     \"\"\"\n89 \n90     try:\n91         from PySAM import PySSC\n92     except ImportError:\n93         raise ImportError(\"Requires NREL's PySAM package at \"\n94                           \"https://pypi.org/project/NREL-PySAM/.\")\n95 \n96     datadict = {'tech_model': '6parsolve', 'financial_model': None,\n97                 'celltype': celltype, 'Vmp': v_mp,\n98                 'Imp': i_mp, 'Voc': v_oc, 'Isc': i_sc, 'alpha_isc': alpha_sc,\n99                 'beta_voc': beta_voc, 'gamma_pmp': gamma_pmp,\n100                 'Nser': cells_in_series, 'Tref': temp_ref}\n101 \n102     result = PySSC.ssc_sim_from_dict(datadict)\n103     if result['cmod_success'] == 1:\n104         return tuple([result[k] for k in ['Il', 'Io', 'Rs', 'Rsh', 'a',\n105                       'Adj']])\n106     else:\n107         raise RuntimeError('Parameter estimation failed')\n108 \n109 \n110 def fit_desoto(v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc, cells_in_series,\n111                EgRef=1.121, dEgdT=-0.0002677, temp_ref=25, irrad_ref=1000,\n112                root_kwargs={}):\n113     \"\"\"\n114     Calculates the parameters for the De Soto single diode model.\n115 \n116     This procedure (described in [1]_) has the advantage of\n117     using common specifications given by manufacturers in the\n118     datasheets of PV modules.\n119 \n120     The solution is found using the scipy.optimize.root() function,\n121     with the corresponding default solver method 'hybr'.\n122     No restriction is put on the fit variables, i.e. series\n123     or shunt resistance could go negative. Nevertheless, if it happens,\n124     check carefully the inputs and their units; alpha_sc and beta_voc are\n125     often given in %/K in manufacturers datasheets and should be given\n126     in A/K and V/K here.\n127 \n128     The parameters returned by this function can be used by\n129     :py:func:`pvlib.pvsystem.calcparams_desoto` to calculate the values at\n130     different irradiance and cell temperature.\n131 \n132     Parameters\n133     ----------\n134     v_mp: float\n135         Module voltage at the maximum-power point at reference conditions [V].\n136     i_mp: float\n137         Module current at the maximum-power point at reference conditions [A].\n138     v_oc: float\n139         Open-circuit voltage at reference conditions [V].\n140     i_sc: float\n141         Short-circuit current at reference conditions [A].\n142     alpha_sc: float\n143         The short-circuit current (i_sc) temperature coefficient of the\n144         module [A/K].\n145     beta_voc: float\n146         The open-circuit voltage (v_oc) temperature coefficient of the\n147         module [V/K].\n148     cells_in_series: integer\n149         Number of cell in the module.\n150     EgRef: float, default 1.121 eV - value for silicon\n151         Energy of bandgap of semi-conductor used [eV]\n152     dEgdT: float, default -0.0002677 - value for silicon\n153         Variation of bandgap according to temperature [eV/K]\n154     temp_ref: float, default 25\n155         Reference temperature condition [C]\n156     irrad_ref: float, default 1000\n157         Reference irradiance condition [W/m2]\n158     root_kwargs: dictionary, default None\n159         Dictionary of arguments to pass onto scipy.optimize.root()\n160 \n161     Returns\n162     -------\n163     dict with the following elements:\n164         I_L_ref: float\n165             Light-generated current at reference conditions [A]\n166         I_o_ref: float\n167             Diode saturation current at reference conditions [A]\n168         R_s: float\n169             Series resistance [ohm]\n170         R_sh_ref: float\n171             Shunt resistance at reference conditions [ohm].\n172         a_ref: float\n173             Modified ideality factor at reference conditions.\n174             The product of the usual diode ideality factor (n, unitless),\n175             number of cells in series (Ns), and cell thermal voltage at\n176             specified effective irradiance and cell temperature.\n177         alpha_sc: float\n178             The short-circuit current (i_sc) temperature coefficient of the\n179             module [A/K].\n180         EgRef: float\n181             Energy of bandgap of semi-conductor used [eV]\n182         dEgdT: float\n183             Variation of bandgap according to temperature [eV/K]\n184         irrad_ref: float\n185             Reference irradiance condition [W/m2]\n186         temp_ref: float\n187             Reference temperature condition [C]\n188 \n189     scipy.optimize.OptimizeResult\n190         Optimization result of scipy.optimize.root().\n191         See scipy.optimize.OptimizeResult for more details.\n192 \n193     References\n194     ----------\n195     .. [1] W. De Soto et al., \"Improvement and validation of a model for\n196        photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n197        2006.\n198     \"\"\"\n199 \n200     try:\n201         from scipy.optimize import root\n202         import scipy.constants\n203     except ImportError:\n204         raise ImportError(\"The fit_desoto function requires scipy.\")\n205 \n206     # Constants\n207     k = scipy.constants.value('Boltzmann constant in eV/K')\n208     Tref = temp_ref + 273.15  # [K]\n209 \n210     # initial guesses of variables for computing convergence:\n211     # Values are taken from [2], p753\n212     Rsh_0 = 100.0\n213     a_0 = 1.5*k*Tref*cells_in_series\n214     IL_0 = i_sc\n215     Io_0 = i_sc * np.exp(-v_oc/a_0)\n216     Rs_0 = (a_0*np.log1p((IL_0-i_mp)/Io_0) - v_mp)/i_mp\n217     # params_i : initial values vector\n218     params_i = np.array([IL_0, Io_0, Rs_0, Rsh_0, a_0])\n219 \n220     # specs of module\n221     specs = (i_sc, v_oc, i_mp, v_mp, beta_voc, alpha_sc, EgRef, dEgdT,\n222              Tref, k)\n223 \n224     # computing with system of equations described in [1]\n225     optimize_result = root(_system_of_equations_desoto, x0=params_i,\n226                            args=(specs,), **root_kwargs)\n227 \n228     if optimize_result.success:\n229         sdm_params = optimize_result.x\n230     else:\n231         raise RuntimeError(\n232             'Parameter estimation failed:\\n' + optimize_result.message)\n233 \n234     # results\n235     return ({'I_L_ref': sdm_params[0],\n236              'I_o_ref': sdm_params[1],\n237              'R_s': sdm_params[2],\n238              'R_sh_ref': sdm_params[3],\n239              'a_ref': sdm_params[4],\n240              'alpha_sc': alpha_sc,\n241              'EgRef': EgRef,\n242              'dEgdT': dEgdT,\n243              'irrad_ref': irrad_ref,\n244              'temp_ref': temp_ref},\n245             optimize_result)\n246 \n247 \n248 def _system_of_equations_desoto(params, specs):\n249     \"\"\"Evaluates the systems of equations used to solve for the single\n250     diode equation parameters. Function designed to be used by\n251     scipy.optimize.root in fit_desoto.\n252 \n253     Parameters\n254     ----------\n255     params: ndarray\n256         Array with parameters of the De Soto single diode model. Must be\n257         given in the following order: IL, Io, a, Rs, Rsh\n258     specs: tuple\n259         Specifications of pv module given by manufacturer. Must be given\n260         in the following order: Isc, Voc, Imp, Vmp, beta_oc, alpha_sc\n261 \n262     Returns\n263     -------\n264     value of the system of equations to solve with scipy.optimize.root().\n265     \"\"\"\n266 \n267     # six input known variables\n268     Isc, Voc, Imp, Vmp, beta_oc, alpha_sc, EgRef, dEgdT, Tref, k = specs\n269 \n270     # five parameters vector to find\n271     IL, Io, Rs, Rsh, a = params\n272 \n273     # five equation vector\n274     y = [0, 0, 0, 0, 0]\n275 \n276     # 1st equation - short-circuit - eq(3) in [1]\n277     y[0] = Isc - IL + Io * np.expm1(Isc * Rs / a) + Isc * Rs / Rsh\n278 \n279     # 2nd equation - open-circuit Tref - eq(4) in [1]\n280     y[1] = -IL + Io * np.expm1(Voc / a) + Voc / Rsh\n281 \n282     # 3rd equation - Imp & Vmp - eq(5) in [1]\n283     y[2] = Imp - IL + Io * np.expm1((Vmp + Imp * Rs) / a) \\\n284         + (Vmp + Imp * Rs) / Rsh\n285 \n286     # 4th equation - Pmp derivated=0 - eq23.2.6 in [2]\n287     # caution: eq(6) in [1] has a sign error\n288     y[3] = Imp \\\n289         - Vmp * ((Io / a) * np.exp((Vmp + Imp * Rs) / a) + 1.0 / Rsh) \\\n290         / (1.0 + (Io * Rs / a) * np.exp((Vmp + Imp * Rs) / a) + Rs / Rsh)\n291 \n292     # 5th equation - open-circuit T2 - eq (4) at temperature T2 in [1]\n293     T2 = Tref + 2\n294     Voc2 = (T2 - Tref) * beta_oc + Voc  # eq (7) in [1]\n295     a2 = a * T2 / Tref  # eq (8) in [1]\n296     IL2 = IL + alpha_sc * (T2 - Tref)  # eq (11) in [1]\n297     Eg2 = EgRef * (1 + dEgdT * (T2 - Tref))  # eq (10) in [1]\n298     Io2 = Io * (T2 / Tref)**3 * np.exp(1 / k * (EgRef/Tref - Eg2/T2))  # eq (9)\n299     y[4] = -IL2 + Io2 * np.expm1(Voc2 / a2) + Voc2 / Rsh  # eq (4) at T2\n300 \n301     return y\n302 \n303 \n304 def fit_pvsyst_sandia(ivcurves, specs, const=constants, maxiter=5, eps1=1.e-3):\n305     \"\"\"\n306     Estimate parameters for the PVsyst module performance model.\n307 \n308     Parameters\n309     ----------\n310     ivcurves : dict\n311         i : array\n312             One array element for each IV curve. The jth element is itself an\n313             array of current for jth IV curve (same length as v[j]) [A]\n314         v : array\n315             One array element for each IV curve. The jth element is itself an\n316             array of voltage for jth IV curve  (same length as i[j]) [V]\n317         ee : array\n318             effective irradiance for each IV curve, i.e., POA broadband\n319             irradiance adjusted by solar spectrum modifier [W / m^2]\n320         tc : array\n321             cell temperature for each IV curve [C]\n322         i_sc : array\n323             short circuit current for each IV curve [A]\n324         v_oc : array\n325             open circuit voltage for each IV curve [V]\n326         i_mp : array\n327             current at max power point for each IV curve [A]\n328         v_mp : array\n329             voltage at max power point for each IV curve [V]\n330 \n331     specs : dict\n332         cells_in_series : int\n333             number of cells in series\n334         alpha_sc : float\n335             temperature coefficient of isc [A/C]\n336 \n337     const : dict\n338         E0 : float\n339             effective irradiance at STC, default 1000 [W/m^2]\n340         T0 : float\n341             cell temperature at STC, default 25 [C]\n342         k : float\n343             1.38066E-23 J/K (Boltzmann's constant)\n344         q : float\n345             1.60218E-19 Coulomb (elementary charge)\n346 \n347     maxiter : int, default 5\n348         input that sets the maximum number of iterations for the parameter\n349         updating part of the algorithm.\n350 \n351     eps1: float, default 1e-3\n352         Tolerance for the IV curve fitting. The parameter updating stops when\n353         absolute values of the percent change in mean, max and standard\n354         deviation of Imp, Vmp and Pmp between iterations are all less than\n355         eps1, or when the number of iterations exceeds maxiter.\n356 \n357     Returns\n358     -------\n359     dict\n360         I_L_ref : float\n361             light current at STC [A]\n362         I_o_ref : float\n363             dark current at STC [A]\n364         EgRef : float\n365             effective band gap at STC [eV]\n366         R_s : float\n367             series resistance at STC [ohm]\n368         R_sh_ref : float\n369             shunt resistance at STC [ohm]\n370         R_sh_0 : float\n371             shunt resistance at zero irradiance [ohm]\n372         R_sh_exp : float\n373             exponential factor defining decrease in shunt resistance with\n374             increasing effective irradiance\n375         gamma_ref : float\n376             diode (ideality) factor at STC [unitless]\n377         mu_gamma : float\n378             temperature coefficient for diode (ideality) factor [1/K]\n379         cells_in_series : int\n380             number of cells in series\n381         iph : array\n382             light current for each IV curve [A]\n383         io : array\n384             dark current for each IV curve [A]\n385         rs : array\n386             series resistance for each IV curve [ohm]\n387         rsh : array\n388             shunt resistance for each IV curve [ohm]\n389         u : array\n390             boolean for each IV curve indicating that the parameter values\n391             are deemed reasonable by the private function ``_filter_params``\n392 \n393     Notes\n394     -----\n395     The PVsyst module performance model is described in [1]_, [2]_, and [3]_.\n396     The fitting method is documented in [4]_, [5]_, and [6]_.\n397     Ported from PVLib Matlab [7]_.\n398 \n399     References\n400     ----------\n401     .. [1] K. Sauer, T. Roessler, C. W. Hansen, Modeling the Irradiance and\n402        Temperature Dependence of Photovoltaic Modules in PVsyst, IEEE Journal\n403        of Photovoltaics v5(1), January 2015.\n404     .. [2] A. Mermoud, PV Modules modeling, Presentation at the 2nd PV\n405        Performance Modeling Workshop, Santa Clara, CA, May 2013\n406     .. [3] A. Mermoud, T. Lejeuene, Performance Assessment of a Simulation\n407        Model for PV modules of any available technology, 25th European\n408        Photovoltaic Solar Energy Conference, Valencia, Spain, Sept. 2010\n409     .. [4] C. Hansen, Estimating Parameters for the PVsyst Version 6\n410        Photovoltaic Module Performance Model, Sandia National Laboratories\n411        Report SAND2015-8598\n412     .. [5] C. Hansen, Parameter Estimation for Single Diode Models of\n413        Photovoltaic Modules, Sandia National Laboratories Report SAND2015-2065\n414     .. [6] C. Hansen, Estimation of Parameters for Single Diode Models using\n415         Measured IV Curves, Proc. of the 39th IEEE PVSC, June 2013.\n416     .. [7] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n417     \"\"\"\n418 \n419     ee = ivcurves['ee']\n420     tc = ivcurves['tc']\n421     tck = tc + 273.15\n422     isc = ivcurves['i_sc']\n423     voc = ivcurves['v_oc']\n424     imp = ivcurves['i_mp']\n425     vmp = ivcurves['v_mp']\n426 \n427     # Cell Thermal Voltage\n428     vth = const['k'] / const['q'] * tck\n429 \n430     n = len(ivcurves['v_oc'])\n431 \n432     # Initial estimate of Rsh used to obtain the diode factor gamma0 and diode\n433     # temperature coefficient mu_gamma. Rsh is estimated using the co-content\n434     # integral method.\n435 \n436     rsh = np.ones(n)\n437     for j in range(n):\n438         voltage, current = rectify_iv_curve(ivcurves['v'][j], ivcurves['i'][j])\n439         # initial estimate of Rsh, from integral over voltage regression\n440         # [5] Step 3a; [6] Step 3a\n441         _, _, _, rsh[j], _ = _fit_sandia_cocontent(\n442             voltage, current, vth[j] * specs['cells_in_series'])\n443 \n444     gamma_ref, mu_gamma = _fit_pvsyst_sandia_gamma(voc, isc, rsh, vth, tck,\n445                                                    specs, const)\n446 \n447     badgamma = np.isnan(gamma_ref) or np.isnan(mu_gamma) \\\n448         or not np.isreal(gamma_ref) or not np.isreal(mu_gamma)\n449 \n450     if badgamma:\n451         raise RuntimeError(\n452             \"Failed to estimate the diode (ideality) factor parameter;\"\n453             \" aborting parameter estimation.\")\n454 \n455     gamma = gamma_ref + mu_gamma * (tc - const['T0'])\n456     nnsvth = gamma * (vth * specs['cells_in_series'])\n457 \n458     # For each IV curve, sequentially determine initial values for Io, Rs,\n459     # and Iph [5] Step 3a; [6] Step 3\n460     iph, io, rs, u = _initial_iv_params(ivcurves, ee, voc, isc, rsh,\n461                                         nnsvth)\n462 \n463     # Update values for each IV curve to converge at vmp, imp, voc and isc\n464     iph, io, rs, rsh, u = _update_iv_params(voc, isc, vmp, imp, ee,\n465                                             iph, io, rs, rsh, nnsvth, u,\n466                                             maxiter, eps1)\n467 \n468     # get single diode models from converged values for each IV curve\n469     pvsyst = _extract_sdm_params(ee, tc, iph, io, rs, rsh, gamma, u,\n470                                  specs, const, model='pvsyst')\n471     # Add parameters estimated in this function\n472     pvsyst['gamma_ref'] = gamma_ref\n473     pvsyst['mu_gamma'] = mu_gamma\n474     pvsyst['cells_in_series'] = specs['cells_in_series']\n475 \n476     return pvsyst\n477 \n478 \n479 def fit_desoto_sandia(ivcurves, specs, const=constants, maxiter=5, eps1=1.e-3):\n480     \"\"\"\n481     Estimate parameters for the De Soto module performance model.\n482 \n483     Parameters\n484     ----------\n485     ivcurves : dict\n486         i : array\n487             One array element for each IV curve. The jth element is itself an\n488             array of current for jth IV curve (same length as v[j]) [A]\n489         v : array\n490             One array element for each IV curve. The jth element is itself an\n491             array of voltage for jth IV curve  (same length as i[j]) [V]\n492         ee : array\n493             effective irradiance for each IV curve, i.e., POA broadband\n494             irradiance adjusted by solar spectrum modifier [W / m^2]\n495         tc : array\n496             cell temperature for each IV curve [C]\n497         i_sc : array\n498             short circuit current for each IV curve [A]\n499         v_oc : array\n500             open circuit voltage for each IV curve [V]\n501         i_mp : array\n502             current at max power point for each IV curve [A]\n503         v_mp : array\n504             voltage at max power point for each IV curve [V]\n505 \n506     specs : dict\n507         cells_in_series : int\n508             number of cells in series\n509         alpha_sc : float\n510             temperature coefficient of Isc [A/C]\n511         beta_voc : float\n512             temperature coefficient of Voc [V/C]\n513 \n514     const : dict\n515         E0 : float\n516             effective irradiance at STC, default 1000 [W/m^2]\n517         T0 : float\n518             cell temperature at STC, default 25 [C]\n519         k : float\n520             1.38066E-23 J/K (Boltzmann's constant)\n521         q : float\n522             1.60218E-19 Coulomb (elementary charge)\n523 \n524     maxiter : int, default 5\n525         input that sets the maximum number of iterations for the parameter\n526         updating part of the algorithm.\n527 \n528     eps1: float, default 1e-3\n529         Tolerance for the IV curve fitting. The parameter updating stops when\n530         absolute values of the percent change in mean, max and standard\n531         deviation of Imp, Vmp and Pmp between iterations are all less than\n532         eps1, or when the number of iterations exceeds maxiter.\n533 \n534     Returns\n535     -------\n536     dict\n537         I_L_ref : float\n538             light current at STC [A]\n539         I_o_ref : float\n540             dark current at STC [A]\n541         EgRef : float\n542             effective band gap at STC [eV]\n543         R_s : float\n544             series resistance at STC [ohm]\n545         R_sh_ref : float\n546             shunt resistance at STC [ohm]\n547         cells_in_series : int\n548             number of cells in series\n549         iph : array\n550             light current for each IV curve [A]\n551         io : array\n552             dark current for each IV curve [A]\n553         rs : array\n554             series resistance for each IV curve [ohm]\n555         rsh : array\n556             shunt resistance for each IV curve [ohm]\n557         u : array\n558             boolean for each IV curve indicating that the parameter values\n559             are deemed reasonable by the private function ``_filter_params``\n560 \n561     Notes\n562     -----\n563     The De Soto module performance model is described in [1]_. The fitting\n564     method is documented in [2]_, [3]_. Ported from PVLib Matlab [4]_.\n565 \n566     References\n567     ----------\n568     .. [1] W. De Soto et al., \"Improvement and validation of a model for\n569        photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n570        2006.\n571     .. [2] C. Hansen, Parameter Estimation for Single Diode Models of\n572        Photovoltaic Modules, Sandia National Laboratories Report SAND2015-2065\n573     .. [3] C. Hansen, Estimation of Parameters for Single Diode Models using\n574         Measured IV Curves, Proc. of the 39th IEEE PVSC, June 2013.\n575     .. [4] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n576     \"\"\"\n577 \n578     ee = ivcurves['ee']\n579     tc = ivcurves['tc']\n580     tck = tc + 273.15\n581     isc = ivcurves['i_sc']\n582     voc = ivcurves['v_oc']\n583     imp = ivcurves['i_mp']\n584     vmp = ivcurves['v_mp']\n585 \n586     # Cell Thermal Voltage\n587     vth = const['k'] / const['q'] * tck\n588 \n589     n = len(voc)\n590 \n591     # Initial estimate of Rsh used to obtain the diode factor gamma0 and diode\n592     # temperature coefficient mu_gamma. Rsh is estimated using the co-content\n593     # integral method.\n594 \n595     rsh = np.ones(n)\n596     for j in range(n):\n597         voltage, current = rectify_iv_curve(ivcurves['v'][j], ivcurves['i'][j])\n598         # initial estimate of Rsh, from integral over voltage regression\n599         # [5] Step 3a; [6] Step 3a\n600         _, _, _, rsh[j], _ = _fit_sandia_cocontent(\n601             voltage, current, vth[j] * specs['cells_in_series'])\n602 \n603     n0 = _fit_desoto_sandia_diode(ee, voc, vth, tc, specs, const)\n604 \n605     bad_n = np.isnan(n0) or not np.isreal(n0)\n606 \n607     if bad_n:\n608         raise RuntimeError(\n609             \"Failed to estimate the diode (ideality) factor parameter;\"\n610             \" aborting parameter estimation.\")\n611 \n612     nnsvth = n0 * specs['cells_in_series'] * vth\n613 \n614     # For each IV curve, sequentially determine initial values for Io, Rs,\n615     # and Iph [5] Step 3a; [6] Step 3\n616     iph, io, rs, u = _initial_iv_params(ivcurves, ee, voc, isc, rsh,\n617                                         nnsvth)\n618 \n619     # Update values for each IV curve to converge at vmp, imp, voc and isc\n620     iph, io, rs, rsh, u = _update_iv_params(voc, isc, vmp, imp, ee,\n621                                             iph, io, rs, rsh, nnsvth, u,\n622                                             maxiter, eps1)\n623 \n624     # get single diode models from converged values for each IV curve\n625     desoto = _extract_sdm_params(ee, tc, iph, io, rs, rsh, n0, u,\n626                                  specs, const, model='desoto')\n627     # Add parameters estimated in this function\n628     desoto['a_ref'] = n0 * specs['cells_in_series'] * const['k'] / \\\n629         const['q'] * (const['T0'] + 273.15)\n630     desoto['cells_in_series'] = specs['cells_in_series']\n631 \n632     return desoto\n633 \n634 \n635 def _fit_pvsyst_sandia_gamma(voc, isc, rsh, vth, tck, specs, const):\n636     # Estimate the diode factor gamma from Isc-Voc data. Method incorporates\n637     # temperature dependence by means of the equation for Io\n638 \n639     y = np.log(isc - voc / rsh) - 3. * np.log(tck / (const['T0'] + 273.15))\n640     x1 = const['q'] / const['k'] * (1. / (const['T0'] + 273.15) - 1. / tck)\n641     x2 = voc / (vth * specs['cells_in_series'])\n642     uu = np.logical_or(np.isnan(y), np.isnan(x1), np.isnan(x2))\n643 \n644     x = np.vstack((np.ones(len(x1[~uu])), x1[~uu], -x1[~uu] *\n645                    (tck[~uu] - (const['T0'] + 273.15)), x2[~uu],\n646                    -x2[~uu] * (tck[~uu] - (const['T0'] + 273.15)))).T\n647     alpha = np.linalg.lstsq(x, y[~uu], rcond=None)[0]\n648 \n649     gamma_ref = 1. / alpha[3]\n650     mu_gamma = alpha[4] / alpha[3] ** 2\n651     return gamma_ref, mu_gamma\n652 \n653 \n654 def _fit_desoto_sandia_diode(ee, voc, vth, tc, specs, const):\n655     # estimates the diode factor for the De Soto model.\n656     # Helper function for fit_desoto_sandia\n657     try:\n658         import statsmodels.api as sm\n659     except ImportError:\n660         raise ImportError('Parameter extraction using Sandia method requires',\n661                           ' statsmodels')\n662 \n663     x = specs['cells_in_series'] * vth * np.log(ee / const['E0'])\n664     y = voc - specs['beta_voc'] * (tc - const['T0'])\n665     new_x = sm.add_constant(x)\n666     res = sm.RLM(y, new_x).fit()\n667     return res.params[1]\n668 \n669 \n670 def _initial_iv_params(ivcurves, ee, voc, isc, rsh, nnsvth):\n671     # sets initial values for iph, io, rs and quality filter u.\n672     # Helper function for fit_<model>_sandia.\n673     n = len(ivcurves['v_oc'])\n674     io = np.ones(n)\n675     iph = np.ones(n)\n676     rs = np.ones(n)\n677 \n678     for j in range(n):\n679 \n680         if rsh[j] > 0:\n681             volt, curr = rectify_iv_curve(ivcurves['v'][j],\n682                                           ivcurves['i'][j])\n683             # Initial estimate of Io, evaluate the single diode model at\n684             # voc and approximate Iph + Io = Isc [5] Step 3a; [6] Step 3b\n685             io[j] = (isc[j] - voc[j] / rsh[j]) * np.exp(-voc[j] /\n686                                                         nnsvth[j])\n687 \n688             # initial estimate of rs from dI/dV near Voc\n689             # [5] Step 3a; [6] Step 3c\n690             [didv, d2id2v] = _numdiff(volt, curr)\n691             t3 = volt > .5 * voc[j]\n692             t4 = volt < .9 * voc[j]\n693             tmp = -rsh[j] * didv - 1.\n694             with np.errstate(invalid=\"ignore\"):  # expect nan in didv\n695                 v = np.logical_and.reduce(np.array([t3, t4, ~np.isnan(tmp),\n696                                                     np.greater(tmp, 0)]))\n697             if np.any(v):\n698                 vtrs = (nnsvth[j] / isc[j] * (\n699                     np.log(tmp[v] * nnsvth[j] / (rsh[j] * io[j]))\n700                     - volt[v] / nnsvth[j]))\n701                 rs[j] = np.mean(vtrs[vtrs > 0], axis=0)\n702             else:\n703                 rs[j] = 0.\n704 \n705             # Initial estimate of Iph, evaluate the single diode model at\n706             # Isc [5] Step 3a; [6] Step 3d\n707             iph[j] = isc[j] + io[j] * np.expm1(isc[j] / nnsvth[j]) \\\n708                 + isc[j] * rs[j] / rsh[j]\n709 \n710         else:\n711             io[j] = np.nan\n712             rs[j] = np.nan\n713             iph[j] = np.nan\n714 \n715         # Filter IV curves for good initial values\n716         # [5] Step 3b\n717         u = _filter_params(ee, isc, io, rs, rsh)\n718 \n719         # [5] Step 3c\n720         # Refine Io to match Voc\n721         io[u] = _update_io(voc[u], iph[u], io[u], rs[u], rsh[u], nnsvth[u])\n722 \n723         # parameters [6], Step 3c\n724         # Calculate Iph to be consistent with Isc and current values of other\n725         iph = isc + io * np.expm1(rs * isc / nnsvth) + isc * rs / rsh\n726 \n727     return iph, io, rs, u\n728 \n729 \n730 def _update_iv_params(voc, isc, vmp, imp, ee, iph, io, rs, rsh, nnsvth, u,\n731                       maxiter, eps1):\n732     # Refine Rsh, Rs, Io and Iph in that order.\n733     # Helper function for fit_<model>_sandia.\n734     counter = 1.  # counter variable for parameter updating while loop,\n735     # counts iterations\n736     prevconvergeparams = {}\n737     prevconvergeparams['state'] = 0.0\n738 \n739     not_converged = np.array([True])\n740 \n741     while not_converged.any() and counter <= maxiter:\n742         # update rsh to match max power point using a fixed point method.\n743         rsh[u] = _update_rsh_fixed_pt(vmp[u], imp[u], iph[u], io[u], rs[u],\n744                                       rsh[u], nnsvth[u])\n745 \n746         # Calculate Rs to be consistent with Rsh and maximum power point\n747         _, phi = _calc_theta_phi_exact(vmp[u], imp[u], iph[u], io[u],\n748                                        rs[u], rsh[u], nnsvth[u])\n749         rs[u] = (iph[u] + io[u] - imp[u]) * rsh[u] / imp[u] - \\\n750             nnsvth[u] * phi / imp[u] - vmp[u] / imp[u]\n751 \n752         # Update filter for good parameters\n753         u = _filter_params(ee, isc, io, rs, rsh)\n754 \n755         # Update value for io to match voc\n756         io[u] = _update_io(voc[u], iph[u], io[u], rs[u], rsh[u], nnsvth[u])\n757 \n758         # Calculate Iph to be consistent with Isc and other parameters\n759         iph = isc + io * np.expm1(rs * isc / nnsvth) + isc * rs / rsh\n760 \n761         # update filter for good parameters\n762         u = _filter_params(ee, isc, io, rs, rsh)\n763 \n764         # compute the IV curve from the current parameter values\n765         result = singlediode(iph[u], io[u], rs[u], rsh[u], nnsvth[u])\n766 \n767         # check convergence criteria\n768         # [5] Step 3d\n769         convergeparams = _check_converge(\n770             prevconvergeparams, result, vmp[u], imp[u], counter)\n771 \n772         prevconvergeparams = convergeparams\n773         counter += 1.\n774         t5 = prevconvergeparams['vmperrmeanchange'] >= eps1\n775         t6 = prevconvergeparams['imperrmeanchange'] >= eps1\n776         t7 = prevconvergeparams['pmperrmeanchange'] >= eps1\n777         t8 = prevconvergeparams['vmperrstdchange'] >= eps1\n778         t9 = prevconvergeparams['imperrstdchange'] >= eps1\n779         t10 = prevconvergeparams['pmperrstdchange'] >= eps1\n780         t11 = prevconvergeparams['vmperrabsmaxchange'] >= eps1\n781         t12 = prevconvergeparams['imperrabsmaxchange'] >= eps1\n782         t13 = prevconvergeparams['pmperrabsmaxchange'] >= eps1\n783         not_converged = np.logical_or.reduce(np.array([t5, t6, t7, t8, t9,\n784                                                        t10, t11, t12, t13]))\n785 \n786     return iph, io, rs, rsh, u\n787 \n788 \n789 def _extract_sdm_params(ee, tc, iph, io, rs, rsh, n, u, specs, const,\n790                         model):\n791     # Get single diode model parameters from five parameters iph, io, rs, rsh\n792     # and n vs. effective irradiance and temperature\n793     try:\n794         from scipy import optimize\n795         import statsmodels.api as sm\n796     except ImportError:\n797         raise ImportError('Parameter extraction using Sandia method requires',\n798                           ' scipy and statsmodels')\n799 \n800     tck = tc + 273.15\n801     tok = const['T0'] + 273.15  # convert to to K\n802 \n803     params = {}\n804 \n805     if model == 'pvsyst':\n806         # Estimate I_o_ref and EgRef\n807         x_for_io = const['q'] / const['k'] * (1. / tok - 1. / tck[u]) / n[u]\n808 \n809         # Estimate R_sh_0, R_sh_ref and R_sh_exp\n810         # Initial guesses. R_sh_0 is value at ee=0.\n811         nans = np.isnan(rsh)\n812         if any(ee < 400):\n813             grsh0 = np.mean(rsh[np.logical_and(~nans, ee < 400)])\n814         else:\n815             grsh0 = np.max(rsh)\n816         # Rsh_ref is value at Ee = 1000\n817         if any(ee > 400):\n818             grshref = np.mean(rsh[np.logical_and(~nans, ee > 400)])\n819         else:\n820             grshref = np.min(rsh)\n821         # PVsyst default for Rshexp is 5.5\n822         R_sh_exp = 5.5\n823 \n824         # Find parameters for Rsh equation\n825 \n826         def fun_rsh(x, rshexp, ee, e0, rsh):\n827             tf = np.log10(_rsh_pvsyst(x, R_sh_exp, ee, e0)) - np.log10(rsh)\n828             return tf\n829 \n830         x0 = np.array([grsh0, grshref])\n831         beta = optimize.least_squares(\n832             fun_rsh, x0, args=(R_sh_exp, ee[u], const['E0'], rsh[u]),\n833             bounds=np.array([[1., 1.], [1.e7, 1.e6]]), verbose=2)\n834         # Extract PVsyst parameter values\n835         R_sh_0 = beta.x[0]\n836         R_sh_ref = beta.x[1]\n837 \n838         # parameters unique to PVsyst\n839         params['R_sh_0'] = R_sh_0\n840         params['R_sh_exp'] = R_sh_exp\n841 \n842     elif model == 'desoto':\n843         dEgdT = 0.0002677\n844         x_for_io = const['q'] / const['k'] * (\n845             1. / tok - 1. / tck[u] + dEgdT * (tc[u] - const['T0']) / tck[u])\n846 \n847         # Estimate R_sh_ref\n848         nans = np.isnan(rsh)\n849         x = const['E0'] / ee[np.logical_and(u, ee > 400, ~nans)]\n850         y = rsh[np.logical_and(u, ee > 400, ~nans)]\n851         new_x = sm.add_constant(x)\n852         beta = sm.RLM(y, new_x).fit()\n853         R_sh_ref = beta.params[1]\n854 \n855         params['dEgdT'] = dEgdT\n856 \n857     # Estimate I_o_ref and EgRef\n858     y = np.log(io[u]) - 3. * np.log(tck[u] / tok)\n859     new_x = sm.add_constant(x_for_io)\n860     res = sm.RLM(y, new_x).fit()\n861     beta = res.params\n862     I_o_ref = np.exp(beta[0])\n863     EgRef = beta[1]\n864 \n865     # Estimate I_L_ref\n866     x = tc[u] - const['T0']\n867     y = iph[u] * (const['E0'] / ee[u])\n868     # average over non-NaN values of Y and X\n869     nans = np.isnan(y - specs['alpha_sc'] * x)\n870     I_L_ref = np.mean(y[~nans] - specs['alpha_sc'] * x[~nans])\n871 \n872     # Estimate R_s\n873     nans = np.isnan(rs)\n874     R_s = np.mean(rs[np.logical_and(u, ee > 400, ~nans)])\n875 \n876     params['I_L_ref'] = I_L_ref\n877     params['I_o_ref'] = I_o_ref\n878     params['EgRef'] = EgRef\n879     params['R_sh_ref'] = R_sh_ref\n880     params['R_s'] = R_s\n881     # save values for each IV curve\n882     params['iph'] = iph\n883     params['io'] = io\n884     params['rsh'] = rsh\n885     params['rs'] = rs\n886     params['u'] = u\n887 \n888     return params\n889 \n890 \n891 def _update_io(voc, iph, io, rs, rsh, nnsvth):\n892     \"\"\"\n893     Adjusts Io to match Voc using other parameter values.\n894 \n895     Helper function for fit_pvsyst_sandia, fit_desoto_sandia\n896 \n897     Description\n898     -----------\n899     Io is updated iteratively 10 times or until successive\n900     values are less than 0.000001 % different. The updating is similar to\n901     Newton's method.\n902 \n903     Parameters\n904     ----------\n905     voc: a numpy array of length N of values for Voc (V)\n906     iph: a numpy array of length N of values for lighbt current IL (A)\n907     io: a numpy array of length N of initial values for Io (A)\n908     rs: a numpy array of length N of values for the series resistance (ohm)\n909     rsh: a numpy array of length N of values for the shunt resistance (ohm)\n910     nnsvth: a numpy array of length N of values for the diode factor x thermal\n911             voltage for the module, equal to Ns (number of cells in series) x\n912             Vth (thermal voltage per cell).\n913 \n914     Returns\n915     -------\n916     new_io - a numpy array of length N of updated values for io\n917 \n918     References\n919     ----------\n920     .. [1] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n921     .. [2] C. Hansen, Parameter Estimation for Single Diode Models of\n922        Photovoltaic Modules, Sandia National Laboratories Report SAND2015-2065\n923     .. [3] C. Hansen, Estimation of Parameteres for Single Diode Models using\n924        Measured IV Curves, Proc. of the 39th IEEE PVSC, June 2013.\n925     \"\"\"\n926 \n927     eps = 1e-6\n928     niter = 10\n929     k = 1\n930     maxerr = 1\n931 \n932     tio = io  # Current Estimate of Io\n933 \n934     while maxerr > eps and k < niter:\n935         # Predict Voc\n936         pvoc = v_from_i(rsh, rs, nnsvth, 0., tio, iph)\n937 \n938         # Difference in Voc\n939         dvoc = pvoc - voc\n940 \n941         # Update Io\n942         new_io = tio * (1. + (2. * dvoc) / (2. * nnsvth - dvoc))\n943 \n944         # Calculate Maximum Percent Difference\n945         maxerr = np.max(np.abs(new_io - tio) / tio) * 100.\n946         tio = new_io\n947         k += 1.\n948 \n949     return new_io\n950 \n951 \n952 def _rsh_pvsyst(x, rshexp, g, go):\n953     # computes rsh for PVsyst model where the parameters are in vector xL\n954     # x[0] = Rsh0\n955     # x[1] = Rshref\n956 \n957     rsho = x[0]\n958     rshref = x[1]\n959 \n960     rshb = np.maximum(\n961         (rshref - rsho * np.exp(-rshexp)) / (1. - np.exp(-rshexp)), 0.)\n962     rsh = rshb + (rsho - rshb) * np.exp(-rshexp * g / go)\n963     return rsh\n964 \n965 \n966 def _filter_params(ee, isc, io, rs, rsh):\n967     # Function _filter_params identifies bad parameter sets. A bad set contains\n968     # Nan, non-positive or imaginary values for parameters; Rs > Rsh; or data\n969     # where effective irradiance Ee differs by more than 5% from a linear fit\n970     # to Isc vs. Ee\n971 \n972     badrsh = np.logical_or(rsh < 0., np.isnan(rsh))\n973     negrs = rs < 0.\n974     badrs = np.logical_or(rs > rsh, np.isnan(rs))\n975     imagrs = ~(np.isreal(rs))\n976     badio = np.logical_or(~(np.isreal(rs)), io <= 0)\n977     goodr = np.logical_and(~badrsh, ~imagrs)\n978     goodr = np.logical_and(goodr, ~negrs)\n979     goodr = np.logical_and(goodr, ~badrs)\n980     goodr = np.logical_and(goodr, ~badio)\n981 \n982     matrix = np.vstack((ee / 1000., np.zeros(len(ee)))).T\n983     eff = np.linalg.lstsq(matrix, isc, rcond=None)[0][0]\n984     pisc = eff * ee / 1000\n985     pisc_error = np.abs(pisc - isc) / isc\n986     # check for departure from linear relation between Isc and Ee\n987     badiph = pisc_error > .05\n988 \n989     u = np.logical_and(goodr, ~badiph)\n990     return u\n991 \n992 \n993 def _check_converge(prevparams, result, vmp, imp, i):\n994     \"\"\"\n995     Function _check_converge computes convergence metrics for all IV curves.\n996 \n997     Helper function for fit_pvsyst_sandia, fit_desoto_sandia\n998 \n999     Parameters\n1000     ----------\n1001     prevparams: Convergence Parameters from the previous Iteration (used to\n1002                 determine Percent Change in values between iterations)\n1003     result: performacne paramters of the (predicted) single diode fitting,\n1004             which includes Voc, Vmp, Imp, Pmp and Isc\n1005     vmp: measured values for each IV curve\n1006     imp: measured values for each IV curve\n1007     i: Index of current iteration in cec_parameter_estimation\n1008 \n1009     Returns\n1010     -------\n1011     convergeparam: dict containing the following for Imp, Vmp and Pmp:\n1012         - maximum percent difference between measured and modeled values\n1013         - minimum percent difference between measured and modeled values\n1014         - maximum absolute percent difference between measured and modeled\n1015           values\n1016         - mean percent difference between measured and modeled values\n1017         - standard deviation of percent difference between measured and modeled\n1018           values\n1019         - absolute difference for previous and current values of maximum\n1020           absolute percent difference (measured vs. modeled)\n1021         - absolute difference for previous and current values of mean percent\n1022           difference (measured vs. modeled)\n1023         - absolute difference for previous and current values of standard\n1024           deviation of percent difference (measured vs. modeled)\n1025     \"\"\"\n1026 \n1027     convergeparam = {}\n1028 \n1029     imperror = (result['i_mp'] - imp) / imp * 100.\n1030     vmperror = (result['v_mp'] - vmp) / vmp * 100.\n1031     pmperror = (result['p_mp'] - (imp * vmp)) / (imp * vmp) * 100.\n1032 \n1033     convergeparam['imperrmax'] = max(imperror)  # max of the error in Imp\n1034     convergeparam['imperrmin'] = min(imperror)  # min of the error in Imp\n1035     # max of the absolute error in Imp\n1036     convergeparam['imperrabsmax'] = max(abs(imperror))\n1037     # mean of the error in Imp\n1038     convergeparam['imperrmean'] = np.mean(imperror, axis=0)\n1039     # std of the error in Imp\n1040     convergeparam['imperrstd'] = np.std(imperror, axis=0, ddof=1)\n1041 \n1042     convergeparam['vmperrmax'] = max(vmperror)  # max of the error in Vmp\n1043     convergeparam['vmperrmin'] = min(vmperror)  # min of the error in Vmp\n1044     # max of the absolute error in Vmp\n1045     convergeparam['vmperrabsmax'] = max(abs(vmperror))\n1046     # mean of the error in Vmp\n1047     convergeparam['vmperrmean'] = np.mean(vmperror, axis=0)\n1048     # std of the error in Vmp\n1049     convergeparam['vmperrstd'] = np.std(vmperror, axis=0, ddof=1)\n1050 \n1051     convergeparam['pmperrmax'] = max(pmperror)  # max of the error in Pmp\n1052     convergeparam['pmperrmin'] = min(pmperror)  # min of the error in Pmp\n1053     # max of the abs err. in Pmp\n1054     convergeparam['pmperrabsmax'] = max(abs(pmperror))\n1055     # mean error in Pmp\n1056     convergeparam['pmperrmean'] = np.mean(pmperror, axis=0)\n1057     # std error Pmp\n1058     convergeparam['pmperrstd'] = np.std(pmperror, axis=0, ddof=1)\n1059 \n1060     if prevparams['state'] != 0.0:\n1061         convergeparam['imperrstdchange'] = np.abs(\n1062             convergeparam['imperrstd'] / prevparams['imperrstd'] - 1.)\n1063         convergeparam['vmperrstdchange'] = np.abs(\n1064             convergeparam['vmperrstd'] / prevparams['vmperrstd'] - 1.)\n1065         convergeparam['pmperrstdchange'] = np.abs(\n1066             convergeparam['pmperrstd'] / prevparams['pmperrstd'] - 1.)\n1067         convergeparam['imperrmeanchange'] = np.abs(\n1068             convergeparam['imperrmean'] / prevparams['imperrmean'] - 1.)\n1069         convergeparam['vmperrmeanchange'] = np.abs(\n1070             convergeparam['vmperrmean'] / prevparams['vmperrmean'] - 1.)\n1071         convergeparam['pmperrmeanchange'] = np.abs(\n1072             convergeparam['pmperrmean'] / prevparams['pmperrmean'] - 1.)\n1073         convergeparam['imperrabsmaxchange'] = np.abs(\n1074             convergeparam['imperrabsmax'] / prevparams['imperrabsmax'] - 1.)\n1075         convergeparam['vmperrabsmaxchange'] = np.abs(\n1076             convergeparam['vmperrabsmax'] / prevparams['vmperrabsmax'] - 1.)\n1077         convergeparam['pmperrabsmaxchange'] = np.abs(\n1078             convergeparam['pmperrabsmax'] / prevparams['pmperrabsmax'] - 1.)\n1079         convergeparam['state'] = 1.0\n1080     else:\n1081         convergeparam['imperrstdchange'] = float(\"Inf\")\n1082         convergeparam['vmperrstdchange'] = float(\"Inf\")\n1083         convergeparam['pmperrstdchange'] = float(\"Inf\")\n1084         convergeparam['imperrmeanchange'] = float(\"Inf\")\n1085         convergeparam['vmperrmeanchange'] = float(\"Inf\")\n1086         convergeparam['pmperrmeanchange'] = float(\"Inf\")\n1087         convergeparam['imperrabsmaxchange'] = float(\"Inf\")\n1088         convergeparam['vmperrabsmaxchange'] = float(\"Inf\")\n1089         convergeparam['pmperrabsmaxchange'] = float(\"Inf\")\n1090         convergeparam['state'] = 1.\n1091     return convergeparam\n1092 \n1093 \n1094 def _update_rsh_fixed_pt(vmp, imp, iph, io, rs, rsh, nnsvth):\n1095     \"\"\"\n1096     Adjust Rsh to match Vmp using other parameter values\n1097 \n1098     Helper function for fit_pvsyst_sandia, fit_desoto_sandia\n1099 \n1100     Description\n1101     -----------\n1102     Rsh is updated iteratively using a fixed point expression\n1103     obtained from combining Vmp = Vmp(Imp) (using the analytic solution to the\n1104     single diode equation) and dP / dI = 0 at Imp. 500 iterations are performed\n1105     because convergence can be very slow.\n1106 \n1107     Parameters\n1108     ----------\n1109     vmp: a numpy array of length N of values for Vmp (V)\n1110     imp: a numpy array of length N of values for Imp (A)\n1111     iph: a numpy array of length N of values for light current IL (A)\n1112     io: a numpy array of length N of values for Io (A)\n1113     rs: a numpy array of length N of values for series resistance (ohm)\n1114     rsh: a numpy array of length N of initial values for shunt resistance (ohm)\n1115     nnsvth: a numpy array length N of values for the diode factor x thermal\n1116             voltage for the module, equal to Ns (number of cells in series) x\n1117             Vth (thermal voltage per cell).\n1118 \n1119     Returns\n1120     -------\n1121     numpy array of length N of updated values for Rsh\n1122 \n1123     References\n1124     ----------\n1125     .. [1] PVLib for MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n1126     .. [2] C. Hansen, Parameter Estimation for Single Diode Models of\n1127        Photovoltaic Modules, Sandia National Laboratories Report SAND2015-2065\n1128     \"\"\"\n1129     niter = 500\n1130     x1 = rsh\n1131 \n1132     for i in range(niter):\n1133         _, z = _calc_theta_phi_exact(vmp, imp, iph, io, rs, x1, nnsvth)\n1134         next_x1 = (1 + z) / z * ((iph + io) * x1 / imp - nnsvth * z / imp - 2 *\n1135                                  vmp / imp)\n1136         x1 = next_x1\n1137 \n1138     return x1\n1139 \n1140 \n1141 def _calc_theta_phi_exact(vmp, imp, iph, io, rs, rsh, nnsvth):\n1142     \"\"\"\n1143     _calc_theta_phi_exact computes Lambert W values appearing in the analytic\n1144     solutions to the single diode equation for the max power point.\n1145 \n1146     Helper function for fit_pvsyst_sandia\n1147 \n1148     Parameters\n1149     ----------\n1150     vmp: a numpy array of length N of values for Vmp (V)\n1151     imp: a numpy array of length N of values for Imp (A)\n1152     iph: a numpy array of length N of values for the light current IL (A)\n1153     io: a numpy array of length N of values for Io (A)\n1154     rs: a numpy array of length N of values for the series resistance (ohm)\n1155     rsh: a numpy array of length N of values for the shunt resistance (ohm)\n1156     nnsvth: a numpy array of length N of values for the diode factor x\n1157             thermal voltage for the module, equal to Ns\n1158             (number of cells in series) x Vth\n1159             (thermal voltage per cell).\n1160 \n1161     Returns\n1162     -------\n1163     theta: a numpy array of values for the Lamber W function for solving\n1164            I = I(V)\n1165     phi: a numpy array of values for the Lambert W function for solving\n1166          V = V(I)\n1167 \n1168     Notes\n1169     -----\n1170     _calc_theta_phi_exact calculates values for the Lambert W function which\n1171     are used in the analytic solutions for the single diode equation at the\n1172     maximum power point. For V=V(I),\n1173     phi = W(Io*Rsh/n*Vth * exp((IL + Io - Imp)*Rsh/n*Vth)). For I=I(V),\n1174     theta = W(Rs*Io/n*Vth *\n1175     Rsh/ (Rsh+Rs) * exp(Rsh/ (Rsh+Rs)*((Rs(IL+Io) + V)/n*Vth))\n1176 \n1177     References\n1178     ----------\n1179     .. [1] PVL MATLAB 2065 https://github.com/sandialabs/MATLAB_PV_LIB\n1180     .. [2] C. Hansen, Parameter Estimation for Single Diode Models of\n1181        Photovoltaic Modules, Sandia National Laboratories Report SAND2015-2065\n1182     .. [3] A. Jain, A. Kapoor, \"Exact analytical solutions of the parameters of\n1183        real solar cells using Lambert W-function\", Solar Energy Materials and\n1184        Solar Cells, 81 (2004) 269-277.\n1185     \"\"\"\n1186 \n1187     try:\n1188         from scipy.special import lambertw\n1189     except ImportError:\n1190         raise ImportError('calc_theta_phi_exact requires scipy')\n1191 \n1192     # handle singleton inputs\n1193     vmp = np.asarray(vmp)\n1194     imp = np.asarray(imp)\n1195     iph = np.asarray(iph)\n1196     io = np.asarray(io)\n1197     rs = np.asarray(rs)\n1198     rsh = np.asarray(rsh)\n1199     nnsvth = np.asarray(nnsvth)\n1200 \n1201     # Argument for Lambert W function involved in V = V(I) [2] Eq. 12; [3]\n1202     # Eq. 3\n1203     with np.errstate(over=\"ignore\"):\n1204         argw = np.where(\n1205             nnsvth == 0,\n1206             np.nan,\n1207             rsh * io / nnsvth * np.exp(rsh * (iph + io - imp) / nnsvth))\n1208     phi = np.where(argw > 0, lambertw(argw).real, np.nan)\n1209 \n1210     # NaN where argw overflows. Switch to log space to evaluate\n1211     u = np.isinf(argw)\n1212     if np.any(u):\n1213         logargw = (\n1214             np.log(rsh[u]) + np.log(io[u]) - np.log(nnsvth[u])\n1215             + rsh[u] * (iph[u] + io[u] - imp[u]) / nnsvth[u])\n1216         # Three iterations of Newton-Raphson method to solve w+log(w)=logargW.\n1217         # The initial guess is w=logargW. Where direct evaluation (above)\n1218         # results in NaN from overflow, 3 iterations of Newton's method gives\n1219         # approximately 8 digits of precision.\n1220         x = logargw\n1221         for i in range(3):\n1222             x *= ((1. - np.log(x) + logargw) / (1. + x))\n1223         phi[u] = x\n1224     phi = np.transpose(phi)\n1225 \n1226     # Argument for Lambert W function involved in I = I(V) [2] Eq. 11; [3]\n1227     # E1. 2\n1228     with np.errstate(over=\"ignore\"):\n1229         argw = np.where(\n1230             nnsvth == 0,\n1231             np.nan,\n1232             rsh / (rsh + rs) * rs * io / nnsvth * np.exp(\n1233                 rsh / (rsh + rs) * (rs * (iph + io) + vmp) / nnsvth))\n1234     theta = np.where(argw > 0, lambertw(argw).real, np.nan)\n1235 \n1236     # NaN where argw overflows. Switch to log space to evaluate\n1237     u = np.isinf(argw)\n1238     if np.any(u):\n1239         logargw = (\n1240             np.log(rsh[u]) / (rsh[u] + rs[u]) + np.log(rs[u]) + np.log(io[u])\n1241             - np.log(nnsvth[u]) + (rsh[u] / (rsh[u] + rs[u]))\n1242             * (rs[u] * (iph[u] + io[u]) + vmp[u]) / nnsvth[u])\n1243         # Three iterations of Newton-Raphson method to solve w+log(w)=logargW.\n1244         # The initial guess is w=logargW. Where direct evaluation (above)\n1245         # results in NaN from overflow, 3 iterations of Newton's method gives\n1246         # approximately 8 digits of precision.\n1247         x = logargw\n1248         for i in range(3):\n1249             x *= ((1. - np.log(x) + logargw) / (1. + x))\n1250         theta[u] = x\n1251     theta = np.transpose(theta)\n1252 \n1253     return theta, phi\n1254 \n[end of pvlib/ivtools/sdm.py]\n[start of pvlib/ivtools/utility.py]\n1 \"\"\"\n2 The ``pvlib.ivtools.utility.py`` module contains utility functions related to\n3 working with IV curves, or fitting equations to IV curve data.\n4 \n5 \"\"\"\n6 \n7 import numpy as np\n8 import pandas as pd\n9 \n10 \n11 # A small number used to decide when a slope is equivalent to zero\n12 EPS = np.finfo('float').eps**(1/3)\n13 \n14 \n15 constants = {'E0': 1000.0, 'T0': 25.0, 'k': 1.38066e-23, 'q': 1.60218e-19}\n16 \n17 \n18 def _numdiff(x, f):\n19     \"\"\"\n20     Compute first and second order derivative using possibly unequally\n21     spaced data.\n22 \n23     Parameters\n24     ----------\n25     x : numeric\n26         a numpy array of values of x\n27     f : numeric\n28         a numpy array of values of the function f for which derivatives are to\n29         be computed. Must be the same length as x.\n30 \n31     Returns\n32     -------\n33     df : numeric\n34         a numpy array of len(x) containing the first derivative of f at each\n35         point x except at the first 2 and last 2 points\n36     df2 : numeric\n37         a numpy array of len(x) containing the second derivative of f at each\n38         point x except at the first 2 and last 2 points.\n39 \n40     Notes\n41     -----\n42     ``numdiff`` computes first and second order derivatives using a 5th order\n43     formula that accounts for possibly unequally spaced data [1]_. Because a\n44     5th order centered difference formula is used, ``numdiff`` returns NaNs\n45     for the first 2 and last 2 points in the input vector for x. Ported from\n46     PVLib Matlab [2]_.\n47 \n48     References\n49     ----------\n50     .. [1] M. K. Bowen, R. Smith, \"Derivative formulae and errors for\n51        non-uniformly spaced points\", Proceedings of the Royal Society A, vol.\n52        461 pp 1975 - 1997, July 2005. DOI: 10.1098/rpsa.2004.1430\n53     .. [2] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n54     \"\"\"\n55 \n56     n = len(f)\n57 \n58     df = np.zeros(n)\n59     df2 = np.zeros(n)\n60 \n61     # first two points are special\n62     df[:2] = float(\"Nan\")\n63     df2[:2] = float(\"Nan\")\n64 \n65     # Last two points are special\n66     df[-2:] = float(\"Nan\")\n67     df2[-2:] = float(\"Nan\")\n68 \n69     # Rest of points. Take reference point to be the middle of each group of 5\n70     # points. Calculate displacements\n71     ff = np.vstack((f[:-4], f[1:-3], f[2:-2], f[3:-1], f[4:])).T\n72 \n73     a0 = (np.vstack((x[:-4], x[1:-3], x[2:-2], x[3:-1], x[4:])).T\n74           - np.tile(x[2:-2], [5, 1]).T)\n75 \n76     u1 = np.zeros(a0.shape)\n77     left = np.zeros(a0.shape)\n78     u2 = np.zeros(a0.shape)\n79 \n80     u1[:, 0] = (\n81         a0[:, 1] * a0[:, 2] * a0[:, 3] + a0[:, 1] * a0[:, 2] * a0[:, 4]\n82         + a0[:, 1] * a0[:, 3] * a0[:, 4] + a0[:, 2] * a0[:, 3] * a0[:, 4])\n83     u1[:, 1] = (\n84         a0[:, 0] * a0[:, 2] * a0[:, 3] + a0[:, 0] * a0[:, 2] * a0[:, 4]\n85         + a0[:, 0] * a0[:, 3] * a0[:, 4] + a0[:, 2] * a0[:, 3] * a0[:, 4])\n86     u1[:, 2] = (\n87         a0[:, 0] * a0[:, 1] * a0[:, 3] + a0[:, 0] * a0[:, 1] * a0[:, 4]\n88         + a0[:, 0] * a0[:, 3] * a0[:, 4] + a0[:, 1] * a0[:, 3] * a0[:, 4])\n89     u1[:, 3] = (\n90         a0[:, 0] * a0[:, 1] * a0[:, 2] + a0[:, 0] * a0[:, 1] * a0[:, 4]\n91         + a0[:, 0] * a0[:, 2] * a0[:, 4] + a0[:, 1] * a0[:, 2] * a0[:, 4])\n92     u1[:, 4] = (\n93         a0[:, 0] * a0[:, 1] * a0[:, 2] + a0[:, 0] * a0[:, 1] * a0[:, 3]\n94         + a0[:, 0] * a0[:, 2] * a0[:, 3] + a0[:, 1] * a0[:, 2] * a0[:, 3])\n95 \n96     left[:, 0] = (a0[:, 0] - a0[:, 1]) * (a0[:, 0] - a0[:, 2]) * \\\n97         (a0[:, 0] - a0[:, 3]) * (a0[:, 0] - a0[:, 4])\n98     left[:, 1] = (a0[:, 1] - a0[:, 0]) * (a0[:, 1] - a0[:, 2]) * \\\n99         (a0[:, 1] - a0[:, 3]) * (a0[:, 1] - a0[:, 4])\n100     left[:, 2] = (a0[:, 2] - a0[:, 0]) * (a0[:, 2] - a0[:, 1]) * \\\n101         (a0[:, 2] - a0[:, 3]) * (a0[:, 2] - a0[:, 4])\n102     left[:, 3] = (a0[:, 3] - a0[:, 0]) * (a0[:, 3] - a0[:, 1]) * \\\n103         (a0[:, 3] - a0[:, 2]) * (a0[:, 3] - a0[:, 4])\n104     left[:, 4] = (a0[:, 4] - a0[:, 0]) * (a0[:, 4] - a0[:, 1]) * \\\n105         (a0[:, 4] - a0[:, 2]) * (a0[:, 4] - a0[:, 3])\n106 \n107     df[2:-2] = np.sum(-(u1 / left) * ff, axis=1)\n108 \n109     # second derivative\n110     u2[:, 0] = (\n111         a0[:, 1] * a0[:, 2] + a0[:, 1] * a0[:, 3] + a0[:, 1] * a0[:, 4]\n112         + a0[:, 2] * a0[:, 3] + a0[:, 2] * a0[:, 4] + a0[:, 3] * a0[:, 4])\n113     u2[:, 1] = (\n114         a0[:, 0] * a0[:, 2] + a0[:, 0] * a0[:, 3] + a0[:, 0] * a0[:, 4]\n115         + a0[:, 2] * a0[:, 3] + a0[:, 2] * a0[:, 4] + a0[:, 3] * a0[:, 4])\n116     u2[:, 2] = (\n117         a0[:, 0] * a0[:, 1] + a0[:, 0] * a0[:, 3] + a0[:, 0] * a0[:, 4]\n118         + a0[:, 1] * a0[:, 3] + a0[:, 1] * a0[:, 3] + a0[:, 3] * a0[:, 4])\n119     u2[:, 3] = (\n120         a0[:, 0] * a0[:, 1] + a0[:, 0] * a0[:, 2] + a0[:, 0] * a0[:, 4]\n121         + a0[:, 1] * a0[:, 2] + a0[:, 1] * a0[:, 4] + a0[:, 2] * a0[:, 4])\n122     u2[:, 4] = (\n123         a0[:, 0] * a0[:, 1] + a0[:, 0] * a0[:, 2] + a0[:, 0] * a0[:, 3]\n124         + a0[:, 1] * a0[:, 2] + a0[:, 1] * a0[:, 4] + a0[:, 2] * a0[:, 3])\n125 \n126     df2[2:-2] = 2. * np.sum(u2 * ff, axis=1)\n127     return df, df2\n128 \n129 \n130 def rectify_iv_curve(voltage, current, decimals=None):\n131     \"\"\"\n132     Sort the IV curve data, remove NaNs and negative\n133     values, and combine points with duplicate voltage.\n134 \n135     Parameters\n136     ----------\n137     voltage : numeric [V]\n138     current : numeric [A]\n139     decimals : int or None, default None\n140         number of decimal places to which voltage is rounded to remove\n141         duplicated points. If None, no rounding is done.\n142 \n143     Returns\n144     -------\n145     voltage : numeric [V]\n146     current : numeric [A]\n147 \n148     Raises\n149     ------\n150     ValueError if voltage and current are different length\n151 \n152     Notes\n153     -----\n154     ``rectify_IV_curve`` ensures that the IV curve lies in the first quadrant\n155     of the (voltage, current) plane. The returned IV curve:\n156         * increases in voltage\n157         * contains no negative current or voltage values\n158         * contains no NaNs\n159         * contains no points with duplicate voltage values. Where voltage\n160           values are repeated, a single data point is substituted with current\n161           equal to the average of current at duplicated voltages.\n162     \"\"\"\n163 \n164     df = pd.DataFrame(data=np.vstack((voltage, current)).T, columns=['v', 'i'])\n165     # restrict to first quadrant\n166     df.dropna(inplace=True)\n167     df = df[(df['v'] >= 0) & (df['i'] >= 0)]\n168     # sort pairs on voltage, then current\n169     df = df.sort_values(by=['v', 'i'], ascending=[True, False])\n170 \n171     # eliminate duplicate voltage points\n172     if decimals is not None:\n173         df['v'] = np.round(df['v'], decimals=decimals)\n174 \n175     _, inv = np.unique(df['v'], return_inverse=True)\n176     df.index = inv\n177     # average current at each common voltage\n178     df = df.groupby(by=inv).mean()\n179 \n180     tmp = np.array(df).T\n181     return tmp[0, ], tmp[1, ]\n182 \n183 \n184 def _schumaker_qspline(x, y):\n185     \"\"\"\n186     Fit a quadratic spline which preserves monotonicity and\n187     convexity in the data.\n188 \n189     Parameters\n190     ----------\n191     x : numeric\n192         independent points between which the spline will interpolate.\n193     y : numeric\n194         dependent points between which the spline will interpolate.\n195 \n196     Returns\n197     -------\n198     t : array\n199         an ordered vector of knots, i.e., X values where the spline\n200         changes coefficients. All values in x are used as knots.\n201         The algorithm may insert additional knots between data points in x\n202         where changes in convexity are indicated by the (numerical)\n203         derivative. Consequently len(t) >= len(x).\n204     c : array\n205         a Nx3 matrix of coefficients where the kth row defines the quadratic\n206         interpolant between t_k and t_(k+1), i.e., y = c[i, 0] *\n207         (x - t_k)^2 + c[i, 1] * (x - t_k) + c[i, 2]\n208     yhat : array\n209         y values corresponding to the knots in t. Contains the original\n210         data points, y, and also y values estimated from the spline at the\n211         inserted knots.\n212     kflag : array\n213         a vector of len(t) of logicals, which are set to true for\n214         elements of t that are knots inserted by the algorithm.\n215 \n216     Notes\n217     -----\n218     Algorithm is taken from [1]_, which relies on prior work described in [2]_.\n219     Ported from PVLib Matlab [3]_.\n220 \n221     References\n222     ----------\n223     .. [1] L. L. Schumaker, \"On Shape Preserving Quadratic Spline\n224        Interpolation\", SIAM Journal on Numerical Analysis 20(4), August 1983,\n225        pp 854 - 864\n226     .. [2] M. H. Lam, \"Monotone and Convex Quadratic Spline Interpolation\",\n227        Virginia Journal of Science 41(1), Spring 1990\n228     .. [3] PVLib MATLAB https://github.com/sandialabs/MATLAB_PV_LIB\n229     \"\"\"\n230     # Make sure vectors are 1D arrays\n231     x = x.flatten()\n232     y = y.flatten()\n233 \n234     n = x.size\n235 \n236     # compute various values used by the algorithm: differences, length of line\n237     # segments between data points, and ratios of differences.\n238     delx = np.diff(x)  # delx[i] = x[i + 1] - x[i]\n239     dely = np.diff(y)\n240 \n241     delta = dely / delx\n242 \n243     # Calculate first derivative at each x value per [3]\n244 \n245     s = np.zeros_like(x)\n246 \n247     left = np.append(0.0, delta)\n248     right = np.append(delta, 0.0)\n249 \n250     pdelta = left * right\n251 \n252     u = pdelta > 0\n253 \n254     # [3], Eq. 9 for interior points\n255     # fix tuning parameters in [2], Eq 9 at chi = .5 and eta = .5\n256     s[u] = pdelta[u] / (0.5*left[u] + 0.5*right[u])\n257 \n258     # [3], Eq. 7 for left endpoint\n259     left_end = 2.0 * delta[0] - s[1]\n260     if delta[0] * left_end > 0:\n261         s[0] = left_end\n262 \n263     # [3], Eq. 8 for right endpoint\n264     right_end = 2.0 * delta[-1] - s[-2]\n265     if delta[-1] * right_end > 0:\n266         s[-1] = right_end\n267 \n268     # determine knots. Start with initial points x\n269     # [2], Algorithm 4.1 first 'if' condition of step 5 defines intervals\n270     # which won't get internal knots\n271     tests = s[:-1] + s[1:]\n272     u = np.isclose(tests, 2.0 * delta, atol=EPS)\n273     # u = true for an interval which will not get an internal knot\n274 \n275     k = n + sum(~u)  # total number of knots = original data + inserted knots\n276 \n277     # set up output arrays\n278     # knot locations, first n - 1 and very last (n + k) are original data\n279     xk = np.zeros(k)\n280     yk = np.zeros(k)  # function values at knot locations\n281     # logicals that will indicate where additional knots are inserted\n282     flag = np.zeros(k, dtype=bool)\n283     a = np.zeros((k, 3))\n284 \n285     # structures needed to compute coefficients, have to be maintained in\n286     # association with each knot\n287 \n288     tmpx = x[:-1]\n289     tmpy = y[:-1]\n290     tmpx2 = x[1:]\n291     tmps = s[:-1]\n292     tmps2 = s[1:]\n293     diffs = np.diff(s)\n294 \n295     # structure to contain information associated with each knot, used to\n296     # calculate coefficients\n297     uu = np.zeros((k, 6))\n298 \n299     uu[:(n - 1), :] = np.array([tmpx, tmpx2, tmpy, tmps, tmps2, delta]).T\n300 \n301     # [2], Algorithm 4.1 subpart 1 of Step 5\n302     # original x values that are left points of intervals without internal\n303     # knots\n304 \n305     # MATLAB differs from NumPy, boolean indices must be same size as\n306     # array\n307     xk[:(n - 1)][u] = tmpx[u]\n308     yk[:(n - 1)][u] = tmpy[u]\n309     # constant term for each polynomial for intervals without knots\n310     a[:(n - 1), 2][u] = tmpy[u]\n311     a[:(n - 1), 1][u] = s[:-1][u]\n312     a[:(n - 1), 0][u] = 0.5 * diffs[u] / delx[u]  # leading coefficients\n313 \n314     # [2], Algorithm 4.1 subpart 2 of Step 5\n315     # original x values that are left points of intervals with internal knots\n316     xk[:(n-1)][~u] = tmpx[~u]\n317     yk[:(n-1)][~u] = tmpy[~u]\n318 \n319     aa = s[:-1] - delta\n320     b = s[1:] - delta\n321 \n322     sbar = np.zeros(k)\n323     eta = np.zeros(k)\n324     # will contain mapping from the left points of intervals containing an\n325     # added knot to each inverval's internal knot value\n326     xi = np.zeros(k)\n327 \n328     t0 = aa * b >= 0\n329     # first 'else' in Algorithm 4.1 Step 5\n330     v = np.logical_and(~u, t0)  # len(u) == (n - 1) always\n331     q = np.sum(v)  # number of this type of knot to add\n332 \n333     if q > 0.:\n334         xk[(n - 1):(n + q - 1)] = .5 * (tmpx[v] + tmpx2[v])  # knot location\n335         uu[(n - 1):(n + q - 1), :] = np.array([tmpx[v], tmpx2[v], tmpy[v],\n336                                                tmps[v], tmps2[v], delta[v]]).T\n337         xi[:(n-1)][v] = xk[(n - 1):(n + q - 1)]\n338 \n339     t1 = np.abs(aa) > np.abs(b)\n340     w = np.logical_and(~u, ~v)  # second 'else' in Algorithm 4.1 Step 5\n341     w = np.logical_and(w, t1)\n342     r = np.sum(w)\n343 \n344     if r > 0.:\n345         xk[(n + q - 1):(n + q + r - 1)] = tmpx2[w] + aa[w] * delx[w] / diffs[w]\n346         uu[(n + q - 1):(n + q + r - 1), :] = np.array([tmpx[w], tmpx2[w],\n347                                                        tmpy[w], tmps[w],\n348                                                        tmps2[w], delta[w]]).T\n349         xi[:(n - 1)][w] = xk[(n + q - 1):(n + q + r - 1)]\n350 \n351     z = np.logical_and(~u, ~v)  # last 'else' in Algorithm 4.1 Step 5\n352     z = np.logical_and(z, ~w)\n353     ss = np.sum(z)\n354 \n355     if ss > 0.:\n356         xk[(n + q + r - 1):(n + q + r + ss - 1)] = \\\n357             tmpx[z] + b[z] * delx[z] / diffs[z]\n358         uu[(n + q + r - 1):(n + q + r + ss - 1), :] = \\\n359             np.array([tmpx[z], tmpx2[z], tmpy[z], tmps[z], tmps2[z],\n360                       delta[z]]).T\n361         xi[:(n-1)][z] = xk[(n + q + r - 1):(n + q + r + ss - 1)]\n362 \n363     # define polynomial coefficients for intervals with added knots\n364     ff = ~u\n365     sbar[:(n-1)][ff] = (\n366         (2 * uu[:(n - 1), 5][ff] - uu[:(n-1), 4][ff])\n367         + (uu[:(n - 1), 4][ff] - uu[:(n-1), 3][ff])\n368         * (xi[:(n - 1)][ff] - uu[:(n-1), 0][ff])\n369         / (uu[:(n - 1), 1][ff] - uu[:(n-1), 0][ff]))\n370     eta[:(n-1)][ff] = (\n371         (sbar[:(n - 1)][ff] - uu[:(n-1), 3][ff])\n372         / (xi[:(n - 1)][ff] - uu[:(n-1), 0][ff]))\n373 \n374     sbar[(n - 1):(n + q + r + ss - 1)] = \\\n375         (2 * uu[(n - 1):(n + q + r + ss - 1), 5] -\n376          uu[(n - 1):(n + q + r + ss - 1), 4]) + \\\n377         (uu[(n - 1):(n + q + r + ss - 1), 4] -\n378          uu[(n - 1):(n + q + r + ss - 1), 3]) * \\\n379         (xk[(n - 1):(n + q + r + ss - 1)] -\n380          uu[(n - 1):(n + q + r + ss - 1), 0]) / \\\n381         (uu[(n - 1):(n + q + r + ss - 1), 1] -\n382          uu[(n - 1):(n + q + r + ss - 1), 0])\n383     eta[(n - 1):(n + q + r + ss - 1)] = \\\n384         (sbar[(n - 1):(n + q + r + ss - 1)] -\n385          uu[(n - 1):(n + q + r + ss - 1), 3]) / \\\n386         (xk[(n - 1):(n + q + r + ss - 1)] -\n387          uu[(n - 1):(n + q + r + ss - 1), 0])\n388 \n389     # constant term for polynomial for intervals with internal knots\n390     a[:(n - 1), 2][~u] = uu[:(n - 1), 2][~u]\n391     a[:(n - 1), 1][~u] = uu[:(n - 1), 3][~u]\n392     a[:(n - 1), 0][~u] = 0.5 * eta[:(n - 1)][~u]  # leading coefficient\n393 \n394     a[(n - 1):(n + q + r + ss - 1), 2] = \\\n395         uu[(n - 1):(n + q + r + ss - 1), 2] + \\\n396         uu[(n - 1):(n + q + r + ss - 1), 3] * \\\n397         (xk[(n - 1):(n + q + r + ss - 1)] -\n398          uu[(n - 1):(n + q + r + ss - 1), 0]) + \\\n399         .5 * eta[(n - 1):(n + q + r + ss - 1)] * \\\n400         (xk[(n - 1):(n + q + r + ss - 1)] -\n401          uu[(n - 1):(n + q + r + ss - 1), 0]) ** 2.\n402     a[(n - 1):(n + q + r + ss - 1), 1] = sbar[(n - 1):(n + q + r + ss - 1)]\n403     a[(n - 1):(n + q + r + ss - 1), 0] = \\\n404         .5 * (uu[(n - 1):(n + q + r + ss - 1), 4] -\n405               sbar[(n - 1):(n + q + r + ss - 1)]) / \\\n406         (uu[(n - 1):(n + q + r + ss - 1), 1] -\n407          uu[(n - 1):(n + q + r + ss - 1), 0])\n408 \n409     yk[(n - 1):(n + q + r + ss - 1)] = a[(n - 1):(n + q + r + ss - 1), 2]\n410 \n411     xk[n + q + r + ss - 1] = x[n - 1]\n412     yk[n + q + r + ss - 1] = y[n - 1]\n413     flag[(n - 1):(n + q + r + ss - 1)] = True  # these are all inserted knots\n414 \n415     tmp = np.vstack((xk, a.T, yk, flag)).T\n416     # sort output in terms of increasing x (original plus added knots)\n417     tmp2 = tmp[tmp[:, 0].argsort(kind='mergesort')]\n418 \n419     t = tmp2[:, 0]\n420     outn = len(t)\n421     c = tmp2[0:(outn - 1), 1:4]\n422     yhat = tmp2[:, 4]\n423     kflag = tmp2[:, 5]\n424     return t, c, yhat, kflag\n425 \n[end of pvlib/ivtools/utility.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/ivtools/sdm.py\n+++ b/pvlib/ivtools/sdm.py\n@@ -1,5 +1,5 @@\n \"\"\"\n The ``sdm`` module contains functions to fit single diode models.\n \n Function names should follow the pattern \"fit_\" + name of model + \"_\" +\n- fitting method.\n+ fitting method.\n \"\"\"\n \n@@ -9,7 +9,7 @@\n \n from pvlib.pvsystem import singlediode, v_from_i\n from pvlib.ivtools.utility import constants, rectify_iv_curve, _numdiff\n-from pvlib.ivtools.sde import _fit_sandia_cocontent\n+from pvlib.ivtools.sde import _fit_sandia_cocontent, _sandia_simple_params\n \n \n def fit_cec_sam(celltype, v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc,\n@@ -110,6 +110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -236,6 +237,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -307,6 +309,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -370,6 +373,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -440,6 +444,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -511,6 +516,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -580,6 +586,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -650,6 +657,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -720,6 +728,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -790,6 +799,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -860,6 +870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -930,6 +941,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1000,6 +1012,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1070,6 +1083,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1140,6 +1154,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1210,6 +1225,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1280,6 +1296,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1350,6 +1367,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1420,6 +1438,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1490,6 +1509,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1560,6 +1580,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1630,6 +1651,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1700,6 +1722,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1770,6 +1793,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1840,6 +1864,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1910,6 +1935,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1980,6 +2006,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2050,6 +2077,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2120,6 +2148,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2190,6 +2219,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2260,6 +2290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2330,6 +2361,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2400,6 +2432,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2470,6 +2503,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2540,6 +2584,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2610,6 +2655,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2680,6 +2726,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2750,6 +2797,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2820,6 +2870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2890,6 +2941,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2960,6 +3012,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3030,6 +3083,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3100,6 +3154,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3170,6 +3225,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3240,6 +3306,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3310,6 +3387,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3380,6 +3460,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3450,6 +3531,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3520,6 +3602,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3590,6 +3683,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3660,6 +3754,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3730,6 +3825,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3800,6 +3906,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3870,6 +3987,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3940,6 +4060,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4010,6 +4131,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4080,6 +4202,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4150,6 +4273,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4220,6 +4345,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4290,6 +4426,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4360,6 +4507,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4430,6 +4588,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4500,6 +4660,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4570,6 +4741,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4640,6 +4822,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4710,6 +4893,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4780,6 +4964,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4850,6 +5035,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4920,6 +5106,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4990,6 +5177,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5060,6 +5248,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5130,6 +5320,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5200,6 +5401,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5270,6 +5472,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5340,6 +5543,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5410,6 +5615,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5480,6 +5686,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5550,6 +5757,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5620,6 +5828,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5690,6 +5900,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5760,6 +5971,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5830,6 +6042,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5900,6 +6113,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5970,6 +6184,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6040,6 +6255,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6110,6 +6326,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6180,6 +6397,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6250,6 +6468,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6320,6 +6540,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6390,6 +6611,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6460,6 +6682,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6530,6 +6753,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6600,6 +6824,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6670,6 +6895,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6740,6 +6966,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6810,6 +7038,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6880,6 +7110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6950,6 +7181,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7020,6 +7252,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7090,6 +7323,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7160,6 +7394,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7230,6 +7465,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7300,6 +7536,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7370,6 +7607,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7440,6 +7678,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7510,6 +7750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7580,6 +7821,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7650,6 +7892,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7720,6 +7963,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7790,6 +8034,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7860,6 +8105,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7930,6 +8176,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8000,6 +8247,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8070,6 +8318,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8140,6 +8390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8210,6 +8461,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8280,6 +8532,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8350,6 +8603,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8420,6 +8674,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8490,6 +8745,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8560,6 +8817,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8630,6 +8898,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8700,6 +8970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8770,6 +9041,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8840,6 +9112,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8910,6 +9184,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8980,6 +9255,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9050,6 +9326,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9120,6 +9398,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9190,6 +9470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9260,6 +9541,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9330,6 +9612,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9400,6 +9683,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9470,6 +9754,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9540,6 +9825,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9610,6 +9896,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9680,6 +9967,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9750,6 +10038,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9820,6 +10109,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9890,6 +10180,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9960,6 +10251,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10030,6 +10322,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10090,6 +10383,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10150,6 +10444,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10210,6 +10505,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10270,6 +10566,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10330,6 +10627,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10390,6 +10688,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10450,6 +10749,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10510,6 +10810,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10570,6 +10871,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10630,6 +10932,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10690,6 +10993,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10750,6 +11054,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10810,6 +11115,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10870,6 +11176,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10930,6 +11237,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10990,6 +11300,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11050,6 +11363,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11110,6 +11426,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11170,6 +11489,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11230,6 +11552,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11290,6 +11615,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11350,6 +11678,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11410,6 +11742,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11470,6 +11806,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11530,6 +11870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11590,6 +11934,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11650,6 +12000,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11710,6 +12066,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11770,6 +12132,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11830,6 +12198,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11890,6 +12264,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11950,6 +12330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12010,6 +12400,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12070,6 +12470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12130,6 +12540,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12190,6 +12610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12250,6 +12680,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12310,6 +12750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12370,6 +12820,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12430,6 +12890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12490,6 +12960,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12550,6 +13030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12610,6 +13100,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12670,6 +13170,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12730,6 +13240,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12790,6 +13310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12850,6 +13380,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12910,6 +13450,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12970,6 +13520,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13030,6 +13590,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13090,6 +13660,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13150,6 +13730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13210,6 +13800,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13270,6 +13870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13330,6 +13940,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13390,6 +14010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13450,6 +14080,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13510,6 +14150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13570,6 +14220,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13630,6 +14290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13690,6 +14360,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13750,6 +14430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13810,6 +14500,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13870,6 +14570,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13930,6 +14640,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13990,6 +14710,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14050,6 +14780,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14110,6 +14850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14170,6 +14920,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14230,6 +14990,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14290,6 +15060,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14350,6 +15130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14410,6 +15200,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14470,6 +15270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14530,6 +15340,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14590,6 +15410,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14650,6 +15480,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14710,6 +15550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14770,6 +15620,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14830,6 +15690,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14890,6 +15760,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14950,6 +15830,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15010,6 +15890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15070,6 +15950,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15130,6 +16010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15190,6 +16070,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15250,6 +16130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15310,6 +16190,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15370,6 +16250,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15430,6 +16310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15490,6 +16370,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15550,6 +16430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15610,6 +16490,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15670,6 +16550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15730,6 +16610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15790,6 +16670,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15850,6 +16730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15910,6 +16790,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15970,6 +16850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16030,6 +16910,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16090,6 +16970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16150,6 +17030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16210,6 +17090,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16270,6 +17150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16330,6 +17210,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16390,6 +17270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16450,6 +17330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16510,6 +17390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16570,6 +17450,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16630,6 +17510,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16690,6 +17570,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16750,6 +17630,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16810,6 +17690,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16870,6 +17750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16930,6 +17810,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16990,6 +17870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17050,6 +17930,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17110,6 +17990,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17170,6 +18050,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17230,6 +18110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17290,6 +18170,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17350,6 +18230,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17410,6 +18290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17470,6 +18350,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17530,6 +18410,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17590,6 +18470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17650,6 +18530,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17710,6 +18590,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17770,6 +18650,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17830,6 +18710,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17890,6 +18770,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17950,6 +18830,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18010,6 +18890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18070,6 +18950,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18130,6 +19010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18190,6 +19070,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18250,6 +19130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18310,6 +19190,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18370,6 +19250,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18430,6 +19310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18490,6 +19370,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18550,6 +19430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18610,6 +19490,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18670,6 +19550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18730,6 +19610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18790,6 +19670,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18850,6 +19730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18910,6 +19790,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18970,6 +19850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19030,6 +19910,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19090,6 +19970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19150,6 +20030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19210,6 +20090,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19270,6 +20150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19330,6 +20210,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19390,6 +20270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19450,6 +20330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19510,6 +20390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19570,6 +20450,7 @@\n", "model_patch": "```patch\n--- a/pvlib/ivtools/sdm.py\n+++ b/pvlib/ivtools/sdm.py\n@@ -1,5 +1,5 @@\n \"\"\"\n The ``sdm`` module contains functions to fit single diode models.\n \n Function names should follow the pattern \"fit_\" + name of model + \"_\" +\n- fitting method.\n+ fitting method.\n \"\"\"\n \n@@ -9,7 +9,7 @@\n \n from pvlib.pvsystem import singlediode, v_from_i\n from pvlib.ivtools.utility import constants, rectify_iv_curve, _numdiff\n-from pvlib.ivtools.sde import _fit_sandia_cocontent\n+from pvlib.ivtools.sde import _fit_sandia_cocontent, _sandia_simple_params\n \n \n def fit_cec_sam(celltype, v_mp, i_mp, v_oc, i_sc, alpha_sc, beta_voc,\n@@ -110,6 +110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -236,6 +237,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -307,6 +309,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -370,6 +373,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -440,6 +444,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -511,6 +516,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -580,6 +586,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -650,6 +657,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -720,6 +728,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -790,6 +799,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -860,6 +870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -930,6 +941,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1000,6 +1012,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1070,6 +1083,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1140,6 +1154,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1210,6 +1225,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1280,6 +1296,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1350,6 +1367,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1420,6 +1438,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1490,6 +1509,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1560,6 +1580,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1630,6 +1651,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1700,6 +1722,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1770,6 +1793,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1840,6 +1864,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1910,6 +1935,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -1980,6 +2006,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2050,6 +2077,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2120,6 +2148,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2190,6 +2219,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2260,6 +2290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2330,6 +2361,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2400,6 +2432,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2470,6 +2503,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2540,6 +2584,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2610,6 +2655,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2680,6 +2726,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2750,6 +2797,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2820,6 +2870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2890,6 +2941,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -2960,6 +3012,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3030,6 +3083,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3100,6 +3154,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3170,6 +3225,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3240,6 +3306,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3310,6 +3387,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3380,6 +3460,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3450,6 +3531,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3520,6 +3602,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3590,6 +3683,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3660,6 +3754,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3730,6 +3825,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3800,6 +3906,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3870,6 +3987,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -3940,6 +4060,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4010,6 +4131,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4080,6 +4202,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4150,6 +4273,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4220,6 +4345,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4290,6 +4426,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4360,6 +4507,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4430,6 +4588,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4500,6 +4660,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4570,6 +4741,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4640,6 +4822,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4710,6 +4893,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4780,6 +4964,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4850,6 +5035,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4920,6 +5106,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -4990,6 +5177,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5060,6 +5248,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5130,6 +5320,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5200,6 +5401,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5270,6 +5472,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5340,6 +5543,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5410,6 +5615,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5480,6 +5686,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5550,6 +5757,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5620,6 +5828,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5690,6 +5900,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5760,6 +5971,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5830,6 +6042,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5900,6 +6113,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -5970,6 +6184,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6040,6 +6255,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6110,6 +6326,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6180,6 +6397,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6250,6 +6468,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6320,6 +6540,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6390,6 +6611,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6460,6 +6682,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6530,6 +6753,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6600,6 +6824,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6670,6 +6895,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6740,6 +6966,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6810,6 +7038,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6880,6 +7110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -6950,6 +7181,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7020,6 +7252,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7090,6 +7323,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7160,6 +7394,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7230,6 +7465,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7300,6 +7536,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7370,6 +7607,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7440,6 +7678,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7510,6 +7750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7580,6 +7821,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7650,6 +7892,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7720,6 +7963,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7790,6 +8034,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7860,6 +8105,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -7930,6 +8176,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8000,6 +8247,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8070,6 +8318,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8140,6 +8390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8210,6 +8461,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8280,6 +8532,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8350,6 +8603,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8420,6 +8674,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8490,6 +8745,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8560,6 +8817,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8630,6 +8898,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8700,6 +8970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8770,6 +9041,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8840,6 +9112,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8910,6 +9184,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -8980,6 +9255,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9050,6 +9326,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9120,6 +9398,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9190,6 +9470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9260,6 +9541,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9330,6 +9612,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9400,6 +9683,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9470,6 +9754,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9540,6 +9825,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9610,6 +9896,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9680,6 +9967,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9750,6 +10038,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9820,6 +10109,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9890,6 +10180,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -9960,6 +10251,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10030,6 +10322,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10090,6 +10383,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10150,6 +10444,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10210,6 +10505,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10270,6 +10566,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10330,6 +10627,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10390,6 +10688,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10450,6 +10749,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10510,6 +10810,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10570,6 +10871,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10630,6 +10932,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10690,6 +10993,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10750,6 +11054,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10810,6 +11115,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10870,6 +11176,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10930,6 +11237,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -10990,6 +11300,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11050,6 +11363,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11110,6 +11426,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11170,6 +11489,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11230,6 +11552,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11290,6 +11615,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11350,6 +11678,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11410,6 +11742,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11470,6 +11806,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11530,6 +11870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11590,6 +11934,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11650,6 +12000,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11710,6 +12066,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11770,6 +12132,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11830,6 +12198,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11890,6 +12264,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -11950,6 +12330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12010,6 +12400,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12070,6 +12470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12130,6 +12540,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12190,6 +12610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12250,6 +12680,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12310,6 +12750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12370,6 +12820,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12430,6 +12890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12490,6 +12960,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12550,6 +13030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12610,6 +13100,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12670,6 +13170,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12730,6 +13240,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12790,6 +13310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12850,6 +13380,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12910,6 +13450,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -12970,6 +13520,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13030,6 +13590,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13090,6 +13660,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13150,6 +13730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13210,6 +13800,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13270,6 +13870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13330,6 +13940,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13390,6 +14010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13450,6 +14080,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13510,6 +14150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13570,6 +14220,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13630,6 +14290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13690,6 +14360,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13750,6 +14430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13810,6 +14500,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13870,6 +14570,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13930,6 +14640,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -13990,6 +14710,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14050,6 +14780,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14110,6 +14850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14170,6 +14920,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14230,6 +14990,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14290,6 +15060,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14350,6 +15130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14410,6 +15200,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14470,6 +15270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14530,6 +15340,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14590,6 +15410,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14650,6 +15480,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14710,6 +15550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14770,6 +15620,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14830,6 +15690,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14890,6 +15760,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -14950,6 +15830,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15010,6 +15890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15070,6 +15950,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15130,6 +16010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15190,6 +16070,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15250,6 +16130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15310,6 +16190,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15370,6 +16250,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15430,6 +16310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15490,6 +16370,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15550,6 +16430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15610,6 +16490,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15670,6 +16550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15730,6 +16610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15790,6 +16670,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15850,6 +16730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15910,6 +16790,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -15970,6 +16850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16030,6 +16910,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16090,6 +16970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16150,6 +17030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16210,6 +17090,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16270,6 +17150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16330,6 +17210,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16390,6 +17270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16450,6 +17330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16510,6 +17390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16570,6 +17450,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16630,6 +17510,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16690,6 +17570,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16750,6 +17630,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16810,6 +17690,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16870,6 +17750,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16930,6 +17810,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -16990,6 +17870,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17050,6 +17930,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17110,6 +17990,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17170,6 +18050,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17230,6 +18110,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17290,6 +18170,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17350,6 +18230,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17410,6 +18290,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17470,6 +18350,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17530,6 +18410,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17590,6 +18470,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17650,6 +18530,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17710,6 +18590,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17770,6 +18650,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17830,6 +18710,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17890,6 +18770,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -17950,6 +18830,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18010,6 +18890,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18070,6 +18950,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18130,6 +19010,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18190,6 +19070,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18250,6 +19130,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18310,6 +19190,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18370,6 +19250,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18430,6 +19310,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18490,6 +19370,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18550,6 +19430,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18610,6 +19490,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18670,6 +19550,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18730,6 +19610,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18790,6 +19670,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18850,6 +19730,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18910,6 +19790,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -18970,6 +19850,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19030,6 +19910,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19090,6 +19970,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19150,6 +20030,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19210,6 +20090,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19270,6 +20150,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19330,6 +20210,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19390,6 +20270,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19450,6 +20330,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19510,6 +20390,7 @@\n \n     Returns\n     -------\n     dict with the following elements:\n+        I_L_ref: float\n         I_o_ref: float\n         R_s: float\n         R_sh_ref: float\n@@ -19570,6 +20450,7 @@\n"}
{"instance_id": "pvlib__pvlib-python-1026", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nmake read_crn accomodate bad files\nA couple of issues with our `read_crn` function. \r\n\r\nFirst, the character sequence '\\x00\\x00\\x00\\x00\\x00\\x00' occasionally shows up and trips up pandas. This can be fixed by adding `na_values=['\\x00\\x00\\x00\\x00\\x00\\x00']` to the reader.\r\n\r\nSecond, we try to set the `CRX_VN` column to dtype int, but it occasionally has floats that cannot be coerced. The [documentation](https://www1.ncdc.noaa.gov/pub/data/uscrn/products/subhourly01/README.txt) says it should be treated like a string.\r\n\r\nExample below shows both issues in `'CRNS0101-05-2020-FL_Titusville_7_E.txt'`\r\n\r\n```\r\n92821 20200706 1145 20200706 0645      3  -80.69   28.62    24.5     0.0    151 0    24.7 C 0    94 0 -99.000 -9999.0   990 0   1.23 0\r\n92821 20200706 1150 20200706 0650      3  -80.69   28.62    24.7     0.0    168 0    25.0 C 0    94 0 -99.000 -9999.0   990 0   1.28 0\r\n92821 20200706 1155 20200706 0655      3  -80.69   28.62    24.9     0.0    173 0    25.3 C 0    93 0 -99.000 -9999.0   990 0   1.48 0\r\n92821 20200706 1200 20200706 0700      3  -80.69   28.62    24.9     0.0    190 0    25.5 C 0    93 0 -99.000 -9999.0   990 0   1.57 0\r\n\\x00\\x00\\x00\\x00\\x00\\x00 repeated\r\n92821 20200706 1305 20200706 0805  2.623  -80.69   28.62    26.8     0.0    409 0    30.0 C 0    87 0 -99.000 -9999.0   988 0   1.44 0\r\n92821 20200706 1310 20200706 0810  2.623  -80.69   28.62    26.9     0.0    430 0    30.2 C 0    87 0 -99.000 -9999.0   989 0   1.64 0\r\n92821 20200706 1315 20200706 0815  2.623  -80.69   28.62    27.0     0.0    445 0    30.4 C 0    86 0 -99.000 -9999.0   989 0   1.94 0\r\n92821 20200706 1320 20200706 0820  2.623  -80.69   28.62    27.3     0.0    463 0    30.8 C 0    86 0 -99.000 -9999.0   988 0   1.50 0\r\n92821 20200706 1325 20200706 0825  2.623  -80.69   28.62    27.6     0.0    478 0    31.1 C 0    85 0 -99.000 -9999.0   988 0   1.54 0\r\n92821 20200706 1330 20200706 0830  2.623  -80.69   28.62    27.6     0.0    496 0    31.5 C 0    84 0 -99.000 -9999.0   988 0   1.48 0\r\n```\r\n\r\nfyi @lboeman \n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"https://travis-ci.org/pvlib/pvlib-python\">\n29     <img src=\"https://travis-ci.org/pvlib/pvlib-python.svg?branch=master\" alt=\"travis build status\" />\n30     </a>\n31     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n32     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n33     </a>\n34     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n35       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n36     </a>\n37   </td>\n38 </tr>\n39 <tr>\n40   <td>Code Quality</td>\n41  \u00a0<td>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n43     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n44     </a>\n45     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n46     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n47     </a>\n48   </td>\n49 </tr>\n50 <tr>\n51   <td>Coverage</td>\n52  \u00a0<td>\n53     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n54     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n55     </a>\n56     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n57     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n58     </a>\n59   </td>\n60 </tr>\n61 <tr>\n62   <td>Publications</td>\n63   <td>\n64     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n65     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n66     </a>\n67     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n68     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n69     </a>\n70   </td>\n71 </tr>\n72 <tr>\n73   <td>Downloads</td>\n74   <td>\n75     <a href=\"https://pypi.org/project/pvlib/\">\n76     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n77     </a>\n78     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n79     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n80     </a>\n81   </td>\n82 </tr>\n83 </table>\n84 \n85 \n86 pvlib python is a community supported tool that provides a set of\n87 functions and classes for simulating the performance of photovoltaic\n88 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n89 toolbox developed at Sandia National Laboratories and it implements many\n90 of the models and methods developed at the Labs. More information on\n91 Sandia Labs PV performance modeling programs can be found at\n92 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n93 but operate independently of it.\n94 \n95 \n96 Documentation\n97 =============\n98 \n99 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n100 \n101 \n102 Installation\n103 ============\n104 \n105 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n106 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n107 \n108 pvlib-python is compatible with Python 3.5 and above.\n109 \n110 **Python 2.7 support ended on June 1, 2019, with pvlib-python 0.6.3.**\n111 \n112 \n113 Contributing\n114 ============\n115 \n116 We need your help to make pvlib-python a great tool!\n117 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n118 The long-term success of pvlib-python requires substantial community support.\n119 \n120 \n121 License\n122 =======\n123 \n124 BSD 3-clause\n125 \n126 \n127 Getting support\n128 ===============\n129 \n130 pvlib usage questions can be asked on\n131 [Stack Overflow](http://stackoverflow.com) and tagged with\n132 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n133 \n134 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n135 is used for discussing various topics of interest to the pvlib-python\n136 community. We also make new version announcements on the google group.\n137 \n138 If you suspect that you may have discovered a bug or if you'd like to\n139 change something about pvlib, then please make an issue on our\n140 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n141 \n142 \n143 Citing\n144 ======\n145 \n146 If you use pvlib-python in a published work, please cite:\n147 \n148   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n149   \"pvlib python: a python package for modeling solar energy systems.\"\n150   Journal of Open Source Software, 3(29), 884, (2018).\n151   https://doi.org/10.21105/joss.00884\n152 \n153 Please also cite the DOI corresponding to the specific version of\n154 pvlib-python that you used. pvlib-python DOIs are listed at\n155 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n156 \n157 NumFOCUS\n158 ========\n159 \n160 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n161 \n162 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n163 \n[end of README.md]\n[start of pvlib/iotools/crn.py]\n1 \"\"\"Functions to read data from the US Climate Reference Network (CRN).\n2 \"\"\"\n3 \n4 import pandas as pd\n5 import numpy as np\n6 \n7 \n8 HEADERS = (\n9     'WBANNO UTC_DATE UTC_TIME LST_DATE LST_TIME CRX_VN LONGITUDE LATITUDE '\n10     'AIR_TEMPERATURE PRECIPITATION SOLAR_RADIATION SR_FLAG '\n11     'SURFACE_TEMPERATURE ST_TYPE ST_FLAG RELATIVE_HUMIDITY RH_FLAG '\n12     'SOIL_MOISTURE_5 SOIL_TEMPERATURE_5 WETNESS WET_FLAG WIND_1_5 WIND_FLAG'\n13 )\n14 \n15 VARIABLE_MAP = {\n16     'LONGITUDE': 'longitude',\n17     'LATITUDE': 'latitude',\n18     'AIR_TEMPERATURE': 'temp_air',\n19     'SOLAR_RADIATION': 'ghi',\n20     'SR_FLAG': 'ghi_flag',\n21     'RELATIVE_HUMIDITY': 'relative_humidity',\n22     'RH_FLAG': 'relative_humidity_flag',\n23     'WIND_1_5': 'wind_speed',\n24     'WIND_FLAG': 'wind_speed_flag'\n25 }\n26 \n27 # as specified in CRN README.txt file. excludes 1 space between columns\n28 WIDTHS = [5, 8, 4, 8, 4, 6, 7, 7, 7, 7, 6, 1, 7, 1, 1, 5, 1, 7, 7, 5, 1, 6, 1]\n29 # add 1 to make fields contiguous (required by pandas.read_fwf)\n30 WIDTHS = [w + 1 for w in WIDTHS]\n31 # no space after last column\n32 WIDTHS[-1] -= 1\n33 \n34 # specify dtypes for potentially problematic values\n35 DTYPES = [\n36     'int64', 'int64', 'int64', 'int64', 'int64', 'int64', 'float64', 'float64',\n37     'float64', 'float64', 'float64', 'int64', 'float64', 'O', 'int64',\n38     'float64', 'int64', 'float64', 'float64', 'int64', 'int64', 'float64',\n39     'int64'\n40 ]\n41 \n42 \n43 def read_crn(filename):\n44     \"\"\"\n45     Read a NOAA USCRN fixed-width file into pandas dataframe.  The CRN is\n46     described in [1]_ and [2]_.\n47 \n48     Parameters\n49     ----------\n50     filename: str, path object, or file-like\n51         filepath or url to read for the fixed-width file.\n52 \n53     Returns\n54     -------\n55     data: Dataframe\n56         A dataframe with DatetimeIndex and all of the variables in the\n57         file.\n58 \n59     Notes\n60     -----\n61     CRN files contain 5 minute averages labeled by the interval ending\n62     time. Here, missing data is flagged as NaN, rather than the lowest\n63     possible integer for a field (e.g. -999 or -99). Air temperature in\n64     deg C. Wind speed in m/s at a height of 1.5 m above ground level.\n65 \n66     Variables corresponding to standard pvlib variables are renamed,\n67     e.g. `SOLAR_RADIATION` becomes `ghi`. See the\n68     `pvlib.iotools.crn.VARIABLE_MAP` dict for the complete mapping.\n69 \n70     References\n71     ----------\n72     .. [1] U.S. Climate Reference Network\n73        `https://www.ncdc.noaa.gov/crn/qcdatasets.html\n74        <https://www.ncdc.noaa.gov/crn/qcdatasets.html>`_\n75 \n76     .. [2] Diamond, H. J. et. al., 2013: U.S. Climate Reference Network\n77        after one decade of operations: status and assessment. Bull.\n78        Amer. Meteor. Soc., 94, 489-498. :doi:`10.1175/BAMS-D-12-00170.1`\n79     \"\"\"\n80 \n81     # read in data\n82     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n83                        widths=WIDTHS)\n84     # loop here because dtype kwarg not supported in read_fwf until 0.20\n85     for (col, _dtype) in zip(data.columns, DTYPES):\n86         data[col] = data[col].astype(_dtype)\n87 \n88     # set index\n89     # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n90     # with %H%M format\n91     dts = data[['UTC_DATE', 'UTC_TIME']].astype(str)\n92     dtindex = pd.to_datetime(dts['UTC_DATE'] + dts['UTC_TIME'].str.zfill(4),\n93                              format='%Y%m%d%H%M', utc=True)\n94     data = data.set_index(dtindex)\n95     try:\n96         # to_datetime(utc=True) does not work in older versions of pandas\n97         data = data.tz_localize('UTC')\n98     except TypeError:\n99         pass\n100 \n101     # set nans\n102     for val in [-99, -999, -9999]:\n103         data = data.where(data != val, np.nan)\n104 \n105     data = data.rename(columns=VARIABLE_MAP)\n106 \n107     return data\n108 \n[end of pvlib/iotools/crn.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/iotools/crn.py\n+++ b/pvlib/iotools/crn.py\n@@ -82,7 +82,8 @@\n                        widths=WIDTHS)\n-     # loop here because dtype kwarg not supported in read_fwf until 0.20\n+     # Add na_values to handle bad files\n+     # Loop here because dtype kwarg not supported in read_fwf until 0.20\n     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n-                        widths=WIDTHS)\n+                        widths=WIDTHS, na_values=['\\x00\\x00\\x00\\x00\\x00\\x00'])\n \n     for (col, _dtype) in zip(data.columns, DTYPES):\n         if col == 'CRX_VN':\n-            data[col] = data[col].astype(str)  # Treat CRX_VN as string\n+            data[col] = data[col].astype(str)  # Treat CRX_VN as string\n         else:\n             data[col] = data[col].astype(_dtype)\n \n```", "model_patch": "--- a/pvlib/iotools/crn.py\n+++ b/pvlib/iotools/crn.py\n@@ -82,7 +82,8 @@\n                        widths=WIDTHS)\n-     # loop here because dtype kwarg not supported in read_fwf until 0.20\n+     # Add na_values to handle bad files\n+     # Loop here because dtype kwarg not supported in read_fwf until 0.20\n     data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n-                        widths=WIDTHS)\n+                        widths=WIDTHS, na_values=['\\x00\\x00\\x00\\x00\\x00\\x00'])\n \n     for (col, _dtype) in zip(data.columns, DTYPES):\n         if col == 'CRX_VN':\n-            data[col] = data[col].astype(str)  # Treat CRX_VN as string\n+            data[col] = data[col].astype(str)  # Treat CRX_VN as string\n         else:\n             data[col] = data[col].astype(_dtype)\n \n"}
{"instance_id": "pvlib__pvlib-python-1176", "model_name_or_path": "gpt-4o-mini", "text": "You will be provided with a partial code base and an issue statement explaining a problem to resolve.\n<issue>\nmake Array play nicely with fixed tilt systems and trackers\n#1076 is adding an `Array` class that largely describes a fixed-tilt array. However, the composition logic of `PVSystem: def __init__(arrays,...)` combined with the inheritance logic of `SingleAxisTracker(PVSystem)` makes for an odd combination of `Array` objects within `SingleAxisTrackers`. See, for example, https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539704316. \r\n\r\nIn https://github.com/pvlib/pvlib-python/pull/1076#discussion_r539686448 I proposed roughly:\r\n\r\nSplit the `Array` into `BaseArray`, `FixedTiltArray(BaseArray)`, `SingleAxisTrackingArray(BaseArray)`? Basic idea:\r\n\r\n```python\r\nclass FixedTiltArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    surface_tilt: float or array-like, default 0\r\n        Surface tilt angles in decimal degrees.\r\n        The tilt angle is defined as degrees from horizontal\r\n        (e.g. surface facing up = 0, surface facing horizon = 90)\r\n\r\n    surface_azimuth: float or array-like, default 180\r\n        Azimuth angle of the module surface.\r\n        North=0, East=90, South=180, West=270.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n\r\n\r\n# could be in pvsystem.py (module is gradually becoming just the objects) or could be in tracking.py\r\nclass SingleAxisTrackerArray(BaseArray)\r\n    \"\"\"\r\n    Parameters\r\n    ----------\r\n    axis_tilt : float, default 0\r\n        The tilt of the axis of rotation (i.e, the y-axis defined by\r\n        axis_azimuth) with respect to horizontal, in decimal degrees.\r\n\r\n    etc.\r\n\r\n    **kwargs\r\n        Passed to Array. Or copy remainder of Array doc string to be explicit.\r\n    \"\"\"\r\n```\r\n\r\nI believe the only major challenge is that the `get_aoi` and `get_irradiance` methods would either need to differ in signature (as they do now, and thus present a challenge to a `PVSystem` wrapper) or in implementation (tracker methods would include a call to `singleaxis`, and thus would be less efficient in some workflows). @wfvining suggests that the consistent signature is more important and I'm inclined to agree.\r\n\r\nWe'd also deprecate the old `SingleAxisTracking` class.\r\n\r\nWe should resolve this issue before releasing the new Array code into the wild in 0.9.\n\n</issue>\n<code>\n[start of README.md]\n1 <img src=\"docs/sphinx/source/_images/pvlib_logo_horiz.png\" width=\"600\">\n2 \n3 <table>\n4 <tr>\n5   <td>Latest Release</td>\n6   <td>\n7     <a href=\"https://pypi.org/project/pvlib/\">\n8     <img src=\"https://img.shields.io/pypi/v/pvlib.svg\" alt=\"latest release\" />\n9     </a>\n10     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n11     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/version.svg\" />\n12     </a>\n13     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n14     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/latest_release_date.svg\" />\n15     </a>\n16 </tr>\n17 <tr>\n18   <td>License</td>\n19   <td>\n20     <a href=\"https://github.com/pvlib/pvlib-python/blob/master/LICENSE\">\n21     <img src=\"https://img.shields.io/pypi/l/pvlib.svg\" alt=\"license\" />\n22     </a>\n23 </td>\n24 </tr>\n25 <tr>\n26   <td>Build Status</td>\n27   <td>\n28     <a href=\"http://pvlib-python.readthedocs.org/en/stable/\">\n29     <img src=\"https://readthedocs.org/projects/pvlib-python/badge/?version=stable\" alt=\"documentation build status\" />\n30     </a>\n31     <a href=\"https://dev.azure.com/solararbiter/pvlib%20python/_build/latest?definitionId=4&branchName=master\">\n32       <img src=\"https://dev.azure.com/solararbiter/pvlib%20python/_apis/build/status/pvlib.pvlib-python?branchName=master\" alt=\"Azure Pipelines build status\" />\n33     </a>\n34   </td>\n35 </tr>\n36 <tr>\n37   <td>Code Quality</td>\n38  \u00a0<td>\n39     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/context:python\">\n40     <img src=\"https://img.shields.io/lgtm/grade/python/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm quality grade\" />\n41     </a>\n42     <a href=\"https://lgtm.com/projects/g/pvlib/pvlib-python/alerts\">\n43     <img src=\"https://img.shields.io/lgtm/alerts/g/pvlib/pvlib-python.svg?logo=lgtm&logoWidth=18\" alt=\"lgtm alters\" />\n44     </a>\n45   </td>\n46 </tr>\n47 <tr>\n48   <td>Coverage</td>\n49  \u00a0<td>\n50     <a href=\"https://coveralls.io/r/pvlib/pvlib-python\">\n51     <img src=\"https://img.shields.io/coveralls/pvlib/pvlib-python.svg\" alt=\"coveralls coverage\" />\n52     </a>\n53     <a href=\"https://codecov.io/gh/pvlib/pvlib-python\">\n54     <img src=\"https://codecov.io/gh/pvlib/pvlib-python/branch/master/graph/badge.svg\" alt=\"codecov coverage\" />\n55     </a>\n56   </td>\n57 </tr>\n58 <tr>\n59   <td>Publications</td>\n60   <td>\n61     <a href=\"https://doi.org/10.5281/zenodo.3762635\">\n62     <img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3762635.svg\" alt=\"zenodo reference\">\n63     </a>\n64     <a href=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1\">\n65     <img src=\"http://joss.theoj.org/papers/41187535cad22dd4b076c89b72f874b1/status.svg\" alt=\"JOSS reference\" />\n66     </a>\n67   </td>\n68 </tr>\n69 <tr>\n70   <td>Downloads</td>\n71   <td>\n72     <a href=\"https://pypi.org/project/pvlib/\">\n73     <img src=\"https://img.shields.io/pypi/dm/pvlib\" alt=\"PyPI downloads\" />\n74     </a>\n75     <a href=\"https://anaconda.org/conda-forge/pvlib-python\">\n76     <img src=\"https://anaconda.org/conda-forge/pvlib-python/badges/downloads.svg\" alt=\"conda-forge downloads\" />\n77     </a>\n78   </td>\n79 </tr>\n80 </table>\n81 \n82 \n83 pvlib python is a community supported tool that provides a set of\n84 functions and classes for simulating the performance of photovoltaic\n85 energy systems. pvlib python was originally ported from the PVLIB MATLAB\n86 toolbox developed at Sandia National Laboratories and it implements many\n87 of the models and methods developed at the Labs. More information on\n88 Sandia Labs PV performance modeling programs can be found at\n89 https://pvpmc.sandia.gov/. We collaborate with the PVLIB MATLAB project,\n90 but operate independently of it.\n91 \n92 \n93 Documentation\n94 =============\n95 \n96 Full documentation can be found at [readthedocs](http://pvlib-python.readthedocs.io/en/stable/).\n97 \n98 \n99 Installation\n100 ============\n101 \n102 pvlib-python releases may be installed using the ``pip`` and ``conda`` tools.\n103 Please see the [Installation page](http://pvlib-python.readthedocs.io/en/stable/installation.html) of the documentation for complete instructions.\n104 \n105 \n106 Contributing\n107 ============\n108 \n109 We need your help to make pvlib-python a great tool!\n110 Please see the [Contributing page](http://pvlib-python.readthedocs.io/en/stable/contributing.html) for more on how you can contribute.\n111 The long-term success of pvlib-python requires substantial community support.\n112 \n113 \n114 License\n115 =======\n116 \n117 BSD 3-clause\n118 \n119 \n120 Getting support\n121 ===============\n122 \n123 pvlib usage questions can be asked on\n124 [Stack Overflow](http://stackoverflow.com) and tagged with\n125 the [pvlib](http://stackoverflow.com/questions/tagged/pvlib) tag.\n126 \n127 The [pvlib-python google group](https://groups.google.com/forum/#!forum/pvlib-python)\n128 is used for discussing various topics of interest to the pvlib-python\n129 community. We also make new version announcements on the google group.\n130 \n131 If you suspect that you may have discovered a bug or if you'd like to\n132 change something about pvlib, then please make an issue on our\n133 [GitHub issues page](https://github.com/pvlib/pvlib-python/issues).\n134 \n135 \n136 Citing\n137 ======\n138 \n139 If you use pvlib-python in a published work, please cite:\n140 \n141   William F. Holmgren, Clifford W. Hansen, and Mark A. Mikofski.\n142   \"pvlib python: a python package for modeling solar energy systems.\"\n143   Journal of Open Source Software, 3(29), 884, (2018).\n144   https://doi.org/10.21105/joss.00884\n145 \n146 Please also cite the DOI corresponding to the specific version of\n147 pvlib-python that you used. pvlib-python DOIs are listed at\n148 [Zenodo.org](https://zenodo.org/search?page=1&size=20&q=conceptrecid:593284&all_versions&sort=-version)\n149 \n150 NumFOCUS\n151 ========\n152 \n153 pvlib python is a [NumFOCUS Affiliated Project](https://numfocus.org/sponsored-projects/affiliated-projects)\n154 \n155 [![NumFocus Affliated Projects](https://i0.wp.com/numfocus.org/wp-content/uploads/2019/06/AffiliatedProject.png)](https://numfocus.org/sponsored-projects/affiliated-projects)\n156 \n[end of README.md]\n[start of pvlib/pvsystem.py]\n1 \"\"\"\n2 The ``pvsystem`` module contains functions for modeling the output and\n3 performance of PV modules and inverters.\n4 \"\"\"\n5 \n6 from collections import OrderedDict\n7 import functools\n8 import io\n9 import itertools\n10 import os\n11 from urllib.request import urlopen\n12 import numpy as np\n13 import pandas as pd\n14 \n15 from pvlib._deprecation import deprecated\n16 \n17 from pvlib import (atmosphere, iam, inverter, irradiance,\n18                    singlediode as _singlediode, temperature)\n19 from pvlib.tools import _build_kwargs, _build_args\n20 \n21 \n22 # a dict of required parameter names for each DC power model\n23 _DC_MODEL_PARAMS = {\n24     'sapm': {\n25         'A0', 'A1', 'A2', 'A3', 'A4', 'B0', 'B1', 'B2', 'B3',\n26         'B4', 'B5', 'C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6',\n27         'C7', 'Isco', 'Impo', 'Voco', 'Vmpo', 'Aisc', 'Aimp', 'Bvoco',\n28         'Mbvoc', 'Bvmpo', 'Mbvmp', 'N', 'Cells_in_Series',\n29         'IXO', 'IXXO', 'FD'},\n30     'desoto': {\n31         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',\n32         'R_sh_ref', 'R_s'},\n33     'cec': {\n34         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',\n35         'R_sh_ref', 'R_s', 'Adjust'},\n36     'pvsyst': {\n37         'gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',\n38         'R_sh_ref', 'R_sh_0', 'R_s', 'alpha_sc', 'EgRef',\n39         'cells_in_series'},\n40     'singlediode': {\n41         'alpha_sc', 'a_ref', 'I_L_ref', 'I_o_ref',\n42         'R_sh_ref', 'R_s'},\n43     'pvwatts': {'pdc0', 'gamma_pdc'}\n44 }\n45 \n46 \n47 def _unwrap_single_value(func):\n48     \"\"\"Decorator for functions that return iterables.\n49 \n50     If the length of the iterable returned by `func` is 1, then\n51     the single member of the iterable is returned. If the length is\n52     greater than 1, then entire iterable is returned.\n53 \n54     Adds 'unwrap' as a keyword argument that can be set to False\n55     to force the return value to be a tuple, regardless of its length.\n56     \"\"\"\n57     @functools.wraps(func)\n58     def f(*args, **kwargs):\n59         unwrap = kwargs.pop('unwrap', True)\n60         x = func(*args, **kwargs)\n61         if unwrap and len(x) == 1:\n62             return x[0]\n63         return x\n64     return f\n65 \n66 \n67 def _check_deprecated_passthrough(func):\n68     \"\"\"\n69     Decorator to warn or error when getting and setting the \"pass-through\"\n70     PVSystem properties that have been moved to Array.  Emits a warning for\n71     PVSystems with only one Array and raises an error for PVSystems with\n72     more than one Array.\n73     \"\"\"\n74 \n75     @functools.wraps(func)\n76     def wrapper(self, *args, **kwargs):\n77         pvsystem_attr = func.__name__\n78         class_name = self.__class__.__name__  # PVSystem or SingleAxisTracker\n79         overrides = {  # some Array attrs aren't the same as PVSystem\n80             'strings_per_inverter': 'strings',\n81         }\n82         array_attr = overrides.get(pvsystem_attr, pvsystem_attr)\n83         alternative = f'{class_name}.arrays[i].{array_attr}'\n84 \n85         if len(self.arrays) > 1:\n86             raise AttributeError(\n87                 f'{class_name}.{pvsystem_attr} not supported for multi-array '\n88                 f'systems. Set {array_attr} for each Array in '\n89                 f'{class_name}.arrays instead.')\n90 \n91         wrapped = deprecated('0.9', alternative=alternative, removal='0.10',\n92                              name=f\"{class_name}.{pvsystem_attr}\")(func)\n93         return wrapped(self, *args, **kwargs)\n94 \n95     return wrapper\n96 \n97 \n98 # not sure if this belongs in the pvsystem module.\n99 # maybe something more like core.py? It may eventually grow to\n100 # import a lot more functionality from other modules.\n101 class PVSystem:\n102     \"\"\"\n103     The PVSystem class defines a standard set of PV system attributes\n104     and modeling functions. This class describes the collection and\n105     interactions of PV system components rather than an installed system\n106     on the ground. It is typically used in combination with\n107     :py:class:`~pvlib.location.Location` and\n108     :py:class:`~pvlib.modelchain.ModelChain`\n109     objects.\n110 \n111     The class supports basic system topologies consisting of:\n112 \n113         * `N` total modules arranged in series\n114           (`modules_per_string=N`, `strings_per_inverter=1`).\n115         * `M` total modules arranged in parallel\n116           (`modules_per_string=1`, `strings_per_inverter=M`).\n117         * `NxM` total modules arranged in `M` strings of `N` modules each\n118           (`modules_per_string=N`, `strings_per_inverter=M`).\n119 \n120     The class is complementary to the module-level functions.\n121 \n122     The attributes should generally be things that don't change about\n123     the system, such the type of module and the inverter. The instance\n124     methods accept arguments for things that do change, such as\n125     irradiance and temperature.\n126 \n127     Parameters\n128     ----------\n129     arrays : iterable of Array, optional\n130         List of arrays that are part of the system. If not specified\n131         a single array is created from the other parameters (e.g.\n132         `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n133         if length of arrays is 0 a ValueError is raised. If `arrays` is\n134         specified the following parameters are ignored:\n135 \n136         - `surface_tilt`\n137         - `surface_azimuth`\n138         - `albedo`\n139         - `surface_type`\n140         - `module`\n141         - `module_type`\n142         - `module_parameters`\n143         - `temperature_model_parameters`\n144         - `modules_per_string`\n145         - `strings_per_inverter`\n146 \n147     surface_tilt: float or array-like, default 0\n148         Surface tilt angles in decimal degrees.\n149         The tilt angle is defined as degrees from horizontal\n150         (e.g. surface facing up = 0, surface facing horizon = 90)\n151 \n152     surface_azimuth: float or array-like, default 180\n153         Azimuth angle of the module surface.\n154         North=0, East=90, South=180, West=270.\n155 \n156     albedo : None or float, default None\n157         The ground albedo. If ``None``, will attempt to use\n158         ``surface_type`` and ``irradiance.SURFACE_ALBEDOS``\n159         to lookup albedo.\n160 \n161     surface_type : None or string, default None\n162         The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n163         for valid values.\n164 \n165     module : None or string, default None\n166         The model name of the modules.\n167         May be used to look up the module_parameters dictionary\n168         via some other method.\n169 \n170     module_type : None or string, default 'glass_polymer'\n171          Describes the module's construction. Valid strings are 'glass_polymer'\n172          and 'glass_glass'. Used for cell and module temperature calculations.\n173 \n174     module_parameters : None, dict or Series, default None\n175         Module parameters as defined by the SAPM, CEC, or other.\n176 \n177     temperature_model_parameters : None, dict or Series, default None.\n178         Temperature model parameters as required by one of the models in\n179         pvlib.temperature (excluding poa_global, temp_air and wind_speed).\n180 \n181     modules_per_string: int or float, default 1\n182         See system topology discussion above.\n183 \n184     strings_per_inverter: int or float, default 1\n185         See system topology discussion above.\n186 \n187     inverter : None or string, default None\n188         The model name of the inverters.\n189         May be used to look up the inverter_parameters dictionary\n190         via some other method.\n191 \n192     inverter_parameters : None, dict or Series, default None\n193         Inverter parameters as defined by the SAPM, CEC, or other.\n194 \n195     racking_model : None or string, default 'open_rack'\n196         Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n197         Used to identify a parameter set for the SAPM cell temperature model.\n198 \n199     losses_parameters : None, dict or Series, default None\n200         Losses parameters as defined by PVWatts or other.\n201 \n202     name : None or string, default None\n203 \n204     **kwargs\n205         Arbitrary keyword arguments.\n206         Included for compatibility, but not used.\n207 \n208     Raises\n209     ------\n210     ValueError\n211         If `arrays` is not None and has length 0.\n212 \n213     See also\n214     --------\n215     pvlib.location.Location\n216     pvlib.tracking.SingleAxisTracker\n217     \"\"\"\n218 \n219     def __init__(self,\n220                  arrays=None,\n221                  surface_tilt=0, surface_azimuth=180,\n222                  albedo=None, surface_type=None,\n223                  module=None, module_type=None,\n224                  module_parameters=None,\n225                  temperature_model_parameters=None,\n226                  modules_per_string=1, strings_per_inverter=1,\n227                  inverter=None, inverter_parameters=None,\n228                  racking_model=None, losses_parameters=None, name=None):\n229 \n230         if arrays is None:\n231             if losses_parameters is None:\n232                 array_losses_parameters = {}\n233             else:\n234                 array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n235                                                         losses_parameters)\n236             self.arrays = (Array(\n237                 surface_tilt,\n238                 surface_azimuth,\n239                 albedo,\n240                 surface_type,\n241                 module,\n242                 module_type,\n243                 module_parameters,\n244                 temperature_model_parameters,\n245                 modules_per_string,\n246                 strings_per_inverter,\n247                 racking_model,\n248                 array_losses_parameters,\n249             ),)\n250         elif len(arrays) == 0:\n251             raise ValueError(\"PVSystem must have at least one Array. \"\n252                              \"If you want to create a PVSystem instance \"\n253                              \"with a single Array pass `arrays=None` and pass \"\n254                              \"values directly to PVSystem attributes, e.g., \"\n255                              \"`surface_tilt=30`\")\n256         else:\n257             self.arrays = tuple(arrays)\n258 \n259         self.inverter = inverter\n260         if inverter_parameters is None:\n261             self.inverter_parameters = {}\n262         else:\n263             self.inverter_parameters = inverter_parameters\n264 \n265         if losses_parameters is None:\n266             self.losses_parameters = {}\n267         else:\n268             self.losses_parameters = losses_parameters\n269 \n270         self.name = name\n271 \n272     def __repr__(self):\n273         repr = f'PVSystem:\\n  name: {self.name}\\n  '\n274         for array in self.arrays:\n275             repr += '\\n  '.join(array.__repr__().split('\\n'))\n276             repr += '\\n  '\n277         repr += f'inverter: {self.inverter}'\n278         return repr\n279 \n280     def _validate_per_array(self, values, system_wide=False):\n281         \"\"\"Check that `values` is a tuple of the same length as\n282         `self.arrays`.\n283 \n284         If `values` is not a tuple it is packed in to a length-1 tuple before\n285         the check. If the lengths are not the same a ValueError is raised,\n286         otherwise the tuple `values` is returned.\n287 \n288         When `system_wide` is True and `values` is not a tuple, `values`\n289         is replicated to a tuple of the same length as `self.arrays` and that\n290         tuple is returned.\n291         \"\"\"\n292         if system_wide and not isinstance(values, tuple):\n293             return (values,) * self.num_arrays\n294         if not isinstance(values, tuple):\n295             values = (values,)\n296         if len(values) != len(self.arrays):\n297             raise ValueError(\"Length mismatch for per-array parameter\")\n298         return values\n299 \n300     @_unwrap_single_value\n301     def _infer_cell_type(self):\n302 \n303         \"\"\"\n304         Examines module_parameters and maps the Technology key for the CEC\n305         database and the Material key for the Sandia database to a common\n306         list of strings for cell type.\n307 \n308         Returns\n309         -------\n310         cell_type: str\n311         \"\"\"\n312         return tuple(array._infer_cell_type() for array in self.arrays)\n313 \n314     @_unwrap_single_value\n315     def get_aoi(self, solar_zenith, solar_azimuth):\n316         \"\"\"Get the angle of incidence on the Array(s) in the system.\n317 \n318         Parameters\n319         ----------\n320         solar_zenith : float or Series.\n321             Solar zenith angle.\n322         solar_azimuth : float or Series.\n323             Solar azimuth angle.\n324 \n325         Returns\n326         -------\n327         aoi : Series or tuple of Series\n328             The angle of incidence\n329         \"\"\"\n330 \n331         return tuple(array.get_aoi(solar_zenith, solar_azimuth)\n332                      for array in self.arrays)\n333 \n334     @_unwrap_single_value\n335     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n336                        dni_extra=None, airmass=None, model='haydavies',\n337                        **kwargs):\n338         \"\"\"\n339         Uses the :py:func:`irradiance.get_total_irradiance` function to\n340         calculate the plane of array irradiance components on a tilted\n341         surface defined by ``self.surface_tilt``,\n342         ``self.surface_azimuth``, and ``self.albedo``.\n343 \n344         Parameters\n345         ----------\n346         solar_zenith : float or Series.\n347             Solar zenith angle.\n348         solar_azimuth : float or Series.\n349             Solar azimuth angle.\n350         dni : float or Series or tuple of float or Series\n351             Direct Normal Irradiance\n352         ghi : float or Series or tuple of float or Series\n353             Global horizontal irradiance\n354         dhi : float or Series or tuple of float or Series\n355             Diffuse horizontal irradiance\n356         dni_extra : None, float or Series, default None\n357             Extraterrestrial direct normal irradiance\n358         airmass : None, float or Series, default None\n359             Airmass\n360         model : String, default 'haydavies'\n361             Irradiance model.\n362 \n363         kwargs\n364             Extra parameters passed to :func:`irradiance.get_total_irradiance`.\n365 \n366         Notes\n367         -----\n368         Each of `dni`, `ghi`, and `dni` parameters may be passed as a tuple\n369         to provide different irradiance for each array in the system. If not\n370         passed as a tuple then the same value is used for input to each Array.\n371         If passed as a tuple the length must be the same as the number of\n372         Arrays.\n373 \n374         Returns\n375         -------\n376         poa_irradiance : DataFrame or tuple of DataFrame\n377             Column names are: ``total, beam, sky, ground``.\n378         \"\"\"\n379         dni = self._validate_per_array(dni, system_wide=True)\n380         ghi = self._validate_per_array(ghi, system_wide=True)\n381         dhi = self._validate_per_array(dhi, system_wide=True)\n382         return tuple(\n383             array.get_irradiance(solar_zenith, solar_azimuth,\n384                                  dni, ghi, dhi,\n385                                  dni_extra, airmass, model,\n386                                  **kwargs)\n387             for array, dni, ghi, dhi in zip(\n388                 self.arrays, dni, ghi, dhi\n389             )\n390         )\n391 \n392     @_unwrap_single_value\n393     def get_iam(self, aoi, iam_model='physical'):\n394         \"\"\"\n395         Determine the incidence angle modifier using the method specified by\n396         ``iam_model``.\n397 \n398         Parameters for the selected IAM model are expected to be in\n399         ``PVSystem.module_parameters``. Default parameters are available for\n400         the 'physical', 'ashrae' and 'martin_ruiz' models.\n401 \n402         Parameters\n403         ----------\n404         aoi : numeric or tuple of numeric\n405             The angle of incidence in degrees.\n406 \n407         aoi_model : string, default 'physical'\n408             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n409             'martin_ruiz' and 'sapm'.\n410         Returns\n411         -------\n412         iam : numeric or tuple of numeric\n413             The AOI modifier.\n414 \n415         Raises\n416         ------\n417         ValueError\n418             if `iam_model` is not a valid model name.\n419         \"\"\"\n420         aoi = self._validate_per_array(aoi)\n421         return tuple(array.get_iam(aoi, iam_model)\n422                      for array, aoi in zip(self.arrays, aoi))\n423 \n424     @_unwrap_single_value\n425     def get_cell_temperature(self, poa_global, temp_air, wind_speed, model,\n426                              effective_irradiance=None):\n427         \"\"\"\n428         Determine cell temperature using the method specified by ``model``.\n429 \n430         Parameters\n431         ----------\n432         poa_global : numeric or tuple of numeric\n433             Total incident irradiance in W/m^2.\n434 \n435         temp_air : numeric or tuple of numeric\n436             Ambient dry bulb temperature in degrees C.\n437 \n438         wind_speed : numeric or tuple of numeric\n439             Wind speed in m/s.\n440 \n441         model : str\n442             Supported models include ``'sapm'``, ``'pvsyst'``,\n443             ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``\n444 \n445         effective_irradiance : numeric or tuple of numeric, optional\n446             The irradiance that is converted to photocurrent in W/m^2.\n447             Only used for some models.\n448 \n449         Returns\n450         -------\n451         numeric or tuple of numeric\n452             Values in degrees C.\n453 \n454         See Also\n455         --------\n456         Array.get_cell_temperature\n457 \n458         Notes\n459         -----\n460         The `temp_air` and `wind_speed` parameters may be passed as tuples\n461         to provide different values for each Array in the system. If passed as\n462         a tuple the length must be the same as the number of Arrays. If not\n463         passed as a tuple then the same value is used for each Array.\n464         \"\"\"\n465         poa_global = self._validate_per_array(poa_global)\n466         temp_air = self._validate_per_array(temp_air, system_wide=True)\n467         wind_speed = self._validate_per_array(wind_speed, system_wide=True)\n468         # Not used for all models, but Array.get_cell_temperature handles it\n469         effective_irradiance = self._validate_per_array(effective_irradiance,\n470                                                         system_wide=True)\n471 \n472         return tuple(\n473             array.get_cell_temperature(poa_global, temp_air, wind_speed,\n474                                        model, effective_irradiance)\n475             for array, poa_global, temp_air, wind_speed, effective_irradiance\n476             in zip(\n477                 self.arrays, poa_global, temp_air, wind_speed,\n478                 effective_irradiance\n479             )\n480         )\n481 \n482     @_unwrap_single_value\n483     def calcparams_desoto(self, effective_irradiance, temp_cell):\n484         \"\"\"\n485         Use the :py:func:`calcparams_desoto` function, the input\n486         parameters and ``self.module_parameters`` to calculate the\n487         module currents and resistances.\n488 \n489         Parameters\n490         ----------\n491         effective_irradiance : numeric or tuple of numeric\n492             The irradiance (W/m2) that is converted to photocurrent.\n493 \n494         temp_cell : float or Series or tuple of float or Series\n495             The average cell temperature of cells within a module in C.\n496 \n497         Returns\n498         -------\n499         See pvsystem.calcparams_desoto for details\n500         \"\"\"\n501         effective_irradiance = self._validate_per_array(effective_irradiance)\n502         temp_cell = self._validate_per_array(temp_cell)\n503 \n504         build_kwargs = functools.partial(\n505             _build_kwargs,\n506             ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n507              'R_s', 'alpha_sc', 'EgRef', 'dEgdT',\n508              'irrad_ref', 'temp_ref']\n509         )\n510 \n511         return tuple(\n512             calcparams_desoto(\n513                 effective_irradiance, temp_cell,\n514                 **build_kwargs(array.module_parameters)\n515             )\n516             for array, effective_irradiance, temp_cell\n517             in zip(self.arrays, effective_irradiance, temp_cell)\n518         )\n519 \n520     @_unwrap_single_value\n521     def calcparams_cec(self, effective_irradiance, temp_cell):\n522         \"\"\"\n523         Use the :py:func:`calcparams_cec` function, the input\n524         parameters and ``self.module_parameters`` to calculate the\n525         module currents and resistances.\n526 \n527         Parameters\n528         ----------\n529         effective_irradiance : numeric or tuple of numeric\n530             The irradiance (W/m2) that is converted to photocurrent.\n531 \n532         temp_cell : float or Series or tuple of float or Series\n533             The average cell temperature of cells within a module in C.\n534 \n535         Returns\n536         -------\n537         See pvsystem.calcparams_cec for details\n538         \"\"\"\n539         effective_irradiance = self._validate_per_array(effective_irradiance)\n540         temp_cell = self._validate_per_array(temp_cell)\n541 \n542         build_kwargs = functools.partial(\n543             _build_kwargs,\n544             ['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n545              'R_s', 'alpha_sc', 'Adjust', 'EgRef', 'dEgdT',\n546              'irrad_ref', 'temp_ref']\n547         )\n548 \n549         return tuple(\n550             calcparams_cec(\n551                 effective_irradiance, temp_cell,\n552                 **build_kwargs(array.module_parameters)\n553             )\n554             for array, effective_irradiance, temp_cell\n555             in zip(self.arrays, effective_irradiance, temp_cell)\n556         )\n557 \n558     @_unwrap_single_value\n559     def calcparams_pvsyst(self, effective_irradiance, temp_cell):\n560         \"\"\"\n561         Use the :py:func:`calcparams_pvsyst` function, the input\n562         parameters and ``self.module_parameters`` to calculate the\n563         module currents and resistances.\n564 \n565         Parameters\n566         ----------\n567         effective_irradiance : numeric or tuple of numeric\n568             The irradiance (W/m2) that is converted to photocurrent.\n569 \n570         temp_cell : float or Series or tuple of float or Series\n571             The average cell temperature of cells within a module in C.\n572 \n573         Returns\n574         -------\n575         See pvsystem.calcparams_pvsyst for details\n576         \"\"\"\n577         effective_irradiance = self._validate_per_array(effective_irradiance)\n578         temp_cell = self._validate_per_array(temp_cell)\n579 \n580         build_kwargs = functools.partial(\n581             _build_kwargs,\n582             ['gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',\n583              'R_sh_ref', 'R_sh_0', 'R_sh_exp',\n584              'R_s', 'alpha_sc', 'EgRef',\n585              'irrad_ref', 'temp_ref',\n586              'cells_in_series']\n587         )\n588 \n589         return tuple(\n590             calcparams_pvsyst(\n591                 effective_irradiance, temp_cell,\n592                 **build_kwargs(array.module_parameters)\n593             )\n594             for array, effective_irradiance, temp_cell\n595             in zip(self.arrays, effective_irradiance, temp_cell)\n596         )\n597 \n598     @_unwrap_single_value\n599     def sapm(self, effective_irradiance, temp_cell):\n600         \"\"\"\n601         Use the :py:func:`sapm` function, the input parameters,\n602         and ``self.module_parameters`` to calculate\n603         Voc, Isc, Ix, Ixx, Vmp, and Imp.\n604 \n605         Parameters\n606         ----------\n607         effective_irradiance : numeric or tuple of numeric\n608             The irradiance (W/m2) that is converted to photocurrent.\n609 \n610         temp_cell : float or Series or tuple of float or Series\n611             The average cell temperature of cells within a module in C.\n612 \n613         Returns\n614         -------\n615         See pvsystem.sapm for details\n616         \"\"\"\n617         effective_irradiance = self._validate_per_array(effective_irradiance)\n618         temp_cell = self._validate_per_array(temp_cell)\n619 \n620         return tuple(\n621             sapm(effective_irradiance, temp_cell, array.module_parameters)\n622             for array, effective_irradiance, temp_cell\n623             in zip(self.arrays, effective_irradiance, temp_cell)\n624         )\n625 \n626     @deprecated('0.9', alternative='PVSystem.get_cell_temperature',\n627                 removal='0.10.0')\n628     def sapm_celltemp(self, poa_global, temp_air, wind_speed):\n629         \"\"\"Uses :py:func:`temperature.sapm_cell` to calculate cell\n630         temperatures.\n631 \n632         Parameters\n633         ----------\n634         poa_global : numeric or tuple of numeric\n635             Total incident irradiance in W/m^2.\n636 \n637         temp_air : numeric or tuple of numeric\n638             Ambient dry bulb temperature in degrees C.\n639 \n640         wind_speed : numeric or tuple of numeric\n641             Wind speed in m/s at a height of 10 meters.\n642 \n643         Returns\n644         -------\n645         numeric or tuple of numeric\n646             values in degrees C.\n647 \n648         Notes\n649         -----\n650         The `temp_air` and `wind_speed` parameters may be passed as tuples\n651         to provide different values for each Array in the system. If not\n652         passed as a tuple then the same value is used for input to each Array.\n653         If passed as a tuple the length must be the same as the number of\n654         Arrays.\n655         \"\"\"\n656         return self.get_cell_temperature(poa_global, temp_air, wind_speed,\n657                                          model='sapm')\n658 \n659     @_unwrap_single_value\n660     def sapm_spectral_loss(self, airmass_absolute):\n661         \"\"\"\n662         Use the :py:func:`sapm_spectral_loss` function, the input\n663         parameters, and ``self.module_parameters`` to calculate F1.\n664 \n665         Parameters\n666         ----------\n667         airmass_absolute : numeric\n668             Absolute airmass.\n669 \n670         Returns\n671         -------\n672         F1 : numeric or tuple of numeric\n673             The SAPM spectral loss coefficient.\n674         \"\"\"\n675         return tuple(\n676             sapm_spectral_loss(airmass_absolute, array.module_parameters)\n677             for array in self.arrays\n678         )\n679 \n680     @_unwrap_single_value\n681     def sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n682                                   airmass_absolute, aoi,\n683                                   reference_irradiance=1000):\n684         \"\"\"\n685         Use the :py:func:`sapm_effective_irradiance` function, the input\n686         parameters, and ``self.module_parameters`` to calculate\n687         effective irradiance.\n688 \n689         Parameters\n690         ----------\n691         poa_direct : numeric or tuple of numeric\n692             The direct irradiance incident upon the module.  [W/m2]\n693 \n694         poa_diffuse : numeric or tuple of numeric\n695             The diffuse irradiance incident on module.  [W/m2]\n696 \n697         airmass_absolute : numeric\n698             Absolute airmass. [unitless]\n699 \n700         aoi : numeric or tuple of numeric\n701             Angle of incidence. [degrees]\n702 \n703         Returns\n704         -------\n705         effective_irradiance : numeric or tuple of numeric\n706             The SAPM effective irradiance. [W/m2]\n707         \"\"\"\n708         poa_direct = self._validate_per_array(poa_direct)\n709         poa_diffuse = self._validate_per_array(poa_diffuse)\n710         aoi = self._validate_per_array(aoi)\n711         return tuple(\n712             sapm_effective_irradiance(\n713                 poa_direct, poa_diffuse, airmass_absolute, aoi,\n714                 array.module_parameters)\n715             for array, poa_direct, poa_diffuse, aoi\n716             in zip(self.arrays, poa_direct, poa_diffuse, aoi)\n717         )\n718 \n719     @deprecated('0.9', alternative='PVSystem.get_cell_temperature',\n720                 removal='0.10.0')\n721     def pvsyst_celltemp(self, poa_global, temp_air, wind_speed=1.0):\n722         \"\"\"Uses :py:func:`temperature.pvsyst_cell` to calculate cell\n723         temperature.\n724 \n725         Parameters\n726         ----------\n727         poa_global : numeric or tuple of numeric\n728             Total incident irradiance in W/m^2.\n729 \n730         temp_air : numeric or tuple of numeric\n731             Ambient dry bulb temperature in degrees C.\n732 \n733         wind_speed : numeric or tuple of numeric, default 1.0\n734             Wind speed in m/s measured at the same height for which the wind\n735             loss factor was determined.  The default value is 1.0, which is\n736             the wind speed at module height used to determine NOCT.\n737 \n738         Returns\n739         -------\n740         numeric or tuple of numeric\n741             values in degrees C.\n742 \n743         Notes\n744         -----\n745         The `temp_air` and `wind_speed` parameters may be passed as tuples\n746         to provide different values for each Array in the system. If not\n747         passed as a tuple then the same value is used for input to each Array.\n748         If passed as a tuple the length must be the same as the number of\n749         Arrays.\n750         \"\"\"\n751         return self.get_cell_temperature(poa_global, temp_air, wind_speed,\n752                                          model='pvsyst')\n753 \n754     @deprecated('0.9', alternative='PVSystem.get_cell_temperature',\n755                 removal='0.10.0')\n756     def faiman_celltemp(self, poa_global, temp_air, wind_speed=1.0):\n757         \"\"\"\n758         Use :py:func:`temperature.faiman` to calculate cell temperature.\n759 \n760         Parameters\n761         ----------\n762         poa_global : numeric or tuple of numeric\n763             Total incident irradiance [W/m^2].\n764 \n765         temp_air : numeric or tuple of numeric\n766             Ambient dry bulb temperature [C].\n767 \n768         wind_speed : numeric or tuple of numeric, default 1.0\n769             Wind speed in m/s measured at the same height for which the wind\n770             loss factor was determined.  The default value 1.0 m/s is the wind\n771             speed at module height used to determine NOCT. [m/s]\n772 \n773         Returns\n774         -------\n775         numeric or tuple of numeric\n776             values in degrees C.\n777 \n778         Notes\n779         -----\n780         The `temp_air` and `wind_speed` parameters may be passed as tuples\n781         to provide different values for each Array in the system. If not\n782         passed as a tuple then the same value is used for input to each Array.\n783         If passed as a tuple the length must be the same as the number of\n784         Arrays.\n785         \"\"\"\n786         return self.get_cell_temperature(poa_global, temp_air, wind_speed,\n787                                          model='faiman')\n788 \n789     @deprecated('0.9', alternative='PVSystem.get_cell_temperature',\n790                 removal='0.10.0')\n791     def fuentes_celltemp(self, poa_global, temp_air, wind_speed):\n792         \"\"\"\n793         Use :py:func:`temperature.fuentes` to calculate cell temperature.\n794 \n795         Parameters\n796         ----------\n797         poa_global : pandas Series or tuple of Series\n798             Total incident irradiance [W/m^2]\n799 \n800         temp_air : pandas Series or tuple of Series\n801             Ambient dry bulb temperature [C]\n802 \n803         wind_speed : pandas Series or tuple of Series\n804             Wind speed [m/s]\n805 \n806         Returns\n807         -------\n808         temperature_cell : Series or tuple of Series\n809             The modeled cell temperature [C]\n810 \n811         Notes\n812         -----\n813         The Fuentes thermal model uses the module surface tilt for convection\n814         modeling. The SAM implementation of PVWatts hardcodes the surface tilt\n815         value at 30 degrees, ignoring whatever value is used for irradiance\n816         transposition. This method defaults to using ``self.surface_tilt``, but\n817         if you want to match the PVWatts behavior, you can override it by\n818         including a ``surface_tilt`` value in ``temperature_model_parameters``.\n819 \n820         The `temp_air` and `wind_speed` parameters may be passed as tuples\n821         to provide different values for each Array in the system. If not\n822         passed as a tuple then the same value is used for input to each Array.\n823         If passed as a tuple the length must be the same as the number of\n824         Arrays.\n825         \"\"\"\n826         return self.get_cell_temperature(poa_global, temp_air, wind_speed,\n827                                          model='fuentes')\n828 \n829     @deprecated('0.9', alternative='PVSystem.get_cell_temperature',\n830                 removal='0.10.0')\n831     def noct_sam_celltemp(self, poa_global, temp_air, wind_speed,\n832                           effective_irradiance=None):\n833         \"\"\"\n834         Use :py:func:`temperature.noct_sam` to calculate cell temperature.\n835 \n836         Parameters\n837         ----------\n838         poa_global : numeric or tuple of numeric\n839             Total incident irradiance in W/m^2.\n840 \n841         temp_air : numeric or tuple of numeric\n842             Ambient dry bulb temperature in degrees C.\n843 \n844         wind_speed : numeric or tuple of numeric\n845             Wind speed in m/s at a height of 10 meters.\n846 \n847         effective_irradiance : numeric, tuple of numeric, or None.\n848             The irradiance that is converted to photocurrent. If None,\n849             assumed equal to ``poa_global``. [W/m^2]\n850 \n851         Returns\n852         -------\n853         temperature_cell : numeric or tuple of numeric\n854             The modeled cell temperature [C]\n855 \n856         Notes\n857         -----\n858         The `temp_air` and `wind_speed` parameters may be passed as tuples\n859         to provide different values for each Array in the system. If not\n860         passed as a tuple then the same value is used for input to each Array.\n861         If passed as a tuple the length must be the same as the number of\n862         Arrays.\n863         \"\"\"\n864         return self.get_cell_temperature(\n865             poa_global, temp_air, wind_speed, model='noct_sam',\n866             effective_irradiance=effective_irradiance)\n867 \n868     @_unwrap_single_value\n869     def first_solar_spectral_loss(self, pw, airmass_absolute):\n870 \n871         \"\"\"\n872         Use the :py:func:`first_solar_spectral_correction` function to\n873         calculate the spectral loss modifier. The model coefficients are\n874         specific to the module's cell type, and are determined by searching\n875         for one of the following keys in self.module_parameters (in order):\n876 \n877         - 'first_solar_spectral_coefficients' (user-supplied coefficients)\n878         - 'Technology' - a string describing the cell type, can be read from\n879           the CEC module parameter database\n880         - 'Material' - a string describing the cell type, can be read from\n881           the Sandia module database.\n882 \n883         Parameters\n884         ----------\n885         pw : array-like\n886             atmospheric precipitable water (cm).\n887 \n888         airmass_absolute : array-like\n889             absolute (pressure corrected) airmass.\n890 \n891         Returns\n892         -------\n893         modifier: array-like or tuple of array-like\n894             spectral mismatch factor (unitless) which can be multiplied\n895             with broadband irradiance reaching a module's cells to estimate\n896             effective irradiance, i.e., the irradiance that is converted to\n897             electrical current.\n898         \"\"\"\n899         pw = self._validate_per_array(pw, system_wide=True)\n900 \n901         def _spectral_correction(array, pw):\n902             if 'first_solar_spectral_coefficients' in \\\n903                     array.module_parameters.keys():\n904                 coefficients = \\\n905                     array.module_parameters[\n906                         'first_solar_spectral_coefficients'\n907                     ]\n908                 module_type = None\n909             else:\n910                 module_type = array._infer_cell_type()\n911                 coefficients = None\n912 \n913             return atmosphere.first_solar_spectral_correction(\n914                 pw, airmass_absolute,\n915                 module_type, coefficients\n916             )\n917         return tuple(\n918             itertools.starmap(_spectral_correction, zip(self.arrays, pw))\n919         )\n920 \n921     def singlediode(self, photocurrent, saturation_current,\n922                     resistance_series, resistance_shunt, nNsVth,\n923                     ivcurve_pnts=None):\n924         \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.singlediode` function.\n925 \n926         See :py:func:`pvsystem.singlediode` for details\n927         \"\"\"\n928         return singlediode(photocurrent, saturation_current,\n929                            resistance_series, resistance_shunt, nNsVth,\n930                            ivcurve_pnts=ivcurve_pnts)\n931 \n932     def i_from_v(self, resistance_shunt, resistance_series, nNsVth, voltage,\n933                  saturation_current, photocurrent):\n934         \"\"\"Wrapper around the :py:func:`pvlib.pvsystem.i_from_v` function.\n935 \n936         See :py:func:`pvsystem.i_from_v` for details\n937         \"\"\"\n938         return i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n939                         saturation_current, photocurrent)\n940 \n941     def get_ac(self, model, p_dc, v_dc=None):\n942         r\"\"\"Calculates AC power from p_dc using the inverter model indicated\n943         by model and self.inverter_parameters.\n944 \n945         Parameters\n946         ----------\n947         model : str\n948             Must be one of 'sandia', 'adr', or 'pvwatts'.\n949         p_dc : numeric, or tuple, list or array of numeric\n950             DC power on each MPPT input of the inverter. Use tuple, list or\n951             array for inverters with multiple MPPT inputs. If type is array,\n952             p_dc must be 2d with axis 0 being the MPPT inputs. [W]\n953         v_dc : numeric, or tuple, list or array of numeric\n954             DC voltage on each MPPT input of the inverter. Required when\n955             model='sandia' or model='adr'. Use tuple, list or\n956             array for inverters with multiple MPPT inputs. If type is array,\n957             v_dc must be 2d with axis 0 being the MPPT inputs. [V]\n958 \n959         Returns\n960         -------\n961         power_ac : numeric\n962             AC power output for the inverter. [W]\n963 \n964         Raises\n965         ------\n966         ValueError\n967             If model is not one of 'sandia', 'adr' or 'pvwatts'.\n968         ValueError\n969             If model='adr' and the PVSystem has more than one array.\n970 \n971         See also\n972         --------\n973         pvlib.inverter.sandia\n974         pvlib.inverter.sandia_multi\n975         pvlib.inverter.adr\n976         pvlib.inverter.pvwatts\n977         pvlib.inverter.pvwatts_multi\n978         \"\"\"\n979         model = model.lower()\n980         multiple_arrays = self.num_arrays > 1\n981         if model == 'sandia':\n982             p_dc = self._validate_per_array(p_dc)\n983             v_dc = self._validate_per_array(v_dc)\n984             if multiple_arrays:\n985                 return inverter.sandia_multi(\n986                     v_dc, p_dc, self.inverter_parameters)\n987             return inverter.sandia(v_dc[0], p_dc[0], self.inverter_parameters)\n988         elif model == 'pvwatts':\n989             kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],\n990                                    self.inverter_parameters)\n991             p_dc = self._validate_per_array(p_dc)\n992             if multiple_arrays:\n993                 return inverter.pvwatts_multi(\n994                     p_dc, self.inverter_parameters['pdc0'], **kwargs)\n995             return inverter.pvwatts(\n996                 p_dc[0], self.inverter_parameters['pdc0'], **kwargs)\n997         elif model == 'adr':\n998             if multiple_arrays:\n999                 raise ValueError(\n1000                     'The adr inverter function cannot be used for an inverter',\n1001                     ' with multiple MPPT inputs')\n1002             # While this is only used for single-array systems, calling\n1003             # _validate_per_arry lets us pass in singleton tuples.\n1004             p_dc = self._validate_per_array(p_dc)\n1005             v_dc = self._validate_per_array(v_dc)\n1006             return inverter.adr(v_dc[0], p_dc[0], self.inverter_parameters)\n1007         else:\n1008             raise ValueError(\n1009                 model + ' is not a valid AC power model.',\n1010                 ' model must be one of \"sandia\", \"adr\" or \"pvwatts\"')\n1011 \n1012     @deprecated('0.9', alternative='PVSystem.get_ac', removal='0.10')\n1013     def snlinverter(self, v_dc, p_dc):\n1014         \"\"\"Uses :py:func:`pvlib.inverter.sandia` to calculate AC power based on\n1015         ``self.inverter_parameters`` and the input voltage and power.\n1016 \n1017         See :py:func:`pvlib.inverter.sandia` for details\n1018         \"\"\"\n1019         return inverter.sandia(v_dc, p_dc, self.inverter_parameters)\n1020 \n1021     @deprecated('0.9', alternative='PVSystem.get_ac', removal='0.10')\n1022     def adrinverter(self, v_dc, p_dc):\n1023         \"\"\"Uses :py:func:`pvlib.inverter.adr` to calculate AC power based on\n1024         ``self.inverter_parameters`` and the input voltage and power.\n1025 \n1026         See :py:func:`pvlib.inverter.adr` for details\n1027         \"\"\"\n1028         return inverter.adr(v_dc, p_dc, self.inverter_parameters)\n1029 \n1030     @_unwrap_single_value\n1031     def scale_voltage_current_power(self, data):\n1032         \"\"\"\n1033         Scales the voltage, current, and power of the `data` DataFrame\n1034         by `self.modules_per_string` and `self.strings_per_inverter`.\n1035 \n1036         Parameters\n1037         ----------\n1038         data: DataFrame or tuple of DataFrame\n1039             May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n1040             'i_sc', 'p_mp'`.\n1041 \n1042         Returns\n1043         -------\n1044         scaled_data: DataFrame or tuple of DataFrame\n1045             A scaled copy of the input data.\n1046         \"\"\"\n1047         data = self._validate_per_array(data)\n1048         return tuple(\n1049             scale_voltage_current_power(data,\n1050                                         voltage=array.modules_per_string,\n1051                                         current=array.strings)\n1052             for array, data in zip(self.arrays, data)\n1053         )\n1054 \n1055     @_unwrap_single_value\n1056     def pvwatts_dc(self, g_poa_effective, temp_cell):\n1057         \"\"\"\n1058         Calcuates DC power according to the PVWatts model using\n1059         :py:func:`pvlib.pvsystem.pvwatts_dc`, `self.module_parameters['pdc0']`,\n1060         and `self.module_parameters['gamma_pdc']`.\n1061 \n1062         See :py:func:`pvlib.pvsystem.pvwatts_dc` for details.\n1063         \"\"\"\n1064         g_poa_effective = self._validate_per_array(g_poa_effective)\n1065         temp_cell = self._validate_per_array(temp_cell)\n1066         return tuple(\n1067             pvwatts_dc(g_poa_effective, temp_cell,\n1068                        array.module_parameters['pdc0'],\n1069                        array.module_parameters['gamma_pdc'],\n1070                        **_build_kwargs(['temp_ref'], array.module_parameters))\n1071             for array, g_poa_effective, temp_cell\n1072             in zip(self.arrays, g_poa_effective, temp_cell)\n1073         )\n1074 \n1075     def pvwatts_losses(self):\n1076         \"\"\"\n1077         Calculates DC power losses according the PVwatts model using\n1078         :py:func:`pvlib.pvsystem.pvwatts_losses` and\n1079         ``self.losses_parameters``.\n1080 \n1081         See :py:func:`pvlib.pvsystem.pvwatts_losses` for details.\n1082         \"\"\"\n1083         kwargs = _build_kwargs(['soiling', 'shading', 'snow', 'mismatch',\n1084                                 'wiring', 'connections', 'lid',\n1085                                 'nameplate_rating', 'age', 'availability'],\n1086                                self.losses_parameters)\n1087         return pvwatts_losses(**kwargs)\n1088 \n1089     @deprecated('0.9', alternative='PVSystem.get_ac', removal='0.10')\n1090     def pvwatts_ac(self, pdc):\n1091         \"\"\"\n1092         Calculates AC power according to the PVWatts model using\n1093         :py:func:`pvlib.inverter.pvwatts`, `self.module_parameters[\"pdc0\"]`,\n1094         and `eta_inv_nom=self.inverter_parameters[\"eta_inv_nom\"]`.\n1095 \n1096         See :py:func:`pvlib.inverter.pvwatts` for details.\n1097         \"\"\"\n1098         kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],\n1099                                self.inverter_parameters)\n1100 \n1101         return inverter.pvwatts(pdc, self.inverter_parameters['pdc0'],\n1102                                 **kwargs)\n1103 \n1104     @_unwrap_single_value\n1105     def dc_ohms_from_percent(self):\n1106         \"\"\"\n1107         Calculates the equivalent resistance of the wires for each array using\n1108         :py:func:`pvlib.pvsystem.dc_ohms_from_percent`\n1109 \n1110         See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for details.\n1111         \"\"\"\n1112 \n1113         return tuple(array.dc_ohms_from_percent() for array in self.arrays)\n1114 \n1115     @property\n1116     @_unwrap_single_value\n1117     @_check_deprecated_passthrough\n1118     def module_parameters(self):\n1119         return tuple(array.module_parameters for array in self.arrays)\n1120 \n1121     @module_parameters.setter\n1122     @_check_deprecated_passthrough\n1123     def module_parameters(self, value):\n1124         for array in self.arrays:\n1125             array.module_parameters = value\n1126 \n1127     @property\n1128     @_unwrap_single_value\n1129     @_check_deprecated_passthrough\n1130     def module(self):\n1131         return tuple(array.module for array in self.arrays)\n1132 \n1133     @module.setter\n1134     @_check_deprecated_passthrough\n1135     def module(self, value):\n1136         for array in self.arrays:\n1137             array.module = value\n1138 \n1139     @property\n1140     @_unwrap_single_value\n1141     @_check_deprecated_passthrough\n1142     def module_type(self):\n1143         return tuple(array.module_type for array in self.arrays)\n1144 \n1145     @module_type.setter\n1146     @_check_deprecated_passthrough\n1147     def module_type(self, value):\n1148         for array in self.arrays:\n1149             array.module_type = value\n1150 \n1151     @property\n1152     @_unwrap_single_value\n1153     @_check_deprecated_passthrough\n1154     def temperature_model_parameters(self):\n1155         return tuple(array.temperature_model_parameters\n1156                      for array in self.arrays)\n1157 \n1158     @temperature_model_parameters.setter\n1159     @_check_deprecated_passthrough\n1160     def temperature_model_parameters(self, value):\n1161         for array in self.arrays:\n1162             array.temperature_model_parameters = value\n1163 \n1164     @property\n1165     @_unwrap_single_value\n1166     @_check_deprecated_passthrough\n1167     def surface_tilt(self):\n1168         return tuple(array.surface_tilt for array in self.arrays)\n1169 \n1170     @surface_tilt.setter\n1171     @_check_deprecated_passthrough\n1172     def surface_tilt(self, value):\n1173         for array in self.arrays:\n1174             array.surface_tilt = value\n1175 \n1176     @property\n1177     @_unwrap_single_value\n1178     @_check_deprecated_passthrough\n1179     def surface_azimuth(self):\n1180         return tuple(array.surface_azimuth for array in self.arrays)\n1181 \n1182     @surface_azimuth.setter\n1183     @_check_deprecated_passthrough\n1184     def surface_azimuth(self, value):\n1185         for array in self.arrays:\n1186             array.surface_azimuth = value\n1187 \n1188     @property\n1189     @_unwrap_single_value\n1190     @_check_deprecated_passthrough\n1191     def albedo(self):\n1192         return tuple(array.albedo for array in self.arrays)\n1193 \n1194     @albedo.setter\n1195     @_check_deprecated_passthrough\n1196     def albedo(self, value):\n1197         for array in self.arrays:\n1198             array.albedo = value\n1199 \n1200     @property\n1201     @_unwrap_single_value\n1202     @_check_deprecated_passthrough\n1203     def racking_model(self):\n1204         return tuple(array.racking_model for array in self.arrays)\n1205 \n1206     @racking_model.setter\n1207     @_check_deprecated_passthrough\n1208     def racking_model(self, value):\n1209         for array in self.arrays:\n1210             array.racking_model = value\n1211 \n1212     @property\n1213     @_unwrap_single_value\n1214     @_check_deprecated_passthrough\n1215     def modules_per_string(self):\n1216         return tuple(array.modules_per_string for array in self.arrays)\n1217 \n1218     @modules_per_string.setter\n1219     @_check_deprecated_passthrough\n1220     def modules_per_string(self, value):\n1221         for array in self.arrays:\n1222             array.modules_per_string = value\n1223 \n1224     @property\n1225     @_unwrap_single_value\n1226     @_check_deprecated_passthrough\n1227     def strings_per_inverter(self):\n1228         return tuple(array.strings for array in self.arrays)\n1229 \n1230     @strings_per_inverter.setter\n1231     @_check_deprecated_passthrough\n1232     def strings_per_inverter(self, value):\n1233         for array in self.arrays:\n1234             array.strings = value\n1235 \n1236     @property\n1237     def num_arrays(self):\n1238         \"\"\"The number of Arrays in the system.\"\"\"\n1239         return len(self.arrays)\n1240 \n1241 \n1242 class Array:\n1243     \"\"\"\n1244     An Array is a set of of modules at the same orientation.\n1245 \n1246     Specifically, an array is defined by tilt, azimuth, the\n1247     module parameters, the number of parallel strings of modules\n1248     and the number of modules on each string.\n1249 \n1250     Parameters\n1251     ----------\n1252     surface_tilt: float or array-like, default 0\n1253         Surface tilt angles in decimal degrees.\n1254         The tilt angle is defined as degrees from horizontal\n1255         (e.g. surface facing up = 0, surface facing horizon = 90)\n1256 \n1257     surface_azimuth: float or array-like, default 180\n1258         Azimuth angle of the module surface.\n1259         North=0, East=90, South=180, West=270.\n1260 \n1261     albedo : None or float, default None\n1262         The ground albedo. If ``None``, will attempt to use\n1263         ``surface_type`` to look up an albedo value in\n1264         ``irradiance.SURFACE_ALBEDOS``. If a surface albedo\n1265         cannot be found then 0.25 is used.\n1266 \n1267     surface_type : None or string, default None\n1268         The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n1269         for valid values.\n1270 \n1271     module : None or string, default None\n1272         The model name of the modules.\n1273         May be used to look up the module_parameters dictionary\n1274         via some other method.\n1275 \n1276     module_type : None or string, default None\n1277          Describes the module's construction. Valid strings are 'glass_polymer'\n1278          and 'glass_glass'. Used for cell and module temperature calculations.\n1279 \n1280     module_parameters : None, dict or Series, default None\n1281         Parameters for the module model, e.g., SAPM, CEC, or other.\n1282 \n1283     temperature_model_parameters : None, dict or Series, default None.\n1284         Parameters for the module temperature model, e.g., SAPM, Pvsyst, or\n1285         other.\n1286 \n1287     modules_per_string: int, default 1\n1288         Number of modules per string in the array.\n1289 \n1290     strings: int, default 1\n1291         Number of parallel strings in the array.\n1292 \n1293     racking_model : None or string, default None\n1294         Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n1295         Used to identify a parameter set for the SAPM cell temperature model.\n1296 \n1297     array_losses_parameters: None, dict or Series, default None.\n1298         Supported keys are 'dc_ohmic_percent'.\n1299 \n1300     \"\"\"\n1301 \n1302     def __init__(self,\n1303                  surface_tilt=0, surface_azimuth=180,\n1304                  albedo=None, surface_type=None,\n1305                  module=None, module_type=None,\n1306                  module_parameters=None,\n1307                  temperature_model_parameters=None,\n1308                  modules_per_string=1, strings=1,\n1309                  racking_model=None, array_losses_parameters=None,\n1310                  name=None):\n1311         self.surface_tilt = surface_tilt\n1312         self.surface_azimuth = surface_azimuth\n1313 \n1314         self.surface_type = surface_type\n1315         if albedo is None:\n1316             self.albedo = irradiance.SURFACE_ALBEDOS.get(surface_type, 0.25)\n1317         else:\n1318             self.albedo = albedo\n1319 \n1320         self.module = module\n1321         if module_parameters is None:\n1322             self.module_parameters = {}\n1323         else:\n1324             self.module_parameters = module_parameters\n1325 \n1326         self.module_type = module_type\n1327         self.racking_model = racking_model\n1328 \n1329         self.strings = strings\n1330         self.modules_per_string = modules_per_string\n1331 \n1332         if temperature_model_parameters is None:\n1333             self.temperature_model_parameters = \\\n1334                 self._infer_temperature_model_params()\n1335         else:\n1336             self.temperature_model_parameters = temperature_model_parameters\n1337 \n1338         if array_losses_parameters is None:\n1339             self.array_losses_parameters = {}\n1340         else:\n1341             self.array_losses_parameters = array_losses_parameters\n1342 \n1343         self.name = name\n1344 \n1345     def __repr__(self):\n1346         attrs = ['name', 'surface_tilt', 'surface_azimuth', 'module',\n1347                  'albedo', 'racking_model', 'module_type',\n1348                  'temperature_model_parameters',\n1349                  'strings', 'modules_per_string']\n1350         return 'Array:\\n  ' + '\\n  '.join(\n1351             f'{attr}: {getattr(self, attr)}' for attr in attrs\n1352         )\n1353 \n1354     def _infer_temperature_model_params(self):\n1355         # try to infer temperature model parameters from from racking_model\n1356         # and module_type\n1357         param_set = f'{self.racking_model}_{self.module_type}'\n1358         if param_set in temperature.TEMPERATURE_MODEL_PARAMETERS['sapm']:\n1359             return temperature._temperature_model_params('sapm', param_set)\n1360         elif 'freestanding' in param_set:\n1361             return temperature._temperature_model_params('pvsyst',\n1362                                                          'freestanding')\n1363         elif 'insulated' in param_set:  # after SAPM to avoid confusing keys\n1364             return temperature._temperature_model_params('pvsyst',\n1365                                                          'insulated')\n1366         else:\n1367             return {}\n1368 \n1369     def _infer_cell_type(self):\n1370         \"\"\"\n1371         Examines module_parameters and maps the Technology key for the CEC\n1372         database and the Material key for the Sandia database to a common\n1373         list of strings for cell type.\n1374 \n1375         Returns\n1376         -------\n1377         cell_type: str\n1378 \n1379         \"\"\"\n1380 \n1381         _cell_type_dict = {'Multi-c-Si': 'multisi',\n1382                            'Mono-c-Si': 'monosi',\n1383                            'Thin Film': 'cigs',\n1384                            'a-Si/nc': 'asi',\n1385                            'CIS': 'cigs',\n1386                            'CIGS': 'cigs',\n1387                            '1-a-Si': 'asi',\n1388                            'CdTe': 'cdte',\n1389                            'a-Si': 'asi',\n1390                            '2-a-Si': None,\n1391                            '3-a-Si': None,\n1392                            'HIT-Si': 'monosi',\n1393                            'mc-Si': 'multisi',\n1394                            'c-Si': 'multisi',\n1395                            'Si-Film': 'asi',\n1396                            'EFG mc-Si': 'multisi',\n1397                            'GaAs': None,\n1398                            'a-Si / mono-Si': 'monosi'}\n1399 \n1400         if 'Technology' in self.module_parameters.keys():\n1401             # CEC module parameter set\n1402             cell_type = _cell_type_dict[self.module_parameters['Technology']]\n1403         elif 'Material' in self.module_parameters.keys():\n1404             # Sandia module parameter set\n1405             cell_type = _cell_type_dict[self.module_parameters['Material']]\n1406         else:\n1407             cell_type = None\n1408 \n1409         return cell_type\n1410 \n1411     def get_aoi(self, solar_zenith, solar_azimuth):\n1412         \"\"\"\n1413         Get the angle of incidence on the array.\n1414 \n1415         Parameters\n1416         ----------\n1417         solar_zenith : float or Series\n1418             Solar zenith angle.\n1419         solar_azimuth : float or Series\n1420             Solar azimuth angle\n1421 \n1422         Returns\n1423         -------\n1424         aoi : Series\n1425             Then angle of incidence.\n1426         \"\"\"\n1427         return irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n1428                               solar_zenith, solar_azimuth)\n1429 \n1430     def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n1431                        dni_extra=None, airmass=None, model='haydavies',\n1432                        **kwargs):\n1433         \"\"\"\n1434         Get plane of array irradiance components.\n1435 \n1436         Uses the :py:func:`pvlib.irradiance.get_total_irradiance` function to\n1437         calculate the plane of array irradiance components for a surface\n1438         defined by ``self.surface_tilt`` and ``self.surface_azimuth`` with\n1439         albedo ``self.albedo``.\n1440 \n1441         Parameters\n1442         ----------\n1443         solar_zenith : float or Series.\n1444             Solar zenith angle.\n1445         solar_azimuth : float or Series.\n1446             Solar azimuth angle.\n1447         dni : float or Series\n1448             Direct Normal Irradiance\n1449         ghi : float or Series\n1450             Global horizontal irradiance\n1451         dhi : float or Series\n1452             Diffuse horizontal irradiance\n1453         dni_extra : None, float or Series, default None\n1454             Extraterrestrial direct normal irradiance\n1455         airmass : None, float or Series, default None\n1456             Airmass\n1457         model : String, default 'haydavies'\n1458             Irradiance model.\n1459 \n1460         kwargs\n1461             Extra parameters passed to\n1462             :py:func:`pvlib.irradiance.get_total_irradiance`.\n1463 \n1464         Returns\n1465         -------\n1466         poa_irradiance : DataFrame\n1467             Column names are: ``total, beam, sky, ground``.\n1468         \"\"\"\n1469         # not needed for all models, but this is easier\n1470         if dni_extra is None:\n1471             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n1472 \n1473         if airmass is None:\n1474             airmass = atmosphere.get_relative_airmass(solar_zenith)\n1475 \n1476         return irradiance.get_total_irradiance(self.surface_tilt,\n1477                                                self.surface_azimuth,\n1478                                                solar_zenith, solar_azimuth,\n1479                                                dni, ghi, dhi,\n1480                                                dni_extra=dni_extra,\n1481                                                airmass=airmass,\n1482                                                model=model,\n1483                                                albedo=self.albedo,\n1484                                                **kwargs)\n1485 \n1486     def get_iam(self, aoi, iam_model='physical'):\n1487         \"\"\"\n1488         Determine the incidence angle modifier using the method specified by\n1489         ``iam_model``.\n1490 \n1491         Parameters for the selected IAM model are expected to be in\n1492         ``Array.module_parameters``. Default parameters are available for\n1493         the 'physical', 'ashrae' and 'martin_ruiz' models.\n1494 \n1495         Parameters\n1496         ----------\n1497         aoi : numeric\n1498             The angle of incidence in degrees.\n1499 \n1500         aoi_model : string, default 'physical'\n1501             The IAM model to be used. Valid strings are 'physical', 'ashrae',\n1502             'martin_ruiz' and 'sapm'.\n1503 \n1504         Returns\n1505         -------\n1506         iam : numeric\n1507             The AOI modifier.\n1508 \n1509         Raises\n1510         ------\n1511         ValueError\n1512             if `iam_model` is not a valid model name.\n1513         \"\"\"\n1514         model = iam_model.lower()\n1515         if model in ['ashrae', 'physical', 'martin_ruiz']:\n1516             param_names = iam._IAM_MODEL_PARAMS[model]\n1517             kwargs = _build_kwargs(param_names, self.module_parameters)\n1518             func = getattr(iam, model)\n1519             return func(aoi, **kwargs)\n1520         elif model == 'sapm':\n1521             return iam.sapm(aoi, self.module_parameters)\n1522         elif model == 'interp':\n1523             raise ValueError(model + ' is not implemented as an IAM model '\n1524                              'option for Array')\n1525         else:\n1526             raise ValueError(model + ' is not a valid IAM model')\n1527 \n1528     def get_cell_temperature(self, poa_global, temp_air, wind_speed, model,\n1529                              effective_irradiance=None):\n1530         \"\"\"\n1531         Determine cell temperature using the method specified by ``model``.\n1532 \n1533         Parameters\n1534         ----------\n1535         poa_global : numeric\n1536             Total incident irradiance [W/m^2]\n1537 \n1538         temp_air : numeric\n1539             Ambient dry bulb temperature [C]\n1540 \n1541         wind_speed : numeric\n1542             Wind speed [m/s]\n1543 \n1544         model : str\n1545             Supported models include ``'sapm'``, ``'pvsyst'``,\n1546             ``'faiman'``, ``'fuentes'``, and ``'noct_sam'``\n1547 \n1548         effective_irradiance : numeric, optional\n1549             The irradiance that is converted to photocurrent in W/m^2.\n1550             Only used for some models.\n1551 \n1552         Returns\n1553         -------\n1554         numeric\n1555             Values in degrees C.\n1556 \n1557         See Also\n1558         --------\n1559         pvlib.temperature.sapm_cell, pvlib.temperature.pvsyst_cell,\n1560         pvlib.temperature.faiman, pvlib.temperature.fuentes,\n1561         pvlib.temperature.noct_sam\n1562 \n1563         Notes\n1564         -----\n1565         Some temperature models have requirements for the input types;\n1566         see the documentation of the underlying model function for details.\n1567         \"\"\"\n1568         # convenience wrapper to avoid passing args 2 and 3 every call\n1569         _build_tcell_args = functools.partial(\n1570             _build_args, input_dict=self.temperature_model_parameters,\n1571             dict_name='temperature_model_parameters')\n1572 \n1573         if model == 'sapm':\n1574             func = temperature.sapm_cell\n1575             required = _build_tcell_args(['a', 'b', 'deltaT'])\n1576             optional = _build_kwargs(['irrad_ref'],\n1577                                      self.temperature_model_parameters)\n1578         elif model == 'pvsyst':\n1579             func = temperature.pvsyst_cell\n1580             required = tuple()\n1581             optional = {\n1582                 # TODO remove 'eta_m' after deprecation of this parameter\n1583                 **_build_kwargs(['eta_m', 'module_efficiency',\n1584                                  'alpha_absorption'],\n1585                                 self.module_parameters),\n1586                 **_build_kwargs(['u_c', 'u_v'],\n1587                                 self.temperature_model_parameters)\n1588             }\n1589         elif model == 'faiman':\n1590             func = temperature.faiman\n1591             required = tuple()\n1592             optional = _build_kwargs(['u0', 'u1'],\n1593                                      self.temperature_model_parameters)\n1594         elif model == 'fuentes':\n1595             func = temperature.fuentes\n1596             required = _build_tcell_args(['noct_installed'])\n1597             optional = _build_kwargs([\n1598                 'module_height', 'wind_height', 'emissivity', 'absorption',\n1599                 'surface_tilt', 'module_width', 'module_length'],\n1600                 self.temperature_model_parameters)\n1601             # default to using the Array attribute, but allow user to override\n1602             # with a custom surface_tilt value in temperature_model_parameters\n1603             if 'surface_tilt' not in optional:\n1604                 optional['surface_tilt'] = self.surface_tilt\n1605         elif model == 'noct_sam':\n1606             func = functools.partial(temperature.noct_sam,\n1607                                      effective_irradiance=effective_irradiance)\n1608             required = _build_tcell_args(['noct', 'module_efficiency'])\n1609             optional = _build_kwargs(['transmittance_absorptance',\n1610                                       'array_height', 'mount_standoff'],\n1611                                      self.temperature_model_parameters)\n1612         else:\n1613             raise ValueError(f'{model} is not a valid cell temperature model')\n1614 \n1615         temperature_cell = func(poa_global, temp_air, wind_speed,\n1616                                 *required, **optional)\n1617         return temperature_cell\n1618 \n1619     def dc_ohms_from_percent(self):\n1620         \"\"\"\n1621         Calculates the equivalent resistance of the wires using\n1622         :py:func:`pvlib.pvsystem.dc_ohms_from_percent`\n1623 \n1624         Makes use of array module parameters according to the\n1625         following DC models:\n1626 \n1627         CEC:\n1628 \n1629             * `self.module_parameters[\"V_mp_ref\"]`\n1630             * `self.module_parameters[\"I_mp_ref\"]`\n1631 \n1632         SAPM:\n1633 \n1634             * `self.module_parameters[\"Vmpo\"]`\n1635             * `self.module_parameters[\"Impo\"]`\n1636 \n1637         PVsyst-like or other:\n1638 \n1639             * `self.module_parameters[\"Vmpp\"]`\n1640             * `self.module_parameters[\"Impp\"]`\n1641 \n1642         Other array parameters that are used are:\n1643         `self.losses_parameters[\"dc_ohmic_percent\"]`,\n1644         `self.modules_per_string`, and\n1645         `self.strings`.\n1646 \n1647         See :py:func:`pvlib.pvsystem.dc_ohms_from_percent` for more details.\n1648         \"\"\"\n1649 \n1650         # get relevent Vmp and Imp parameters from CEC parameters\n1651         if all([elem in self.module_parameters\n1652                 for elem in ['V_mp_ref', 'I_mp_ref']]):\n1653             vmp_ref = self.module_parameters['V_mp_ref']\n1654             imp_ref = self.module_parameters['I_mp_ref']\n1655 \n1656         # get relevant Vmp and Imp parameters from SAPM parameters\n1657         elif all([elem in self.module_parameters\n1658                   for elem in ['Vmpo', 'Impo']]):\n1659             vmp_ref = self.module_parameters['Vmpo']\n1660             imp_ref = self.module_parameters['Impo']\n1661 \n1662         # get relevant Vmp and Imp parameters if they are PVsyst-like\n1663         elif all([elem in self.module_parameters\n1664                   for elem in ['Vmpp', 'Impp']]):\n1665             vmp_ref = self.module_parameters['Vmpp']\n1666             imp_ref = self.module_parameters['Impp']\n1667 \n1668         # raise error if relevant Vmp and Imp parameters are not found\n1669         else:\n1670             raise ValueError('Parameters for Vmp and Imp could not be found '\n1671                              'in the array module parameters. Module '\n1672                              'parameters must include one set of '\n1673                              '{\"V_mp_ref\", \"I_mp_Ref\"}, '\n1674                              '{\"Vmpo\", \"Impo\"}, or '\n1675                              '{\"Vmpp\", \"Impp\"}.'\n1676                              )\n1677 \n1678         return dc_ohms_from_percent(\n1679             vmp_ref,\n1680             imp_ref,\n1681             self.array_losses_parameters['dc_ohmic_percent'],\n1682             self.modules_per_string,\n1683             self.strings)\n1684 \n1685 \n1686 def calcparams_desoto(effective_irradiance, temp_cell,\n1687                       alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n1688                       EgRef=1.121, dEgdT=-0.0002677,\n1689                       irrad_ref=1000, temp_ref=25):\n1690     '''\n1691     Calculates five parameter values for the single diode equation at\n1692     effective irradiance and cell temperature using the De Soto et al.\n1693     model described in [1]_. The five values returned by calcparams_desoto\n1694     can be used by singlediode to calculate an IV curve.\n1695 \n1696     Parameters\n1697     ----------\n1698     effective_irradiance : numeric\n1699         The irradiance (W/m2) that is converted to photocurrent.\n1700 \n1701     temp_cell : numeric\n1702         The average cell temperature of cells within a module in C.\n1703 \n1704     alpha_sc : float\n1705         The short-circuit current temperature coefficient of the\n1706         module in units of A/C.\n1707 \n1708     a_ref : float\n1709         The product of the usual diode ideality factor (n, unitless),\n1710         number of cells in series (Ns), and cell thermal voltage at reference\n1711         conditions, in units of V.\n1712 \n1713     I_L_ref : float\n1714         The light-generated current (or photocurrent) at reference conditions,\n1715         in amperes.\n1716 \n1717     I_o_ref : float\n1718         The dark or diode reverse saturation current at reference conditions,\n1719         in amperes.\n1720 \n1721     R_sh_ref : float\n1722         The shunt resistance at reference conditions, in ohms.\n1723 \n1724     R_s : float\n1725         The series resistance at reference conditions, in ohms.\n1726 \n1727     EgRef : float\n1728         The energy bandgap at reference temperature in units of eV.\n1729         1.121 eV for crystalline silicon. EgRef must be >0.  For parameters\n1730         from the SAM CEC module database, EgRef=1.121 is implicit for all\n1731         cell types in the parameter estimation algorithm used by NREL.\n1732 \n1733     dEgdT : float\n1734         The temperature dependence of the energy bandgap at reference\n1735         conditions in units of 1/K. May be either a scalar value\n1736         (e.g. -0.0002677 as in [1]_) or a DataFrame (this may be useful if\n1737         dEgdT is a modeled as a function of temperature). For parameters from\n1738         the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell\n1739         types in the parameter estimation algorithm used by NREL.\n1740 \n1741     irrad_ref : float (optional, default=1000)\n1742         Reference irradiance in W/m^2.\n1743 \n1744     temp_ref : float (optional, default=25)\n1745         Reference cell temperature in C.\n1746 \n1747     Returns\n1748     -------\n1749     Tuple of the following results:\n1750 \n1751     photocurrent : numeric\n1752         Light-generated current in amperes\n1753 \n1754     saturation_current : numeric\n1755         Diode saturation curent in amperes\n1756 \n1757     resistance_series : float\n1758         Series resistance in ohms\n1759 \n1760     resistance_shunt : numeric\n1761         Shunt resistance in ohms\n1762 \n1763     nNsVth : numeric\n1764         The product of the usual diode ideality factor (n, unitless),\n1765         number of cells in series (Ns), and cell thermal voltage at\n1766         specified effective irradiance and cell temperature.\n1767 \n1768     References\n1769     ----------\n1770     .. [1] W. De Soto et al., \"Improvement and validation of a model for\n1771        photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n1772        2006.\n1773 \n1774     .. [2] System Advisor Model web page. https://sam.nrel.gov.\n1775 \n1776     .. [3] A. Dobos, \"An Improved Coefficient Calculator for the California\n1777        Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n1778        Solar Energy Engineering, vol 134, 2012.\n1779 \n1780     .. [4] O. Madelung, \"Semiconductors: Data Handbook, 3rd ed.\" ISBN\n1781        3-540-40488-0\n1782 \n1783     See Also\n1784     --------\n1785     singlediode\n1786     retrieve_sam\n1787 \n1788     Notes\n1789     -----\n1790     If the reference parameters in the ModuleParameters struct are read\n1791     from a database or library of parameters (e.g. System Advisor\n1792     Model), it is important to use the same EgRef and dEgdT values that\n1793     were used to generate the reference parameters, regardless of the\n1794     actual bandgap characteristics of the semiconductor. For example, in\n1795     the case of the System Advisor Model library, created as described\n1796     in [3], EgRef and dEgdT for all modules were 1.121 and -0.0002677,\n1797     respectively.\n1798 \n1799     This table of reference bandgap energies (EgRef), bandgap energy\n1800     temperature dependence (dEgdT), and \"typical\" airmass response (M)\n1801     is provided purely as reference to those who may generate their own\n1802     reference module parameters (a_ref, IL_ref, I0_ref, etc.) based upon\n1803     the various PV semiconductors. Again, we stress the importance of\n1804     using identical EgRef and dEgdT when generation reference parameters\n1805     and modifying the reference parameters (for irradiance, temperature,\n1806     and airmass) per DeSoto's equations.\n1807 \n1808      Crystalline Silicon (Si):\n1809          * EgRef = 1.121\n1810          * dEgdT = -0.0002677\n1811 \n1812          >>> M = np.polyval([-1.26E-4, 2.816E-3, -0.024459, 0.086257, 0.9181],\n1813          ...                AMa) # doctest: +SKIP\n1814 \n1815          Source: [1]\n1816 \n1817      Cadmium Telluride (CdTe):\n1818          * EgRef = 1.475\n1819          * dEgdT = -0.0003\n1820 \n1821          >>> M = np.polyval([-2.46E-5, 9.607E-4, -0.0134, 0.0716, 0.9196],\n1822          ...                AMa) # doctest: +SKIP\n1823 \n1824          Source: [4]\n1825 \n1826      Copper Indium diSelenide (CIS):\n1827          * EgRef = 1.010\n1828          * dEgdT = -0.00011\n1829 \n1830          >>> M = np.polyval([-3.74E-5, 0.00125, -0.01462, 0.0718, 0.9210],\n1831          ...                AMa) # doctest: +SKIP\n1832 \n1833          Source: [4]\n1834 \n1835      Copper Indium Gallium diSelenide (CIGS):\n1836          * EgRef = 1.15\n1837          * dEgdT = ????\n1838 \n1839          >>> M = np.polyval([-9.07E-5, 0.0022, -0.0202, 0.0652, 0.9417],\n1840          ...                AMa) # doctest: +SKIP\n1841 \n1842          Source: Wikipedia\n1843 \n1844      Gallium Arsenide (GaAs):\n1845          * EgRef = 1.424\n1846          * dEgdT = -0.000433\n1847          * M = unknown\n1848 \n1849          Source: [4]\n1850     '''\n1851 \n1852     # Boltzmann constant in eV/K\n1853     k = 8.617332478e-05\n1854 \n1855     # reference temperature\n1856     Tref_K = temp_ref + 273.15\n1857     Tcell_K = temp_cell + 273.15\n1858 \n1859     E_g = EgRef * (1 + dEgdT*(Tcell_K - Tref_K))\n1860 \n1861     nNsVth = a_ref * (Tcell_K / Tref_K)\n1862 \n1863     # In the equation for IL, the single factor effective_irradiance is\n1864     # used, in place of the product S*M in [1]. effective_irradiance is\n1865     # equivalent to the product of S (irradiance reaching a module's cells) *\n1866     # M (spectral adjustment factor) as described in [1].\n1867     IL = effective_irradiance / irrad_ref * \\\n1868         (I_L_ref + alpha_sc * (Tcell_K - Tref_K))\n1869     I0 = (I_o_ref * ((Tcell_K / Tref_K) ** 3) *\n1870           (np.exp(EgRef / (k*(Tref_K)) - (E_g / (k*(Tcell_K))))))\n1871     # Note that the equation for Rsh differs from [1]. In [1] Rsh is given as\n1872     # Rsh = Rsh_ref * (S_ref / S) where S is broadband irradiance reaching\n1873     # the module's cells. If desired this model behavior can be duplicated\n1874     # by applying reflection and soiling losses to broadband plane of array\n1875     # irradiance and not applying a spectral loss modifier, i.e.,\n1876     # spectral_modifier = 1.0.\n1877     # use errstate to silence divide by warning\n1878     with np.errstate(divide='ignore'):\n1879         Rsh = R_sh_ref * (irrad_ref / effective_irradiance)\n1880     Rs = R_s\n1881 \n1882     return IL, I0, Rs, Rsh, nNsVth\n1883 \n1884 \n1885 def calcparams_cec(effective_irradiance, temp_cell,\n1886                    alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n1887                    Adjust, EgRef=1.121, dEgdT=-0.0002677,\n1888                    irrad_ref=1000, temp_ref=25):\n1889     '''\n1890     Calculates five parameter values for the single diode equation at\n1891     effective irradiance and cell temperature using the CEC\n1892     model. The CEC model [1]_ differs from the De soto et al.\n1893     model [3]_ by the parameter Adjust. The five values returned by\n1894     calcparams_cec can be used by singlediode to calculate an IV curve.\n1895 \n1896     Parameters\n1897     ----------\n1898     effective_irradiance : numeric\n1899         The irradiance (W/m2) that is converted to photocurrent.\n1900 \n1901     temp_cell : numeric\n1902         The average cell temperature of cells within a module in C.\n1903 \n1904     alpha_sc : float\n1905         The short-circuit current temperature coefficient of the\n1906         module in units of A/C.\n1907 \n1908     a_ref : float\n1909         The product of the usual diode ideality factor (n, unitless),\n1910         number of cells in series (Ns), and cell thermal voltage at reference\n1911         conditions, in units of V.\n1912 \n1913     I_L_ref : float\n1914         The light-generated current (or photocurrent) at reference conditions,\n1915         in amperes.\n1916 \n1917     I_o_ref : float\n1918         The dark or diode reverse saturation current at reference conditions,\n1919         in amperes.\n1920 \n1921     R_sh_ref : float\n1922         The shunt resistance at reference conditions, in ohms.\n1923 \n1924     R_s : float\n1925         The series resistance at reference conditions, in ohms.\n1926 \n1927     Adjust : float\n1928         The adjustment to the temperature coefficient for short circuit\n1929         current, in percent\n1930 \n1931     EgRef : float\n1932         The energy bandgap at reference temperature in units of eV.\n1933         1.121 eV for crystalline silicon. EgRef must be >0.  For parameters\n1934         from the SAM CEC module database, EgRef=1.121 is implicit for all\n1935         cell types in the parameter estimation algorithm used by NREL.\n1936 \n1937     dEgdT : float\n1938         The temperature dependence of the energy bandgap at reference\n1939         conditions in units of 1/K. May be either a scalar value\n1940         (e.g. -0.0002677 as in [3]) or a DataFrame (this may be useful if\n1941         dEgdT is a modeled as a function of temperature). For parameters from\n1942         the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell\n1943         types in the parameter estimation algorithm used by NREL.\n1944 \n1945     irrad_ref : float (optional, default=1000)\n1946         Reference irradiance in W/m^2.\n1947 \n1948     temp_ref : float (optional, default=25)\n1949         Reference cell temperature in C.\n1950 \n1951     Returns\n1952     -------\n1953     Tuple of the following results:\n1954 \n1955     photocurrent : numeric\n1956         Light-generated current in amperes\n1957 \n1958     saturation_current : numeric\n1959         Diode saturation curent in amperes\n1960 \n1961     resistance_series : float\n1962         Series resistance in ohms\n1963 \n1964     resistance_shunt : numeric\n1965         Shunt resistance in ohms\n1966 \n1967     nNsVth : numeric\n1968         The product of the usual diode ideality factor (n, unitless),\n1969         number of cells in series (Ns), and cell thermal voltage at\n1970         specified effective irradiance and cell temperature.\n1971 \n1972     References\n1973     ----------\n1974     .. [1] A. Dobos, \"An Improved Coefficient Calculator for the California\n1975        Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n1976        Solar Energy Engineering, vol 134, 2012.\n1977 \n1978     .. [2] System Advisor Model web page. https://sam.nrel.gov.\n1979 \n1980     .. [3] W. De Soto et al., \"Improvement and validation of a model for\n1981        photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n1982        2006.\n1983 \n1984     See Also\n1985     --------\n1986     calcparams_desoto\n1987     singlediode\n1988     retrieve_sam\n1989 \n1990     '''\n1991 \n1992     # pass adjusted temperature coefficient to desoto\n1993     return calcparams_desoto(effective_irradiance, temp_cell,\n1994                              alpha_sc*(1.0 - Adjust/100),\n1995                              a_ref, I_L_ref, I_o_ref,\n1996                              R_sh_ref, R_s,\n1997                              EgRef=EgRef, dEgdT=dEgdT,\n1998                              irrad_ref=irrad_ref, temp_ref=temp_ref)\n1999 \n2000 \n2001 def calcparams_pvsyst(effective_irradiance, temp_cell,\n2002                       alpha_sc, gamma_ref, mu_gamma,\n2003                       I_L_ref, I_o_ref,\n2004                       R_sh_ref, R_sh_0, R_s,\n2005                       cells_in_series,\n2006                       R_sh_exp=5.5,\n2007                       EgRef=1.121,\n2008                       irrad_ref=1000, temp_ref=25):\n2009     '''\n2010     Calculates five parameter values for the single diode equation at\n2011     effective irradiance and cell temperature using the PVsyst v6\n2012     model.  The PVsyst v6 model is described in [1]_, [2]_, [3]_.\n2013     The five values returned by calcparams_pvsyst can be used by singlediode\n2014     to calculate an IV curve.\n2015 \n2016     Parameters\n2017     ----------\n2018     effective_irradiance : numeric\n2019         The irradiance (W/m2) that is converted to photocurrent.\n2020 \n2021     temp_cell : numeric\n2022         The average cell temperature of cells within a module in C.\n2023 \n2024     alpha_sc : float\n2025         The short-circuit current temperature coefficient of the\n2026         module in units of A/C.\n2027 \n2028     gamma_ref : float\n2029         The diode ideality factor\n2030 \n2031     mu_gamma : float\n2032         The temperature coefficient for the diode ideality factor, 1/K\n2033 \n2034     I_L_ref : float\n2035         The light-generated current (or photocurrent) at reference conditions,\n2036         in amperes.\n2037 \n2038     I_o_ref : float\n2039         The dark or diode reverse saturation current at reference conditions,\n2040         in amperes.\n2041 \n2042     R_sh_ref : float\n2043         The shunt resistance at reference conditions, in ohms.\n2044 \n2045     R_sh_0 : float\n2046         The shunt resistance at zero irradiance conditions, in ohms.\n2047 \n2048     R_s : float\n2049         The series resistance at reference conditions, in ohms.\n2050 \n2051     cells_in_series : integer\n2052         The number of cells connected in series.\n2053 \n2054     R_sh_exp : float\n2055         The exponent in the equation for shunt resistance, unitless. Defaults\n2056         to 5.5.\n2057 \n2058     EgRef : float\n2059         The energy bandgap at reference temperature in units of eV.\n2060         1.121 eV for crystalline silicon. EgRef must be >0.\n2061 \n2062     irrad_ref : float (optional, default=1000)\n2063         Reference irradiance in W/m^2.\n2064 \n2065     temp_ref : float (optional, default=25)\n2066         Reference cell temperature in C.\n2067 \n2068     Returns\n2069     -------\n2070     Tuple of the following results:\n2071 \n2072     photocurrent : numeric\n2073         Light-generated current in amperes\n2074 \n2075     saturation_current : numeric\n2076         Diode saturation current in amperes\n2077 \n2078     resistance_series : float\n2079         Series resistance in ohms\n2080 \n2081     resistance_shunt : numeric\n2082         Shunt resistance in ohms\n2083 \n2084     nNsVth : numeric\n2085         The product of the usual diode ideality factor (n, unitless),\n2086         number of cells in series (Ns), and cell thermal voltage at\n2087         specified effective irradiance and cell temperature.\n2088 \n2089     References\n2090     ----------\n2091     .. [1] K. Sauer, T. Roessler, C. W. Hansen, Modeling the Irradiance and\n2092        Temperature Dependence of Photovoltaic Modules in PVsyst,\n2093        IEEE Journal of Photovoltaics v5(1), January 2015.\n2094 \n2095     .. [2] A. Mermoud, PV modules modelling, Presentation at the 2nd PV\n2096        Performance Modeling Workshop, Santa Clara, CA, May 2013\n2097 \n2098     .. [3] A. Mermoud, T. Lejeune, Performance Assessment of a Simulation Model\n2099        for PV modules of any available technology, 25th European Photovoltaic\n2100        Solar Energy Conference, Valencia, Spain, Sept. 2010\n2101 \n2102     See Also\n2103     --------\n2104     calcparams_desoto\n2105     singlediode\n2106 \n2107     '''\n2108 \n2109     # Boltzmann constant in J/K\n2110     k = 1.38064852e-23\n2111 \n2112     # elementary charge in coulomb\n2113     q = 1.6021766e-19\n2114 \n2115     # reference temperature\n2116     Tref_K = temp_ref + 273.15\n2117     Tcell_K = temp_cell + 273.15\n2118 \n2119     gamma = gamma_ref + mu_gamma * (Tcell_K - Tref_K)\n2120     nNsVth = gamma * k / q * cells_in_series * Tcell_K\n2121 \n2122     IL = effective_irradiance / irrad_ref * \\\n2123         (I_L_ref + alpha_sc * (Tcell_K - Tref_K))\n2124 \n2125     I0 = I_o_ref * ((Tcell_K / Tref_K) ** 3) * \\\n2126         (np.exp((q * EgRef) / (k * gamma) * (1 / Tref_K - 1 / Tcell_K)))\n2127 \n2128     Rsh_tmp = \\\n2129         (R_sh_ref - R_sh_0 * np.exp(-R_sh_exp)) / (1.0 - np.exp(-R_sh_exp))\n2130     Rsh_base = np.maximum(0.0, Rsh_tmp)\n2131 \n2132     Rsh = Rsh_base + (R_sh_0 - Rsh_base) * \\\n2133         np.exp(-R_sh_exp * effective_irradiance / irrad_ref)\n2134 \n2135     Rs = R_s\n2136 \n2137     return IL, I0, Rs, Rsh, nNsVth\n2138 \n2139 \n2140 def retrieve_sam(name=None, path=None):\n2141     '''\n2142     Retrieve latest module and inverter info from a local file or the\n2143     SAM website.\n2144 \n2145     This function will retrieve either:\n2146 \n2147         * CEC module database\n2148         * Sandia Module database\n2149         * CEC Inverter database\n2150         * Anton Driesse Inverter database\n2151 \n2152     and return it as a pandas DataFrame.\n2153 \n2154     Parameters\n2155     ----------\n2156     name : None or string, default None\n2157         Name can be one of:\n2158 \n2159         * 'CECMod' - returns the CEC module database\n2160         * 'CECInverter' - returns the CEC Inverter database\n2161         * 'SandiaInverter' - returns the CEC Inverter database\n2162           (CEC is only current inverter db available; tag kept for\n2163           backwards compatibility)\n2164         * 'SandiaMod' - returns the Sandia Module database\n2165         * 'ADRInverter' - returns the ADR Inverter database\n2166 \n2167     path : None or string, default None\n2168         Path to the SAM file. May also be a URL.\n2169 \n2170     Returns\n2171     -------\n2172     samfile : DataFrame\n2173         A DataFrame containing all the elements of the desired database.\n2174         Each column represents a module or inverter, and a specific\n2175         dataset can be retrieved by the command\n2176 \n2177     Raises\n2178     ------\n2179     ValueError\n2180         If no name or path is provided.\n2181 \n2182     Notes\n2183     -----\n2184     Files available at\n2185         https://github.com/NREL/SAM/tree/develop/deploy/libraries\n2186     Documentation for module and inverter data sets:\n2187         https://sam.nrel.gov/photovoltaic/pv-sub-page-2.html\n2188 \n2189     Examples\n2190     --------\n2191 \n2192     >>> from pvlib import pvsystem\n2193     >>> invdb = pvsystem.retrieve_sam('CECInverter')\n2194     >>> inverter = invdb.AE_Solar_Energy__AE6_0__277V__277V__CEC_2012_\n2195     >>> inverter\n2196     Vac           277.000000\n2197     Paco         6000.000000\n2198     Pdco         6165.670000\n2199     Vdco          361.123000\n2200     Pso            36.792300\n2201     C0             -0.000002\n2202     C1             -0.000047\n2203     C2             -0.001861\n2204     C3              0.000721\n2205     Pnt             0.070000\n2206     Vdcmax        600.000000\n2207     Idcmax         32.000000\n2208     Mppt_low      200.000000\n2209     Mppt_high     500.000000\n2210     Name: AE_Solar_Energy__AE6_0__277V__277V__CEC_2012_, dtype: float64\n2211     '''\n2212 \n2213     if name is not None:\n2214         name = name.lower()\n2215         data_path = os.path.join(\n2216             os.path.dirname(os.path.abspath(__file__)), 'data')\n2217         if name == 'cecmod':\n2218             csvdata = os.path.join(\n2219                 data_path, 'sam-library-cec-modules-2019-03-05.csv')\n2220         elif name == 'sandiamod':\n2221             csvdata = os.path.join(\n2222                 data_path, 'sam-library-sandia-modules-2015-6-30.csv')\n2223         elif name == 'adrinverter':\n2224             csvdata = os.path.join(data_path, 'adr-library-2013-10-01.csv')\n2225         elif name in ['cecinverter', 'sandiainverter']:\n2226             # Allowing either, to provide for old code,\n2227             # while aligning with current expectations\n2228             csvdata = os.path.join(\n2229                 data_path, 'sam-library-cec-inverters-2019-03-05.csv')\n2230         else:\n2231             raise ValueError(f'invalid name {name}')\n2232     elif path is not None:\n2233         if path.startswith('http'):\n2234             response = urlopen(path)\n2235             csvdata = io.StringIO(response.read().decode(errors='ignore'))\n2236         else:\n2237             csvdata = path\n2238     elif name is None and path is None:\n2239         raise ValueError(\"A name or path must be provided!\")\n2240 \n2241     return _parse_raw_sam_df(csvdata)\n2242 \n2243 \n2244 def _normalize_sam_product_names(names):\n2245     '''\n2246     Replace special characters within the product names to make them more\n2247     suitable for use as Dataframe column names.\n2248     '''\n2249     # Contributed by Anton Driesse (@adriesse), PV Performance Labs. July, 2019\n2250 \n2251     import warnings\n2252 \n2253     BAD_CHARS = ' -.()[]:+/\",'\n2254     GOOD_CHARS = '____________'\n2255 \n2256     mapping = str.maketrans(BAD_CHARS, GOOD_CHARS)\n2257     names = pd.Series(data=names)\n2258     norm_names = names.str.translate(mapping)\n2259 \n2260     n_duplicates = names.duplicated().sum()\n2261     if n_duplicates > 0:\n2262         warnings.warn('Original names contain %d duplicate(s).' % n_duplicates)\n2263 \n2264     n_duplicates = norm_names.duplicated().sum()\n2265     if n_duplicates > 0:\n2266         warnings.warn(\n2267             'Normalized names contain %d duplicate(s).' % n_duplicates)\n2268 \n2269     return norm_names.values\n2270 \n2271 \n2272 def _parse_raw_sam_df(csvdata):\n2273 \n2274     df = pd.read_csv(csvdata, index_col=0, skiprows=[1, 2])\n2275 \n2276     df.columns = df.columns.str.replace(' ', '_')\n2277     df.index = _normalize_sam_product_names(df.index)\n2278     df = df.transpose()\n2279 \n2280     if 'ADRCoefficients' in df.index:\n2281         ad_ce = 'ADRCoefficients'\n2282         # for each inverter, parses a string of coefficients like\n2283         # ' 1.33, 2.11, 3.12' into a list containing floats:\n2284         # [1.33, 2.11, 3.12]\n2285         df.loc[ad_ce] = df.loc[ad_ce].map(lambda x: list(\n2286             map(float, x.strip(' []').split())))\n2287 \n2288     return df\n2289 \n2290 \n2291 def sapm(effective_irradiance, temp_cell, module):\n2292     '''\n2293     The Sandia PV Array Performance Model (SAPM) generates 5 points on a\n2294     PV module's I-V curve (Voc, Isc, Ix, Ixx, Vmp/Imp) according to\n2295     SAND2004-3535. Assumes a reference cell temperature of 25 C.\n2296 \n2297     Parameters\n2298     ----------\n2299     effective_irradiance : numeric\n2300         Irradiance reaching the module's cells, after reflections and\n2301         adjustment for spectrum. [W/m2]\n2302 \n2303     temp_cell : numeric\n2304         Cell temperature [C].\n2305 \n2306     module : dict-like\n2307         A dict or Series defining the SAPM parameters. See the notes section\n2308         for more details.\n2309 \n2310     Returns\n2311     -------\n2312     A DataFrame with the columns:\n2313 \n2314         * i_sc : Short-circuit current (A)\n2315         * i_mp : Current at the maximum-power point (A)\n2316         * v_oc : Open-circuit voltage (V)\n2317         * v_mp : Voltage at maximum-power point (V)\n2318         * p_mp : Power at maximum-power point (W)\n2319         * i_x : Current at module V = 0.5Voc, defines 4th point on I-V\n2320           curve for modeling curve shape\n2321         * i_xx : Current at module V = 0.5(Voc+Vmp), defines 5th point on\n2322           I-V curve for modeling curve shape\n2323 \n2324     Notes\n2325     -----\n2326     The SAPM parameters which are required in ``module`` are\n2327     listed in the following table.\n2328 \n2329     The Sandia module database contains parameter values for a limited set\n2330     of modules. The CEC module database does not contain these parameters.\n2331     Both databases can be accessed using :py:func:`retrieve_sam`.\n2332 \n2333     ================   ========================================================\n2334     Key                Description\n2335     ================   ========================================================\n2336     A0-A4              The airmass coefficients used in calculating\n2337                        effective irradiance\n2338     B0-B5              The angle of incidence coefficients used in calculating\n2339                        effective irradiance\n2340     C0-C7              The empirically determined coefficients relating\n2341                        Imp, Vmp, Ix, and Ixx to effective irradiance\n2342     Isco               Short circuit current at reference condition (amps)\n2343     Impo               Maximum power current at reference condition (amps)\n2344     Voco               Open circuit voltage at reference condition (amps)\n2345     Vmpo               Maximum power voltage at reference condition (amps)\n2346     Aisc               Short circuit current temperature coefficient at\n2347                        reference condition (1/C)\n2348     Aimp               Maximum power current temperature coefficient at\n2349                        reference condition (1/C)\n2350     Bvoco              Open circuit voltage temperature coefficient at\n2351                        reference condition (V/C)\n2352     Mbvoc              Coefficient providing the irradiance dependence for the\n2353                        BetaVoc temperature coefficient at reference irradiance\n2354                        (V/C)\n2355     Bvmpo              Maximum power voltage temperature coefficient at\n2356                        reference condition\n2357     Mbvmp              Coefficient providing the irradiance dependence for the\n2358                        BetaVmp temperature coefficient at reference irradiance\n2359                        (V/C)\n2360     N                  Empirically determined \"diode factor\" (dimensionless)\n2361     Cells_in_Series    Number of cells in series in a module's cell string(s)\n2362     IXO                Ix at reference conditions\n2363     IXXO               Ixx at reference conditions\n2364     FD                 Fraction of diffuse irradiance used by module\n2365     ================   ========================================================\n2366 \n2367     References\n2368     ----------\n2369     .. [1] King, D. et al, 2004, \"Sandia Photovoltaic Array Performance\n2370        Model\", SAND Report 3535, Sandia National Laboratories, Albuquerque,\n2371        NM.\n2372 \n2373     See Also\n2374     --------\n2375     retrieve_sam\n2376     pvlib.temperature.sapm_cell\n2377     pvlib.temperature.sapm_module\n2378     '''\n2379 \n2380     # TODO: someday, change temp_ref and irrad_ref to reference_temperature and\n2381     # reference_irradiance and expose\n2382     temp_ref = 25\n2383     irrad_ref = 1000\n2384 \n2385     q = 1.60218e-19  # Elementary charge in units of coulombs\n2386     kb = 1.38066e-23  # Boltzmann's constant in units of J/K\n2387 \n2388     # avoid problem with integer input\n2389     Ee = np.array(effective_irradiance, dtype='float64') / irrad_ref\n2390 \n2391     # set up masking for 0, positive, and nan inputs\n2392     Ee_gt_0 = np.full_like(Ee, False, dtype='bool')\n2393     Ee_eq_0 = np.full_like(Ee, False, dtype='bool')\n2394     notnan = ~np.isnan(Ee)\n2395     np.greater(Ee, 0, where=notnan, out=Ee_gt_0)\n2396     np.equal(Ee, 0, where=notnan, out=Ee_eq_0)\n2397 \n2398     Bvmpo = module['Bvmpo'] + module['Mbvmp']*(1 - Ee)\n2399     Bvoco = module['Bvoco'] + module['Mbvoc']*(1 - Ee)\n2400     delta = module['N'] * kb * (temp_cell + 273.15) / q\n2401 \n2402     # avoid repeated computation\n2403     logEe = np.full_like(Ee, np.nan)\n2404     np.log(Ee, where=Ee_gt_0, out=logEe)\n2405     logEe = np.where(Ee_eq_0, -np.inf, logEe)\n2406     # avoid repeated __getitem__\n2407     cells_in_series = module['Cells_in_Series']\n2408 \n2409     out = OrderedDict()\n2410 \n2411     out['i_sc'] = (\n2412         module['Isco'] * Ee * (1 + module['Aisc']*(temp_cell - temp_ref)))\n2413 \n2414     out['i_mp'] = (\n2415         module['Impo'] * (module['C0']*Ee + module['C1']*(Ee**2)) *\n2416         (1 + module['Aimp']*(temp_cell - temp_ref)))\n2417 \n2418     out['v_oc'] = np.maximum(0, (\n2419         module['Voco'] + cells_in_series * delta * logEe +\n2420         Bvoco*(temp_cell - temp_ref)))\n2421 \n2422     out['v_mp'] = np.maximum(0, (\n2423         module['Vmpo'] +\n2424         module['C2'] * cells_in_series * delta * logEe +\n2425         module['C3'] * cells_in_series * ((delta * logEe) ** 2) +\n2426         Bvmpo*(temp_cell - temp_ref)))\n2427 \n2428     out['p_mp'] = out['i_mp'] * out['v_mp']\n2429 \n2430     out['i_x'] = (\n2431         module['IXO'] * (module['C4']*Ee + module['C5']*(Ee**2)) *\n2432         (1 + module['Aisc']*(temp_cell - temp_ref)))\n2433 \n2434     # the Ixx calculation in King 2004 has a typo (mixes up Aisc and Aimp)\n2435     out['i_xx'] = (\n2436         module['IXXO'] * (module['C6']*Ee + module['C7']*(Ee**2)) *\n2437         (1 + module['Aisc']*(temp_cell - temp_ref)))\n2438 \n2439     if isinstance(out['i_sc'], pd.Series):\n2440         out = pd.DataFrame(out)\n2441 \n2442     return out\n2443 \n2444 \n2445 def sapm_spectral_loss(airmass_absolute, module):\n2446     \"\"\"\n2447     Calculates the SAPM spectral loss coefficient, F1.\n2448 \n2449     Parameters\n2450     ----------\n2451     airmass_absolute : numeric\n2452         Absolute airmass\n2453 \n2454     module : dict-like\n2455         A dict, Series, or DataFrame defining the SAPM performance\n2456         parameters. See the :py:func:`sapm` notes section for more\n2457         details.\n2458 \n2459     Returns\n2460     -------\n2461     F1 : numeric\n2462         The SAPM spectral loss coefficient.\n2463 \n2464     Notes\n2465     -----\n2466     nan airmass values will result in 0 output.\n2467     \"\"\"\n2468 \n2469     am_coeff = [module['A4'], module['A3'], module['A2'], module['A1'],\n2470                 module['A0']]\n2471 \n2472     spectral_loss = np.polyval(am_coeff, airmass_absolute)\n2473 \n2474     spectral_loss = np.where(np.isnan(spectral_loss), 0, spectral_loss)\n2475 \n2476     spectral_loss = np.maximum(0, spectral_loss)\n2477 \n2478     if isinstance(airmass_absolute, pd.Series):\n2479         spectral_loss = pd.Series(spectral_loss, airmass_absolute.index)\n2480 \n2481     return spectral_loss\n2482 \n2483 \n2484 def sapm_effective_irradiance(poa_direct, poa_diffuse, airmass_absolute, aoi,\n2485                               module):\n2486     r\"\"\"\n2487     Calculates the SAPM effective irradiance using the SAPM spectral\n2488     loss and SAPM angle of incidence loss functions.\n2489 \n2490     Parameters\n2491     ----------\n2492     poa_direct : numeric\n2493         The direct irradiance incident upon the module. [W/m2]\n2494 \n2495     poa_diffuse : numeric\n2496         The diffuse irradiance incident on module.  [W/m2]\n2497 \n2498     airmass_absolute : numeric\n2499         Absolute airmass. [unitless]\n2500 \n2501     aoi : numeric\n2502         Angle of incidence. [degrees]\n2503 \n2504     module : dict-like\n2505         A dict, Series, or DataFrame defining the SAPM performance\n2506         parameters. See the :py:func:`sapm` notes section for more\n2507         details.\n2508 \n2509     Returns\n2510     -------\n2511     effective_irradiance : numeric\n2512         Effective irradiance accounting for reflections and spectral content.\n2513         [W/m2]\n2514 \n2515     Notes\n2516     -----\n2517     The SAPM model for effective irradiance [1]_ translates broadband direct\n2518     and diffuse irradiance on the plane of array to the irradiance absorbed by\n2519     a module's cells.\n2520 \n2521     The model is\n2522     .. math::\n2523 \n2524         `Ee = f_1(AM_a) (E_b f_2(AOI) + f_d E_d)`\n2525 \n2526     where :math:`Ee` is effective irradiance (W/m2), :math:`f_1` is a fourth\n2527     degree polynomial in air mass :math:`AM_a`, :math:`E_b` is beam (direct)\n2528     irradiance on the plane of array, :math:`E_d` is diffuse irradiance on the\n2529     plane of array, :math:`f_2` is a fifth degree polynomial in the angle of\n2530     incidence :math:`AOI`, and :math:`f_d` is the fraction of diffuse\n2531     irradiance on the plane of array that is not reflected away.\n2532 \n2533     References\n2534     ----------\n2535     .. [1] D. King et al, \"Sandia Photovoltaic Array Performance Model\",\n2536        SAND2004-3535, Sandia National Laboratories, Albuquerque, NM\n2537 \n2538     See also\n2539     --------\n2540     pvlib.iam.sapm\n2541     pvlib.pvsystem.sapm_spectral_loss\n2542     pvlib.pvsystem.sapm\n2543     \"\"\"\n2544 \n2545     F1 = sapm_spectral_loss(airmass_absolute, module)\n2546     F2 = iam.sapm(aoi, module)\n2547 \n2548     Ee = F1 * (poa_direct * F2 + module['FD'] * poa_diffuse)\n2549 \n2550     return Ee\n2551 \n2552 \n2553 def singlediode(photocurrent, saturation_current, resistance_series,\n2554                 resistance_shunt, nNsVth, ivcurve_pnts=None,\n2555                 method='lambertw'):\n2556     r\"\"\"\n2557     Solve the single-diode equation to obtain a photovoltaic IV curve.\n2558 \n2559     Solves the single diode equation [1]_\n2560 \n2561     .. math::\n2562 \n2563         I = I_L -\n2564             I_0 \\left[\n2565                 \\exp \\left(\\frac{V+I R_s}{n N_s V_{th}} \\right)-1\n2566             \\right] -\n2567             \\frac{V + I R_s}{R_{sh}}\n2568 \n2569     for :math:`I` and :math:`V` when given :math:`I_L, I_0, R_s, R_{sh},` and\n2570     :math:`n N_s V_{th}` which are described later. Returns a DataFrame\n2571     which contains the 5 points on the I-V curve specified in\n2572     [3]_. If all :math:`I_L, I_0, R_s, R_{sh},` and\n2573     :math:`n N_s V_{th}` are scalar, a single curve is returned, if any\n2574     are Series (of the same length), multiple IV curves are calculated.\n2575 \n2576     The input parameters can be calculated from meteorological data using a\n2577     function for a single diode model, e.g.,\n2578     :py:func:`~pvlib.pvsystem.calcparams_desoto`.\n2579 \n2580     Parameters\n2581     ----------\n2582     photocurrent : numeric\n2583         Light-generated current :math:`I_L` (photocurrent)\n2584         ``0 <= photocurrent``. [A]\n2585 \n2586     saturation_current : numeric\n2587         Diode saturation :math:`I_0` current under desired IV curve\n2588         conditions. ``0 < saturation_current``. [A]\n2589 \n2590     resistance_series : numeric\n2591         Series resistance :math:`R_s` under desired IV curve conditions.\n2592         ``0 <= resistance_series < numpy.inf``.  [ohm]\n2593 \n2594     resistance_shunt : numeric\n2595         Shunt resistance :math:`R_{sh}` under desired IV curve conditions.\n2596         ``0 < resistance_shunt <= numpy.inf``.  [ohm]\n2597 \n2598     nNsVth : numeric\n2599         The product of three components: 1) the usual diode ideality factor\n2600         :math:`n`, 2) the number of cells in series :math:`N_s`, and 3)\n2601         the cell thermal voltage\n2602         :math:`V_{th}`. The thermal voltage of the cell (in volts) may be\n2603         calculated as :math:`k_B T_c / q`, where :math:`k_B` is\n2604         Boltzmann's constant (J/K), :math:`T_c` is the temperature of the p-n\n2605         junction in Kelvin, and :math:`q` is the charge of an electron\n2606         (coulombs). ``0 < nNsVth``.  [V]\n2607 \n2608     ivcurve_pnts : None or int, default None\n2609         Number of points in the desired IV curve. If None or 0, no points on\n2610         the IV curves will be produced.\n2611 \n2612     method : str, default 'lambertw'\n2613         Determines the method used to calculate points on the IV curve. The\n2614         options are ``'lambertw'``, ``'newton'``, or ``'brentq'``.\n2615 \n2616     Returns\n2617     -------\n2618     OrderedDict or DataFrame\n2619 \n2620     The returned dict-like object always contains the keys/columns:\n2621 \n2622         * i_sc - short circuit current in amperes.\n2623         * v_oc - open circuit voltage in volts.\n2624         * i_mp - current at maximum power point in amperes.\n2625         * v_mp - voltage at maximum power point in volts.\n2626         * p_mp - power at maximum power point in watts.\n2627         * i_x - current, in amperes, at ``v = 0.5*v_oc``.\n2628         * i_xx - current, in amperes, at ``V = 0.5*(v_oc+v_mp)``.\n2629 \n2630     If ivcurve_pnts is greater than 0, the output dictionary will also\n2631     include the keys:\n2632 \n2633         * i - IV curve current in amperes.\n2634         * v - IV curve voltage in volts.\n2635 \n2636     The output will be an OrderedDict if photocurrent is a scalar,\n2637     array, or ivcurve_pnts is not None.\n2638 \n2639     The output will be a DataFrame if photocurrent is a Series and\n2640     ivcurve_pnts is None.\n2641 \n2642     See also\n2643     --------\n2644     calcparams_desoto\n2645     calcparams_cec\n2646     calcparams_pvsyst\n2647     sapm\n2648     pvlib.singlediode.bishop88\n2649 \n2650     Notes\n2651     -----\n2652     If the method is ``'lambertw'`` then the solution employed to solve the\n2653     implicit diode equation utilizes the Lambert W function to obtain an\n2654     explicit function of :math:`V=f(I)` and :math:`I=f(V)` as shown in [2]_.\n2655 \n2656     If the method is ``'newton'`` then the root-finding Newton-Raphson method\n2657     is used. It should be safe for well behaved IV-curves, but the ``'brentq'``\n2658     method is recommended for reliability.\n2659 \n2660     If the method is ``'brentq'`` then Brent's bisection search method is used\n2661     that guarantees convergence by bounding the voltage between zero and\n2662     open-circuit.\n2663 \n2664     If the method is either ``'newton'`` or ``'brentq'`` and ``ivcurve_pnts``\n2665     are indicated, then :func:`pvlib.singlediode.bishop88` [4]_ is used to\n2666     calculate the points on the IV curve points at diode voltages from zero to\n2667     open-circuit voltage with a log spacing that gets closer as voltage\n2668     increases. If the method is ``'lambertw'`` then the calculated points on\n2669     the IV curve are linearly spaced.\n2670 \n2671     References\n2672     ----------\n2673     .. [1] S.R. Wenham, M.A. Green, M.E. Watt, \"Applied Photovoltaics\" ISBN\n2674        0 86758 909 4\n2675 \n2676     .. [2] A. Jain, A. Kapoor, \"Exact analytical solutions of the\n2677        parameters of real solar cells using Lambert W-function\", Solar\n2678        Energy Materials and Solar Cells, 81 (2004) 269-277.\n2679 \n2680     .. [3] D. King et al, \"Sandia Photovoltaic Array Performance Model\",\n2681        SAND2004-3535, Sandia National Laboratories, Albuquerque, NM\n2682 \n2683     .. [4] \"Computer simulation of the effects of electrical mismatches in\n2684        photovoltaic cell interconnection circuits\" JW Bishop, Solar Cell (1988)\n2685        https://doi.org/10.1016/0379-6787(88)90059-2\n2686     \"\"\"\n2687     # Calculate points on the IV curve using the LambertW solution to the\n2688     # single diode equation\n2689     if method.lower() == 'lambertw':\n2690         out = _singlediode._lambertw(\n2691             photocurrent, saturation_current, resistance_series,\n2692             resistance_shunt, nNsVth, ivcurve_pnts\n2693         )\n2694         i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx = out[:7]\n2695         if ivcurve_pnts:\n2696             ivcurve_i, ivcurve_v = out[7:]\n2697     else:\n2698         # Calculate points on the IV curve using either 'newton' or 'brentq'\n2699         # methods. Voltages are determined by first solving the single diode\n2700         # equation for the diode voltage V_d then backing out voltage\n2701         args = (photocurrent, saturation_current, resistance_series,\n2702                 resistance_shunt, nNsVth)  # collect args\n2703         v_oc = _singlediode.bishop88_v_from_i(\n2704             0.0, *args, method=method.lower()\n2705         )\n2706         i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n2707             *args, method=method.lower()\n2708         )\n2709         i_sc = _singlediode.bishop88_i_from_v(\n2710             0.0, *args, method=method.lower()\n2711         )\n2712         i_x = _singlediode.bishop88_i_from_v(\n2713             v_oc / 2.0, *args, method=method.lower()\n2714         )\n2715         i_xx = _singlediode.bishop88_i_from_v(\n2716             (v_oc + v_mp) / 2.0, *args, method=method.lower()\n2717         )\n2718 \n2719         # calculate the IV curve if requested using bishop88\n2720         if ivcurve_pnts:\n2721             vd = v_oc * (\n2722                 (11.0 - np.logspace(np.log10(11.0), 0.0, ivcurve_pnts)) / 10.0\n2723             )\n2724             ivcurve_i, ivcurve_v, _ = _singlediode.bishop88(vd, *args)\n2725 \n2726     out = OrderedDict()\n2727     out['i_sc'] = i_sc\n2728     out['v_oc'] = v_oc\n2729     out['i_mp'] = i_mp\n2730     out['v_mp'] = v_mp\n2731     out['p_mp'] = p_mp\n2732     out['i_x'] = i_x\n2733     out['i_xx'] = i_xx\n2734 \n2735     if ivcurve_pnts:\n2736 \n2737         out['v'] = ivcurve_v\n2738         out['i'] = ivcurve_i\n2739 \n2740     if isinstance(photocurrent, pd.Series) and not ivcurve_pnts:\n2741         out = pd.DataFrame(out, index=photocurrent.index)\n2742 \n2743     return out\n2744 \n2745 \n2746 def max_power_point(photocurrent, saturation_current, resistance_series,\n2747                     resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n2748                     method='brentq'):\n2749     \"\"\"\n2750     Given the single diode equation coefficients, calculates the maximum power\n2751     point (MPP).\n2752 \n2753     Parameters\n2754     ----------\n2755     photocurrent : numeric\n2756         photo-generated current [A]\n2757     saturation_current : numeric\n2758         diode reverse saturation current [A]\n2759     resistance_series : numeric\n2760         series resitance [ohms]\n2761     resistance_shunt : numeric\n2762         shunt resitance [ohms]\n2763     nNsVth : numeric\n2764         product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n2765         and number of serices cells ``Ns``\n2766     d2mutau : numeric, default 0\n2767         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n2768         (a-Si) modules that accounts for recombination current in the\n2769         intrinsic layer. The value is the ratio of intrinsic layer thickness\n2770         squared :math:`d^2` to the diffusion length of charge carriers\n2771         :math:`\\\\mu \\\\tau`. [V]\n2772     NsVbi : numeric, default np.inf\n2773         PVsyst parameter for cadmium-telluride (CdTe) and amorphous-silicon\n2774         (a-Si) modules that is the product of the PV module number of series\n2775         cells ``Ns`` and the builtin voltage ``Vbi`` of the intrinsic layer.\n2776         [V].\n2777     method : str\n2778         either ``'newton'`` or ``'brentq'``\n2779 \n2780     Returns\n2781     -------\n2782     OrderedDict or pandas.Datafrane\n2783         ``(i_mp, v_mp, p_mp)``\n2784 \n2785     Notes\n2786     -----\n2787     Use this function when you only want to find the maximum power point. Use\n2788     :func:`singlediode` when you need to find additional points on the IV\n2789     curve. This function uses Brent's method by default because it is\n2790     guaranteed to converge.\n2791     \"\"\"\n2792     i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n2793         photocurrent, saturation_current, resistance_series,\n2794         resistance_shunt, nNsVth, d2mutau=0, NsVbi=np.Inf,\n2795         method=method.lower()\n2796     )\n2797     if isinstance(photocurrent, pd.Series):\n2798         ivp = {'i_mp': i_mp, 'v_mp': v_mp, 'p_mp': p_mp}\n2799         out = pd.DataFrame(ivp, index=photocurrent.index)\n2800     else:\n2801         out = OrderedDict()\n2802         out['i_mp'] = i_mp\n2803         out['v_mp'] = v_mp\n2804         out['p_mp'] = p_mp\n2805     return out\n2806 \n2807 \n2808 def v_from_i(resistance_shunt, resistance_series, nNsVth, current,\n2809              saturation_current, photocurrent, method='lambertw'):\n2810     '''\n2811     Device voltage at the given device current for the single diode model.\n2812 \n2813     Uses the single diode model (SDM) as described in, e.g.,\n2814     Jain and Kapoor 2004 [1]_.\n2815     The solution is per Eq 3 of [1]_ except when resistance_shunt=numpy.inf,\n2816     in which case the explict solution for voltage is used.\n2817     Ideal device parameters are specified by resistance_shunt=np.inf and\n2818     resistance_series=0.\n2819     Inputs to this function can include scalars and pandas.Series, but it is\n2820     the caller's responsibility to ensure that the arguments are all float64\n2821     and within the proper ranges.\n2822 \n2823     Parameters\n2824     ----------\n2825     resistance_shunt : numeric\n2826         Shunt resistance in ohms under desired IV curve conditions.\n2827         Often abbreviated ``Rsh``.\n2828         0 < resistance_shunt <= numpy.inf\n2829 \n2830     resistance_series : numeric\n2831         Series resistance in ohms under desired IV curve conditions.\n2832         Often abbreviated ``Rs``.\n2833         0 <= resistance_series < numpy.inf\n2834 \n2835     nNsVth : numeric\n2836         The product of three components. 1) The usual diode ideal factor\n2837         (n), 2) the number of cells in series (Ns), and 3) the cell\n2838         thermal voltage under the desired IV curve conditions (Vth). The\n2839         thermal voltage of the cell (in volts) may be calculated as\n2840         ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),\n2841         temp_cell is the temperature of the p-n junction in Kelvin, and\n2842         q is the charge of an electron (coulombs).\n2843         0 < nNsVth\n2844 \n2845     current : numeric\n2846         The current in amperes under desired IV curve conditions.\n2847 \n2848     saturation_current : numeric\n2849         Diode saturation current in amperes under desired IV curve\n2850         conditions. Often abbreviated ``I_0``.\n2851         0 < saturation_current\n2852 \n2853     photocurrent : numeric\n2854         Light-generated current (photocurrent) in amperes under desired\n2855         IV curve conditions. Often abbreviated ``I_L``.\n2856         0 <= photocurrent\n2857 \n2858     method : str\n2859         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n2860         ``'brentq'`` is limited to 1st quadrant only.\n2861 \n2862     Returns\n2863     -------\n2864     current : np.ndarray or scalar\n2865 \n2866     References\n2867     ----------\n2868     .. [1] A. Jain, A. Kapoor, \"Exact analytical solutions of the\n2869        parameters of real solar cells using Lambert W-function\", Solar\n2870        Energy Materials and Solar Cells, 81 (2004) 269-277.\n2871     '''\n2872     if method.lower() == 'lambertw':\n2873         return _singlediode._lambertw_v_from_i(\n2874             resistance_shunt, resistance_series, nNsVth, current,\n2875             saturation_current, photocurrent\n2876         )\n2877     else:\n2878         # Calculate points on the IV curve using either 'newton' or 'brentq'\n2879         # methods. Voltages are determined by first solving the single diode\n2880         # equation for the diode voltage V_d then backing out voltage\n2881         args = (current, photocurrent, saturation_current,\n2882                 resistance_series, resistance_shunt, nNsVth)\n2883         V = _singlediode.bishop88_v_from_i(*args, method=method.lower())\n2884         # find the right size and shape for returns\n2885         size, shape = _singlediode._get_size_and_shape(args)\n2886         if size <= 1:\n2887             if shape is not None:\n2888                 V = np.tile(V, shape)\n2889         if np.isnan(V).any() and size <= 1:\n2890             V = np.repeat(V, size)\n2891             if shape is not None:\n2892                 V = V.reshape(shape)\n2893         return V\n2894 \n2895 \n2896 def i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n2897              saturation_current, photocurrent, method='lambertw'):\n2898     '''\n2899     Device current at the given device voltage for the single diode model.\n2900 \n2901     Uses the single diode model (SDM) as described in, e.g.,\n2902      Jain and Kapoor 2004 [1]_.\n2903     The solution is per Eq 2 of [1] except when resistance_series=0,\n2904      in which case the explict solution for current is used.\n2905     Ideal device parameters are specified by resistance_shunt=np.inf and\n2906      resistance_series=0.\n2907     Inputs to this function can include scalars and pandas.Series, but it is\n2908      the caller's responsibility to ensure that the arguments are all float64\n2909      and within the proper ranges.\n2910 \n2911     Parameters\n2912     ----------\n2913     resistance_shunt : numeric\n2914         Shunt resistance in ohms under desired IV curve conditions.\n2915         Often abbreviated ``Rsh``.\n2916         0 < resistance_shunt <= numpy.inf\n2917 \n2918     resistance_series : numeric\n2919         Series resistance in ohms under desired IV curve conditions.\n2920         Often abbreviated ``Rs``.\n2921         0 <= resistance_series < numpy.inf\n2922 \n2923     nNsVth : numeric\n2924         The product of three components. 1) The usual diode ideal factor\n2925         (n), 2) the number of cells in series (Ns), and 3) the cell\n2926         thermal voltage under the desired IV curve conditions (Vth). The\n2927         thermal voltage of the cell (in volts) may be calculated as\n2928         ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),\n2929         temp_cell is the temperature of the p-n junction in Kelvin, and\n2930         q is the charge of an electron (coulombs).\n2931         0 < nNsVth\n2932 \n2933     voltage : numeric\n2934         The voltage in Volts under desired IV curve conditions.\n2935 \n2936     saturation_current : numeric\n2937         Diode saturation current in amperes under desired IV curve\n2938         conditions. Often abbreviated ``I_0``.\n2939         0 < saturation_current\n2940 \n2941     photocurrent : numeric\n2942         Light-generated current (photocurrent) in amperes under desired\n2943         IV curve conditions. Often abbreviated ``I_L``.\n2944         0 <= photocurrent\n2945 \n2946     method : str\n2947         Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n2948         ``'brentq'`` is limited to 1st quadrant only.\n2949 \n2950     Returns\n2951     -------\n2952     current : np.ndarray or scalar\n2953 \n2954     References\n2955     ----------\n2956     .. [1] A. Jain, A. Kapoor, \"Exact analytical solutions of the\n2957        parameters of real solar cells using Lambert W-function\", Solar\n2958        Energy Materials and Solar Cells, 81 (2004) 269-277.\n2959     '''\n2960     if method.lower() == 'lambertw':\n2961         return _singlediode._lambertw_i_from_v(\n2962             resistance_shunt, resistance_series, nNsVth, voltage,\n2963             saturation_current, photocurrent\n2964         )\n2965     else:\n2966         # Calculate points on the IV curve using either 'newton' or 'brentq'\n2967         # methods. Voltages are determined by first solving the single diode\n2968         # equation for the diode voltage V_d then backing out voltage\n2969         args = (voltage, photocurrent, saturation_current, resistance_series,\n2970                 resistance_shunt, nNsVth)\n2971         current = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n2972         # find the right size and shape for returns\n2973         size, shape = _singlediode._get_size_and_shape(args)\n2974         if size <= 1:\n2975             if shape is not None:\n2976                 current = np.tile(current, shape)\n2977         if np.isnan(current).any() and size <= 1:\n2978             current = np.repeat(current, size)\n2979             if shape is not None:\n2980                 current = current.reshape(shape)\n2981         return current\n2982 \n2983 \n2984 def scale_voltage_current_power(data, voltage=1, current=1):\n2985     \"\"\"\n2986     Scales the voltage, current, and power in data by the voltage\n2987     and current factors.\n2988 \n2989     Parameters\n2990     ----------\n2991     data: DataFrame\n2992         May contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n2993         'i_sc', 'p_mp'`.\n2994     voltage: numeric, default 1\n2995         The amount by which to multiply the voltages.\n2996     current: numeric, default 1\n2997         The amount by which to multiply the currents.\n2998 \n2999     Returns\n3000     -------\n3001     scaled_data: DataFrame\n3002         A scaled copy of the input data.\n3003         `'p_mp'` is scaled by `voltage * current`.\n3004     \"\"\"\n3005 \n3006     # as written, only works with a DataFrame\n3007     # could make it work with a dict, but it would be more verbose\n3008     voltage_keys = ['v_mp', 'v_oc']\n3009     current_keys = ['i_mp', 'i_x', 'i_xx', 'i_sc']\n3010     power_keys = ['p_mp']\n3011     voltage_df = data.filter(voltage_keys, axis=1) * voltage\n3012     current_df = data.filter(current_keys, axis=1) * current\n3013     power_df = data.filter(power_keys, axis=1) * voltage * current\n3014     df = pd.concat([voltage_df, current_df, power_df], axis=1)\n3015     df_sorted = df[data.columns]  # retain original column order\n3016     return df_sorted\n3017 \n3018 \n3019 def pvwatts_dc(g_poa_effective, temp_cell, pdc0, gamma_pdc, temp_ref=25.):\n3020     r\"\"\"\n3021     Implements NREL's PVWatts DC power model. The PVWatts DC model [1]_ is:\n3022 \n3023     .. math::\n3024 \n3025         P_{dc} = \\frac{G_{poa eff}}{1000} P_{dc0} ( 1 + \\gamma_{pdc} (T_{cell} - T_{ref}))\n3026 \n3027     Note that the pdc0 is also used as a symbol in\n3028     :py:func:`pvlib.inverter.pvwatts`. pdc0 in this function refers to the DC\n3029     power of the modules at reference conditions. pdc0 in\n3030     :py:func:`pvlib.inverter.pvwatts` refers to the DC power input limit of\n3031     the inverter.\n3032 \n3033     Parameters\n3034     ----------\n3035     g_poa_effective: numeric\n3036         Irradiance transmitted to the PV cells. To be\n3037         fully consistent with PVWatts, the user must have already\n3038         applied angle of incidence losses, but not soiling, spectral,\n3039         etc. [W/m^2]\n3040     temp_cell: numeric\n3041         Cell temperature [C].\n3042     pdc0: numeric\n3043         Power of the modules at 1000 W/m^2 and cell reference temperature. [W]\n3044     gamma_pdc: numeric\n3045         The temperature coefficient of power. Typically -0.002 to\n3046         -0.005 per degree C. [1/C]\n3047     temp_ref: numeric, default 25.0\n3048         Cell reference temperature. PVWatts defines it to be 25 C and\n3049         is included here for flexibility. [C]\n3050 \n3051     Returns\n3052     -------\n3053     pdc: numeric\n3054         DC power.\n3055 \n3056     References\n3057     ----------\n3058     .. [1] A. P. Dobos, \"PVWatts Version 5 Manual\"\n3059            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf\n3060            (2014).\n3061     \"\"\"  # noqa: E501\n3062 \n3063     pdc = (g_poa_effective * 0.001 * pdc0 *\n3064            (1 + gamma_pdc * (temp_cell - temp_ref)))\n3065 \n3066     return pdc\n3067 \n3068 \n3069 def pvwatts_losses(soiling=2, shading=3, snow=0, mismatch=2, wiring=2,\n3070                    connections=0.5, lid=1.5, nameplate_rating=1, age=0,\n3071                    availability=3):\n3072     r\"\"\"\n3073     Implements NREL's PVWatts system loss model.\n3074     The PVWatts loss model [1]_ is:\n3075 \n3076     .. math::\n3077 \n3078         L_{total}(\\%) = 100 [ 1 - \\Pi_i ( 1 - \\frac{L_i}{100} ) ]\n3079 \n3080     All parameters must be in units of %. Parameters may be\n3081     array-like, though all array sizes must match.\n3082 \n3083     Parameters\n3084     ----------\n3085     soiling: numeric, default 2\n3086     shading: numeric, default 3\n3087     snow: numeric, default 0\n3088     mismatch: numeric, default 2\n3089     wiring: numeric, default 2\n3090     connections: numeric, default 0.5\n3091     lid: numeric, default 1.5\n3092         Light induced degradation\n3093     nameplate_rating: numeric, default 1\n3094     age: numeric, default 0\n3095     availability: numeric, default 3\n3096 \n3097     Returns\n3098     -------\n3099     losses: numeric\n3100         System losses in units of %.\n3101 \n3102     References\n3103     ----------\n3104     .. [1] A. P. Dobos, \"PVWatts Version 5 Manual\"\n3105            http://pvwatts.nrel.gov/downloads/pvwattsv5.pdf\n3106            (2014).\n3107     \"\"\"\n3108 \n3109     params = [soiling, shading, snow, mismatch, wiring, connections, lid,\n3110               nameplate_rating, age, availability]\n3111 \n3112     # manually looping over params allows for numpy/pandas to handle any\n3113     # array-like broadcasting that might be necessary.\n3114     perf = 1\n3115     for param in params:\n3116         perf *= 1 - param/100\n3117 \n3118     losses = (1 - perf) * 100.\n3119 \n3120     return losses\n3121 \n3122 \n3123 def dc_ohms_from_percent(vmp_ref, imp_ref, dc_ohmic_percent,\n3124                          modules_per_string=1,\n3125                          strings=1):\n3126     \"\"\"\n3127     Calculates the equivalent resistance of the wires from a percent\n3128     ohmic loss at STC.\n3129 \n3130     Equivalent resistance is calculated with the function:\n3131 \n3132     .. math::\n3133         Rw = (L_{stc} / 100) * (Varray / Iarray)\n3134 \n3135     :math:`Rw` is the equivalent resistance in ohms\n3136     :math:`Varray` is the Vmp of the modules times modules per string\n3137     :math:`Iarray` is the Imp of the modules times strings per array\n3138     :math:`L_{stc}` is the input dc loss percent\n3139 \n3140     Parameters\n3141     ----------\n3142     vmp_ref: numeric\n3143         Voltage at maximum power in reference conditions [V]\n3144     imp_ref: numeric\n3145         Current at maximum power in reference conditions [V]\n3146     dc_ohmic_percent: numeric, default 0\n3147         input dc loss as a percent, e.g. 1.5% loss is input as 1.5\n3148     modules_per_string: int, default 1\n3149         Number of modules per string in the array.\n3150     strings: int, default 1\n3151         Number of parallel strings in the array.\n3152 \n3153     Returns\n3154     ----------\n3155     Rw: numeric\n3156         Equivalent resistance [ohm]\n3157 \n3158     See Also\n3159     --------\n3160     :py:func:`~pvlib.pvsystem.dc_ohmic_losses`\n3161 \n3162     References\n3163     ----------\n3164     .. [1] PVsyst 7 Help. \"Array ohmic wiring loss\".\n3165        https://www.pvsyst.com/help/ohmic_loss.htm\n3166     \"\"\"\n3167     vmp = modules_per_string * vmp_ref\n3168 \n3169     imp = strings * imp_ref\n3170 \n3171     Rw = (dc_ohmic_percent / 100) * (vmp / imp)\n3172 \n3173     return Rw\n3174 \n3175 \n3176 def dc_ohmic_losses(resistance, current):\n3177     \"\"\"\n3178     Returns ohmic losses in units of power from the equivalent\n3179     resistance of the wires and the operating current.\n3180 \n3181     Parameters\n3182     ----------\n3183     resistance: numeric\n3184         Equivalent resistance of wires [ohm]\n3185     current: numeric, float or array-like\n3186         Operating current [A]\n3187 \n3188     Returns\n3189     ----------\n3190     loss: numeric\n3191         Power Loss [W]\n3192 \n3193     See Also\n3194     --------\n3195     :py:func:`~pvlib.pvsystem.dc_ohms_from_percent`\n3196 \n3197     References\n3198     ----------\n3199     .. [1] PVsyst 7 Help. \"Array ohmic wiring loss\".\n3200        https://www.pvsyst.com/help/ohmic_loss.htm\n3201     \"\"\"\n3202     return resistance * current * current\n3203 \n3204 \n3205 def combine_loss_factors(index, *losses, fill_method='ffill'):\n3206     r\"\"\"\n3207     Combines Series loss fractions while setting a common index.\n3208 \n3209     The separate losses are compounded using the following equation:\n3210 \n3211     .. math::\n3212 \n3213         L_{total} = 1 - [ 1 - \\Pi_i ( 1 - L_i ) ]\n3214 \n3215     :math:`L_{total}` is the total loss returned\n3216     :math:`L_i` is each individual loss factor input\n3217 \n3218     Note the losses must each be a series with a DatetimeIndex.\n3219     All losses will be resampled to match the index parameter using\n3220     the fill method specified (defaults to \"fill forward\").\n3221 \n3222     Parameters\n3223     ----------\n3224     index : DatetimeIndex\n3225         The index of the returned loss factors\n3226 \n3227     *losses : Series\n3228         One or more Series of fractions to be compounded\n3229 \n3230     fill_method : {'ffill', 'bfill', 'nearest'}, default 'ffill'\n3231         Method to use for filling holes in reindexed DataFrame\n3232 \n3233     Returns\n3234     -------\n3235     Series\n3236         Fractions resulting from the combination of each loss factor\n3237     \"\"\"\n3238     combined_factor = 1\n3239 \n3240     for loss in losses:\n3241         loss = loss.reindex(index, method=fill_method)\n3242         combined_factor *= (1 - loss)\n3243 \n3244     return 1 - combined_factor\n3245 \n[end of pvlib/pvsystem.py]\n[start of pvlib/tracking.py]\n1 import numpy as np\n2 import pandas as pd\n3 \n4 from pvlib.tools import cosd, sind, tand\n5 from pvlib.pvsystem import PVSystem, _unwrap_single_value\n6 from pvlib import irradiance, atmosphere\n7 \n8 \n9 class SingleAxisTracker(PVSystem):\n10     \"\"\"\n11     A class for single-axis trackers that inherits the PV modeling methods from\n12     :py:class:`~pvlib.pvsystem.PVSystem`. For details on calculating tracker\n13     rotation see :py:func:`pvlib.tracking.singleaxis`.\n14 \n15     Parameters\n16     ----------\n17     axis_tilt : float, default 0\n18         The tilt of the axis of rotation (i.e, the y-axis defined by\n19         axis_azimuth) with respect to horizontal, in decimal degrees.\n20 \n21     axis_azimuth : float, default 0\n22         A value denoting the compass direction along which the axis of\n23         rotation lies. Measured in decimal degrees east of north.\n24 \n25     max_angle : float, default 90\n26         A value denoting the maximum rotation angle, in decimal degrees,\n27         of the one-axis tracker from its horizontal position (horizontal\n28         if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n29         to rotate to a vertical position to point the panel towards a\n30         horizon. max_angle of 180 degrees allows for full rotation.\n31 \n32     backtrack : bool, default True\n33         Controls whether the tracker has the capability to \"backtrack\"\n34         to avoid row-to-row shading. False denotes no backtrack\n35         capability. True denotes backtrack capability.\n36 \n37     gcr : float, default 2.0/7.0\n38         A value denoting the ground coverage ratio of a tracker system\n39         which utilizes backtracking; i.e. the ratio between the PV array\n40         surface area to total ground area. A tracker system with modules\n41         2 meters wide, centered on the tracking axis, with 6 meters\n42         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n43         provided, a gcr of 2/7 is default. gcr must be <=1.\n44 \n45     cross_axis_tilt : float, default 0.0\n46         The angle, relative to horizontal, of the line formed by the\n47         intersection between the slope containing the tracker axes and a plane\n48         perpendicular to the tracker axes. Cross-axis tilt should be specified\n49         using a right-handed convention. For example, trackers with axis\n50         azimuth of 180 degrees (heading south) will have a negative cross-axis\n51         tilt if the tracker axes plane slopes down to the east and positive\n52         cross-axis tilt if the tracker axes plane slopes up to the east. Use\n53         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n54         `cross_axis_tilt`. [degrees]\n55 \n56     **kwargs\n57         Passed to :py:class:`~pvlib.pvsystem.PVSystem`. If the `arrays`\n58         parameter is specified it must have only a single Array. Furthermore\n59         if a :py:class:`~pvlib.pvsystem.Array` is provided it must have\n60         ``surface_tilt`` and ``surface_azimuth`` equal to None.\n61 \n62     Raises\n63     ------\n64     ValueError\n65         If more than one Array is specified.\n66     ValueError\n67         If an Array is provided with a surface tilt or azimuth not None.\n68 \n69     See also\n70     --------\n71     pvlib.tracking.singleaxis\n72     pvlib.tracking.calc_axis_tilt\n73     pvlib.tracking.calc_cross_axis_tilt\n74     \"\"\"\n75 \n76     def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n77                  backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n78 \n79         arrays = kwargs.get('arrays', [])\n80         if len(arrays) > 1:\n81             raise ValueError(\"SingleAxisTracker does not support \"\n82                              \"multiple arrays.\")\n83         elif len(arrays) == 1:\n84             surface_tilt = arrays[0].surface_tilt\n85             surface_azimuth = arrays[0].surface_azimuth\n86             if surface_tilt is not None or surface_azimuth is not None:\n87                 raise ValueError(\n88                     \"Array must not have surface_tilt or \"\n89                     \"surface_azimuth assigned. You must pass an \"\n90                     \"Array with these fields set to None.\"\n91                 )\n92 \n93         self.axis_tilt = axis_tilt\n94         self.axis_azimuth = axis_azimuth\n95         self.max_angle = max_angle\n96         self.backtrack = backtrack\n97         self.gcr = gcr\n98         self.cross_axis_tilt = cross_axis_tilt\n99 \n100         kwargs['surface_tilt'] = None\n101         kwargs['surface_azimuth'] = None\n102 \n103         super().__init__(**kwargs)\n104 \n105     def __repr__(self):\n106         attrs = ['axis_tilt', 'axis_azimuth', 'max_angle', 'backtrack', 'gcr',\n107                  'cross_axis_tilt']\n108         sat_repr = ('SingleAxisTracker:\\n  ' + '\\n  '.join(\n109             f'{attr}: {getattr(self, attr)}' for attr in attrs))\n110         # get the parent PVSystem info\n111         pvsystem_repr = super().__repr__()\n112         # remove the first line (contains 'PVSystem: \\n')\n113         pvsystem_repr = '\\n'.join(pvsystem_repr.split('\\n')[1:])\n114         return sat_repr + '\\n' + pvsystem_repr\n115 \n116     def singleaxis(self, apparent_zenith, apparent_azimuth):\n117         \"\"\"\n118         Get tracking data. See :py:func:`pvlib.tracking.singleaxis` more\n119         detail.\n120 \n121         Parameters\n122         ----------\n123         apparent_zenith : float, 1d array, or Series\n124             Solar apparent zenith angles in decimal degrees.\n125 \n126         apparent_azimuth : float, 1d array, or Series\n127             Solar apparent azimuth angles in decimal degrees.\n128 \n129         Returns\n130         -------\n131         tracking data\n132         \"\"\"\n133         tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n134                                    self.axis_tilt, self.axis_azimuth,\n135                                    self.max_angle, self.backtrack,\n136                                    self.gcr, self.cross_axis_tilt)\n137 \n138         return tracking_data\n139 \n140     def get_aoi(self, surface_tilt, surface_azimuth, solar_zenith,\n141                 solar_azimuth):\n142         \"\"\"Get the angle of incidence on the system.\n143 \n144         For a given set of solar zenith and azimuth angles, the\n145         surface tilt and azimuth parameters are typically determined\n146         by :py:meth:`~SingleAxisTracker.singleaxis`. The\n147         :py:meth:`~SingleAxisTracker.singleaxis` method also returns\n148         the angle of incidence, so this method is only needed\n149         if using a different tracking algorithm.\n150 \n151         Parameters\n152         ----------\n153         surface_tilt : numeric\n154             Panel tilt from horizontal.\n155         surface_azimuth : numeric\n156             Panel azimuth from north\n157         solar_zenith : float or Series.\n158             Solar zenith angle.\n159         solar_azimuth : float or Series.\n160             Solar azimuth angle.\n161 \n162         Returns\n163         -------\n164         aoi : Series\n165             The angle of incidence in degrees from normal.\n166         \"\"\"\n167 \n168         aoi = irradiance.aoi(surface_tilt, surface_azimuth,\n169                              solar_zenith, solar_azimuth)\n170         return aoi\n171 \n172     @_unwrap_single_value\n173     def get_irradiance(self, surface_tilt, surface_azimuth,\n174                        solar_zenith, solar_azimuth, dni, ghi, dhi,\n175                        dni_extra=None, airmass=None, model='haydavies',\n176                        **kwargs):\n177         \"\"\"\n178         Uses the :func:`irradiance.get_total_irradiance` function to\n179         calculate the plane of array irradiance components on a tilted\n180         surface defined by the input data and ``self.albedo``.\n181 \n182         For a given set of solar zenith and azimuth angles, the\n183         surface tilt and azimuth parameters are typically determined\n184         by :py:meth:`~SingleAxisTracker.singleaxis`.\n185 \n186         Parameters\n187         ----------\n188         surface_tilt : numeric\n189             Panel tilt from horizontal.\n190         surface_azimuth : numeric\n191             Panel azimuth from north\n192         solar_zenith : numeric\n193             Solar zenith angle.\n194         solar_azimuth : numeric\n195             Solar azimuth angle.\n196         dni : float or Series\n197             Direct Normal Irradiance\n198         ghi : float or Series\n199             Global horizontal irradiance\n200         dhi : float or Series\n201             Diffuse horizontal irradiance\n202         dni_extra : float or Series, default None\n203             Extraterrestrial direct normal irradiance\n204         airmass : float or Series, default None\n205             Airmass\n206         model : String, default 'haydavies'\n207             Irradiance model.\n208 \n209         **kwargs\n210             Passed to :func:`irradiance.get_total_irradiance`.\n211 \n212         Returns\n213         -------\n214         poa_irradiance : DataFrame\n215             Column names are: ``total, beam, sky, ground``.\n216         \"\"\"\n217 \n218         # not needed for all models, but this is easier\n219         if dni_extra is None:\n220             dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n221 \n222         if airmass is None:\n223             airmass = atmosphere.get_relative_airmass(solar_zenith)\n224 \n225         # SingleAxisTracker only supports a single Array, but we need the\n226         # validate/iterate machinery so that single length tuple input/output\n227         # is handled the same as PVSystem.get_irradiance. GH 1159\n228         dni = self._validate_per_array(dni, system_wide=True)\n229         ghi = self._validate_per_array(ghi, system_wide=True)\n230         dhi = self._validate_per_array(dhi, system_wide=True)\n231 \n232         return tuple(\n233             irradiance.get_total_irradiance(\n234                 surface_tilt,\n235                 surface_azimuth,\n236                 solar_zenith,\n237                 solar_azimuth,\n238                 dni, ghi, dhi,\n239                 dni_extra=dni_extra,\n240                 airmass=airmass,\n241                 model=model,\n242                 albedo=self.arrays[0].albedo,\n243                 **kwargs)\n244             for array, dni, ghi, dhi in zip(\n245                 self.arrays, dni, ghi, dhi\n246             )\n247         )\n248 \n249 \n250 def singleaxis(apparent_zenith, apparent_azimuth,\n251                axis_tilt=0, axis_azimuth=0, max_angle=90,\n252                backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0):\n253     \"\"\"\n254     Determine the rotation angle of a single-axis tracker when given particular\n255     solar zenith and azimuth angles.\n256 \n257     See [1]_ for details about the equations. Backtracking may be specified,\n258     and if so, a ground coverage ratio is required.\n259 \n260     Rotation angle is determined in a right-handed coordinate system. The\n261     tracker `axis_azimuth` defines the positive y-axis, the positive x-axis is\n262     90 degrees clockwise from the y-axis and parallel to the Earth's surface,\n263     and the positive z-axis is normal to both x & y-axes and oriented skyward.\n264     Rotation angle `tracker_theta` is a right-handed rotation around the y-axis\n265     in the x, y, z coordinate system and indicates tracker position relative to\n266     horizontal. For example, if tracker `axis_azimuth` is 180 (oriented south)\n267     and `axis_tilt` is zero, then a `tracker_theta` of zero is horizontal, a\n268     `tracker_theta` of 30 degrees is a rotation of 30 degrees towards the west,\n269     and a `tracker_theta` of -90 degrees is a rotation to the vertical plane\n270     facing east.\n271 \n272     Parameters\n273     ----------\n274     apparent_zenith : float, 1d array, or Series\n275         Solar apparent zenith angles in decimal degrees.\n276 \n277     apparent_azimuth : float, 1d array, or Series\n278         Solar apparent azimuth angles in decimal degrees.\n279 \n280     axis_tilt : float, default 0\n281         The tilt of the axis of rotation (i.e, the y-axis defined by\n282         axis_azimuth) with respect to horizontal, in decimal degrees.\n283 \n284     axis_azimuth : float, default 0\n285         A value denoting the compass direction along which the axis of\n286         rotation lies. Measured in decimal degrees east of north.\n287 \n288     max_angle : float, default 90\n289         A value denoting the maximum rotation angle, in decimal degrees,\n290         of the one-axis tracker from its horizontal position (horizontal\n291         if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n292         to rotate to a vertical position to point the panel towards a\n293         horizon. max_angle of 180 degrees allows for full rotation.\n294 \n295     backtrack : bool, default True\n296         Controls whether the tracker has the capability to \"backtrack\"\n297         to avoid row-to-row shading. False denotes no backtrack\n298         capability. True denotes backtrack capability.\n299 \n300     gcr : float, default 2.0/7.0\n301         A value denoting the ground coverage ratio of a tracker system\n302         which utilizes backtracking; i.e. the ratio between the PV array\n303         surface area to total ground area. A tracker system with modules\n304         2 meters wide, centered on the tracking axis, with 6 meters\n305         between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n306         provided, a gcr of 2/7 is default. gcr must be <=1.\n307 \n308     cross_axis_tilt : float, default 0.0\n309         The angle, relative to horizontal, of the line formed by the\n310         intersection between the slope containing the tracker axes and a plane\n311         perpendicular to the tracker axes. Cross-axis tilt should be specified\n312         using a right-handed convention. For example, trackers with axis\n313         azimuth of 180 degrees (heading south) will have a negative cross-axis\n314         tilt if the tracker axes plane slopes down to the east and positive\n315         cross-axis tilt if the tracker axes plane slopes up to the east. Use\n316         :func:`~pvlib.tracking.calc_cross_axis_tilt` to calculate\n317         `cross_axis_tilt`. [degrees]\n318 \n319     Returns\n320     -------\n321     dict or DataFrame with the following columns:\n322         * `tracker_theta`: The rotation angle of the tracker.\n323           tracker_theta = 0 is horizontal, and positive rotation angles are\n324           clockwise. [degrees]\n325         * `aoi`: The angle-of-incidence of direct irradiance onto the\n326           rotated panel surface. [degrees]\n327         * `surface_tilt`: The angle between the panel surface and the earth\n328           surface, accounting for panel rotation. [degrees]\n329         * `surface_azimuth`: The azimuth of the rotated panel, determined by\n330           projecting the vector normal to the panel's surface to the earth's\n331           surface. [degrees]\n332 \n333     See also\n334     --------\n335     pvlib.tracking.calc_axis_tilt\n336     pvlib.tracking.calc_cross_axis_tilt\n337 \n338     References\n339     ----------\n340     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n341        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n342        https://www.nrel.gov/docs/fy20osti/76626.pdf\n343     \"\"\"\n344 \n345     # MATLAB to Python conversion by\n346     # Will Holmgren (@wholmgren), U. Arizona. March, 2015.\n347 \n348     if isinstance(apparent_zenith, pd.Series):\n349         index = apparent_zenith.index\n350     else:\n351         index = None\n352 \n353     # convert scalars to arrays\n354     apparent_azimuth = np.atleast_1d(apparent_azimuth)\n355     apparent_zenith = np.atleast_1d(apparent_zenith)\n356 \n357     if apparent_azimuth.ndim > 1 or apparent_zenith.ndim > 1:\n358         raise ValueError('Input dimensions must not exceed 1')\n359 \n360     # Calculate sun position x, y, z using coordinate system as in [1], Eq 1.\n361 \n362     # NOTE: solar elevation = 90 - solar zenith, then use trig identities:\n363     # sin(90-x) = cos(x) & cos(90-x) = sin(x)\n364     sin_zenith = sind(apparent_zenith)\n365     x = sin_zenith * sind(apparent_azimuth)\n366     y = sin_zenith * cosd(apparent_azimuth)\n367     z = cosd(apparent_zenith)\n368 \n369     # Assume the tracker reference frame is right-handed. Positive y-axis is\n370     # oriented along tracking axis; from north, the y-axis is rotated clockwise\n371     # by the axis azimuth and tilted from horizontal by the axis tilt. The\n372     # positive x-axis is 90 deg clockwise from the y-axis and parallel to\n373     # horizontal (e.g., if the y-axis is south, the x-axis is west); the\n374     # positive z-axis is normal to the x and y axes, pointed upward.\n375 \n376     # Calculate sun position (xp, yp, zp) in tracker coordinate system using\n377     # [1] Eq 4.\n378 \n379     cos_axis_azimuth = cosd(axis_azimuth)\n380     sin_axis_azimuth = sind(axis_azimuth)\n381     cos_axis_tilt = cosd(axis_tilt)\n382     sin_axis_tilt = sind(axis_tilt)\n383     xp = x*cos_axis_azimuth - y*sin_axis_azimuth\n384     yp = (x*cos_axis_tilt*sin_axis_azimuth\n385           + y*cos_axis_tilt*cos_axis_azimuth\n386           - z*sin_axis_tilt)\n387     zp = (x*sin_axis_tilt*sin_axis_azimuth\n388           + y*sin_axis_tilt*cos_axis_azimuth\n389           + z*cos_axis_tilt)\n390 \n391     # The ideal tracking angle wid is the rotation to place the sun position\n392     # vector (xp, yp, zp) in the (y, z) plane, which is normal to the panel and\n393     # contains the axis of rotation.  wid = 0 indicates that the panel is\n394     # horizontal. Here, our convention is that a clockwise rotation is\n395     # positive, to view rotation angles in the same frame of reference as\n396     # azimuth. For example, for a system with tracking axis oriented south, a\n397     # rotation toward the east is negative, and a rotation to the west is\n398     # positive. This is a right-handed rotation around the tracker y-axis.\n399 \n400     # Calculate angle from x-y plane to projection of sun vector onto x-z plane\n401     # using [1] Eq. 5.\n402 \n403     wid = np.degrees(np.arctan2(xp, zp))\n404 \n405     # filter for sun above panel horizon\n406     zen_gt_90 = apparent_zenith > 90\n407     wid[zen_gt_90] = np.nan\n408 \n409     # Account for backtracking\n410     if backtrack:\n411         # distance between rows in terms of rack lengths relative to cross-axis\n412         # tilt\n413         axes_distance = 1/(gcr * cosd(cross_axis_tilt))\n414 \n415         # NOTE: account for rare angles below array, see GH 824\n416         temp = np.abs(axes_distance * cosd(wid - cross_axis_tilt))\n417 \n418         # backtrack angle using [1], Eq. 14\n419         with np.errstate(invalid='ignore'):\n420             wc = np.degrees(-np.sign(wid)*np.arccos(temp))\n421 \n422         # NOTE: in the middle of the day, arccos(temp) is out of range because\n423         # there's no row-to-row shade to avoid, & backtracking is unnecessary\n424         # [1], Eqs. 15-16\n425         with np.errstate(invalid='ignore'):\n426             tracker_theta = wid + np.where(temp < 1, wc, 0)\n427     else:\n428         tracker_theta = wid\n429 \n430     # NOTE: max_angle defined relative to zero-point rotation, not the\n431     # system-plane normal\n432     tracker_theta = np.clip(tracker_theta, -max_angle, max_angle)\n433 \n434     # Calculate panel normal vector in panel-oriented x, y, z coordinates.\n435     # y-axis is axis of tracker rotation. tracker_theta is a compass angle\n436     # (clockwise is positive) rather than a trigonometric angle.\n437     # NOTE: the *0 is a trick to preserve NaN values.\n438     panel_norm = np.array([sind(tracker_theta),\n439                            tracker_theta*0,\n440                            cosd(tracker_theta)])\n441 \n442     # sun position in vector format in panel-oriented x, y, z coordinates\n443     sun_vec = np.array([xp, yp, zp])\n444 \n445     # calculate angle-of-incidence on panel\n446     aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\n447 \n448     # Calculate panel tilt and azimuth in a coordinate system where the panel\n449     # tilt is the angle from horizontal, and the panel azimuth is the compass\n450     # angle (clockwise from north) to the projection of the panel's normal to\n451     # the earth's surface. These outputs are provided for convenience and\n452     # comparison with other PV software which use these angle conventions.\n453 \n454     # Project normal vector to earth surface. First rotate about x-axis by\n455     # angle -axis_tilt so that y-axis is also parallel to earth surface, then\n456     # project.\n457 \n458     # Calculate standard rotation matrix\n459     rot_x = np.array([[1, 0, 0],\n460                       [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n461                       [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n462 \n463     # panel_norm_earth contains the normal vector expressed in earth-surface\n464     # coordinates (z normal to surface, y aligned with tracker axis parallel to\n465     # earth)\n466     panel_norm_earth = np.dot(rot_x, panel_norm).T\n467 \n468     # projection to plane tangent to earth surface, in earth surface\n469     # coordinates\n470     projected_normal = np.array([panel_norm_earth[:, 0],\n471                                  panel_norm_earth[:, 1],\n472                                  panel_norm_earth[:, 2]*0]).T\n473 \n474     # calculate vector magnitudes\n475     projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n476 \n477     # renormalize the projected vector, avoid creating nan values.\n478     non_zeros = projected_normal_mag != 0\n479     projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n480                                    projected_normal_mag[non_zeros]).T\n481 \n482     # calculation of surface_azimuth\n483     surface_azimuth = \\\n484         np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n485 \n486     # Rotate 0 reference from panel's x-axis to its y-axis and then back to\n487     # north.\n488     surface_azimuth = 90 - surface_azimuth + axis_azimuth\n489 \n490     # Map azimuth into [0,360) domain.\n491     with np.errstate(invalid='ignore'):\n492         surface_azimuth = surface_azimuth % 360\n493 \n494     # Calculate surface_tilt\n495     dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n496     surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n497 \n498     # Bundle DataFrame for return values and filter for sun below horizon.\n499     out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n500            'surface_tilt': surface_tilt, 'surface_azimuth': surface_azimuth}\n501     if index is not None:\n502         out = pd.DataFrame(out, index=index)\n503         out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n504         out[zen_gt_90] = np.nan\n505     else:\n506         out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n507 \n508     return out\n509 \n510 \n511 def calc_axis_tilt(slope_azimuth, slope_tilt, axis_azimuth):\n512     \"\"\"\n513     Calculate tracker axis tilt in the global reference frame when on a sloped\n514     plane.\n515 \n516     Parameters\n517     ----------\n518     slope_azimuth : float\n519         direction of normal to slope on horizontal [degrees]\n520     slope_tilt : float\n521         tilt of normal to slope relative to vertical [degrees]\n522     axis_azimuth : float\n523         direction of tracker axes on horizontal [degrees]\n524 \n525     Returns\n526     -------\n527     axis_tilt : float\n528         tilt of tracker [degrees]\n529 \n530     See also\n531     --------\n532     pvlib.tracking.singleaxis\n533     pvlib.tracking.calc_cross_axis_tilt\n534 \n535     Notes\n536     -----\n537     See [1]_ for derivation of equations.\n538 \n539     References\n540     ----------\n541     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n542        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n543        https://www.nrel.gov/docs/fy20osti/76626.pdf\n544     \"\"\"\n545     delta_gamma = axis_azimuth - slope_azimuth\n546     # equations 18-19\n547     tan_axis_tilt = cosd(delta_gamma) * tand(slope_tilt)\n548     return np.degrees(np.arctan(tan_axis_tilt))\n549 \n550 \n551 def _calc_tracker_norm(ba, bg, dg):\n552     \"\"\"\n553     Calculate tracker normal, v, cross product of tracker axis and unit normal,\n554     N, to the system slope plane.\n555 \n556     Parameters\n557     ----------\n558     ba : float\n559         axis tilt [degrees]\n560     bg : float\n561         ground tilt [degrees]\n562     dg : float\n563         delta gamma, difference between axis and ground azimuths [degrees]\n564 \n565     Returns\n566     -------\n567     vector : tuple\n568         vx, vy, vz\n569     \"\"\"\n570     cos_ba = cosd(ba)\n571     cos_bg = cosd(bg)\n572     sin_bg = sind(bg)\n573     sin_dg = sind(dg)\n574     vx = sin_dg * cos_ba * cos_bg\n575     vy = sind(ba)*sin_bg + cosd(dg)*cos_ba*cos_bg\n576     vz = -sin_dg*sin_bg*cos_ba\n577     return vx, vy, vz\n578 \n579 \n580 def _calc_beta_c(v, dg, ba):\n581     \"\"\"\n582     Calculate the cross-axis tilt angle.\n583 \n584     Parameters\n585     ----------\n586     v : tuple\n587         tracker normal\n588     dg : float\n589         delta gamma, difference between axis and ground azimuths [degrees]\n590     ba : float\n591         axis tilt [degrees]\n592 \n593     Returns\n594     -------\n595     beta_c : float\n596         cross-axis tilt angle [radians]\n597     \"\"\"\n598     vnorm = np.sqrt(np.dot(v, v))\n599     beta_c = np.arcsin(\n600         ((v[0]*cosd(dg) - v[1]*sind(dg)) * sind(ba) + v[2]*cosd(ba)) / vnorm)\n601     return beta_c\n602 \n603 \n604 def calc_cross_axis_tilt(\n605         slope_azimuth, slope_tilt, axis_azimuth, axis_tilt):\n606     \"\"\"\n607     Calculate the angle, relative to horizontal, of the line formed by the\n608     intersection between the slope containing the tracker axes and a plane\n609     perpendicular to the tracker axes.\n610 \n611     Use the cross-axis tilt to avoid row-to-row shade when backtracking on a\n612     slope not parallel with the axis azimuth. Cross-axis tilt should be\n613     specified using a right-handed convention. For example, trackers with axis\n614     azimuth of 180 degrees (heading south) will have a negative cross-axis tilt\n615     if the tracker axes plane slopes down to the east and positive cross-axis\n616     tilt if the tracker axes plane slopes up to the east.\n617 \n618     Parameters\n619     ----------\n620     slope_azimuth : float\n621         direction of the normal to the slope containing the tracker axes, when\n622         projected on the horizontal [degrees]\n623     slope_tilt : float\n624         angle of the slope containing the tracker axes, relative to horizontal\n625         [degrees]\n626     axis_azimuth : float\n627         direction of tracker axes projected on the horizontal [degrees]\n628     axis_tilt : float\n629         tilt of trackers relative to horizontal [degrees]\n630 \n631     Returns\n632     -------\n633     cross_axis_tilt : float\n634         angle, relative to horizontal, of the line formed by the intersection\n635         between the slope containing the tracker axes and a plane perpendicular\n636         to the tracker axes [degrees]\n637 \n638     See also\n639     --------\n640     pvlib.tracking.singleaxis\n641     pvlib.tracking.calc_axis_tilt\n642 \n643     Notes\n644     -----\n645     See [1]_ for derivation of equations.\n646 \n647     References\n648     ----------\n649     .. [1] Kevin Anderson and Mark Mikofski, \"Slope-Aware Backtracking for\n650        Single-Axis Trackers\", Technical Report NREL/TP-5K00-76626, July 2020.\n651        https://www.nrel.gov/docs/fy20osti/76626.pdf\n652     \"\"\"\n653     # delta-gamma, difference between axis and slope azimuths\n654     delta_gamma = axis_azimuth - slope_azimuth\n655     # equation 22\n656     v = _calc_tracker_norm(axis_tilt, slope_tilt, delta_gamma)\n657     # equation 26\n658     beta_c = _calc_beta_c(v, delta_gamma, axis_tilt)\n659     return np.degrees(beta_c)\n660 \n[end of pvlib/tracking.py]\n</code>\nI need you to solve this issue by generating a single patch file that I can apply directly to this repository using git apply. Please respond with a single patch file in the following format.\n<patch>\n--- a/file.py\n+++ b/file.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n</patch>\n\n\n\n", "full_output": "```patch\n--- a/pvlib/pvsystem.py\n+++ b/pvlib/pvsystem.py\n@@ -1242,6 +1242,16 @@\n \n class Array:\n     \"\"\"\n+    Base class for different types of arrays in a PV system.\n+    This class can be extended to create specific types of arrays\n+    such as fixed tilt and single-axis tracking arrays.\n+    \"\"\"\n+\n+class FixedTiltArray(Array):\n+    \"\"\"\n+    Parameters\n+    ----------\n+    surface_tilt: float or array-like, default 0\n+        Surface tilt angles in decimal degrees.\n+        The tilt angle is defined as degrees from horizontal\n+        (e.g. surface facing up = 0, surface facing horizon = 90)\n+\n+    surface_azimuth: float or array-like, default 180\n+        Azimuth angle of the module surface.\n+        North=0, East=90, South=180, West=270.\n+\n+    **kwargs\n+        Passed to Array. Or copy remainder of Array doc string to be explicit.\n+    \"\"\"\n+\n+class SingleAxisTrackingArray(Array):\n+    \"\"\"\n+    Parameters\n+    ----------\n+    axis_tilt : float, default 0\n+        The tilt of the axis of rotation (i.e, the y-axis defined by\n+        axis_azimuth) with respect to horizontal, in decimal degrees.\n+\n+    axis_azimuth : float, default 0\n+        A value denoting the compass direction along which the axis of\n+        rotation lies. Measured in decimal degrees east of north.\n+\n+    **kwargs\n+        Passed to Array. Or copy remainder of Array doc string to be explicit.\n+    \"\"\"\n+\n+    def __init__(self, axis_tilt=0, axis_azimuth=0, **kwargs):\n+        super().__init__(**kwargs)\n+        self.axis_tilt = axis_tilt\n+        self.axis_azimuth = axis_azimuth\n+\n+    def get_aoi(self, solar_zenith, solar_azimuth):\n+        \"\"\"Get the angle of incidence on the Array(s) in the system.\n+\n+        Parameters\n+        ----------\n+        solar_zenith : float or Series.\n+            Solar zenith angle.\n+        solar_azimuth : float or Series.\n+            Solar azimuth angle.\n+\n+        Returns\n+        -------\n+        aoi : Series or tuple of Series\n+            The angle of incidence\n+        \"\"\"\n+\n+        return irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n+                              solar_zenith, solar_azimuth)\n+\n+    def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n+                       dni_extra=None, airmass=None, model='haydavies',\n+                       **kwargs):\n+        \"\"\"\n+        Uses the :py:func:`irradiance.get_total_irradiance` function to\n+        calculate the plane of array irradiance components on a tilted\n+        surface defined by ``self.surface_tilt``,\n+        ``self.surface_azimuth``, and ``self.albedo``.\n+\n+        Parameters\n+        ----------\n+        solar_zenith : float or Series.\n+            Solar zenith angle.\n+        solar_azimuth : float or Series.\n+            Solar azimuth angle.\n+        dni : float or Series or tuple of float or Series\n+            Direct Normal Irradiance\n+        ghi : float or Series or tuple of float or Series\n+            Global horizontal irradiance\n+        dhi : float or Series or tuple of float or Series\n+            Diffuse horizontal irradiance\n+        dni_extra : None, float or Series, default None\n+            Extraterrestrial direct normal irradiance\n+        airmass : None, float or Series, default None\n+            Airmass\n+        model : String, default 'haydavies'\n+            Irradiance model.\n+\n+        kwargs\n+            Extra parameters passed to :func:`irradiance.get_total_irradiance`.\n+\n+        Returns\n+        -------\n+        poa_irradiance : DataFrame or tuple of DataFrame\n+            Column names are: ``total, beam, sky, ground``.\n+        \"\"\"\n+\n+        dni = self._validate_per_array(dni, system_wide=True)\n+        ghi = self._validate_per_array(ghi, system_wide=True)\n+        dhi = self._validate_per_array(dhi, system_wide=True)\n+\n+        return tuple(\n+            irradiance.get_total_irradiance(\n+                self.surface_tilt,\n+                self.surface_azimuth,\n+                solar_zenith,\n+                solar_azimuth,\n+                dni, ghi, dhi,\n+                dni_extra=dni_extra,\n+                airmass=airmass,\n+                model=model,\n+                albedo=self.albedo,\n+                **kwargs)\n+            for array, dni, ghi, dhi in zip(\n+                self.arrays, dni, ghi, dhi\n+            )\n+        )\n+\n+    # Deprecate old SingleAxisTracking class\n+    @deprecated('0.9', alternative='SingleAxisTrackingArray', removal='0.10')\n+    class SingleAxisTracking(SingleAxisTrackingArray):\n+        pass\n+\n+    # Update PVSystem to use new Array classes\n+    class PVSystem:\n+        \"\"\"\n+        The PVSystem class defines a standard set of PV system attributes\n+        and modeling functions. This class describes the collection and\n+        interactions of PV system components rather than an installed system\n+        on the ground. It is typically used in combination with\n+        :py:class:`~pvlib.location.Location` and\n+        :py:class:`~pvlib.modelchain.ModelChain`\n+        objects.\n+\n+        The class supports basic system topologies consisting of:\n+\n+            * `N` total modules arranged in series\n+              (`modules_per_string=N`, `strings_per_inverter=1`).\n+            * `M` total modules arranged in parallel\n+              (`modules_per_string=1`, `strings_per_inverter=M`).\n+            * `NxM` total modules arranged in `M` strings of `N` modules each\n+              (`modules_per_string=N`, `strings_per_inverter=M`).\n+\n+        The class is complementary to the module-level functions.\n+\n+        The attributes should generally be things that don't change about\n+        the system, such the type of module and the inverter. The instance\n+        methods accept arguments for things that do change, such as\n+        irradiance and temperature.\n+\n+        Parameters\n+        ----------\n+        arrays : iterable of Array, optional\n+            List of arrays that are part of the system. If not specified\n+            a single array is created from the other parameters (e.g.\n+            `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n+            if length of arrays is 0 a ValueError is raised. If `arrays` is\n+            specified the following parameters are ignored:\n+\n+            - `surface_tilt`\n+            - `surface_azimuth`\n+            - `albedo`\n+            - `surface_type`\n+            - `module`\n+            - `module_type`\n+            - `module_parameters`\n+            - `temperature_model_parameters`\n+            - `modules_per_string`\n+            - `strings_per_inverter`\n+\n+        surface_tilt: float or array-like, default 0\n+            Surface tilt angles in decimal degrees.\n+            The tilt angle is defined as degrees from horizontal\n+            (e.g. surface facing up = 0, surface facing horizon = 90)\n+\n+        surface_azimuth: float or array-like, default 180\n+            Azimuth angle of the module surface.\n+            North=0, East=90, South=180, West=270.\n+\n+        albedo : None or float, default None\n+            The ground albedo. If ``None``, will attempt to use\n+            ``surface_type`` and ``irradiance.SURFACE_ALBEDOS``\n+            to lookup albedo.\n+\n+        surface_type : None or string, default None\n+            The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n+            for valid values.\n+\n+        module : None or string, default None\n+            The model name of the modules.\n+            May be used to look up the module_parameters dictionary\n+            via some other method.\n+\n+        module_type : None or string, default 'glass_polymer'\n+            Describes the module's construction. Valid strings are 'glass_polymer'\n+            and 'glass_glass'. Used for cell and module temperature calculations.\n+\n+        module_parameters : None, dict or Series, default None\n+            Module parameters as defined by the SAPM, CEC, or other.\n+\n+        temperature_model_parameters : None, dict or Series, default None.\n+            Temperature model parameters as required by one of the models in\n+            pvlib.temperature (excluding poa_global, temp_air and wind_speed).\n+\n+        modules_per_string: int or float, default 1\n+            See system topology discussion above.\n+\n+        strings_per_inverter: int or float, default 1\n+            See system topology discussion above.\n+\n+        inverter : None or string, default None\n+            The model name of the inverters.\n+            May be used to look up the inverter_parameters dictionary\n+            via some other method.\n+\n+        inverter_parameters : None, dict or Series, default None\n+            Inverter parameters as defined by the SAPM, CEC, or other.\n+\n+        racking_model : None or string, default 'open_rack'\n+            Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n+            Used to identify a parameter set for the SAPM cell temperature model.\n+\n+        losses_parameters : None, dict or Series, default None\n+            Losses parameters as defined by PVWatts or other.\n+\n+        name : None or string, default None\n+\n+        **kwargs\n+            Arbitrary keyword arguments.\n+            Included for compatibility, but not used.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If `arrays` is not None and has length 0.\n+\n+        See also\n+        --------\n+        pvlib.location.Location\n+        pvlib.tracking.SingleAxisTracker\n+        \"\"\"\n+\n+        def __init__(self,\n+                     arrays=None,\n+                     surface_tilt=0, surface_azimuth=180,\n+                     albedo=None, surface_type=None,\n+                     module=None, module_type=None,\n+                     module_parameters=None,\n+                     temperature_model_parameters=None,\n+                     modules_per_string=1, strings_per_inverter=1,\n+                     inverter=None, inverter_parameters=None,\n+                     racking_model=None, losses_parameters=None, name=None):\n+            if arrays is None:\n+                if losses_parameters is None:\n+                    array_losses_parameters = {}\n+                else:\n+                    array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n+                                                            losses_parameters)\n+                self.arrays = (FixedTiltArray(\n+                    surface_tilt,\n+                    surface_azimuth,\n+                    albedo,\n+                    surface_type,\n+                    module,\n+                    module_type,\n+                    module_parameters,\n+                    temperature_model_parameters,\n+                    modules_per_string,\n+                    strings_per_inverter,\n+                    racking_model,\n+                    array_losses_parameters,\n+                ),)\n+            elif len(arrays) == 0:\n+                raise ValueError(\"PVSystem must have at least one Array. \"\n+                                 \"If you want to create a PVSystem instance \"\n+                                 \"with a single Array pass `arrays=None` and pass \"\n+                                 \"values directly to PVSystem attributes, e.g., \"\n+                                 \"`surface_tilt=30`\")\n+            else:\n+                self.arrays = tuple(arrays)\n+\n+            self.inverter = inverter\n+            if inverter_parameters is None:\n+                self.inverter_parameters = {}\n+            else:\n+                self.inverter_parameters = inverter_parameters\n+\n+            if losses_parameters is None:\n+                self.losses_parameters = {}\n+            else:\n+                self.losses_parameters = losses_parameters\n+\n+            self.name = name\n+\n+        def __repr__(self):\n+            repr = f'PVSystem:\\n  name: {self.name}\\n  '\n+            for array in self.arrays:\n+                repr += '\\n  '.join(array.__repr__().split('\\n'))\n+                repr += '\\n  '\n+            repr += f'inverter: {self.inverter}'\n+            return repr\n+\n+        # ... (rest of the PVSystem methods remain unchanged)\n```", "model_patch": "--- a/pvlib/pvsystem.py\n+++ b/pvlib/pvsystem.py\n@@ -1242,6 +1242,16 @@\n \n class Array:\n     \"\"\"\n+    Base class for different types of arrays in a PV system.\n+    This class can be extended to create specific types of arrays\n+    such as fixed tilt and single-axis tracking arrays.\n+    \"\"\"\n+\n+class FixedTiltArray(Array):\n+    \"\"\"\n+    Parameters\n+    ----------\n+    surface_tilt: float or array-like, default 0\n+        Surface tilt angles in decimal degrees.\n+        The tilt angle is defined as degrees from horizontal\n+        (e.g. surface facing up = 0, surface facing horizon = 90)\n+\n+    surface_azimuth: float or array-like, default 180\n+        Azimuth angle of the module surface.\n+        North=0, East=90, South=180, West=270.\n+\n+    **kwargs\n+        Passed to Array. Or copy remainder of Array doc string to be explicit.\n+    \"\"\"\n+\n+class SingleAxisTrackingArray(Array):\n+    \"\"\"\n+    Parameters\n+    ----------\n+    axis_tilt : float, default 0\n+        The tilt of the axis of rotation (i.e, the y-axis defined by\n+        axis_azimuth) with respect to horizontal, in decimal degrees.\n+\n+    axis_azimuth : float, default 0\n+        A value denoting the compass direction along which the axis of\n+        rotation lies. Measured in decimal degrees east of north.\n+\n+    **kwargs\n+        Passed to Array. Or copy remainder of Array doc string to be explicit.\n+    \"\"\"\n+\n+    def __init__(self, axis_tilt=0, axis_azimuth=0, **kwargs):\n+        super().__init__(**kwargs)\n+        self.axis_tilt = axis_tilt\n+        self.axis_azimuth = axis_azimuth\n+\n+    def get_aoi(self, solar_zenith, solar_azimuth):\n+        \"\"\"Get the angle of incidence on the Array(s) in the system.\n+\n+        Parameters\n+        ----------\n+        solar_zenith : float or Series.\n+            Solar zenith angle.\n+        solar_azimuth : float or Series.\n+            Solar azimuth angle.\n+\n+        Returns\n+        -------\n+        aoi : Series or tuple of Series\n+            The angle of incidence\n+        \"\"\"\n+\n+        return irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n+                              solar_zenith, solar_azimuth)\n+\n+    def get_irradiance(self, solar_zenith, solar_azimuth, dni, ghi, dhi,\n+                       dni_extra=None, airmass=None, model='haydavies',\n+                       **kwargs):\n+        \"\"\"\n+        Uses the :py:func:`irradiance.get_total_irradiance` function to\n+        calculate the plane of array irradiance components on a tilted\n+        surface defined by ``self.surface_tilt``,\n+        ``self.surface_azimuth``, and ``self.albedo``.\n+\n+        Parameters\n+        ----------\n+        solar_zenith : float or Series.\n+            Solar zenith angle.\n+        solar_azimuth : float or Series.\n+            Solar azimuth angle.\n+        dni : float or Series or tuple of float or Series\n+            Direct Normal Irradiance\n+        ghi : float or Series or tuple of float or Series\n+            Global horizontal irradiance\n+        dhi : float or Series or tuple of float or Series\n+            Diffuse horizontal irradiance\n+        dni_extra : None, float or Series, default None\n+            Extraterrestrial direct normal irradiance\n+        airmass : None, float or Series, default None\n+            Airmass\n+        model : String, default 'haydavies'\n+            Irradiance model.\n+\n+        kwargs\n+            Extra parameters passed to :func:`irradiance.get_total_irradiance`.\n+\n+        Returns\n+        -------\n+        poa_irradiance : DataFrame or tuple of DataFrame\n+            Column names are: ``total, beam, sky, ground``.\n+        \"\"\"\n+\n+        dni = self._validate_per_array(dni, system_wide=True)\n+        ghi = self._validate_per_array(ghi, system_wide=True)\n+        dhi = self._validate_per_array(dhi, system_wide=True)\n+\n+        return tuple(\n+            irradiance.get_total_irradiance(\n+                self.surface_tilt,\n+                self.surface_azimuth,\n+                solar_zenith,\n+                solar_azimuth,\n+                dni, ghi, dhi,\n+                dni_extra=dni_extra,\n+                airmass=airmass,\n+                model=model,\n+                albedo=self.albedo,\n+                **kwargs)\n+            for array, dni, ghi, dhi in zip(\n+                self.arrays, dni, ghi, dhi\n+            )\n+        )\n+\n+    # Deprecate old SingleAxisTracking class\n+    @deprecated('0.9', alternative='SingleAxisTrackingArray', removal='0.10')\n+    class SingleAxisTracking(SingleAxisTrackingArray):\n+        pass\n+\n+    # Update PVSystem to use new Array classes\n+    class PVSystem:\n+        \"\"\"\n+        The PVSystem class defines a standard set of PV system attributes\n+        and modeling functions. This class describes the collection and\n+        interactions of PV system components rather than an installed system\n+        on the ground. It is typically used in combination with\n+        :py:class:`~pvlib.location.Location` and\n+        :py:class:`~pvlib.modelchain.ModelChain`\n+        objects.\n+\n+        The class supports basic system topologies consisting of:\n+\n+            * `N` total modules arranged in series\n+              (`modules_per_string=N`, `strings_per_inverter=1`).\n+            * `M` total modules arranged in parallel\n+              (`modules_per_string=1`, `strings_per_inverter=M`).\n+            * `NxM` total modules arranged in `M` strings of `N` modules each\n+              (`modules_per_string=N`, `strings_per_inverter=M`).\n+\n+        The class is complementary to the module-level functions.\n+\n+        The attributes should generally be things that don't change about\n+        the system, such the type of module and the inverter. The instance\n+        methods accept arguments for things that do change, such as\n+        irradiance and temperature.\n+\n+        Parameters\n+        ----------\n+        arrays : iterable of Array, optional\n+            List of arrays that are part of the system. If not specified\n+            a single array is created from the other parameters (e.g.\n+            `surface_tilt`, `surface_azimuth`). Must contain at least one Array,\n+            if length of arrays is 0 a ValueError is raised. If `arrays` is\n+            specified the following parameters are ignored:\n+\n+            - `surface_tilt`\n+            - `surface_azimuth`\n+            - `albedo`\n+            - `surface_type`\n+            - `module`\n+            - `module_type`\n+            - `module_parameters`\n+            - `temperature_model_parameters`\n+            - `modules_per_string`\n+            - `strings_per_inverter`\n+\n+        surface_tilt: float or array-like, default 0\n+            Surface tilt angles in decimal degrees.\n+            The tilt angle is defined as degrees from horizontal\n+            (e.g. surface facing up = 0, surface facing horizon = 90)\n+\n+        surface_azimuth: float or array-like, default 180\n+            Azimuth angle of the module surface.\n+            North=0, East=90, South=180, West=270.\n+\n+        albedo : None or float, default None\n+            The ground albedo. If ``None``, will attempt to use\n+            ``surface_type`` and ``irradiance.SURFACE_ALBEDOS``\n+            to lookup albedo.\n+\n+        surface_type : None or string, default None\n+            The ground surface type. See ``irradiance.SURFACE_ALBEDOS``\n+            for valid values.\n+\n+        module : None or string, default None\n+            The model name of the modules.\n+            May be used to look up the module_parameters dictionary\n+            via some other method.\n+\n+        module_type : None or string, default 'glass_polymer'\n+            Describes the module's construction. Valid strings are 'glass_polymer'\n+            and 'glass_glass'. Used for cell and module temperature calculations.\n+\n+        module_parameters : None, dict or Series, default None\n+            Module parameters as defined by the SAPM, CEC, or other.\n+\n+        temperature_model_parameters : None, dict or Series, default None.\n+            Temperature model parameters as required by one of the models in\n+            pvlib.temperature (excluding poa_global, temp_air and wind_speed).\n+\n+        modules_per_string: int or float, default 1\n+            See system topology discussion above.\n+\n+        strings_per_inverter: int or float, default 1\n+            See system topology discussion above.\n+\n+        inverter : None or string, default None\n+            The model name of the inverters.\n+            May be used to look up the inverter_parameters dictionary\n+            via some other method.\n+\n+        inverter_parameters : None, dict or Series, default None\n+            Inverter parameters as defined by the SAPM, CEC, or other.\n+\n+        racking_model : None or string, default 'open_rack'\n+            Valid strings are 'open_rack', 'close_mount', and 'insulated_back'.\n+            Used to identify a parameter set for the SAPM cell temperature model.\n+\n+        losses_parameters : None, dict or Series, default None\n+            Losses parameters as defined by PVWatts or other.\n+\n+        name : None or string, default None\n+\n+        **kwargs\n+            Arbitrary keyword arguments.\n+            Included for compatibility, but not used.\n+\n+        Raises\n+        ------\n+        ValueError\n+            If `arrays` is not None and has length 0.\n+\n+        See also\n+        --------\n+        pvlib.location.Location\n+        pvlib.tracking.SingleAxisTracker\n+        \"\"\"\n+\n+        def __init__(self,\n+                     arrays=None,\n+                     surface_tilt=0, surface_azimuth=180,\n+                     albedo=None, surface_type=None,\n+                     module=None, module_type=None,\n+                     module_parameters=None,\n+                     temperature_model_parameters=None,\n+                     modules_per_string=1, strings_per_inverter=1,\n+                     inverter=None, inverter_parameters=None,\n+                     racking_model=None, losses_parameters=None, name=None):\n+            if arrays is None:\n+                if losses_parameters is None:\n+                    array_losses_parameters = {}\n+                else:\n+                    array_losses_parameters = _build_kwargs(['dc_ohmic_percent'],\n+                                                            losses_parameters)\n+                self.arrays = (FixedTiltArray(\n+                    surface_tilt,\n+                    surface_azimuth,\n+                    albedo,\n+                    surface_type,\n+                    module,\n+                    module_type,\n+                    module_parameters,\n+                    temperature_model_parameters,\n+                    modules_per_string,\n+                    strings_per_inverter,\n+                    racking_model,\n+                    array_losses_parameters,\n+                ),)\n+            elif len(arrays) == 0:\n+                raise ValueError(\"PVSystem must have at least one Array. \"\n+                                 \"If you want to create a PVSystem instance \"\n+                                 \"with a single Array pass `arrays=None` and pass \"\n+                                 \"values directly to PVSystem attributes, e.g., \"\n+                                 \"`surface_tilt=30`\")\n+            else:\n+                self.arrays = tuple(arrays)\n+\n+            self.inverter = inverter\n+            if inverter_parameters is None:\n+                self.inverter_parameters = {}\n+            else:\n+                self.inverter_parameters = inverter_parameters\n+\n+            if losses_parameters is None:\n+                self.losses_parameters = {}\n+            else:\n+                self.losses_parameters = losses_parameters\n+\n+            self.name = name\n+\n+        def __repr__(self):\n+            repr = f'PVSystem:\\n  name: {self.name}\\n  '\n+            for array in self.arrays:\n+                repr += '\\n  '.join(array.__repr__().split('\\n'))\n+                repr += '\\n  '\n+            repr += f'inverter: {self.inverter}'\n+            return repr\n+\n+        # ... (rest of the PVSystem methods remain unchanged)\n"}
