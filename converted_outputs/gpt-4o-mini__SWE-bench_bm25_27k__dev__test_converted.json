[
  {
    "instance_id": "sqlfluff__sqlfluff-4764",
    "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -173,6 +173,15 @@\n \n     return f\n     \n+    f = click.option(\n+        \"--quiet\",\n+        is_flag=True,\n+        default=False,\n+        help=\"Enable quiet mode, suppressing detailed output.\",\n+    )(f)\n+\n+\n def core_options(f: Callable) -> Callable:\n     \"\"\"Add core operation options to commands via a decorator.\n \n@@ -613,6 +622,10 @@\n     if verbose >= 1 and not non_human_output:\n         click.echo(format_linting_result_header())\n \n+    if kwargs.get(\"quiet\"):\n+        verbose = 0  # Set verbosity to 0 if quiet mode is enabled\n+\n     with PathAndUserErrorHandler(formatter):\n         # add stdin if specified via lone '-'\n         if (\"-\",) == paths:\n--- a/src/sqlfluff/core/linter/linted_file.py\n+++ b/src/sqlfluff/core/linter/linted_file.py\n@@ -559,6 +559,7 @@\n         return success\n     \n     @staticmethod\n+    def _safe_create_replace_file(\n     def _safe_create_replace_file(\n         input_path: str, output_path: str, write_buff: str, encoding: str\n     ):\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-884",
    "model_patch": "--- a/src/sqlfluff/core/parser/segments/base.py\n+++ b/src/sqlfluff/core/parser/segments/base.py\n@@ -1,5 +1,5 @@\n class BaseSegment:\n     \"\"\"Base class for all segments.\"\"\"\n \n-    def __init__(self, raw: str):\n+    def __init__(self, raw: str, is_whitespace: bool = False):\n         self.raw = raw\n-        self.is_whitespace = False\n+        self.is_whitespace = is_whitespace\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-3904",
    "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -511,7 +511,7 @@\n \n @click.option(\n     \"--disable_progress_bar\",\n-    is_flag=True,\n+    is_flag=True, \n     help=\"Disables progress bars.\",\n )\n \n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,4 +1,4 @@\n \n     def lint_paths(\n         self,\n         paths: Tuple[str, ...],\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n         processes: Optional[int] = None,\n     ) -> LintingResult:\n         \"\"\"Lint an iterable of paths.\"\"\"\n         paths_count = len(paths)\n \n         # If no paths specified - assume local\n         if not paths_count:  # pragma: no cover\n             paths = (os.getcwd(),)\n         # Set up the result to hold what we get back\n         result = LintingResult()\n \n         progress_bar_paths = tqdm(\n             total=paths_count,\n             desc=\"path\",\n             leave=False,\n             disable=paths_count <= 1 or progress_bar_configuration.disable_progress_bar,\n         )\n         for path in paths:\n             progress_bar_paths.set_description(f\"path {path}\")\n \n             # Iterate through files recursively in the specified directory (if it's a\n             # directory) or read the file directly if it's not\n             result.add(\n                 self.lint_path(\n                     path,\n                     fix=fix,\n                     ignore_non_existent_files=ignore_non_existent_files,\n                     ignore_files=ignore_files,\n                     processes=processes,\n                 )\n             )\n \n             progress_bar_paths.update(1)\n \n         result.stop_timer()\n         return result\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -707,7 +707,7 @@\n \n @click.option(\n     \"--disable-noqa\",\n-    is_flag=True,\n+    is_flag=True, \n     default=None,\n     help=\"Set this flag to ignore inline noqa comments.\",\n )\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-1625",
    "model_patch": "--- a/src/sqlfluff/rules/L031.py\n+++ b/src/sqlfluff/rules/L031.py\n@@ -13,7 +13,7 @@\n     \"\"\"Avoid table aliases in from clauses and join conditions.\n \n     | **Anti-pattern**\n-    | In this example, alias 'o' is used for the orders table, and 'c' is used for 'customers' table.\n+    | In this example, alias 'o' is used for the orders table, and 'c' is used for the customers table.\n     .. code-block:: sql\n     \n         SELECT\n@@ -56,7 +56,7 @@\n \n             from_expression_element = None\n             if from_expression:\n-                from_expression_element = from_expression.get_child(\n+                from_expression_element = from_expression.get_child(\n                     \"from_expression_element\"\n                 )\n \n@@ -74,7 +74,7 @@\n             # Find base table\n             base_table = None\n             if from_expression_element:\n-                base_table = from_expression_element.get_child(\"object_reference\")\n+                base_table = from_expression_element.get_child(\"object_reference\")\n \n             from_clause_index = segment.segments.index(from_clause_segment)\n             from_clause_and_after = segment.segments[from_clause_index:]\n@@ -99,7 +99,7 @@\n \n             return (\n                 self._lint_aliases_in_join(\n                     base_table,\n-                    from_expression_elements,\n+                    from_expression_elements,\n                     column_reference_segments,\n                     segment,\n                 )\n@@ -105,7 +105,7 @@\n \n     class TableAliasInfo(NamedTuple):\n         \"\"\"Structure yielded by_filter_table_expressions().\"\"\"\n \n@@ -115,7 +115\n         table_ref: BaseSegment\n         whitespace_ref: BaseSegment\n         alias_exp_ref: BaseSegment\n         alias_identifier_ref: BaseSegment\n \n@@ -117,7 +117\n     @classmethod\n     def _filter_table_expressions(\n         cls, base_table, from_expression_elements\n     ) -> Generator[TableAliasInfo, None, None]:\n@@ -149,7 +149\n         \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n         # A buffer to keep any violations.\n         violation_buff = []\n \n@@ -157,7 +157\n         # How many times does each table appear in the FROM clause?\n         table_counts = Counter(ai.table_ref.raw for ai in to_check)\n \n@@ -169,7 +169\n         # For each aliased table, check whether to keep or remove it.\n         for alias_info in to_check:\n             # If the same table appears more than once in the FROM clause with\n             # different alias names, do not consider removing its aliases.\n@@ -173,7 +173\n             # The aliases may have been introduced simply to make each\n             # occurrence of the table independent within the query.\n             if (\n                 table_counts[alias_info.table_ref.raw] > 1\n                 and len(table_aliases[alias_info.table_ref.raw]) > 1\n             ):\n                 continue\n \n@@ -181,7 +181\n             select_clause = segment.get_child(\"select_clause\")\n \n             ids_refs = []\n \n@@ -187,7 +187\n             # Find all references to alias in select clause\n             alias_name = alias_info.alias_identifier_ref.raw\n             for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n                 used_alias_ref = alias_with_column.get_child(\"identifier\")\n                 if used_alias_ref and used_alias_ref.raw == alias_name:\n                     ids_refs.append(used_alias_ref)\n \n@@ -192,7 +192\n             # Find all references to alias in column references\n             for exp_ref in column_reference_segments:\n                 used_alias_ref = exp_ref.get_child(\"identifier\")\n                 # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n                 if used_alias_ref.raw == alias_name and exp_ref.get_child(\"dot\"):\n                     ids_refs.append(used_alias_ref)\n \n@@ -199,7 +199\n             # Fixes for deleting ` as sth` and for editing references to aliased tables\n             fixes = [\n                 *[\n                     LintFix(\"delete\", d)\n                     for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n                 ],\n                 *[\n                     LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n                     for alias in [alias_info.alias_identifier_ref, *ids_refs]\n                 ],\n             ]\n \n@@ -211,7 +211\n             violation_buff.append(\n                 LintResult(\n                     anchor=alias_info.alias_identifier_ref,\n                     description=\"Avoid using aliases in join condition\",\n                     fixes=fixes,\n                 )\n             )\n \n@@ -219,7 +219\n         return violation_buff or None\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-3662",
    "model_patch": "--- a/src/sqlfluff/core/config.py\n+++ b/src/sqlfluff/core/config.py\n@@ -1,6 +1,7 @@\n import os\n import json\n import logging\n+import multiprocessing\n from typing import Any, Dict, List, Optional, Tuple, Union\n from sqlfluff.core.errors import SQLFluffUserError\n from sqlfluff.core.plugin.host import get_plugin_manager\n@@ -16,6 +17,7 @@\n \n class FluffConfig:\n     \"\"\"Configuration class for SQLFluff.\"\"\"\n \n+    default_processes = 1\n     def __init__(self, **kwargs: Any) -> None:\n         \"\"\"Initialize the configuration.\"\"\"\n         self.config: Dict[str, Any] = {}\n@@ -100,6 +102,9 @@\n         return config\n \n     @classmethod\n     def from_root(cls, extra_config_path: Optional[str] = None, ignore_local_config: bool = False, **kwargs: Any) -> \"FluffConfig\":\n         \"\"\"Create a FluffConfig object from the root directory.\"\"\"\n         config = cls(**kwargs)\n \n+        # Set the number of processes from the config or use the default\n+        config.processes = kwargs.get(\"processes\", cls.default_processes)\n+\n         # Load additional configuration files if specified\n         if extra_config_path:\n             config.load_config_file(extra_config_path)\n--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -488,6 +488,9 @@\n     )\n )\n @click.option(\n+    \"--processes\",\n+    type=int,\n+    default=None,\n     help=\"The number of parallel processes to run.\",\n )\n @click.argument(\"paths\", nargs=-1, type=click.Path(allow_dash=True))\n@@ -500,6 +503\n     processes: int,\n+    extra_config_path: Optional[str] = None,\n     **kwargs,\n ) -> None:\n     \"\"\"Lint SQL files via passing a list of files or using stdin.\n@@ -531,6 +535,9 @@\n     )\n     config = get_config(\n         extra_config_path, ignore_local_config, require_dialect=False, **kwargs\n     )\n+    # Override the processes if specified in the command line\n+    if processes is not None:\n+        config.processes = processes\n \n     non_human_output = (format != FormatType.human.value) or (write_output is not None)\n     file_output = None\n@@ -566,7 +573\n                 processes=processes,\n+                processes=config.processes,\n             )\n \n     # Output the final stats\n--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1129,7 +1129,7 @@\n     def lint_path(\n         self,\n         path: str,\n         fix: bool = False,\n         ignore_non_existent_files: bool = False,\n         ignore_files: bool = True,\n-        processes: int = 1,\n+        processes: int = 1,  # Default processes\n     ) -> LintedDir:\n         \"\"\"Lint a path.\"\"\"\n         linted_path = LintedDir(path)\n@@ -1136,7 +1136\n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n         runner = get_runner(\n             self,\n             self.config,\n-            processes=processes,\n+            processes=self.config.processes,  # Use config processes\n             allow_process_parallelism=self.allow_process_parallelism,\n         )\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-2509",
    "model_patch": "--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1,6 +1,7 @@\n \n import fnmatch\n import os\n import time\n+import logging\n from typing import (\n     Any,\n     List,\n@@ -22,6 +23,7 @@\n from sqlfluff.core.errors import (\n     SQLBaseError,\n     SQLLexError,\n     SQLLintError,\n     SQLParseError,\n     SQLTemplaterSkipFile,\n )\n+from sqlfluff.core.templaters import TemplatedFile\n from sqlfluff.core.parser import Lexer, Parser\n from sqlfluff.core.file_helpers import get_encoding\n from sqlfluff.core.rules import get_ruleset\n@@ -86,6 +88,7 @@\n \n     def lint_fix_parsed(\n         cls,\n         tree: BaseSegment,\n         config: FluffConfig,\n         rule_set: List[BaseRule],\n         fix: bool = False,\n         fname: Optional[str] = None,\n         templated_file: Optional[TemplatedFile] = None,\n         formatter: Any = None,\n     ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n+        \"\"\"Lint and optionally fix a tree object.\"\"\"\n         # Keep track of the linting errors\n         all_linting_errors = []\n         # A placeholder for the fixes we had on the previous loop\n@@ -150,6 +153,7 @@\n \n     def lint_parsed(\n         cls,\n         parsed: ParsedString,\n         rule_set: List[BaseRule],\n         fix: bool = False,\n         formatter: Any = None,\n         encoding: str = \"utf8\",\n     ):\n+        \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n         violations = parsed.violations\n         time_dict = parsed.time_dict\n         tree: Optional[BaseSegment]\n@@ -180,6 +184,7 @@\n \n     def lint_rendered(\n         cls,\n         rendered: RenderedFile,\n         rule_set: List[BaseRule],\n         fix: bool = False,\n         formatter: Any = None,\n     ) -> LintedFile:\n+        \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n         parsed = cls.parse_rendered(rendered)\n         return cls.lint_parsed(\n             parsed,\n             rule_set=rule_set,\n             fix=fix,\n             formatter=formatter,\n             encoding=rendered.encoding,\n         )\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-4777",
    "model_patch": "--- a/src/sqlfluff/cli/commands.py\n+++ b/src/sqlfluff/cli/commands.py\n@@ -690,6 +690,7 @@\n     # Output the final stats\n     if verbose >= 1 and not non_human_output:\n         click.echo(formatter.format_linting_stats(result, verbose=verbose))\n \n+    # Write output sequentially instead of all at once\n     for filepath in result.paths:\n         if filepath.files:\n             for file in filepath.files:\n@@ -691,6 +692,7 @@\n                 dump_file_payload(file.fname, file.fix_string()[0])\n \n     if format == FormatType.json.value:\n         file_output = json.dumps(result.as_records())\n     elif format == FormatType.yaml.value:\n         file_output = yaml.dump(result.as_records(), sort_keys=False)\n     elif format == FormatType.none.value:\n         file_output = \"\"\n     elif format == FormatType.github_annotation.value:\n         if annotation_level == \"error\":\n             annotation_level = \"failure\"\n \n         github_result = []\n         for record in result.as_records():\n             filepath = record[\"filepath\"]\n             for violation in record[\"violations\"]:\n                 # NOTE: The output format is designed for this GitHub action:\n                 # https://github.com/yuzutech/annotations-action\n                 # It is similar, but not identical, to the native GitHub format:\n                 # https://docs.github.com/en/rest/reference/checks#annotations-items\n                 github_result.append(\n                     {\n                         \"file\": filepath,\n                         \"line\": violation[\"line_no\"],\n                         \"start_column\": violation[\"line_pos\"],\n                         \"end_column\": violation[\"line_pos\"],\n                         \"title\": \"SQLFluff\",\n                         \"message\": f\"{violation['code']}: {violation['description']}\",\n                         \"annotation_level\": annotation_level,\n                     }\n                 )\n         file_output = json.dumps(github_result)\n     elif format == FormatType.github_annotation_native.value:\n         if annotation_level == \"failure\":\n             annotation_level = \"error\"\n \n         github_result_native = []\n         for record in result.as_records():\n             filepath = record[\"filepath\"]\n             for violation in record[\"violations\"]:\n                 # NOTE: The output format is designed for GitHub action:\n                 # https://docs.github.com/en/actions/using-workflows/workflow-commands-for-github-actions#setting-a-notice-message\n                 line = f\"::{annotation_level} \"\n                 line += \"title=SQLFluff,\"\n                 line += f\"file={filepath},\"\n                 line += f\"line={violation['line_no']},\"\n                 line += f\"col={violation['line_pos']}\"\n                 line += \"::\"\n                 line += f\"{violation['code']}: {violation['description']}\"\n                 if violation[\"name\"]:\n                     line += f\" [{violation['name']}]\"\n \n                 github_result_native.append(line)\n \n         file_output = \"\\n\".join(github_result_native)\n \n     if file_output:\n         dump_file_payload(write_output, cast(str, file_output))\n \n     if persist_timing:\n         result.persist_timing_records(persist_timing)\n \n     output_stream.close()\n     if bench:\n         click.echo(\"==== overall timings ====\")\n         click.echo(formatter.cli_table([(\"Clock time\", result.total_time)]))\n         timing_summary = result.timing_summary()\n         for step in timing_summary:\n             click.echo(f\"=== {step} ===\")\n             click.echo(\n                 formatter.cli_table(timing_summary[step].items(), cols=3, col_width=20)\n             )\n \n     if not nofail:\n         if not non_human_output:\n             formatter.completion_message()\n         sys.exit(result.stats()[\"exit code\"])\n     else:\n         sys.exit(EXIT_SUCCESS)\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-3066",
    "model_patch": "--- src/sqlfluff/core/templaters/slicers/tracer.py\n+++ src/sqlfluff/core/templaters/slicers/tracer.py\n@@ -244,7 +244,7 @@\n             # Entering a set/macro block.\n             self.inside_set_or_macro = True\n         elif block_type == \"block_end\" and (trimmed_parts[0] in (\"endmacro\", \"endset\")):\n             # Exiting a set/macro block.\n-            self.inside_set_or_macro = False\n+            self.inside_set_or_macro = False\n+        elif block_type == \"block_start\" and trimmed_parts[0] == \"set\" and trimmed_parts.count(\"=\") == 1:\n+            # Handle single target assignments\n+            self.inside_set_or_macro = True\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "sqlfluff__sqlfluff-4084",
    "model_patch": "--- a/src/sqlfluff/core/linter/linter.py\n+++ b/src/sqlfluff/core/linter/linter.py\n@@ -1164,7 +1164,7 @@\n         \"\"\"Lint a path.\"\"\"\n         linted_path = LintedDir(path)\n         if self.formatter:\n             self.formatter.dispatch_path(path)\n         fnames = list(\n-            self.paths_from_path(\n+            self.paths_from_path(\n                 path,\n                 ignore_non_existent_files=ignore_non_existent_files,\n                 ignore_files=ignore_files,\n@@ -1192,7 +1192,7 @@\n         # to avoid circular import\n         from sqlfluff.core.linter.runner import get_runner\n \n-        runner, effective_processes = get_runner(\n+        runner, effective_processes = get_runner(\n             self,\n             self.config,\n             processes=processes,\n@@ -1204,7 +1204,7 @@\n         if self.formatter and effective_processes != 1:\n             self.formatter.dispatch_processing_header(effective_processes)\n \n-        # Show files progress bar only when there is more than one.\n+        # Show files progress bar only when there is more than one.\n         files_count = len(fnames)\n         progress_bar_files = tqdm(\n             total=files_count,\n@@ -1210,7 +1210\n             desc=f\"file {os.path.basename(fnames[0] if fnames else '')}\",\n             leave=False,\n             disable=files_count <= 1 or progress_bar_configuration.disable_progress_bar,\n@@ -1219,7 +1219\n             linted_path.add(linted_file)\n             # If any fatal errors, then stop iteration.\n             if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                 linter_logger.error(\"Fatal linting error. Halting further linting.\")\n@@ -1225,7 +1225\n             # Progress bar for files is rendered only when there is more than one file.\n             # Additionally as it's updated after each loop, we need to get file name\n             # from the next loop. This is why `enumerate` starts with `1` and there\n             # is `i < len` to not exceed files list length.\n             progress_bar_files.update(n=1)\n             if i < len(fnames):\n                 progress_bar_files.set_description(\n                     f\"file {os.path.basename(fnames[i])}\"\n                 )\n \n-        return linted_path\n+        return linted_path\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "marshmallow-code__marshmallow-2123",
    "model_patch": "--- a/src/marshmallow/fields.py\n+++ b/src/marshmallow/fields.py\n@@ -1710,6 +1710,14 @@\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n+        # Ensure that only relative URLs are allowed\n+        if relative and (schemes is not None or require_tld):\n+            raise ValueError(\"When 'relative' is True, 'schemes' and 'require_tld' cannot be set.\")\n+        if not relative:\n+            # Insert validation into self.validators for absolute URLs\n+            validator = validate.URL(\n+                relative=False,\n+                schemes=schemes,\n+                require_tld=require_tld,\n+                error=self.error_messages[\"invalid\"],\n+            )\n+            self.validators.insert(0, validator)\n \n     def _deserialize(self, value, attr, data, **kwargs):\n         # Custom deserialization logic for URLs\n         if value is None:\n             return None\n         if not self.relative and not self.is_absolute(value):\n             raise self.make_error(\"invalid\")\n         return super()._deserialize(value, attr, data, **kwargs)\n\n     def is_absolute(self, url):\n         \"\"\"Check if the URL is absolute.\"\"\"\n         return bool(urlparse(url).netloc)\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1738",
    "model_patch": "--- a/pvlib/soiling.py\n+++ b/pvlib/soiling.py\n@@ -10,7 +10,7 @@\n \n def hsu(snowfall, surface_tilt, threshold_snowfall=1.):\n     \"\"\"\n-    Calculates the fraction of the slant height of a row of modules covered by\n+    Calculates the fraction of the slant height of a row of modules covered by snow at every time step.\n \n     Parameters\n     ----------\n@@ -11,7 +11,7 @@\n     snowfall : Series\n         Accumulated snowfall within each time period. [cm]\n     poa_irradiance : Series\n     temp_air : Series\n     surface_tilt : numeric\n-        Tilt of module's from horizontal, e.g. surface facing up = 0,\n+        Tilt of module's from horizontal, e.g. surface facing up = 0, surface facing horizon = 90. [degrees]\n     initial_coverage : float, default 0\n         Fraction of row's slant height that is covered with snow at the beginning of the simulation. [unitless]\n     threshold_snowfall : float, default 1.0\n@@ -27,7 +27,7 @@\n     Returns\n     -------\n     snow_coverage : Series\n         The fraction of the slant height of a row of modules that is covered\n         by snow at each time step.\n \n     Notes\n     -----\n     In [1]_, `can_slide_coefficient` is termed `m`, and the value of\n     `slide_amount_coefficient` is given in tenths of a module's slant height.\n \n     References\n     ----------\n     .. [1] Marion, B.; Schaefer, R.; Caine, H.; Sanchez, G. (2013).\n        \"Measured and modeled photovoltaic system energy losses from snow for\n        Colorado and Wisconsin locations.\" Solar Energy 97; pp.112-121.\n     .. [2] Ryberg, D; Freeman, J. (2017). \"Integration, Validation, and\n        Application of a PV Snow Coverage Model in SAM\" NREL Technical Report\n        NREL/TP-6A20-68705\n     \"\"\"\n \n-    # find times with new snowfall\n+    # find times with new snowfall\n     new_snowfall = fully_covered_nrel(snowfall, threshold_snowfall)\n \n     # set up output Series\n     snow_coverage = pd.Series(np.nan, index=poa_irradiance.index)\n \n     # determine amount that snow can slide in each timestep\n     can_slide = temp_air > poa_irradiance / can_slide_coefficient\n     slide_amt = slide_amount_coefficient * sind(surface_tilt) * \\\n         _time_delta_in_hours(poa_irradiance.index)\n     slide_amt[~can_slide] = 0.\n     # don't slide during snow events\n     slide_amt[new_snowfall] = 0.\n     # don't slide in the interval preceding the snowfall data\n     slide_amt.iloc[0] = 0\n \n     # build time series of cumulative slide amounts\n     sliding_period_ID = new_snowfall.cumsum()\n     cumulative_sliding = slide_amt.groupby(sliding_period_ID).cumsum()\n \n     # set up time series of snow coverage without any sliding applied\n     snow_coverage[new_snowfall] = 1.0\n     if np.isnan(snow_coverage.iloc[0]):\n         snow_coverage.iloc[0] = initial_coverage\n     snow_coverage.ffill(inplace=True)\n     snow_coverage -= cumulative_sliding\n \n     # clean up periods where row is completely uncovered\n     return snow_coverage.clip(lower=0)\n \n-def dc_loss_nrel(snow_coverage, num_strings):\n+def dc_loss_nrel(snow_coverage, num_strings):\n     '''\n     Calculates the fraction of DC capacity lost due to snow coverage.\n \n     DC capacity loss assumes that if a string is partially covered by snow,\n     the string's capacity is lost; see [1]_, Eq. 11.8.\n \n     Module orientation is accounted for by specifying the number of cell\n     strings in parallel along the slant height.\n     For example, a typical 60-cell module has 3 parallel strings, each\n     comprising 20 cells in series, with the cells arranged in 6 columns of 10\n     cells each. For a row consisting of single modules, if the module is\n     mounted in portrait orientation, i.e., the row slant height is along a\n     column of 10 cells, there is 1 string in parallel along the row slant\n     height, so `num_strings=1`. In contrast, if the module is mounted in\n     landscape orientation with the row slant height comprising 6 cells, there\n     are 3 parallel strings along the row slant height, so `num_strings=3`.\n \n     Parameters\n     ----------\n     snow_coverage : numeric\n         The fraction of row slant height covered by snow at each time step.\n \n     num_strings: int\n         The number of parallel-connected strings along a row slant height.\n \n     Returns\n     -------\n     loss : numeric\n         fraction of DC capacity loss due to snow coverage at each time step.\n \n     References\n     ----------\n     .. [1] Gilman, P. et al., (2018). \"SAM Photovoltaic Model Technical\n        Reference Update\", NREL Technical Report NREL/TP-6A20-67399.\n        Available at https://www.nrel.gov/docs/fy18osti/67399.pdf\n     '''\n     return np.ceil(snow_coverage * num_strings) / num_strings\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1213",
    "model_patch": "--- a/pvlib/scaling.py\n+++ b/pvlib/scaling.py\n@@ -239,7 +239,7 @@\n     # Clip off the padding and just return the original time window\n     wavelet = np.zeros([max_tmscale, len(vals)])\n     for i in np.arange(0, max_tmscale):\n-        wavelet[i, :] = wavelet_long[i, len(vals)+1: 2*len(vals)+1]\n+        wavelet[i, :] = wavelet_long[i, len(vals): 2*len(vals)]\n \n     return wavelet, tmscales\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1368",
    "model_patch": "--- a/pvlib/iotools/crn.py\n+++ b/pvlib/iotools/crn.py\n@@ -112,6 +112,9 @@\n \n     # Map missing values to NaN\n     missing_values = [-99999, -999999]\n+    for missing_value in missing_values:\n+        df[df == missing_value] = np.nan\n+\n     # Convert the DataFrame to a proper format\n     df = df.astype(float)\n \n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1048",
    "model_patch": "--- a/pvlib/singlediode.py\n+++ b/pvlib/singlediode.py\n@@ -1,27 +1,35 @@\n def euclidean(a, b):\n-    while b:\n-        a, b = b, a % b\n-    return a\n+    if b == 0:\n+        return a\n+    return euclidean(b, a % b)\n \n \n def bresenham(x0, y0, x1, y1):\n     points = []\n     dx = abs(x1 - x0)\n     dy = abs(y1 - y0)\n-    sx = 1 if x0 < x1 else -1\n-    sy = 1 if y0 < y1 else -1\n-    err = dx - dy\n+    x, y = x0, y0\n+    sx = -1 if x0 > x1 else 1\n+    sy = -1 if y0 > y1 else 1\n \n-    while True:\n-        points.append((x0, y0))\n-        if x0 == x1 and y0 == y1:\n-            break\n-        e2 = 2 * err\n-        if e2 > -dy:\n+    if dx > dy:\n+        err = dx / 2.0\n+        while x != x1:\n+            points.append((x, y))\n             err -= dy\n-            x0 += sx\n-        if e2 < dx:\n-            err += dx\n-            y0 += sy\n+            if err < 0:\n+                y += sy\n+                err += dx\n+            x += sx\n+    else:\n+        err = dy / 2.0\n+        while y != y1:\n+            points.append((x, y))\n+            err -= dx\n+            if err < 0:\n+                x += sx\n+                err += dy\n+            y += sy\n \n+    points.append((x, y))\n     return points\n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1026",
    "model_patch": "--- a/pvlib/read_crn.py\n+++ b/pvlib/read_crn.py\n@@ -1,6 +1,7 @@\n import pandas as pd\n \n def read_crn(filename):\n+    na_values = ['\\x00\\x00\\x00\\x00\\x00\\x00']\n     # Read the data\n     df = pd.read_csv(filename, na_values=na_values, delim_whitespace=True, header=None)\n \n@@ -10,7 +11,8 @@\n     # Set the column names\n     df.columns = ['CRX_VN', 'DATE', 'TIME', 'DATE_TIME', 'DATA_TYPE', 'LAT', 'LON', 'ELEV', 'GHI', 'DNI', 'DHI', 'WS', 'WD', 'T', 'RH', 'P', 'SNOW', 'Q']\n \n-    # Convert CRX_VN to int\n+    # Convert CRX_VN to string to accommodate floats\n+    df['CRX_VN'] = df['CRX_VN'].astype(str)\n \n     # Convert DATE and TIME to datetime\n     df['DATE_TIME'] = pd.to_datetime(df['DATE'].astype(str) + ' ' + df['TIME'].astype(str), format='%Y%m%d %H%M')\n \n",
    "model_name_or_path": "gpt-4o-mini"
  },
  {
    "instance_id": "pvlib__pvlib-python-1176",
    "model_patch": "--- a/pvlib/tracking.py\n+++ b/pvlib/tracking.py\n@@ -1,6 +1,7 @@\n import numpy as np\n import pandas as pd\n \n from pvlib.tools import cosd, sind, tand\n from pvlib.pvsystem import PVSystem, _unwrap_single_value\n from pvlib import irradiance, atmosphere\n \n+class BaseArray:\n+    pass\n \n class SingleAxisTracker(PVSystem):\n     \"\"\"\n@@ -17,6 +18,7 @@\n     axis_azimuth : float, default 0\n         A value denoting the compass direction along which the axis of\n         rotation lies. Measured in decimal degrees east of north.\n \n+    surface_tilt: float or array-like, default 0\n+        Surface tilt angles in decimal degrees.\n+        The tilt angle is defined as degrees from horizontal\n+        (e.g. surface facing up = 0, surface facing horizon = 90)\n \n     **kwargs\n         Passed to :py:class:`~pvlib.pvsystem.PVSystem`. If the `arrays`\n@@ -78,6 +79,7 @@\n \n     def __init__(self, axis_tilt=0, axis_azimuth=0, max_angle=90,\n                  backtrack=True, gcr=2.0/7.0, cross_axis_tilt=0.0, **kwargs):\n \n+        arrays = kwargs.get('arrays', [])\n         if len(arrays) > 1:\n             raise ValueError(\"SingleAxisTracker does not support \"\n                              \"multiple arrays.\")\n@@ -84,6 +85\n             surface_tilt = arrays[0].surface_tilt\n             surface_azimuth = arrays[0].surface_azimuth\n             if surface_tilt is not None or surface_azimuth is not None:\n                 raise ValueError(\n                     \"Array must not have surface_tilt or \"\n                     \"surface_azimuth assigned. You must pass an \"\n                     \"Array with these fields set to None.\"\n                 )\n \n+        self.surface_tilt = surface_tilt\n+        self.surface_azimuth = surface_azimuth\n         self.axis_tilt = axis_tilt\n         self.axis_azimuth = axis_azimuth\n         self.max_angle = max_angle\n         self.backtrack = backtrack\n         self.gcr = gcr\n         self.cross_axis_tilt = cross_axis_tilt\n \n         kwargs['surface_tilt'] = None\n         kwargs['surface_azimuth'] = None\n \n@@ -116,6 +117,7 @@\n \n     def singleaxis(self, apparent_zenith, apparent_azimuth):\n         \"\"\"\n         Get tracking data. See :py:func:`pvlib.tracking.singleaxis` more\n         detail.\n \n+        Parameters\n+        ----------\n+        surface_tilt : numeric\n+            Panel tilt from horizontal.\n+        surface_azimuth : numeric\n+            Panel azimuth from north\n+        solar_zenith : float or Series.\n+            Solar zenith angle.\n+        solar_azimuth : float or Series.\n+            Solar azimuth angle.\n \n         Returns\n         -------\n         tracking data\n         \"\"\"\n         tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n                                    self.axis_tilt, self.axis_azimuth,\n                                    self.max_angle, self.backtrack,\n                                    self.gcr, self.cross_axis_tilt)\n \n         return tracking_data\n",
    "model_name_or_path": "gpt-4o-mini"
  }
]